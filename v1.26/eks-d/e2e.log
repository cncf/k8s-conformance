I0315 21:43:21.731633      20 e2e.go:126] Starting e2e run "5f02fcbf-8b49-43fa-a80d-391846325739" on Ginkgo node 1
Mar 15 21:43:21.750: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1678916601 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Mar 15 21:43:21.909: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:43:21.910: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0315 21:43:21.912062      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar 15 21:43:21.925: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 15 21:43:21.965: INFO: 28 / 28 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 15 21:43:21.965: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Mar 15 21:43:21.965: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 15 21:43:21.978: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'aws-cloud-controller-manager' (0 seconds elapsed)
Mar 15 21:43:21.978: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'aws-iam-authenticator' (0 seconds elapsed)
Mar 15 21:43:21.978: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
Mar 15 21:43:21.978: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
Mar 15 21:43:21.978: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'kops-controller' (0 seconds elapsed)
Mar 15 21:43:21.978: INFO: e2e test version: v1.26.2
Mar 15 21:43:21.980: INFO: kube-apiserver version: v1.26.2-eks-b106822
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Mar 15 21:43:21.980: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:43:21.984: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.074 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Mar 15 21:43:21.909: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:43:21.910: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0315 21:43:21.912062      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Mar 15 21:43:21.925: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Mar 15 21:43:21.965: INFO: 28 / 28 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Mar 15 21:43:21.965: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
    Mar 15 21:43:21.965: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Mar 15 21:43:21.978: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'aws-cloud-controller-manager' (0 seconds elapsed)
    Mar 15 21:43:21.978: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'aws-iam-authenticator' (0 seconds elapsed)
    Mar 15 21:43:21.978: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
    Mar 15 21:43:21.978: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
    Mar 15 21:43:21.978: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'kops-controller' (0 seconds elapsed)
    Mar 15 21:43:21.978: INFO: e2e test version: v1.26.2
    Mar 15 21:43:21.980: INFO: kube-apiserver version: v1.26.2-eks-b106822
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Mar 15 21:43:21.980: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:43:21.984: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:43:22.011
Mar 15 21:43:22.011: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 21:43:22.012
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:43:22.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:43:22.03
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/15/23 21:43:22.033
Mar 15 21:43:22.034: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:43:24.616: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:43:31.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3066" for this suite. 03/15/23 21:43:31.354
------------------------------
• [SLOW TEST] [9.348 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:43:22.011
    Mar 15 21:43:22.011: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 21:43:22.012
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:43:22.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:43:22.03
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/15/23 21:43:22.033
    Mar 15 21:43:22.034: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:43:24.616: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:43:31.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3066" for this suite. 03/15/23 21:43:31.354
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:43:31.362
Mar 15 21:43:31.362: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename sysctl 03/15/23 21:43:31.363
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:43:31.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:43:31.376
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/15/23 21:43:31.379
STEP: Watching for error events or started pod 03/15/23 21:43:31.387
STEP: Waiting for pod completion 03/15/23 21:43:33.4
Mar 15 21:43:33.400: INFO: Waiting up to 3m0s for pod "sysctl-7dcbbbb5-5311-4485-9851-a54a3127d557" in namespace "sysctl-2205" to be "completed"
Mar 15 21:43:33.404: INFO: Pod "sysctl-7dcbbbb5-5311-4485-9851-a54a3127d557": Phase="Pending", Reason="", readiness=false. Elapsed: 4.538854ms
Mar 15 21:43:35.408: INFO: Pod "sysctl-7dcbbbb5-5311-4485-9851-a54a3127d557": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007966704s
Mar 15 21:43:37.408: INFO: Pod "sysctl-7dcbbbb5-5311-4485-9851-a54a3127d557": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007742467s
Mar 15 21:43:37.408: INFO: Pod "sysctl-7dcbbbb5-5311-4485-9851-a54a3127d557" satisfied condition "completed"
STEP: Checking that the pod succeeded 03/15/23 21:43:37.41
STEP: Getting logs from the pod 03/15/23 21:43:37.41
STEP: Checking that the sysctl is actually updated 03/15/23 21:43:37.424
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:43:37.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-2205" for this suite. 03/15/23 21:43:37.428
------------------------------
• [SLOW TEST] [6.070 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:43:31.362
    Mar 15 21:43:31.362: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename sysctl 03/15/23 21:43:31.363
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:43:31.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:43:31.376
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/15/23 21:43:31.379
    STEP: Watching for error events or started pod 03/15/23 21:43:31.387
    STEP: Waiting for pod completion 03/15/23 21:43:33.4
    Mar 15 21:43:33.400: INFO: Waiting up to 3m0s for pod "sysctl-7dcbbbb5-5311-4485-9851-a54a3127d557" in namespace "sysctl-2205" to be "completed"
    Mar 15 21:43:33.404: INFO: Pod "sysctl-7dcbbbb5-5311-4485-9851-a54a3127d557": Phase="Pending", Reason="", readiness=false. Elapsed: 4.538854ms
    Mar 15 21:43:35.408: INFO: Pod "sysctl-7dcbbbb5-5311-4485-9851-a54a3127d557": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007966704s
    Mar 15 21:43:37.408: INFO: Pod "sysctl-7dcbbbb5-5311-4485-9851-a54a3127d557": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007742467s
    Mar 15 21:43:37.408: INFO: Pod "sysctl-7dcbbbb5-5311-4485-9851-a54a3127d557" satisfied condition "completed"
    STEP: Checking that the pod succeeded 03/15/23 21:43:37.41
    STEP: Getting logs from the pod 03/15/23 21:43:37.41
    STEP: Checking that the sysctl is actually updated 03/15/23 21:43:37.424
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:43:37.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-2205" for this suite. 03/15/23 21:43:37.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:43:37.434
Mar 15 21:43:37.434: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-runtime 03/15/23 21:43:37.435
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:43:37.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:43:37.463
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 03/15/23 21:43:37.468
STEP: wait for the container to reach Failed 03/15/23 21:43:37.479
STEP: get the container status 03/15/23 21:43:41.505
STEP: the container should be terminated 03/15/23 21:43:41.508
STEP: the termination message should be set 03/15/23 21:43:41.508
Mar 15 21:43:41.508: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/15/23 21:43:41.508
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 15 21:43:41.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8128" for this suite. 03/15/23 21:43:41.532
------------------------------
• [4.103 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:43:37.434
    Mar 15 21:43:37.434: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-runtime 03/15/23 21:43:37.435
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:43:37.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:43:37.463
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 03/15/23 21:43:37.468
    STEP: wait for the container to reach Failed 03/15/23 21:43:37.479
    STEP: get the container status 03/15/23 21:43:41.505
    STEP: the container should be terminated 03/15/23 21:43:41.508
    STEP: the termination message should be set 03/15/23 21:43:41.508
    Mar 15 21:43:41.508: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/15/23 21:43:41.508
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:43:41.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8128" for this suite. 03/15/23 21:43:41.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:43:41.54
Mar 15 21:43:41.540: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 21:43:41.541
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:43:41.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:43:41.56
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 03/15/23 21:43:41.563
Mar 15 21:43:41.583: INFO: Waiting up to 5m0s for pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9" in namespace "downward-api-630" to be "running and ready"
Mar 15 21:43:41.599: INFO: Pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.173368ms
Mar 15 21:43:41.599: INFO: The phase of Pod labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:43:43.603: INFO: Pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019771727s
Mar 15 21:43:43.603: INFO: The phase of Pod labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:43:45.603: INFO: Pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01943304s
Mar 15 21:43:45.603: INFO: The phase of Pod labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:43:47.602: INFO: Pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9": Phase="Running", Reason="", readiness=true. Elapsed: 6.018282968s
Mar 15 21:43:47.602: INFO: The phase of Pod labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9 is Running (Ready = true)
Mar 15 21:43:47.602: INFO: Pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9" satisfied condition "running and ready"
Mar 15 21:43:48.122: INFO: Successfully updated pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 15 21:43:50.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-630" for this suite. 03/15/23 21:43:50.15
------------------------------
• [SLOW TEST] [8.625 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:43:41.54
    Mar 15 21:43:41.540: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 21:43:41.541
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:43:41.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:43:41.56
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 03/15/23 21:43:41.563
    Mar 15 21:43:41.583: INFO: Waiting up to 5m0s for pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9" in namespace "downward-api-630" to be "running and ready"
    Mar 15 21:43:41.599: INFO: Pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.173368ms
    Mar 15 21:43:41.599: INFO: The phase of Pod labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:43:43.603: INFO: Pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019771727s
    Mar 15 21:43:43.603: INFO: The phase of Pod labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:43:45.603: INFO: Pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01943304s
    Mar 15 21:43:45.603: INFO: The phase of Pod labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:43:47.602: INFO: Pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9": Phase="Running", Reason="", readiness=true. Elapsed: 6.018282968s
    Mar 15 21:43:47.602: INFO: The phase of Pod labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9 is Running (Ready = true)
    Mar 15 21:43:47.602: INFO: Pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9" satisfied condition "running and ready"
    Mar 15 21:43:48.122: INFO: Successfully updated pod "labelsupdatec69f55a6-1d42-4904-8e7d-a19efa122dd9"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:43:50.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-630" for this suite. 03/15/23 21:43:50.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:43:50.169
Mar 15 21:43:50.170: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 21:43:50.171
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:43:50.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:43:50.212
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 03/15/23 21:43:50.216
Mar 15 21:43:50.234: INFO: Waiting up to 5m0s for pod "downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726" in namespace "projected-2197" to be "Succeeded or Failed"
Mar 15 21:43:50.242: INFO: Pod "downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726": Phase="Pending", Reason="", readiness=false. Elapsed: 7.699286ms
Mar 15 21:43:52.257: INFO: Pod "downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022882029s
Mar 15 21:43:54.247: INFO: Pod "downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012254996s
Mar 15 21:43:56.246: INFO: Pod "downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011485222s
STEP: Saw pod success 03/15/23 21:43:56.246
Mar 15 21:43:56.246: INFO: Pod "downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726" satisfied condition "Succeeded or Failed"
Mar 15 21:43:56.249: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726 container client-container: <nil>
STEP: delete the pod 03/15/23 21:43:56.271
Mar 15 21:43:56.285: INFO: Waiting for pod downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726 to disappear
Mar 15 21:43:56.290: INFO: Pod downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 15 21:43:56.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2197" for this suite. 03/15/23 21:43:56.294
------------------------------
• [SLOW TEST] [6.130 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:43:50.169
    Mar 15 21:43:50.170: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 21:43:50.171
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:43:50.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:43:50.212
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 03/15/23 21:43:50.216
    Mar 15 21:43:50.234: INFO: Waiting up to 5m0s for pod "downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726" in namespace "projected-2197" to be "Succeeded or Failed"
    Mar 15 21:43:50.242: INFO: Pod "downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726": Phase="Pending", Reason="", readiness=false. Elapsed: 7.699286ms
    Mar 15 21:43:52.257: INFO: Pod "downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022882029s
    Mar 15 21:43:54.247: INFO: Pod "downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012254996s
    Mar 15 21:43:56.246: INFO: Pod "downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011485222s
    STEP: Saw pod success 03/15/23 21:43:56.246
    Mar 15 21:43:56.246: INFO: Pod "downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726" satisfied condition "Succeeded or Failed"
    Mar 15 21:43:56.249: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726 container client-container: <nil>
    STEP: delete the pod 03/15/23 21:43:56.271
    Mar 15 21:43:56.285: INFO: Waiting for pod downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726 to disappear
    Mar 15 21:43:56.290: INFO: Pod downwardapi-volume-858eeb2d-5ac6-41bb-b817-cc38d51e0726 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:43:56.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2197" for this suite. 03/15/23 21:43:56.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:43:56.303
Mar 15 21:43:56.303: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 21:43:56.304
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:43:56.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:43:56.321
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-c95e7ee0-029a-4cc0-986b-337a148f546a 03/15/23 21:43:56.328
STEP: Creating a pod to test consume secrets 03/15/23 21:43:56.333
Mar 15 21:43:56.344: INFO: Waiting up to 5m0s for pod "pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888" in namespace "secrets-494" to be "Succeeded or Failed"
Mar 15 21:43:56.357: INFO: Pod "pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888": Phase="Pending", Reason="", readiness=false. Elapsed: 12.90304ms
Mar 15 21:43:58.360: INFO: Pod "pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016805891s
Mar 15 21:44:00.367: INFO: Pod "pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022972238s
STEP: Saw pod success 03/15/23 21:44:00.367
Mar 15 21:44:00.367: INFO: Pod "pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888" satisfied condition "Succeeded or Failed"
Mar 15 21:44:00.378: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888 container secret-volume-test: <nil>
STEP: delete the pod 03/15/23 21:44:00.403
Mar 15 21:44:00.421: INFO: Waiting for pod pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888 to disappear
Mar 15 21:44:00.426: INFO: Pod pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 21:44:00.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-494" for this suite. 03/15/23 21:44:00.436
------------------------------
• [4.144 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:43:56.303
    Mar 15 21:43:56.303: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 21:43:56.304
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:43:56.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:43:56.321
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-c95e7ee0-029a-4cc0-986b-337a148f546a 03/15/23 21:43:56.328
    STEP: Creating a pod to test consume secrets 03/15/23 21:43:56.333
    Mar 15 21:43:56.344: INFO: Waiting up to 5m0s for pod "pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888" in namespace "secrets-494" to be "Succeeded or Failed"
    Mar 15 21:43:56.357: INFO: Pod "pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888": Phase="Pending", Reason="", readiness=false. Elapsed: 12.90304ms
    Mar 15 21:43:58.360: INFO: Pod "pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016805891s
    Mar 15 21:44:00.367: INFO: Pod "pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022972238s
    STEP: Saw pod success 03/15/23 21:44:00.367
    Mar 15 21:44:00.367: INFO: Pod "pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888" satisfied condition "Succeeded or Failed"
    Mar 15 21:44:00.378: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888 container secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 21:44:00.403
    Mar 15 21:44:00.421: INFO: Waiting for pod pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888 to disappear
    Mar 15 21:44:00.426: INFO: Pod pod-secrets-7ff2a26f-b27e-4a96-9d87-775354e2c888 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:44:00.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-494" for this suite. 03/15/23 21:44:00.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:44:00.45
Mar 15 21:44:00.450: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 21:44:00.451
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:00.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:00.502
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-8647 03/15/23 21:44:00.512
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8647 to expose endpoints map[] 03/15/23 21:44:00.524
Mar 15 21:44:00.536: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar 15 21:44:01.548: INFO: successfully validated that service multi-endpoint-test in namespace services-8647 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8647 03/15/23 21:44:01.548
Mar 15 21:44:01.555: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8647" to be "running and ready"
Mar 15 21:44:01.558: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48516ms
Mar 15 21:44:01.558: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:44:03.563: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008378077s
Mar 15 21:44:03.563: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 15 21:44:03.563: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8647 to expose endpoints map[pod1:[100]] 03/15/23 21:44:03.566
Mar 15 21:44:03.574: INFO: successfully validated that service multi-endpoint-test in namespace services-8647 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8647 03/15/23 21:44:03.575
Mar 15 21:44:03.582: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8647" to be "running and ready"
Mar 15 21:44:03.586: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032822ms
Mar 15 21:44:03.586: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:44:05.593: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010939291s
Mar 15 21:44:05.593: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:44:07.590: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.007995928s
Mar 15 21:44:07.590: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 15 21:44:07.590: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8647 to expose endpoints map[pod1:[100] pod2:[101]] 03/15/23 21:44:07.595
Mar 15 21:44:07.614: INFO: successfully validated that service multi-endpoint-test in namespace services-8647 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 03/15/23 21:44:07.614
Mar 15 21:44:07.614: INFO: Creating new exec pod
Mar 15 21:44:07.624: INFO: Waiting up to 5m0s for pod "execpodj6lqr" in namespace "services-8647" to be "running"
Mar 15 21:44:07.631: INFO: Pod "execpodj6lqr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.961216ms
Mar 15 21:44:09.642: INFO: Pod "execpodj6lqr": Phase="Running", Reason="", readiness=true. Elapsed: 2.018392292s
Mar 15 21:44:09.642: INFO: Pod "execpodj6lqr" satisfied condition "running"
Mar 15 21:44:10.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-8647 exec execpodj6lqr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Mar 15 21:44:10.832: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar 15 21:44:10.832: INFO: stdout: ""
Mar 15 21:44:10.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-8647 exec execpodj6lqr -- /bin/sh -x -c nc -v -z -w 2 100.70.123.206 80'
Mar 15 21:44:10.974: INFO: stderr: "+ nc -v -z -w 2 100.70.123.206 80\nConnection to 100.70.123.206 80 port [tcp/http] succeeded!\n"
Mar 15 21:44:10.974: INFO: stdout: ""
Mar 15 21:44:10.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-8647 exec execpodj6lqr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Mar 15 21:44:11.222: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar 15 21:44:11.222: INFO: stdout: ""
Mar 15 21:44:11.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-8647 exec execpodj6lqr -- /bin/sh -x -c nc -v -z -w 2 100.70.123.206 81'
Mar 15 21:44:11.374: INFO: stderr: "+ nc -v -z -w 2 100.70.123.206 81\nConnection to 100.70.123.206 81 port [tcp/*] succeeded!\n"
Mar 15 21:44:11.374: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-8647 03/15/23 21:44:11.374
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8647 to expose endpoints map[pod2:[101]] 03/15/23 21:44:11.391
Mar 15 21:44:11.432: INFO: successfully validated that service multi-endpoint-test in namespace services-8647 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8647 03/15/23 21:44:11.432
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8647 to expose endpoints map[] 03/15/23 21:44:11.47
Mar 15 21:44:11.498: INFO: successfully validated that service multi-endpoint-test in namespace services-8647 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 21:44:11.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8647" for this suite. 03/15/23 21:44:11.535
------------------------------
• [SLOW TEST] [11.095 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:44:00.45
    Mar 15 21:44:00.450: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 21:44:00.451
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:00.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:00.502
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-8647 03/15/23 21:44:00.512
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8647 to expose endpoints map[] 03/15/23 21:44:00.524
    Mar 15 21:44:00.536: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Mar 15 21:44:01.548: INFO: successfully validated that service multi-endpoint-test in namespace services-8647 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8647 03/15/23 21:44:01.548
    Mar 15 21:44:01.555: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8647" to be "running and ready"
    Mar 15 21:44:01.558: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48516ms
    Mar 15 21:44:01.558: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:44:03.563: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008378077s
    Mar 15 21:44:03.563: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 15 21:44:03.563: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8647 to expose endpoints map[pod1:[100]] 03/15/23 21:44:03.566
    Mar 15 21:44:03.574: INFO: successfully validated that service multi-endpoint-test in namespace services-8647 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-8647 03/15/23 21:44:03.575
    Mar 15 21:44:03.582: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8647" to be "running and ready"
    Mar 15 21:44:03.586: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032822ms
    Mar 15 21:44:03.586: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:44:05.593: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010939291s
    Mar 15 21:44:05.593: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:44:07.590: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.007995928s
    Mar 15 21:44:07.590: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 15 21:44:07.590: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8647 to expose endpoints map[pod1:[100] pod2:[101]] 03/15/23 21:44:07.595
    Mar 15 21:44:07.614: INFO: successfully validated that service multi-endpoint-test in namespace services-8647 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 03/15/23 21:44:07.614
    Mar 15 21:44:07.614: INFO: Creating new exec pod
    Mar 15 21:44:07.624: INFO: Waiting up to 5m0s for pod "execpodj6lqr" in namespace "services-8647" to be "running"
    Mar 15 21:44:07.631: INFO: Pod "execpodj6lqr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.961216ms
    Mar 15 21:44:09.642: INFO: Pod "execpodj6lqr": Phase="Running", Reason="", readiness=true. Elapsed: 2.018392292s
    Mar 15 21:44:09.642: INFO: Pod "execpodj6lqr" satisfied condition "running"
    Mar 15 21:44:10.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-8647 exec execpodj6lqr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Mar 15 21:44:10.832: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Mar 15 21:44:10.832: INFO: stdout: ""
    Mar 15 21:44:10.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-8647 exec execpodj6lqr -- /bin/sh -x -c nc -v -z -w 2 100.70.123.206 80'
    Mar 15 21:44:10.974: INFO: stderr: "+ nc -v -z -w 2 100.70.123.206 80\nConnection to 100.70.123.206 80 port [tcp/http] succeeded!\n"
    Mar 15 21:44:10.974: INFO: stdout: ""
    Mar 15 21:44:10.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-8647 exec execpodj6lqr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Mar 15 21:44:11.222: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Mar 15 21:44:11.222: INFO: stdout: ""
    Mar 15 21:44:11.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-8647 exec execpodj6lqr -- /bin/sh -x -c nc -v -z -w 2 100.70.123.206 81'
    Mar 15 21:44:11.374: INFO: stderr: "+ nc -v -z -w 2 100.70.123.206 81\nConnection to 100.70.123.206 81 port [tcp/*] succeeded!\n"
    Mar 15 21:44:11.374: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-8647 03/15/23 21:44:11.374
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8647 to expose endpoints map[pod2:[101]] 03/15/23 21:44:11.391
    Mar 15 21:44:11.432: INFO: successfully validated that service multi-endpoint-test in namespace services-8647 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-8647 03/15/23 21:44:11.432
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8647 to expose endpoints map[] 03/15/23 21:44:11.47
    Mar 15 21:44:11.498: INFO: successfully validated that service multi-endpoint-test in namespace services-8647 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:44:11.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8647" for this suite. 03/15/23 21:44:11.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:44:11.547
Mar 15 21:44:11.547: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 21:44:11.547
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:11.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:11.576
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 03/15/23 21:44:11.582
Mar 15 21:44:11.590: INFO: Waiting up to 5m0s for pod "downward-api-59510cc7-ead8-4637-80b4-25032d3fd901" in namespace "downward-api-8578" to be "Succeeded or Failed"
Mar 15 21:44:11.597: INFO: Pod "downward-api-59510cc7-ead8-4637-80b4-25032d3fd901": Phase="Pending", Reason="", readiness=false. Elapsed: 6.446712ms
Mar 15 21:44:13.600: INFO: Pod "downward-api-59510cc7-ead8-4637-80b4-25032d3fd901": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009845197s
Mar 15 21:44:15.601: INFO: Pod "downward-api-59510cc7-ead8-4637-80b4-25032d3fd901": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010961118s
STEP: Saw pod success 03/15/23 21:44:15.601
Mar 15 21:44:15.602: INFO: Pod "downward-api-59510cc7-ead8-4637-80b4-25032d3fd901" satisfied condition "Succeeded or Failed"
Mar 15 21:44:15.604: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downward-api-59510cc7-ead8-4637-80b4-25032d3fd901 container dapi-container: <nil>
STEP: delete the pod 03/15/23 21:44:15.611
Mar 15 21:44:15.623: INFO: Waiting for pod downward-api-59510cc7-ead8-4637-80b4-25032d3fd901 to disappear
Mar 15 21:44:15.626: INFO: Pod downward-api-59510cc7-ead8-4637-80b4-25032d3fd901 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 15 21:44:15.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8578" for this suite. 03/15/23 21:44:15.63
------------------------------
• [4.089 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:44:11.547
    Mar 15 21:44:11.547: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 21:44:11.547
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:11.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:11.576
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 03/15/23 21:44:11.582
    Mar 15 21:44:11.590: INFO: Waiting up to 5m0s for pod "downward-api-59510cc7-ead8-4637-80b4-25032d3fd901" in namespace "downward-api-8578" to be "Succeeded or Failed"
    Mar 15 21:44:11.597: INFO: Pod "downward-api-59510cc7-ead8-4637-80b4-25032d3fd901": Phase="Pending", Reason="", readiness=false. Elapsed: 6.446712ms
    Mar 15 21:44:13.600: INFO: Pod "downward-api-59510cc7-ead8-4637-80b4-25032d3fd901": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009845197s
    Mar 15 21:44:15.601: INFO: Pod "downward-api-59510cc7-ead8-4637-80b4-25032d3fd901": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010961118s
    STEP: Saw pod success 03/15/23 21:44:15.601
    Mar 15 21:44:15.602: INFO: Pod "downward-api-59510cc7-ead8-4637-80b4-25032d3fd901" satisfied condition "Succeeded or Failed"
    Mar 15 21:44:15.604: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downward-api-59510cc7-ead8-4637-80b4-25032d3fd901 container dapi-container: <nil>
    STEP: delete the pod 03/15/23 21:44:15.611
    Mar 15 21:44:15.623: INFO: Waiting for pod downward-api-59510cc7-ead8-4637-80b4-25032d3fd901 to disappear
    Mar 15 21:44:15.626: INFO: Pod downward-api-59510cc7-ead8-4637-80b4-25032d3fd901 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:44:15.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8578" for this suite. 03/15/23 21:44:15.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:44:15.64
Mar 15 21:44:15.641: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename custom-resource-definition 03/15/23 21:44:15.642
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:15.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:15.664
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Mar 15 21:44:15.668: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:44:21.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8102" for this suite. 03/15/23 21:44:21.997
------------------------------
• [SLOW TEST] [6.363 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:44:15.64
    Mar 15 21:44:15.641: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename custom-resource-definition 03/15/23 21:44:15.642
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:15.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:15.664
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Mar 15 21:44:15.668: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:44:21.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8102" for this suite. 03/15/23 21:44:21.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:44:22.005
Mar 15 21:44:22.005: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 21:44:22.006
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:22.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:22.027
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-b379fb4c-8fed-423e-b6fd-738acac225cd 03/15/23 21:44:22.03
STEP: Creating a pod to test consume secrets 03/15/23 21:44:22.034
Mar 15 21:44:22.043: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c" in namespace "projected-557" to be "Succeeded or Failed"
Mar 15 21:44:22.046: INFO: Pod "pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765107ms
Mar 15 21:44:24.057: INFO: Pod "pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014534439s
Mar 15 21:44:26.050: INFO: Pod "pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00731881s
STEP: Saw pod success 03/15/23 21:44:26.05
Mar 15 21:44:26.050: INFO: Pod "pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c" satisfied condition "Succeeded or Failed"
Mar 15 21:44:26.055: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c container projected-secret-volume-test: <nil>
STEP: delete the pod 03/15/23 21:44:26.067
Mar 15 21:44:26.086: INFO: Waiting for pod pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c to disappear
Mar 15 21:44:26.089: INFO: Pod pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 15 21:44:26.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-557" for this suite. 03/15/23 21:44:26.093
------------------------------
• [4.095 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:44:22.005
    Mar 15 21:44:22.005: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 21:44:22.006
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:22.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:22.027
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-b379fb4c-8fed-423e-b6fd-738acac225cd 03/15/23 21:44:22.03
    STEP: Creating a pod to test consume secrets 03/15/23 21:44:22.034
    Mar 15 21:44:22.043: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c" in namespace "projected-557" to be "Succeeded or Failed"
    Mar 15 21:44:22.046: INFO: Pod "pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765107ms
    Mar 15 21:44:24.057: INFO: Pod "pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014534439s
    Mar 15 21:44:26.050: INFO: Pod "pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00731881s
    STEP: Saw pod success 03/15/23 21:44:26.05
    Mar 15 21:44:26.050: INFO: Pod "pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c" satisfied condition "Succeeded or Failed"
    Mar 15 21:44:26.055: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 21:44:26.067
    Mar 15 21:44:26.086: INFO: Waiting for pod pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c to disappear
    Mar 15 21:44:26.089: INFO: Pod pod-projected-secrets-997f02bd-5a4c-4bb3-aad5-b9ba0275db5c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:44:26.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-557" for this suite. 03/15/23 21:44:26.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:44:26.106
Mar 15 21:44:26.106: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir-wrapper 03/15/23 21:44:26.107
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:26.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:26.136
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Mar 15 21:44:26.167: INFO: Waiting up to 5m0s for pod "pod-secrets-7cc80f45-7152-4db0-b67a-5d5442645ec7" in namespace "emptydir-wrapper-9761" to be "running and ready"
Mar 15 21:44:26.172: INFO: Pod "pod-secrets-7cc80f45-7152-4db0-b67a-5d5442645ec7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810296ms
Mar 15 21:44:26.172: INFO: The phase of Pod pod-secrets-7cc80f45-7152-4db0-b67a-5d5442645ec7 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:44:28.176: INFO: Pod "pod-secrets-7cc80f45-7152-4db0-b67a-5d5442645ec7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009062664s
Mar 15 21:44:28.176: INFO: The phase of Pod pod-secrets-7cc80f45-7152-4db0-b67a-5d5442645ec7 is Running (Ready = true)
Mar 15 21:44:28.177: INFO: Pod "pod-secrets-7cc80f45-7152-4db0-b67a-5d5442645ec7" satisfied condition "running and ready"
STEP: Cleaning up the secret 03/15/23 21:44:28.179
STEP: Cleaning up the configmap 03/15/23 21:44:28.183
STEP: Cleaning up the pod 03/15/23 21:44:28.187
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 21:44:28.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-9761" for this suite. 03/15/23 21:44:28.213
------------------------------
• [2.116 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:44:26.106
    Mar 15 21:44:26.106: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir-wrapper 03/15/23 21:44:26.107
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:26.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:26.136
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Mar 15 21:44:26.167: INFO: Waiting up to 5m0s for pod "pod-secrets-7cc80f45-7152-4db0-b67a-5d5442645ec7" in namespace "emptydir-wrapper-9761" to be "running and ready"
    Mar 15 21:44:26.172: INFO: Pod "pod-secrets-7cc80f45-7152-4db0-b67a-5d5442645ec7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810296ms
    Mar 15 21:44:26.172: INFO: The phase of Pod pod-secrets-7cc80f45-7152-4db0-b67a-5d5442645ec7 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:44:28.176: INFO: Pod "pod-secrets-7cc80f45-7152-4db0-b67a-5d5442645ec7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009062664s
    Mar 15 21:44:28.176: INFO: The phase of Pod pod-secrets-7cc80f45-7152-4db0-b67a-5d5442645ec7 is Running (Ready = true)
    Mar 15 21:44:28.177: INFO: Pod "pod-secrets-7cc80f45-7152-4db0-b67a-5d5442645ec7" satisfied condition "running and ready"
    STEP: Cleaning up the secret 03/15/23 21:44:28.179
    STEP: Cleaning up the configmap 03/15/23 21:44:28.183
    STEP: Cleaning up the pod 03/15/23 21:44:28.187
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:44:28.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-9761" for this suite. 03/15/23 21:44:28.213
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:44:28.222
Mar 15 21:44:28.222: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename containers 03/15/23 21:44:28.223
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:28.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:28.241
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Mar 15 21:44:28.251: INFO: Waiting up to 5m0s for pod "client-containers-70f07ffc-11d5-4209-a3b1-96903db6bf38" in namespace "containers-3566" to be "running"
Mar 15 21:44:28.254: INFO: Pod "client-containers-70f07ffc-11d5-4209-a3b1-96903db6bf38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.659845ms
Mar 15 21:44:30.259: INFO: Pod "client-containers-70f07ffc-11d5-4209-a3b1-96903db6bf38": Phase="Running", Reason="", readiness=true. Elapsed: 2.008264999s
Mar 15 21:44:30.259: INFO: Pod "client-containers-70f07ffc-11d5-4209-a3b1-96903db6bf38" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 15 21:44:30.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3566" for this suite. 03/15/23 21:44:30.277
------------------------------
• [2.061 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:44:28.222
    Mar 15 21:44:28.222: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename containers 03/15/23 21:44:28.223
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:28.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:28.241
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Mar 15 21:44:28.251: INFO: Waiting up to 5m0s for pod "client-containers-70f07ffc-11d5-4209-a3b1-96903db6bf38" in namespace "containers-3566" to be "running"
    Mar 15 21:44:28.254: INFO: Pod "client-containers-70f07ffc-11d5-4209-a3b1-96903db6bf38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.659845ms
    Mar 15 21:44:30.259: INFO: Pod "client-containers-70f07ffc-11d5-4209-a3b1-96903db6bf38": Phase="Running", Reason="", readiness=true. Elapsed: 2.008264999s
    Mar 15 21:44:30.259: INFO: Pod "client-containers-70f07ffc-11d5-4209-a3b1-96903db6bf38" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:44:30.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3566" for this suite. 03/15/23 21:44:30.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:44:30.285
Mar 15 21:44:30.286: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename gc 03/15/23 21:44:30.286
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:30.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:30.304
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 03/15/23 21:44:30.311
STEP: delete the rc 03/15/23 21:44:35.318
STEP: wait for the rc to be deleted 03/15/23 21:44:35.325
Mar 15 21:44:36.419: INFO: 83 pods remaining
Mar 15 21:44:36.420: INFO: 80 pods has nil DeletionTimestamp
Mar 15 21:44:36.420: INFO: 
Mar 15 21:44:37.388: INFO: 76 pods remaining
Mar 15 21:44:37.389: INFO: 70 pods has nil DeletionTimestamp
Mar 15 21:44:37.389: INFO: 
Mar 15 21:44:38.409: INFO: 69 pods remaining
Mar 15 21:44:38.409: INFO: 60 pods has nil DeletionTimestamp
Mar 15 21:44:38.409: INFO: 
Mar 15 21:44:39.365: INFO: 59 pods remaining
Mar 15 21:44:39.365: INFO: 40 pods has nil DeletionTimestamp
Mar 15 21:44:39.365: INFO: 
Mar 15 21:44:40.383: INFO: 55 pods remaining
Mar 15 21:44:40.383: INFO: 30 pods has nil DeletionTimestamp
Mar 15 21:44:40.383: INFO: 
Mar 15 21:44:41.361: INFO: 54 pods remaining
Mar 15 21:44:41.361: INFO: 20 pods has nil DeletionTimestamp
Mar 15 21:44:41.361: INFO: 
Mar 15 21:44:42.337: INFO: 53 pods remaining
Mar 15 21:44:42.337: INFO: 0 pods has nil DeletionTimestamp
Mar 15 21:44:42.338: INFO: 
Mar 15 21:44:43.336: INFO: 49 pods remaining
Mar 15 21:44:43.336: INFO: 0 pods has nil DeletionTimestamp
Mar 15 21:44:43.336: INFO: 
Mar 15 21:44:44.335: INFO: 44 pods remaining
Mar 15 21:44:44.335: INFO: 0 pods has nil DeletionTimestamp
Mar 15 21:44:44.335: INFO: 
Mar 15 21:44:45.334: INFO: 35 pods remaining
Mar 15 21:44:45.334: INFO: 0 pods has nil DeletionTimestamp
Mar 15 21:44:45.334: INFO: 
Mar 15 21:44:46.359: INFO: 31 pods remaining
Mar 15 21:44:46.359: INFO: 0 pods has nil DeletionTimestamp
Mar 15 21:44:46.359: INFO: 
Mar 15 21:44:47.345: INFO: 25 pods remaining
Mar 15 21:44:47.345: INFO: 0 pods has nil DeletionTimestamp
Mar 15 21:44:47.345: INFO: 
Mar 15 21:44:48.334: INFO: 16 pods remaining
Mar 15 21:44:48.334: INFO: 0 pods has nil DeletionTimestamp
Mar 15 21:44:48.334: INFO: 
Mar 15 21:44:49.332: INFO: 11 pods remaining
Mar 15 21:44:49.332: INFO: 0 pods has nil DeletionTimestamp
Mar 15 21:44:49.332: INFO: 
Mar 15 21:44:50.334: INFO: 2 pods remaining
Mar 15 21:44:50.334: INFO: 0 pods has nil DeletionTimestamp
Mar 15 21:44:50.334: INFO: 
Mar 15 21:44:51.331: INFO: 0 pods remaining
Mar 15 21:44:51.332: INFO: 0 pods has nil DeletionTimestamp
Mar 15 21:44:51.332: INFO: 
STEP: Gathering metrics 03/15/23 21:44:52.332
Mar 15 21:44:52.362: INFO: Waiting up to 5m0s for pod "kube-controller-manager-i-00864a1c8c75a434c" in namespace "kube-system" to be "running and ready"
Mar 15 21:44:52.365: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c": Phase="Running", Reason="", readiness=true. Elapsed: 2.918672ms
Mar 15 21:44:52.365: INFO: The phase of Pod kube-controller-manager-i-00864a1c8c75a434c is Running (Ready = true)
Mar 15 21:44:52.365: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c" satisfied condition "running and ready"
Mar 15 21:44:52.495: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 15 21:44:52.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6924" for this suite. 03/15/23 21:44:52.502
------------------------------
• [SLOW TEST] [22.222 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:44:30.285
    Mar 15 21:44:30.286: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename gc 03/15/23 21:44:30.286
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:30.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:30.304
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 03/15/23 21:44:30.311
    STEP: delete the rc 03/15/23 21:44:35.318
    STEP: wait for the rc to be deleted 03/15/23 21:44:35.325
    Mar 15 21:44:36.419: INFO: 83 pods remaining
    Mar 15 21:44:36.420: INFO: 80 pods has nil DeletionTimestamp
    Mar 15 21:44:36.420: INFO: 
    Mar 15 21:44:37.388: INFO: 76 pods remaining
    Mar 15 21:44:37.389: INFO: 70 pods has nil DeletionTimestamp
    Mar 15 21:44:37.389: INFO: 
    Mar 15 21:44:38.409: INFO: 69 pods remaining
    Mar 15 21:44:38.409: INFO: 60 pods has nil DeletionTimestamp
    Mar 15 21:44:38.409: INFO: 
    Mar 15 21:44:39.365: INFO: 59 pods remaining
    Mar 15 21:44:39.365: INFO: 40 pods has nil DeletionTimestamp
    Mar 15 21:44:39.365: INFO: 
    Mar 15 21:44:40.383: INFO: 55 pods remaining
    Mar 15 21:44:40.383: INFO: 30 pods has nil DeletionTimestamp
    Mar 15 21:44:40.383: INFO: 
    Mar 15 21:44:41.361: INFO: 54 pods remaining
    Mar 15 21:44:41.361: INFO: 20 pods has nil DeletionTimestamp
    Mar 15 21:44:41.361: INFO: 
    Mar 15 21:44:42.337: INFO: 53 pods remaining
    Mar 15 21:44:42.337: INFO: 0 pods has nil DeletionTimestamp
    Mar 15 21:44:42.338: INFO: 
    Mar 15 21:44:43.336: INFO: 49 pods remaining
    Mar 15 21:44:43.336: INFO: 0 pods has nil DeletionTimestamp
    Mar 15 21:44:43.336: INFO: 
    Mar 15 21:44:44.335: INFO: 44 pods remaining
    Mar 15 21:44:44.335: INFO: 0 pods has nil DeletionTimestamp
    Mar 15 21:44:44.335: INFO: 
    Mar 15 21:44:45.334: INFO: 35 pods remaining
    Mar 15 21:44:45.334: INFO: 0 pods has nil DeletionTimestamp
    Mar 15 21:44:45.334: INFO: 
    Mar 15 21:44:46.359: INFO: 31 pods remaining
    Mar 15 21:44:46.359: INFO: 0 pods has nil DeletionTimestamp
    Mar 15 21:44:46.359: INFO: 
    Mar 15 21:44:47.345: INFO: 25 pods remaining
    Mar 15 21:44:47.345: INFO: 0 pods has nil DeletionTimestamp
    Mar 15 21:44:47.345: INFO: 
    Mar 15 21:44:48.334: INFO: 16 pods remaining
    Mar 15 21:44:48.334: INFO: 0 pods has nil DeletionTimestamp
    Mar 15 21:44:48.334: INFO: 
    Mar 15 21:44:49.332: INFO: 11 pods remaining
    Mar 15 21:44:49.332: INFO: 0 pods has nil DeletionTimestamp
    Mar 15 21:44:49.332: INFO: 
    Mar 15 21:44:50.334: INFO: 2 pods remaining
    Mar 15 21:44:50.334: INFO: 0 pods has nil DeletionTimestamp
    Mar 15 21:44:50.334: INFO: 
    Mar 15 21:44:51.331: INFO: 0 pods remaining
    Mar 15 21:44:51.332: INFO: 0 pods has nil DeletionTimestamp
    Mar 15 21:44:51.332: INFO: 
    STEP: Gathering metrics 03/15/23 21:44:52.332
    Mar 15 21:44:52.362: INFO: Waiting up to 5m0s for pod "kube-controller-manager-i-00864a1c8c75a434c" in namespace "kube-system" to be "running and ready"
    Mar 15 21:44:52.365: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c": Phase="Running", Reason="", readiness=true. Elapsed: 2.918672ms
    Mar 15 21:44:52.365: INFO: The phase of Pod kube-controller-manager-i-00864a1c8c75a434c is Running (Ready = true)
    Mar 15 21:44:52.365: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c" satisfied condition "running and ready"
    Mar 15 21:44:52.495: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:44:52.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6924" for this suite. 03/15/23 21:44:52.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:44:52.508
Mar 15 21:44:52.508: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 21:44:52.509
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:52.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:52.524
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 03/15/23 21:44:52.527
Mar 15 21:44:52.535: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292" in namespace "downward-api-1724" to be "Succeeded or Failed"
Mar 15 21:44:52.542: INFO: Pod "downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292": Phase="Pending", Reason="", readiness=false. Elapsed: 6.540334ms
Mar 15 21:44:54.546: INFO: Pod "downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010251693s
Mar 15 21:44:56.546: INFO: Pod "downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010678822s
Mar 15 21:44:58.546: INFO: Pod "downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010881117s
STEP: Saw pod success 03/15/23 21:44:58.546
Mar 15 21:44:58.547: INFO: Pod "downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292" satisfied condition "Succeeded or Failed"
Mar 15 21:44:58.550: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292 container client-container: <nil>
STEP: delete the pod 03/15/23 21:44:58.555
Mar 15 21:44:58.566: INFO: Waiting for pod downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292 to disappear
Mar 15 21:44:58.570: INFO: Pod downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 15 21:44:58.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1724" for this suite. 03/15/23 21:44:58.576
------------------------------
• [SLOW TEST] [6.076 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:44:52.508
    Mar 15 21:44:52.508: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 21:44:52.509
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:52.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:52.524
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 03/15/23 21:44:52.527
    Mar 15 21:44:52.535: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292" in namespace "downward-api-1724" to be "Succeeded or Failed"
    Mar 15 21:44:52.542: INFO: Pod "downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292": Phase="Pending", Reason="", readiness=false. Elapsed: 6.540334ms
    Mar 15 21:44:54.546: INFO: Pod "downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010251693s
    Mar 15 21:44:56.546: INFO: Pod "downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010678822s
    Mar 15 21:44:58.546: INFO: Pod "downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010881117s
    STEP: Saw pod success 03/15/23 21:44:58.546
    Mar 15 21:44:58.547: INFO: Pod "downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292" satisfied condition "Succeeded or Failed"
    Mar 15 21:44:58.550: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292 container client-container: <nil>
    STEP: delete the pod 03/15/23 21:44:58.555
    Mar 15 21:44:58.566: INFO: Waiting for pod downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292 to disappear
    Mar 15 21:44:58.570: INFO: Pod downwardapi-volume-1e559a49-2e99-422c-a166-5352ceae8292 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:44:58.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1724" for this suite. 03/15/23 21:44:58.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:44:58.584
Mar 15 21:44:58.584: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 21:44:58.585
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:58.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:58.601
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 03/15/23 21:44:58.606
Mar 15 21:44:58.615: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2" in namespace "downward-api-5374" to be "Succeeded or Failed"
Mar 15 21:44:58.620: INFO: Pod "downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.057643ms
Mar 15 21:45:00.623: INFO: Pod "downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008898382s
Mar 15 21:45:02.625: INFO: Pod "downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010154237s
STEP: Saw pod success 03/15/23 21:45:02.625
Mar 15 21:45:02.625: INFO: Pod "downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2" satisfied condition "Succeeded or Failed"
Mar 15 21:45:02.628: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2 container client-container: <nil>
STEP: delete the pod 03/15/23 21:45:02.64
Mar 15 21:45:02.654: INFO: Waiting for pod downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2 to disappear
Mar 15 21:45:02.664: INFO: Pod downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 15 21:45:02.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5374" for this suite. 03/15/23 21:45:02.672
------------------------------
• [4.098 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:44:58.584
    Mar 15 21:44:58.584: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 21:44:58.585
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:44:58.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:44:58.601
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 03/15/23 21:44:58.606
    Mar 15 21:44:58.615: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2" in namespace "downward-api-5374" to be "Succeeded or Failed"
    Mar 15 21:44:58.620: INFO: Pod "downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.057643ms
    Mar 15 21:45:00.623: INFO: Pod "downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008898382s
    Mar 15 21:45:02.625: INFO: Pod "downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010154237s
    STEP: Saw pod success 03/15/23 21:45:02.625
    Mar 15 21:45:02.625: INFO: Pod "downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2" satisfied condition "Succeeded or Failed"
    Mar 15 21:45:02.628: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2 container client-container: <nil>
    STEP: delete the pod 03/15/23 21:45:02.64
    Mar 15 21:45:02.654: INFO: Waiting for pod downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2 to disappear
    Mar 15 21:45:02.664: INFO: Pod downwardapi-volume-8fae8738-242b-45a7-9768-8976f0c907c2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:45:02.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5374" for this suite. 03/15/23 21:45:02.672
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:45:02.683
Mar 15 21:45:02.683: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 21:45:02.684
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:02.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:02.72
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 21:45:02.756
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 21:45:03.006
STEP: Deploying the webhook pod 03/15/23 21:45:03.015
STEP: Wait for the deployment to be ready 03/15/23 21:45:03.035
Mar 15 21:45:03.066: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 21:45:05.075
STEP: Verifying the service has paired with the endpoint 03/15/23 21:45:05.088
Mar 15 21:45:06.089: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Mar 15 21:45:06.092: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9959-crds.webhook.example.com via the AdmissionRegistration API 03/15/23 21:45:06.609
STEP: Creating a custom resource that should be mutated by the webhook 03/15/23 21:45:06.628
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:45:09.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3254" for this suite. 03/15/23 21:45:09.268
STEP: Destroying namespace "webhook-3254-markers" for this suite. 03/15/23 21:45:09.28
------------------------------
• [SLOW TEST] [6.605 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:45:02.683
    Mar 15 21:45:02.683: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 21:45:02.684
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:02.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:02.72
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 21:45:02.756
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 21:45:03.006
    STEP: Deploying the webhook pod 03/15/23 21:45:03.015
    STEP: Wait for the deployment to be ready 03/15/23 21:45:03.035
    Mar 15 21:45:03.066: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 21:45:05.075
    STEP: Verifying the service has paired with the endpoint 03/15/23 21:45:05.088
    Mar 15 21:45:06.089: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Mar 15 21:45:06.092: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9959-crds.webhook.example.com via the AdmissionRegistration API 03/15/23 21:45:06.609
    STEP: Creating a custom resource that should be mutated by the webhook 03/15/23 21:45:06.628
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:45:09.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3254" for this suite. 03/15/23 21:45:09.268
    STEP: Destroying namespace "webhook-3254-markers" for this suite. 03/15/23 21:45:09.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:45:09.304
Mar 15 21:45:09.305: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 21:45:09.306
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:09.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:09.348
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-90ca72d4-d607-4f2a-9d98-4614e3101959 03/15/23 21:45:09.354
STEP: Creating a pod to test consume secrets 03/15/23 21:45:09.362
Mar 15 21:45:09.370: INFO: Waiting up to 5m0s for pod "pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b" in namespace "secrets-9837" to be "Succeeded or Failed"
Mar 15 21:45:09.379: INFO: Pod "pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.064985ms
Mar 15 21:45:11.383: INFO: Pod "pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012309136s
Mar 15 21:45:13.382: INFO: Pod "pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011533317s
Mar 15 21:45:15.383: INFO: Pod "pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012185293s
STEP: Saw pod success 03/15/23 21:45:15.383
Mar 15 21:45:15.383: INFO: Pod "pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b" satisfied condition "Succeeded or Failed"
Mar 15 21:45:15.386: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b container secret-volume-test: <nil>
STEP: delete the pod 03/15/23 21:45:15.391
Mar 15 21:45:15.401: INFO: Waiting for pod pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b to disappear
Mar 15 21:45:15.404: INFO: Pod pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 21:45:15.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9837" for this suite. 03/15/23 21:45:15.408
------------------------------
• [SLOW TEST] [6.111 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:45:09.304
    Mar 15 21:45:09.305: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 21:45:09.306
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:09.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:09.348
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-90ca72d4-d607-4f2a-9d98-4614e3101959 03/15/23 21:45:09.354
    STEP: Creating a pod to test consume secrets 03/15/23 21:45:09.362
    Mar 15 21:45:09.370: INFO: Waiting up to 5m0s for pod "pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b" in namespace "secrets-9837" to be "Succeeded or Failed"
    Mar 15 21:45:09.379: INFO: Pod "pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.064985ms
    Mar 15 21:45:11.383: INFO: Pod "pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012309136s
    Mar 15 21:45:13.382: INFO: Pod "pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011533317s
    Mar 15 21:45:15.383: INFO: Pod "pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012185293s
    STEP: Saw pod success 03/15/23 21:45:15.383
    Mar 15 21:45:15.383: INFO: Pod "pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b" satisfied condition "Succeeded or Failed"
    Mar 15 21:45:15.386: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b container secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 21:45:15.391
    Mar 15 21:45:15.401: INFO: Waiting for pod pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b to disappear
    Mar 15 21:45:15.404: INFO: Pod pod-secrets-d994fdfe-3094-413e-84af-17cabe9bcf3b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:45:15.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9837" for this suite. 03/15/23 21:45:15.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:45:15.417
Mar 15 21:45:15.417: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename events 03/15/23 21:45:15.418
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:15.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:15.432
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 03/15/23 21:45:15.436
STEP: get a list of Events with a label in the current namespace 03/15/23 21:45:15.448
STEP: delete a list of events 03/15/23 21:45:15.455
Mar 15 21:45:15.455: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/15/23 21:45:15.468
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Mar 15 21:45:15.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4261" for this suite. 03/15/23 21:45:15.474
------------------------------
• [0.065 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:45:15.417
    Mar 15 21:45:15.417: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename events 03/15/23 21:45:15.418
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:15.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:15.432
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 03/15/23 21:45:15.436
    STEP: get a list of Events with a label in the current namespace 03/15/23 21:45:15.448
    STEP: delete a list of events 03/15/23 21:45:15.455
    Mar 15 21:45:15.455: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/15/23 21:45:15.468
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:45:15.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4261" for this suite. 03/15/23 21:45:15.474
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:45:15.483
Mar 15 21:45:15.483: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 21:45:15.484
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:15.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:15.499
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 21:45:15.513
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 21:45:16.199
STEP: Deploying the webhook pod 03/15/23 21:45:16.203
STEP: Wait for the deployment to be ready 03/15/23 21:45:16.218
Mar 15 21:45:16.228: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 21:45:18.237
STEP: Verifying the service has paired with the endpoint 03/15/23 21:45:18.248
Mar 15 21:45:19.248: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 03/15/23 21:45:19.252
STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/15/23 21:45:19.284
STEP: Creating a configMap that should not be mutated 03/15/23 21:45:19.296
STEP: Patching a mutating webhook configuration's rules to include the create operation 03/15/23 21:45:19.31
STEP: Creating a configMap that should be mutated 03/15/23 21:45:19.319
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:45:19.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2834" for this suite. 03/15/23 21:45:19.444
STEP: Destroying namespace "webhook-2834-markers" for this suite. 03/15/23 21:45:19.46
------------------------------
• [4.068 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:45:15.483
    Mar 15 21:45:15.483: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 21:45:15.484
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:15.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:15.499
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 21:45:15.513
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 21:45:16.199
    STEP: Deploying the webhook pod 03/15/23 21:45:16.203
    STEP: Wait for the deployment to be ready 03/15/23 21:45:16.218
    Mar 15 21:45:16.228: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 21:45:18.237
    STEP: Verifying the service has paired with the endpoint 03/15/23 21:45:18.248
    Mar 15 21:45:19.248: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 03/15/23 21:45:19.252
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/15/23 21:45:19.284
    STEP: Creating a configMap that should not be mutated 03/15/23 21:45:19.296
    STEP: Patching a mutating webhook configuration's rules to include the create operation 03/15/23 21:45:19.31
    STEP: Creating a configMap that should be mutated 03/15/23 21:45:19.319
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:45:19.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2834" for this suite. 03/15/23 21:45:19.444
    STEP: Destroying namespace "webhook-2834-markers" for this suite. 03/15/23 21:45:19.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:45:19.554
Mar 15 21:45:19.554: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 21:45:19.555
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:19.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:19.611
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 03/15/23 21:45:19.617
Mar 15 21:45:19.625: INFO: Waiting up to 5m0s for pod "pod-61328bd8-c3a5-495c-80df-61995335f952" in namespace "emptydir-6720" to be "Succeeded or Failed"
Mar 15 21:45:19.639: INFO: Pod "pod-61328bd8-c3a5-495c-80df-61995335f952": Phase="Pending", Reason="", readiness=false. Elapsed: 13.9312ms
Mar 15 21:45:21.643: INFO: Pod "pod-61328bd8-c3a5-495c-80df-61995335f952": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018243545s
Mar 15 21:45:23.644: INFO: Pod "pod-61328bd8-c3a5-495c-80df-61995335f952": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018462183s
STEP: Saw pod success 03/15/23 21:45:23.644
Mar 15 21:45:23.644: INFO: Pod "pod-61328bd8-c3a5-495c-80df-61995335f952" satisfied condition "Succeeded or Failed"
Mar 15 21:45:23.648: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-61328bd8-c3a5-495c-80df-61995335f952 container test-container: <nil>
STEP: delete the pod 03/15/23 21:45:23.663
Mar 15 21:45:23.686: INFO: Waiting for pod pod-61328bd8-c3a5-495c-80df-61995335f952 to disappear
Mar 15 21:45:23.691: INFO: Pod pod-61328bd8-c3a5-495c-80df-61995335f952 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 21:45:23.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6720" for this suite. 03/15/23 21:45:23.698
------------------------------
• [4.152 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:45:19.554
    Mar 15 21:45:19.554: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 21:45:19.555
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:19.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:19.611
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/15/23 21:45:19.617
    Mar 15 21:45:19.625: INFO: Waiting up to 5m0s for pod "pod-61328bd8-c3a5-495c-80df-61995335f952" in namespace "emptydir-6720" to be "Succeeded or Failed"
    Mar 15 21:45:19.639: INFO: Pod "pod-61328bd8-c3a5-495c-80df-61995335f952": Phase="Pending", Reason="", readiness=false. Elapsed: 13.9312ms
    Mar 15 21:45:21.643: INFO: Pod "pod-61328bd8-c3a5-495c-80df-61995335f952": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018243545s
    Mar 15 21:45:23.644: INFO: Pod "pod-61328bd8-c3a5-495c-80df-61995335f952": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018462183s
    STEP: Saw pod success 03/15/23 21:45:23.644
    Mar 15 21:45:23.644: INFO: Pod "pod-61328bd8-c3a5-495c-80df-61995335f952" satisfied condition "Succeeded or Failed"
    Mar 15 21:45:23.648: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-61328bd8-c3a5-495c-80df-61995335f952 container test-container: <nil>
    STEP: delete the pod 03/15/23 21:45:23.663
    Mar 15 21:45:23.686: INFO: Waiting for pod pod-61328bd8-c3a5-495c-80df-61995335f952 to disappear
    Mar 15 21:45:23.691: INFO: Pod pod-61328bd8-c3a5-495c-80df-61995335f952 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:45:23.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6720" for this suite. 03/15/23 21:45:23.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:45:23.715
Mar 15 21:45:23.716: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename endpointslice 03/15/23 21:45:23.717
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:23.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:23.735
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 03/15/23 21:45:28.869
STEP: referencing matching pods with named port 03/15/23 21:45:33.887
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/15/23 21:45:38.902
STEP: recreating EndpointSlices after they've been deleted 03/15/23 21:45:43.911
Mar 15 21:45:43.926: INFO: EndpointSlice for Service endpointslice-2699/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 15 21:45:53.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2699" for this suite. 03/15/23 21:45:53.937
------------------------------
• [SLOW TEST] [30.230 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:45:23.715
    Mar 15 21:45:23.716: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename endpointslice 03/15/23 21:45:23.717
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:23.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:23.735
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 03/15/23 21:45:28.869
    STEP: referencing matching pods with named port 03/15/23 21:45:33.887
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/15/23 21:45:38.902
    STEP: recreating EndpointSlices after they've been deleted 03/15/23 21:45:43.911
    Mar 15 21:45:43.926: INFO: EndpointSlice for Service endpointslice-2699/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:45:53.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2699" for this suite. 03/15/23 21:45:53.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:45:53.946
Mar 15 21:45:53.946: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename sysctl 03/15/23 21:45:53.947
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:53.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:53.963
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 03/15/23 21:45:53.966
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:45:53.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-9687" for this suite. 03/15/23 21:45:53.979
------------------------------
• [0.045 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:45:53.946
    Mar 15 21:45:53.946: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename sysctl 03/15/23 21:45:53.947
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:53.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:53.963
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 03/15/23 21:45:53.966
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:45:53.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-9687" for this suite. 03/15/23 21:45:53.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:45:53.992
Mar 15 21:45:53.992: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 21:45:53.993
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:54.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:54.025
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/15/23 21:45:54.032
Mar 15 21:45:54.041: INFO: Waiting up to 5m0s for pod "pod-da19f891-c9cc-4e87-8cc6-0c13578d6710" in namespace "emptydir-3745" to be "Succeeded or Failed"
Mar 15 21:45:54.045: INFO: Pod "pod-da19f891-c9cc-4e87-8cc6-0c13578d6710": Phase="Pending", Reason="", readiness=false. Elapsed: 4.325688ms
Mar 15 21:45:56.048: INFO: Pod "pod-da19f891-c9cc-4e87-8cc6-0c13578d6710": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007363727s
Mar 15 21:45:58.049: INFO: Pod "pod-da19f891-c9cc-4e87-8cc6-0c13578d6710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00847597s
STEP: Saw pod success 03/15/23 21:45:58.049
Mar 15 21:45:58.049: INFO: Pod "pod-da19f891-c9cc-4e87-8cc6-0c13578d6710" satisfied condition "Succeeded or Failed"
Mar 15 21:45:58.052: INFO: Trying to get logs from node i-077ee0eb7ec5a02aa pod pod-da19f891-c9cc-4e87-8cc6-0c13578d6710 container test-container: <nil>
STEP: delete the pod 03/15/23 21:45:58.066
Mar 15 21:45:58.078: INFO: Waiting for pod pod-da19f891-c9cc-4e87-8cc6-0c13578d6710 to disappear
Mar 15 21:45:58.081: INFO: Pod pod-da19f891-c9cc-4e87-8cc6-0c13578d6710 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 21:45:58.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3745" for this suite. 03/15/23 21:45:58.086
------------------------------
• [4.102 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:45:53.992
    Mar 15 21:45:53.992: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 21:45:53.993
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:54.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:54.025
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/15/23 21:45:54.032
    Mar 15 21:45:54.041: INFO: Waiting up to 5m0s for pod "pod-da19f891-c9cc-4e87-8cc6-0c13578d6710" in namespace "emptydir-3745" to be "Succeeded or Failed"
    Mar 15 21:45:54.045: INFO: Pod "pod-da19f891-c9cc-4e87-8cc6-0c13578d6710": Phase="Pending", Reason="", readiness=false. Elapsed: 4.325688ms
    Mar 15 21:45:56.048: INFO: Pod "pod-da19f891-c9cc-4e87-8cc6-0c13578d6710": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007363727s
    Mar 15 21:45:58.049: INFO: Pod "pod-da19f891-c9cc-4e87-8cc6-0c13578d6710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00847597s
    STEP: Saw pod success 03/15/23 21:45:58.049
    Mar 15 21:45:58.049: INFO: Pod "pod-da19f891-c9cc-4e87-8cc6-0c13578d6710" satisfied condition "Succeeded or Failed"
    Mar 15 21:45:58.052: INFO: Trying to get logs from node i-077ee0eb7ec5a02aa pod pod-da19f891-c9cc-4e87-8cc6-0c13578d6710 container test-container: <nil>
    STEP: delete the pod 03/15/23 21:45:58.066
    Mar 15 21:45:58.078: INFO: Waiting for pod pod-da19f891-c9cc-4e87-8cc6-0c13578d6710 to disappear
    Mar 15 21:45:58.081: INFO: Pod pod-da19f891-c9cc-4e87-8cc6-0c13578d6710 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:45:58.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3745" for this suite. 03/15/23 21:45:58.086
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:45:58.096
Mar 15 21:45:58.096: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename endpointslice 03/15/23 21:45:58.099
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:58.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:58.117
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 15 21:46:00.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3868" for this suite. 03/15/23 21:46:00.227
------------------------------
• [2.157 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:45:58.096
    Mar 15 21:45:58.096: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename endpointslice 03/15/23 21:45:58.099
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:45:58.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:45:58.117
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:46:00.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3868" for this suite. 03/15/23 21:46:00.227
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:46:00.296
Mar 15 21:46:00.297: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 21:46:00.309
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:46:00.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:46:00.358
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 03/15/23 21:46:00.365
Mar 15 21:46:00.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 15 21:46:00.532: INFO: stderr: ""
Mar 15 21:46:00.532: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 03/15/23 21:46:00.532
Mar 15 21:46:00.532: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 15 21:46:00.533: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9270" to be "running and ready, or succeeded"
Mar 15 21:46:00.538: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.362422ms
Mar 15 21:46:00.538: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'i-077ee0eb7ec5a02aa' to be 'Running' but was 'Pending'
Mar 15 21:46:02.542: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.008695796s
Mar 15 21:46:02.542: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 15 21:46:02.542: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 03/15/23 21:46:02.542
Mar 15 21:46:02.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 logs logs-generator logs-generator'
Mar 15 21:46:02.617: INFO: stderr: ""
Mar 15 21:46:02.617: INFO: stdout: "I0315 21:46:01.347947       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/hgfr 254\nI0315 21:46:01.548364       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/6jn 549\nI0315 21:46:01.748657       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/ndkk 537\nI0315 21:46:01.948839       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/hs8l 473\nI0315 21:46:02.148083       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/r68d 433\nI0315 21:46:02.348488       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/rbhl 356\nI0315 21:46:02.548682       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5gz 256\n"
STEP: limiting log lines 03/15/23 21:46:02.617
Mar 15 21:46:02.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 logs logs-generator logs-generator --tail=1'
Mar 15 21:46:02.719: INFO: stderr: ""
Mar 15 21:46:02.719: INFO: stdout: "I0315 21:46:02.548682       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5gz 256\n"
Mar 15 21:46:02.719: INFO: got output "I0315 21:46:02.548682       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5gz 256\n"
STEP: limiting log bytes 03/15/23 21:46:02.719
Mar 15 21:46:02.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 logs logs-generator logs-generator --limit-bytes=1'
Mar 15 21:46:02.864: INFO: stderr: ""
Mar 15 21:46:02.864: INFO: stdout: "I"
Mar 15 21:46:02.864: INFO: got output "I"
STEP: exposing timestamps 03/15/23 21:46:02.864
Mar 15 21:46:02.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 15 21:46:03.017: INFO: stderr: ""
Mar 15 21:46:03.017: INFO: stdout: "2023-03-15T21:46:02.949185834Z I0315 21:46:02.948852       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/2p9x 540\n"
Mar 15 21:46:03.017: INFO: got output "2023-03-15T21:46:02.949185834Z I0315 21:46:02.948852       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/2p9x 540\n"
STEP: restricting to a time range 03/15/23 21:46:03.017
Mar 15 21:46:05.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 logs logs-generator logs-generator --since=1s'
Mar 15 21:46:05.607: INFO: stderr: ""
Mar 15 21:46:05.607: INFO: stdout: "I0315 21:46:04.748758       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/89wz 273\nI0315 21:46:04.948070       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/txm 350\nI0315 21:46:05.148406       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/g2d 229\nI0315 21:46:05.348776       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/msdw 574\nI0315 21:46:05.548061       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/sw28 334\n"
Mar 15 21:46:05.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 logs logs-generator logs-generator --since=24h'
Mar 15 21:46:05.692: INFO: stderr: ""
Mar 15 21:46:05.692: INFO: stdout: "I0315 21:46:01.347947       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/hgfr 254\nI0315 21:46:01.548364       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/6jn 549\nI0315 21:46:01.748657       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/ndkk 537\nI0315 21:46:01.948839       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/hs8l 473\nI0315 21:46:02.148083       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/r68d 433\nI0315 21:46:02.348488       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/rbhl 356\nI0315 21:46:02.548682       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5gz 256\nI0315 21:46:02.748105       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/jzh 481\nI0315 21:46:02.948852       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/2p9x 540\nI0315 21:46:03.148560       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/b7ss 249\nI0315 21:46:03.348948       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/gpg 479\nI0315 21:46:03.548295       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/lvmc 515\nI0315 21:46:03.750984       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/k78 465\nI0315 21:46:03.948526       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/xc68 350\nI0315 21:46:04.148861       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/prdp 485\nI0315 21:46:04.348115       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/wxl 231\nI0315 21:46:04.548416       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/s28 425\nI0315 21:46:04.748758       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/89wz 273\nI0315 21:46:04.948070       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/txm 350\nI0315 21:46:05.148406       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/g2d 229\nI0315 21:46:05.348776       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/msdw 574\nI0315 21:46:05.548061       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/sw28 334\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Mar 15 21:46:05.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 delete pod logs-generator'
Mar 15 21:46:06.821: INFO: stderr: ""
Mar 15 21:46:06.821: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 21:46:06.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9270" for this suite. 03/15/23 21:46:06.824
------------------------------
• [SLOW TEST] [6.578 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:46:00.296
    Mar 15 21:46:00.297: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 21:46:00.309
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:46:00.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:46:00.358
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 03/15/23 21:46:00.365
    Mar 15 21:46:00.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Mar 15 21:46:00.532: INFO: stderr: ""
    Mar 15 21:46:00.532: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 03/15/23 21:46:00.532
    Mar 15 21:46:00.532: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Mar 15 21:46:00.533: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9270" to be "running and ready, or succeeded"
    Mar 15 21:46:00.538: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.362422ms
    Mar 15 21:46:00.538: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'i-077ee0eb7ec5a02aa' to be 'Running' but was 'Pending'
    Mar 15 21:46:02.542: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.008695796s
    Mar 15 21:46:02.542: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Mar 15 21:46:02.542: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 03/15/23 21:46:02.542
    Mar 15 21:46:02.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 logs logs-generator logs-generator'
    Mar 15 21:46:02.617: INFO: stderr: ""
    Mar 15 21:46:02.617: INFO: stdout: "I0315 21:46:01.347947       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/hgfr 254\nI0315 21:46:01.548364       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/6jn 549\nI0315 21:46:01.748657       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/ndkk 537\nI0315 21:46:01.948839       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/hs8l 473\nI0315 21:46:02.148083       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/r68d 433\nI0315 21:46:02.348488       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/rbhl 356\nI0315 21:46:02.548682       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5gz 256\n"
    STEP: limiting log lines 03/15/23 21:46:02.617
    Mar 15 21:46:02.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 logs logs-generator logs-generator --tail=1'
    Mar 15 21:46:02.719: INFO: stderr: ""
    Mar 15 21:46:02.719: INFO: stdout: "I0315 21:46:02.548682       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5gz 256\n"
    Mar 15 21:46:02.719: INFO: got output "I0315 21:46:02.548682       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5gz 256\n"
    STEP: limiting log bytes 03/15/23 21:46:02.719
    Mar 15 21:46:02.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 logs logs-generator logs-generator --limit-bytes=1'
    Mar 15 21:46:02.864: INFO: stderr: ""
    Mar 15 21:46:02.864: INFO: stdout: "I"
    Mar 15 21:46:02.864: INFO: got output "I"
    STEP: exposing timestamps 03/15/23 21:46:02.864
    Mar 15 21:46:02.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 logs logs-generator logs-generator --tail=1 --timestamps'
    Mar 15 21:46:03.017: INFO: stderr: ""
    Mar 15 21:46:03.017: INFO: stdout: "2023-03-15T21:46:02.949185834Z I0315 21:46:02.948852       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/2p9x 540\n"
    Mar 15 21:46:03.017: INFO: got output "2023-03-15T21:46:02.949185834Z I0315 21:46:02.948852       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/2p9x 540\n"
    STEP: restricting to a time range 03/15/23 21:46:03.017
    Mar 15 21:46:05.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 logs logs-generator logs-generator --since=1s'
    Mar 15 21:46:05.607: INFO: stderr: ""
    Mar 15 21:46:05.607: INFO: stdout: "I0315 21:46:04.748758       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/89wz 273\nI0315 21:46:04.948070       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/txm 350\nI0315 21:46:05.148406       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/g2d 229\nI0315 21:46:05.348776       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/msdw 574\nI0315 21:46:05.548061       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/sw28 334\n"
    Mar 15 21:46:05.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 logs logs-generator logs-generator --since=24h'
    Mar 15 21:46:05.692: INFO: stderr: ""
    Mar 15 21:46:05.692: INFO: stdout: "I0315 21:46:01.347947       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/hgfr 254\nI0315 21:46:01.548364       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/6jn 549\nI0315 21:46:01.748657       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/ndkk 537\nI0315 21:46:01.948839       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/hs8l 473\nI0315 21:46:02.148083       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/r68d 433\nI0315 21:46:02.348488       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/rbhl 356\nI0315 21:46:02.548682       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5gz 256\nI0315 21:46:02.748105       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/jzh 481\nI0315 21:46:02.948852       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/2p9x 540\nI0315 21:46:03.148560       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/b7ss 249\nI0315 21:46:03.348948       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/gpg 479\nI0315 21:46:03.548295       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/lvmc 515\nI0315 21:46:03.750984       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/k78 465\nI0315 21:46:03.948526       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/xc68 350\nI0315 21:46:04.148861       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/prdp 485\nI0315 21:46:04.348115       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/wxl 231\nI0315 21:46:04.548416       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/s28 425\nI0315 21:46:04.748758       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/89wz 273\nI0315 21:46:04.948070       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/txm 350\nI0315 21:46:05.148406       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/g2d 229\nI0315 21:46:05.348776       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/msdw 574\nI0315 21:46:05.548061       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/sw28 334\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Mar 15 21:46:05.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-9270 delete pod logs-generator'
    Mar 15 21:46:06.821: INFO: stderr: ""
    Mar 15 21:46:06.821: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:46:06.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9270" for this suite. 03/15/23 21:46:06.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:46:06.833
Mar 15 21:46:06.833: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename watch 03/15/23 21:46:06.834
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:46:06.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:46:06.85
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 03/15/23 21:46:06.853
STEP: creating a new configmap 03/15/23 21:46:06.854
STEP: modifying the configmap once 03/15/23 21:46:06.858
STEP: changing the label value of the configmap 03/15/23 21:46:06.866
STEP: Expecting to observe a delete notification for the watched object 03/15/23 21:46:06.882
Mar 15 21:46:06.882: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4442  8bd8debe-ebfb-4655-9f0e-1ceb4240e611 3510 0 2023-03-15 21:46:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-15 21:46:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 21:46:06.882: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4442  8bd8debe-ebfb-4655-9f0e-1ceb4240e611 3511 0 2023-03-15 21:46:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-15 21:46:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 21:46:06.882: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4442  8bd8debe-ebfb-4655-9f0e-1ceb4240e611 3512 0 2023-03-15 21:46:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-15 21:46:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 03/15/23 21:46:06.882
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/15/23 21:46:06.901
STEP: changing the label value of the configmap back 03/15/23 21:46:16.901
STEP: modifying the configmap a third time 03/15/23 21:46:16.908
STEP: deleting the configmap 03/15/23 21:46:16.913
STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/15/23 21:46:16.917
Mar 15 21:46:16.917: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4442  8bd8debe-ebfb-4655-9f0e-1ceb4240e611 3554 0 2023-03-15 21:46:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-15 21:46:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 21:46:16.918: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4442  8bd8debe-ebfb-4655-9f0e-1ceb4240e611 3555 0 2023-03-15 21:46:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-15 21:46:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 21:46:16.918: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4442  8bd8debe-ebfb-4655-9f0e-1ceb4240e611 3556 0 2023-03-15 21:46:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-15 21:46:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 15 21:46:16.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4442" for this suite. 03/15/23 21:46:16.923
------------------------------
• [SLOW TEST] [10.097 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:46:06.833
    Mar 15 21:46:06.833: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename watch 03/15/23 21:46:06.834
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:46:06.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:46:06.85
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 03/15/23 21:46:06.853
    STEP: creating a new configmap 03/15/23 21:46:06.854
    STEP: modifying the configmap once 03/15/23 21:46:06.858
    STEP: changing the label value of the configmap 03/15/23 21:46:06.866
    STEP: Expecting to observe a delete notification for the watched object 03/15/23 21:46:06.882
    Mar 15 21:46:06.882: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4442  8bd8debe-ebfb-4655-9f0e-1ceb4240e611 3510 0 2023-03-15 21:46:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-15 21:46:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 21:46:06.882: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4442  8bd8debe-ebfb-4655-9f0e-1ceb4240e611 3511 0 2023-03-15 21:46:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-15 21:46:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 21:46:06.882: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4442  8bd8debe-ebfb-4655-9f0e-1ceb4240e611 3512 0 2023-03-15 21:46:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-15 21:46:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 03/15/23 21:46:06.882
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/15/23 21:46:06.901
    STEP: changing the label value of the configmap back 03/15/23 21:46:16.901
    STEP: modifying the configmap a third time 03/15/23 21:46:16.908
    STEP: deleting the configmap 03/15/23 21:46:16.913
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/15/23 21:46:16.917
    Mar 15 21:46:16.917: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4442  8bd8debe-ebfb-4655-9f0e-1ceb4240e611 3554 0 2023-03-15 21:46:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-15 21:46:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 21:46:16.918: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4442  8bd8debe-ebfb-4655-9f0e-1ceb4240e611 3555 0 2023-03-15 21:46:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-15 21:46:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 21:46:16.918: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4442  8bd8debe-ebfb-4655-9f0e-1ceb4240e611 3556 0 2023-03-15 21:46:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-15 21:46:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:46:16.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4442" for this suite. 03/15/23 21:46:16.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:46:16.938
Mar 15 21:46:16.938: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename subpath 03/15/23 21:46:16.939
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:46:16.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:46:16.959
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/15/23 21:46:16.964
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-vt5w 03/15/23 21:46:16.975
STEP: Creating a pod to test atomic-volume-subpath 03/15/23 21:46:16.975
Mar 15 21:46:16.984: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vt5w" in namespace "subpath-8167" to be "Succeeded or Failed"
Mar 15 21:46:16.992: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017855ms
Mar 15 21:46:18.998: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 2.014109838s
Mar 15 21:46:20.996: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 4.012265237s
Mar 15 21:46:22.995: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 6.011107651s
Mar 15 21:46:25.002: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 8.017952732s
Mar 15 21:46:26.997: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 10.013258733s
Mar 15 21:46:28.997: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 12.013013376s
Mar 15 21:46:30.997: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 14.012591956s
Mar 15 21:46:32.996: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 16.012339349s
Mar 15 21:46:34.997: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 18.012918241s
Mar 15 21:46:36.996: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 20.011961835s
Mar 15 21:46:38.996: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=false. Elapsed: 22.012449015s
Mar 15 21:46:40.996: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012179179s
STEP: Saw pod success 03/15/23 21:46:40.996
Mar 15 21:46:40.996: INFO: Pod "pod-subpath-test-downwardapi-vt5w" satisfied condition "Succeeded or Failed"
Mar 15 21:46:41.000: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-subpath-test-downwardapi-vt5w container test-container-subpath-downwardapi-vt5w: <nil>
STEP: delete the pod 03/15/23 21:46:41.007
Mar 15 21:46:41.022: INFO: Waiting for pod pod-subpath-test-downwardapi-vt5w to disappear
Mar 15 21:46:41.024: INFO: Pod pod-subpath-test-downwardapi-vt5w no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-vt5w 03/15/23 21:46:41.025
Mar 15 21:46:41.025: INFO: Deleting pod "pod-subpath-test-downwardapi-vt5w" in namespace "subpath-8167"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 15 21:46:41.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8167" for this suite. 03/15/23 21:46:41.031
------------------------------
• [SLOW TEST] [24.099 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:46:16.938
    Mar 15 21:46:16.938: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename subpath 03/15/23 21:46:16.939
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:46:16.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:46:16.959
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/15/23 21:46:16.964
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-vt5w 03/15/23 21:46:16.975
    STEP: Creating a pod to test atomic-volume-subpath 03/15/23 21:46:16.975
    Mar 15 21:46:16.984: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vt5w" in namespace "subpath-8167" to be "Succeeded or Failed"
    Mar 15 21:46:16.992: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017855ms
    Mar 15 21:46:18.998: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 2.014109838s
    Mar 15 21:46:20.996: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 4.012265237s
    Mar 15 21:46:22.995: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 6.011107651s
    Mar 15 21:46:25.002: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 8.017952732s
    Mar 15 21:46:26.997: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 10.013258733s
    Mar 15 21:46:28.997: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 12.013013376s
    Mar 15 21:46:30.997: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 14.012591956s
    Mar 15 21:46:32.996: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 16.012339349s
    Mar 15 21:46:34.997: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 18.012918241s
    Mar 15 21:46:36.996: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=true. Elapsed: 20.011961835s
    Mar 15 21:46:38.996: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Running", Reason="", readiness=false. Elapsed: 22.012449015s
    Mar 15 21:46:40.996: INFO: Pod "pod-subpath-test-downwardapi-vt5w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012179179s
    STEP: Saw pod success 03/15/23 21:46:40.996
    Mar 15 21:46:40.996: INFO: Pod "pod-subpath-test-downwardapi-vt5w" satisfied condition "Succeeded or Failed"
    Mar 15 21:46:41.000: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-subpath-test-downwardapi-vt5w container test-container-subpath-downwardapi-vt5w: <nil>
    STEP: delete the pod 03/15/23 21:46:41.007
    Mar 15 21:46:41.022: INFO: Waiting for pod pod-subpath-test-downwardapi-vt5w to disappear
    Mar 15 21:46:41.024: INFO: Pod pod-subpath-test-downwardapi-vt5w no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-vt5w 03/15/23 21:46:41.025
    Mar 15 21:46:41.025: INFO: Deleting pod "pod-subpath-test-downwardapi-vt5w" in namespace "subpath-8167"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:46:41.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8167" for this suite. 03/15/23 21:46:41.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:46:41.041
Mar 15 21:46:41.041: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 21:46:41.042
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:46:41.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:46:41.078
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 03/15/23 21:46:41.081
Mar 15 21:46:41.091: INFO: Waiting up to 5m0s for pod "pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828" in namespace "emptydir-9569" to be "Succeeded or Failed"
Mar 15 21:46:41.095: INFO: Pod "pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828": Phase="Pending", Reason="", readiness=false. Elapsed: 4.590858ms
Mar 15 21:46:43.099: INFO: Pod "pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008431761s
Mar 15 21:46:45.099: INFO: Pod "pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008576871s
STEP: Saw pod success 03/15/23 21:46:45.099
Mar 15 21:46:45.100: INFO: Pod "pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828" satisfied condition "Succeeded or Failed"
Mar 15 21:46:45.103: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828 container test-container: <nil>
STEP: delete the pod 03/15/23 21:46:45.112
Mar 15 21:46:45.132: INFO: Waiting for pod pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828 to disappear
Mar 15 21:46:45.138: INFO: Pod pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 21:46:45.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9569" for this suite. 03/15/23 21:46:45.163
------------------------------
• [4.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:46:41.041
    Mar 15 21:46:41.041: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 21:46:41.042
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:46:41.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:46:41.078
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/15/23 21:46:41.081
    Mar 15 21:46:41.091: INFO: Waiting up to 5m0s for pod "pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828" in namespace "emptydir-9569" to be "Succeeded or Failed"
    Mar 15 21:46:41.095: INFO: Pod "pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828": Phase="Pending", Reason="", readiness=false. Elapsed: 4.590858ms
    Mar 15 21:46:43.099: INFO: Pod "pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008431761s
    Mar 15 21:46:45.099: INFO: Pod "pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008576871s
    STEP: Saw pod success 03/15/23 21:46:45.099
    Mar 15 21:46:45.100: INFO: Pod "pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828" satisfied condition "Succeeded or Failed"
    Mar 15 21:46:45.103: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828 container test-container: <nil>
    STEP: delete the pod 03/15/23 21:46:45.112
    Mar 15 21:46:45.132: INFO: Waiting for pod pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828 to disappear
    Mar 15 21:46:45.138: INFO: Pod pod-a2ac1f3a-0aed-498e-b00e-b4e5adbb4828 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:46:45.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9569" for this suite. 03/15/23 21:46:45.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:46:45.197
Mar 15 21:46:45.197: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 21:46:45.2
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:46:45.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:46:45.299
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 21:46:45.387
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 21:46:46.241
STEP: Deploying the webhook pod 03/15/23 21:46:46.25
STEP: Wait for the deployment to be ready 03/15/23 21:46:46.263
Mar 15 21:46:46.278: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 21:46:48.286
STEP: Verifying the service has paired with the endpoint 03/15/23 21:46:48.297
Mar 15 21:46:49.297: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 03/15/23 21:46:49.3
STEP: Creating a configMap that does not comply to the validation webhook rules 03/15/23 21:46:49.338
STEP: Updating a validating webhook configuration's rules to not include the create operation 03/15/23 21:46:49.35
STEP: Creating a configMap that does not comply to the validation webhook rules 03/15/23 21:46:49.364
STEP: Patching a validating webhook configuration's rules to include the create operation 03/15/23 21:46:49.382
STEP: Creating a configMap that does not comply to the validation webhook rules 03/15/23 21:46:49.392
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:46:49.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5889" for this suite. 03/15/23 21:46:49.537
STEP: Destroying namespace "webhook-5889-markers" for this suite. 03/15/23 21:46:49.548
------------------------------
• [4.362 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:46:45.197
    Mar 15 21:46:45.197: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 21:46:45.2
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:46:45.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:46:45.299
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 21:46:45.387
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 21:46:46.241
    STEP: Deploying the webhook pod 03/15/23 21:46:46.25
    STEP: Wait for the deployment to be ready 03/15/23 21:46:46.263
    Mar 15 21:46:46.278: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 21:46:48.286
    STEP: Verifying the service has paired with the endpoint 03/15/23 21:46:48.297
    Mar 15 21:46:49.297: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 03/15/23 21:46:49.3
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/15/23 21:46:49.338
    STEP: Updating a validating webhook configuration's rules to not include the create operation 03/15/23 21:46:49.35
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/15/23 21:46:49.364
    STEP: Patching a validating webhook configuration's rules to include the create operation 03/15/23 21:46:49.382
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/15/23 21:46:49.392
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:46:49.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5889" for this suite. 03/15/23 21:46:49.537
    STEP: Destroying namespace "webhook-5889-markers" for this suite. 03/15/23 21:46:49.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:46:49.564
Mar 15 21:46:49.564: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pod-network-test 03/15/23 21:46:49.565
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:46:49.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:46:49.59
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1870 03/15/23 21:46:49.594
STEP: creating a selector 03/15/23 21:46:49.594
STEP: Creating the service pods in kubernetes 03/15/23 21:46:49.594
Mar 15 21:46:49.594: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 15 21:46:49.651: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1870" to be "running and ready"
Mar 15 21:46:49.658: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.112218ms
Mar 15 21:46:49.658: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:46:51.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011239229s
Mar 15 21:46:51.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:46:53.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010801486s
Mar 15 21:46:53.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:46:55.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011714368s
Mar 15 21:46:55.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:46:57.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010930437s
Mar 15 21:46:57.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:46:59.675: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023370015s
Mar 15 21:46:59.675: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:47:01.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011104751s
Mar 15 21:47:01.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:47:03.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011769393s
Mar 15 21:47:03.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:47:05.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012575913s
Mar 15 21:47:05.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:47:07.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010325591s
Mar 15 21:47:07.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:47:09.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011787498s
Mar 15 21:47:09.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:47:11.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.012825935s
Mar 15 21:47:11.664: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 15 21:47:11.665: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 15 21:47:11.667: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1870" to be "running and ready"
Mar 15 21:47:11.672: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.200538ms
Mar 15 21:47:11.672: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 15 21:47:11.672: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 15 21:47:11.676: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1870" to be "running and ready"
Mar 15 21:47:11.680: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.34902ms
Mar 15 21:47:11.680: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 15 21:47:11.680: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/15/23 21:47:11.685
Mar 15 21:47:11.692: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1870" to be "running"
Mar 15 21:47:11.697: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034019ms
Mar 15 21:47:13.700: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007413771s
Mar 15 21:47:13.700: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 15 21:47:13.704: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 15 21:47:13.704: INFO: Breadth first check of 100.96.2.22 on host 172.20.75.105...
Mar 15 21:47:13.706: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.239:9080/dial?request=hostname&protocol=http&host=100.96.2.22&port=8083&tries=1'] Namespace:pod-network-test-1870 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:47:13.706: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:47:13.707: INFO: ExecWithOptions: Clientset creation
Mar 15 21:47:13.707: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-1870/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.96.2.22%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 15 21:47:13.863: INFO: Waiting for responses: map[]
Mar 15 21:47:13.863: INFO: reached 100.96.2.22 after 0/1 tries
Mar 15 21:47:13.863: INFO: Breadth first check of 100.96.1.24 on host 172.20.60.208...
Mar 15 21:47:13.866: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.239:9080/dial?request=hostname&protocol=http&host=100.96.1.24&port=8083&tries=1'] Namespace:pod-network-test-1870 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:47:13.866: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:47:13.867: INFO: ExecWithOptions: Clientset creation
Mar 15 21:47:13.867: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-1870/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.96.1.24%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 15 21:47:13.968: INFO: Waiting for responses: map[]
Mar 15 21:47:13.968: INFO: reached 100.96.1.24 after 0/1 tries
Mar 15 21:47:13.968: INFO: Breadth first check of 100.96.3.42 on host 172.20.126.23...
Mar 15 21:47:13.972: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.239:9080/dial?request=hostname&protocol=http&host=100.96.3.42&port=8083&tries=1'] Namespace:pod-network-test-1870 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:47:13.972: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:47:13.972: INFO: ExecWithOptions: Clientset creation
Mar 15 21:47:13.972: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-1870/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.96.3.42%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 15 21:47:14.052: INFO: Waiting for responses: map[]
Mar 15 21:47:14.052: INFO: reached 100.96.3.42 after 0/1 tries
Mar 15 21:47:14.052: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 15 21:47:14.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1870" for this suite. 03/15/23 21:47:14.058
------------------------------
• [SLOW TEST] [24.502 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:46:49.564
    Mar 15 21:46:49.564: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pod-network-test 03/15/23 21:46:49.565
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:46:49.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:46:49.59
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1870 03/15/23 21:46:49.594
    STEP: creating a selector 03/15/23 21:46:49.594
    STEP: Creating the service pods in kubernetes 03/15/23 21:46:49.594
    Mar 15 21:46:49.594: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 15 21:46:49.651: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1870" to be "running and ready"
    Mar 15 21:46:49.658: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.112218ms
    Mar 15 21:46:49.658: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:46:51.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011239229s
    Mar 15 21:46:51.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:46:53.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010801486s
    Mar 15 21:46:53.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:46:55.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011714368s
    Mar 15 21:46:55.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:46:57.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010930437s
    Mar 15 21:46:57.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:46:59.675: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023370015s
    Mar 15 21:46:59.675: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:47:01.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011104751s
    Mar 15 21:47:01.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:47:03.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011769393s
    Mar 15 21:47:03.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:47:05.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012575913s
    Mar 15 21:47:05.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:47:07.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010325591s
    Mar 15 21:47:07.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:47:09.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011787498s
    Mar 15 21:47:09.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:47:11.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.012825935s
    Mar 15 21:47:11.664: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 15 21:47:11.665: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 15 21:47:11.667: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1870" to be "running and ready"
    Mar 15 21:47:11.672: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.200538ms
    Mar 15 21:47:11.672: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 15 21:47:11.672: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 15 21:47:11.676: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1870" to be "running and ready"
    Mar 15 21:47:11.680: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.34902ms
    Mar 15 21:47:11.680: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 15 21:47:11.680: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/15/23 21:47:11.685
    Mar 15 21:47:11.692: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1870" to be "running"
    Mar 15 21:47:11.697: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034019ms
    Mar 15 21:47:13.700: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007413771s
    Mar 15 21:47:13.700: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 15 21:47:13.704: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 15 21:47:13.704: INFO: Breadth first check of 100.96.2.22 on host 172.20.75.105...
    Mar 15 21:47:13.706: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.239:9080/dial?request=hostname&protocol=http&host=100.96.2.22&port=8083&tries=1'] Namespace:pod-network-test-1870 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:47:13.706: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:47:13.707: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:47:13.707: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-1870/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.96.2.22%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 15 21:47:13.863: INFO: Waiting for responses: map[]
    Mar 15 21:47:13.863: INFO: reached 100.96.2.22 after 0/1 tries
    Mar 15 21:47:13.863: INFO: Breadth first check of 100.96.1.24 on host 172.20.60.208...
    Mar 15 21:47:13.866: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.239:9080/dial?request=hostname&protocol=http&host=100.96.1.24&port=8083&tries=1'] Namespace:pod-network-test-1870 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:47:13.866: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:47:13.867: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:47:13.867: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-1870/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.96.1.24%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 15 21:47:13.968: INFO: Waiting for responses: map[]
    Mar 15 21:47:13.968: INFO: reached 100.96.1.24 after 0/1 tries
    Mar 15 21:47:13.968: INFO: Breadth first check of 100.96.3.42 on host 172.20.126.23...
    Mar 15 21:47:13.972: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.239:9080/dial?request=hostname&protocol=http&host=100.96.3.42&port=8083&tries=1'] Namespace:pod-network-test-1870 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:47:13.972: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:47:13.972: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:47:13.972: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-1870/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.96.3.42%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 15 21:47:14.052: INFO: Waiting for responses: map[]
    Mar 15 21:47:14.052: INFO: reached 100.96.3.42 after 0/1 tries
    Mar 15 21:47:14.052: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:47:14.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1870" for this suite. 03/15/23 21:47:14.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:47:14.067
Mar 15 21:47:14.067: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-runtime 03/15/23 21:47:14.068
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:47:14.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:47:14.093
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/15/23 21:47:14.11
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/15/23 21:47:31.193
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/15/23 21:47:31.198
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/15/23 21:47:31.203
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/15/23 21:47:31.203
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/15/23 21:47:31.254
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/15/23 21:47:34.28
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/15/23 21:47:36.29
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/15/23 21:47:36.296
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/15/23 21:47:36.296
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/15/23 21:47:36.319
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/15/23 21:47:37.331
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/15/23 21:47:40.344
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/15/23 21:47:40.349
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/15/23 21:47:40.349
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 15 21:47:40.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2435" for this suite. 03/15/23 21:47:40.397
------------------------------
• [SLOW TEST] [26.352 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:47:14.067
    Mar 15 21:47:14.067: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-runtime 03/15/23 21:47:14.068
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:47:14.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:47:14.093
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/15/23 21:47:14.11
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/15/23 21:47:31.193
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/15/23 21:47:31.198
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/15/23 21:47:31.203
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/15/23 21:47:31.203
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/15/23 21:47:31.254
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/15/23 21:47:34.28
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/15/23 21:47:36.29
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/15/23 21:47:36.296
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/15/23 21:47:36.296
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/15/23 21:47:36.319
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/15/23 21:47:37.331
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/15/23 21:47:40.344
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/15/23 21:47:40.349
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/15/23 21:47:40.349
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:47:40.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2435" for this suite. 03/15/23 21:47:40.397
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:47:40.419
Mar 15 21:47:40.419: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 21:47:40.42
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:47:40.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:47:40.469
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-7209/secret-test-7e8912ec-a47a-4ead-b4f5-f0d82730876c 03/15/23 21:47:40.476
STEP: Creating a pod to test consume secrets 03/15/23 21:47:40.485
Mar 15 21:47:40.501: INFO: Waiting up to 5m0s for pod "pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58" in namespace "secrets-7209" to be "Succeeded or Failed"
Mar 15 21:47:40.508: INFO: Pod "pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58": Phase="Pending", Reason="", readiness=false. Elapsed: 7.633536ms
Mar 15 21:47:42.514: INFO: Pod "pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013519434s
Mar 15 21:47:44.512: INFO: Pod "pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011804569s
STEP: Saw pod success 03/15/23 21:47:44.512
Mar 15 21:47:44.513: INFO: Pod "pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58" satisfied condition "Succeeded or Failed"
Mar 15 21:47:44.515: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58 container env-test: <nil>
STEP: delete the pod 03/15/23 21:47:44.522
Mar 15 21:47:44.532: INFO: Waiting for pod pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58 to disappear
Mar 15 21:47:44.542: INFO: Pod pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 21:47:44.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7209" for this suite. 03/15/23 21:47:44.546
------------------------------
• [4.132 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:47:40.419
    Mar 15 21:47:40.419: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 21:47:40.42
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:47:40.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:47:40.469
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-7209/secret-test-7e8912ec-a47a-4ead-b4f5-f0d82730876c 03/15/23 21:47:40.476
    STEP: Creating a pod to test consume secrets 03/15/23 21:47:40.485
    Mar 15 21:47:40.501: INFO: Waiting up to 5m0s for pod "pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58" in namespace "secrets-7209" to be "Succeeded or Failed"
    Mar 15 21:47:40.508: INFO: Pod "pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58": Phase="Pending", Reason="", readiness=false. Elapsed: 7.633536ms
    Mar 15 21:47:42.514: INFO: Pod "pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013519434s
    Mar 15 21:47:44.512: INFO: Pod "pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011804569s
    STEP: Saw pod success 03/15/23 21:47:44.512
    Mar 15 21:47:44.513: INFO: Pod "pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58" satisfied condition "Succeeded or Failed"
    Mar 15 21:47:44.515: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58 container env-test: <nil>
    STEP: delete the pod 03/15/23 21:47:44.522
    Mar 15 21:47:44.532: INFO: Waiting for pod pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58 to disappear
    Mar 15 21:47:44.542: INFO: Pod pod-configmaps-cbd00b91-8c44-4bd0-a422-69333cfe3c58 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:47:44.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7209" for this suite. 03/15/23 21:47:44.546
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:47:44.551
Mar 15 21:47:44.552: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename deployment 03/15/23 21:47:44.553
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:47:44.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:47:44.569
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 03/15/23 21:47:44.577
STEP: waiting for Deployment to be created 03/15/23 21:47:44.581
STEP: waiting for all Replicas to be Ready 03/15/23 21:47:44.584
Mar 15 21:47:44.589: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 15 21:47:44.589: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 15 21:47:44.598: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 15 21:47:44.598: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 15 21:47:44.626: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 15 21:47:44.626: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 15 21:47:44.663: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 15 21:47:44.663: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 15 21:47:46.034: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 15 21:47:46.034: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 15 21:47:46.066: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 03/15/23 21:47:46.066
W0315 21:47:46.072768      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 15 21:47:46.076: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 03/15/23 21:47:46.076
Mar 15 21:47:46.078: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
Mar 15 21:47:46.078: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:46.088: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:46.088: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:46.107: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:46.108: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:46.119: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
Mar 15 21:47:46.119: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
Mar 15 21:47:46.165: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
Mar 15 21:47:46.165: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
Mar 15 21:47:49.104: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:49.104: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:49.141: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
STEP: listing Deployments 03/15/23 21:47:49.141
Mar 15 21:47:49.149: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 03/15/23 21:47:49.149
Mar 15 21:47:49.162: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 03/15/23 21:47:49.162
Mar 15 21:47:49.172: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 15 21:47:49.178: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 15 21:47:49.234: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 15 21:47:49.273: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 15 21:47:49.307: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 15 21:47:51.066: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 15 21:47:53.126: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Mar 15 21:47:53.149: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 15 21:47:53.192: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 15 21:47:57.093: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 03/15/23 21:47:57.128
STEP: fetching the DeploymentStatus 03/15/23 21:47:57.138
Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 3
Mar 15 21:47:57.144: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:57.144: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
Mar 15 21:47:57.144: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 3
STEP: deleting the Deployment 03/15/23 21:47:57.144
Mar 15 21:47:57.168: INFO: observed event type MODIFIED
Mar 15 21:47:57.169: INFO: observed event type MODIFIED
Mar 15 21:47:57.169: INFO: observed event type MODIFIED
Mar 15 21:47:57.169: INFO: observed event type MODIFIED
Mar 15 21:47:57.169: INFO: observed event type MODIFIED
Mar 15 21:47:57.169: INFO: observed event type MODIFIED
Mar 15 21:47:57.169: INFO: observed event type MODIFIED
Mar 15 21:47:57.169: INFO: observed event type MODIFIED
Mar 15 21:47:57.169: INFO: observed event type MODIFIED
Mar 15 21:47:57.169: INFO: observed event type MODIFIED
Mar 15 21:47:57.169: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 15 21:47:57.179: INFO: Log out all the ReplicaSets if there is no deployment created
Mar 15 21:47:57.196: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-4658  2b81b228-2c3a-47f0-9965-47f9a93adaa2 4225 2 2023-03-15 21:47:49 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 3b0cd630-e7f0-4106-8cda-c8cc5bc15c3f 0xc005df1177 0xc005df1178}] [] [{kube-controller-manager Update apps/v1 2023-03-15 21:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b0cd630-e7f0-4106-8cda-c8cc5bc15c3f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 21:47:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005df1200 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Mar 15 21:47:57.200: INFO: pod: "test-deployment-7b7876f9d6-jkjsl":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-jkjsl test-deployment-7b7876f9d6- deployment-4658  bb3d65d3-437f-4208-b90f-788663fa81c8 4189 0 2023-03-15 21:47:49 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 2b81b228-2c3a-47f0-9965-47f9a93adaa2 0xc006250c37 0xc006250c38}] [] [{kube-controller-manager Update v1 2023-03-15 21:47:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b81b228-2c3a-47f0-9965-47f9a93adaa2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 21:47:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9q66w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9q66w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.177,StartTime:2023-03-15 21:47:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 21:47:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a7d83da873759ccdb41a0c28ce01bf88bd8a3fa2487dd4265b1a4a6018adfc27,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 15 21:47:57.200: INFO: pod: "test-deployment-7b7876f9d6-q5sp6":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-q5sp6 test-deployment-7b7876f9d6- deployment-4658  5266a875-bcd8-4e4c-a664-7a275106c48e 4224 0 2023-03-15 21:47:53 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 2b81b228-2c3a-47f0-9965-47f9a93adaa2 0xc006250e17 0xc006250e18}] [] [{kube-controller-manager Update v1 2023-03-15 21:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b81b228-2c3a-47f0-9965-47f9a93adaa2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 21:47:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5dgm8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5dgm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.192,StartTime:2023-03-15 21:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 21:47:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0267f3b7eceb389bfbb352795e8bff7221db101607ddf648d4a656905c990663,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 15 21:47:57.200: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-4658  44672b1a-f276-4a2b-898c-49106efc7ea8 4230 4 2023-03-15 21:47:46 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 3b0cd630-e7f0-4106-8cda-c8cc5bc15c3f 0xc005df1267 0xc005df1268}] [] [{kube-controller-manager Update apps/v1 2023-03-15 21:47:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b0cd630-e7f0-4106-8cda-c8cc5bc15c3f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 21:47:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005df12f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar 15 21:47:57.207: INFO: pod: "test-deployment-7df74c55ff-k6qfk":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-k6qfk test-deployment-7df74c55ff- deployment-4658  75e6e553-60ae-4f32-9d7f-69a6cf23e9c3 4228 0 2023-03-15 21:47:49 +0000 UTC 2023-03-15 21:47:58 +0000 UTC 0xc005df1650 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 44672b1a-f276-4a2b-898c-49106efc7ea8 0xc005df1687 0xc005df1688}] [] [{kube-controller-manager Update v1 2023-03-15 21:47:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44672b1a-f276-4a2b-898c-49106efc7ea8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 21:47:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xbqr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xbqr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.65,StartTime:2023-03-15 21:47:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 21:47:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://db8d4f0cf5f1af8903df9bb9bcfed1790c50e82b5c37785148bebf2eacf2a321,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 15 21:47:57.207: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-4658  f0c1b6b0-8683-493f-8501-4adb2fc7bade 4145 3 2023-03-15 21:47:44 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 3b0cd630-e7f0-4106-8cda-c8cc5bc15c3f 0xc005df1357 0xc005df1358}] [] [{kube-controller-manager Update apps/v1 2023-03-15 21:47:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b0cd630-e7f0-4106-8cda-c8cc5bc15c3f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 21:47:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005df13e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 15 21:47:57.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4658" for this suite. 03/15/23 21:47:57.22
------------------------------
• [SLOW TEST] [12.681 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:47:44.551
    Mar 15 21:47:44.552: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename deployment 03/15/23 21:47:44.553
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:47:44.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:47:44.569
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 03/15/23 21:47:44.577
    STEP: waiting for Deployment to be created 03/15/23 21:47:44.581
    STEP: waiting for all Replicas to be Ready 03/15/23 21:47:44.584
    Mar 15 21:47:44.589: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 15 21:47:44.589: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 15 21:47:44.598: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 15 21:47:44.598: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 15 21:47:44.626: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 15 21:47:44.626: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 15 21:47:44.663: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 15 21:47:44.663: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 15 21:47:46.034: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 15 21:47:46.034: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 15 21:47:46.066: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 03/15/23 21:47:46.066
    W0315 21:47:46.072768      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 15 21:47:46.076: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 03/15/23 21:47:46.076
    Mar 15 21:47:46.078: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
    Mar 15 21:47:46.078: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
    Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
    Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
    Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
    Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
    Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
    Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 0
    Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:46.079: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:46.088: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:46.088: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:46.107: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:46.108: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:46.119: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    Mar 15 21:47:46.119: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    Mar 15 21:47:46.165: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    Mar 15 21:47:46.165: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    Mar 15 21:47:49.104: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:49.104: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:49.141: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    STEP: listing Deployments 03/15/23 21:47:49.141
    Mar 15 21:47:49.149: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 03/15/23 21:47:49.149
    Mar 15 21:47:49.162: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 03/15/23 21:47:49.162
    Mar 15 21:47:49.172: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 15 21:47:49.178: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 15 21:47:49.234: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 15 21:47:49.273: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 15 21:47:49.307: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 15 21:47:51.066: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 15 21:47:53.126: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 15 21:47:53.149: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 15 21:47:53.192: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 15 21:47:57.093: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 03/15/23 21:47:57.128
    STEP: fetching the DeploymentStatus 03/15/23 21:47:57.138
    Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 1
    Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:57.143: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 3
    Mar 15 21:47:57.144: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:57.144: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 2
    Mar 15 21:47:57.144: INFO: observed Deployment test-deployment in namespace deployment-4658 with ReadyReplicas 3
    STEP: deleting the Deployment 03/15/23 21:47:57.144
    Mar 15 21:47:57.168: INFO: observed event type MODIFIED
    Mar 15 21:47:57.169: INFO: observed event type MODIFIED
    Mar 15 21:47:57.169: INFO: observed event type MODIFIED
    Mar 15 21:47:57.169: INFO: observed event type MODIFIED
    Mar 15 21:47:57.169: INFO: observed event type MODIFIED
    Mar 15 21:47:57.169: INFO: observed event type MODIFIED
    Mar 15 21:47:57.169: INFO: observed event type MODIFIED
    Mar 15 21:47:57.169: INFO: observed event type MODIFIED
    Mar 15 21:47:57.169: INFO: observed event type MODIFIED
    Mar 15 21:47:57.169: INFO: observed event type MODIFIED
    Mar 15 21:47:57.169: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 15 21:47:57.179: INFO: Log out all the ReplicaSets if there is no deployment created
    Mar 15 21:47:57.196: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-4658  2b81b228-2c3a-47f0-9965-47f9a93adaa2 4225 2 2023-03-15 21:47:49 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 3b0cd630-e7f0-4106-8cda-c8cc5bc15c3f 0xc005df1177 0xc005df1178}] [] [{kube-controller-manager Update apps/v1 2023-03-15 21:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b0cd630-e7f0-4106-8cda-c8cc5bc15c3f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 21:47:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005df1200 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Mar 15 21:47:57.200: INFO: pod: "test-deployment-7b7876f9d6-jkjsl":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-jkjsl test-deployment-7b7876f9d6- deployment-4658  bb3d65d3-437f-4208-b90f-788663fa81c8 4189 0 2023-03-15 21:47:49 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 2b81b228-2c3a-47f0-9965-47f9a93adaa2 0xc006250c37 0xc006250c38}] [] [{kube-controller-manager Update v1 2023-03-15 21:47:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b81b228-2c3a-47f0-9965-47f9a93adaa2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 21:47:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9q66w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9q66w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.177,StartTime:2023-03-15 21:47:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 21:47:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a7d83da873759ccdb41a0c28ce01bf88bd8a3fa2487dd4265b1a4a6018adfc27,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 15 21:47:57.200: INFO: pod: "test-deployment-7b7876f9d6-q5sp6":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-q5sp6 test-deployment-7b7876f9d6- deployment-4658  5266a875-bcd8-4e4c-a664-7a275106c48e 4224 0 2023-03-15 21:47:53 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 2b81b228-2c3a-47f0-9965-47f9a93adaa2 0xc006250e17 0xc006250e18}] [] [{kube-controller-manager Update v1 2023-03-15 21:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b81b228-2c3a-47f0-9965-47f9a93adaa2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 21:47:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5dgm8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5dgm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.192,StartTime:2023-03-15 21:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 21:47:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0267f3b7eceb389bfbb352795e8bff7221db101607ddf648d4a656905c990663,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 15 21:47:57.200: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-4658  44672b1a-f276-4a2b-898c-49106efc7ea8 4230 4 2023-03-15 21:47:46 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 3b0cd630-e7f0-4106-8cda-c8cc5bc15c3f 0xc005df1267 0xc005df1268}] [] [{kube-controller-manager Update apps/v1 2023-03-15 21:47:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b0cd630-e7f0-4106-8cda-c8cc5bc15c3f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 21:47:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005df12f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Mar 15 21:47:57.207: INFO: pod: "test-deployment-7df74c55ff-k6qfk":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-k6qfk test-deployment-7df74c55ff- deployment-4658  75e6e553-60ae-4f32-9d7f-69a6cf23e9c3 4228 0 2023-03-15 21:47:49 +0000 UTC 2023-03-15 21:47:58 +0000 UTC 0xc005df1650 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 44672b1a-f276-4a2b-898c-49106efc7ea8 0xc005df1687 0xc005df1688}] [] [{kube-controller-manager Update v1 2023-03-15 21:47:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44672b1a-f276-4a2b-898c-49106efc7ea8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 21:47:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xbqr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xbqr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 21:47:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.65,StartTime:2023-03-15 21:47:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 21:47:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://db8d4f0cf5f1af8903df9bb9bcfed1790c50e82b5c37785148bebf2eacf2a321,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 15 21:47:57.207: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-4658  f0c1b6b0-8683-493f-8501-4adb2fc7bade 4145 3 2023-03-15 21:47:44 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 3b0cd630-e7f0-4106-8cda-c8cc5bc15c3f 0xc005df1357 0xc005df1358}] [] [{kube-controller-manager Update apps/v1 2023-03-15 21:47:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b0cd630-e7f0-4106-8cda-c8cc5bc15c3f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 21:47:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005df13e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:47:57.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4658" for this suite. 03/15/23 21:47:57.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:47:57.235
Mar 15 21:47:57.235: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pods 03/15/23 21:47:57.236
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:47:57.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:47:57.274
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Mar 15 21:47:57.278: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: creating the pod 03/15/23 21:47:57.279
STEP: submitting the pod to kubernetes 03/15/23 21:47:57.279
Mar 15 21:47:57.291: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-cfa1202b-bf7a-4b46-beef-3498fb6e3085" in namespace "pods-6698" to be "running and ready"
Mar 15 21:47:57.297: INFO: Pod "pod-logs-websocket-cfa1202b-bf7a-4b46-beef-3498fb6e3085": Phase="Pending", Reason="", readiness=false. Elapsed: 5.845531ms
Mar 15 21:47:57.297: INFO: The phase of Pod pod-logs-websocket-cfa1202b-bf7a-4b46-beef-3498fb6e3085 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:47:59.301: INFO: Pod "pod-logs-websocket-cfa1202b-bf7a-4b46-beef-3498fb6e3085": Phase="Running", Reason="", readiness=true. Elapsed: 2.009538611s
Mar 15 21:47:59.301: INFO: The phase of Pod pod-logs-websocket-cfa1202b-bf7a-4b46-beef-3498fb6e3085 is Running (Ready = true)
Mar 15 21:47:59.301: INFO: Pod "pod-logs-websocket-cfa1202b-bf7a-4b46-beef-3498fb6e3085" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 15 21:47:59.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6698" for this suite. 03/15/23 21:47:59.32
------------------------------
• [2.090 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:47:57.235
    Mar 15 21:47:57.235: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pods 03/15/23 21:47:57.236
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:47:57.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:47:57.274
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Mar 15 21:47:57.278: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: creating the pod 03/15/23 21:47:57.279
    STEP: submitting the pod to kubernetes 03/15/23 21:47:57.279
    Mar 15 21:47:57.291: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-cfa1202b-bf7a-4b46-beef-3498fb6e3085" in namespace "pods-6698" to be "running and ready"
    Mar 15 21:47:57.297: INFO: Pod "pod-logs-websocket-cfa1202b-bf7a-4b46-beef-3498fb6e3085": Phase="Pending", Reason="", readiness=false. Elapsed: 5.845531ms
    Mar 15 21:47:57.297: INFO: The phase of Pod pod-logs-websocket-cfa1202b-bf7a-4b46-beef-3498fb6e3085 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:47:59.301: INFO: Pod "pod-logs-websocket-cfa1202b-bf7a-4b46-beef-3498fb6e3085": Phase="Running", Reason="", readiness=true. Elapsed: 2.009538611s
    Mar 15 21:47:59.301: INFO: The phase of Pod pod-logs-websocket-cfa1202b-bf7a-4b46-beef-3498fb6e3085 is Running (Ready = true)
    Mar 15 21:47:59.301: INFO: Pod "pod-logs-websocket-cfa1202b-bf7a-4b46-beef-3498fb6e3085" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:47:59.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6698" for this suite. 03/15/23 21:47:59.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:47:59.327
Mar 15 21:47:59.327: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 21:47:59.332
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:47:59.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:47:59.357
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/15/23 21:47:59.367
Mar 15 21:47:59.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3722 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Mar 15 21:47:59.546: INFO: stderr: ""
Mar 15 21:47:59.546: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 03/15/23 21:47:59.546
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Mar 15 21:47:59.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3722 delete pods e2e-test-httpd-pod'
Mar 15 21:48:02.160: INFO: stderr: ""
Mar 15 21:48:02.160: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 21:48:02.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3722" for this suite. 03/15/23 21:48:02.165
------------------------------
• [2.843 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:47:59.327
    Mar 15 21:47:59.327: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 21:47:59.332
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:47:59.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:47:59.357
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/15/23 21:47:59.367
    Mar 15 21:47:59.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3722 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Mar 15 21:47:59.546: INFO: stderr: ""
    Mar 15 21:47:59.546: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 03/15/23 21:47:59.546
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Mar 15 21:47:59.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3722 delete pods e2e-test-httpd-pod'
    Mar 15 21:48:02.160: INFO: stderr: ""
    Mar 15 21:48:02.160: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:48:02.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3722" for this suite. 03/15/23 21:48:02.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:48:02.171
Mar 15 21:48:02.171: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename namespaces 03/15/23 21:48:02.172
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:02.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:02.188
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-9969" 03/15/23 21:48:02.193
Mar 15 21:48:02.204: INFO: Namespace "namespaces-9969" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"5f02fcbf-8b49-43fa-a80d-391846325739", "kubernetes.io/metadata.name":"namespaces-9969", "namespaces-9969":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:48:02.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9969" for this suite. 03/15/23 21:48:02.209
------------------------------
• [0.054 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:48:02.171
    Mar 15 21:48:02.171: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename namespaces 03/15/23 21:48:02.172
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:02.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:02.188
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-9969" 03/15/23 21:48:02.193
    Mar 15 21:48:02.204: INFO: Namespace "namespaces-9969" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"5f02fcbf-8b49-43fa-a80d-391846325739", "kubernetes.io/metadata.name":"namespaces-9969", "namespaces-9969":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:48:02.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9969" for this suite. 03/15/23 21:48:02.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:48:02.227
Mar 15 21:48:02.227: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 21:48:02.228
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:02.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:02.274
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 21:48:02.29
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 21:48:03.108
STEP: Deploying the webhook pod 03/15/23 21:48:03.118
STEP: Wait for the deployment to be ready 03/15/23 21:48:03.13
Mar 15 21:48:03.139: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 21:48:05.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 21, 48, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 21, 48, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 21, 48, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 21, 48, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/15/23 21:48:07.153
STEP: Verifying the service has paired with the endpoint 03/15/23 21:48:07.169
Mar 15 21:48:08.169: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/15/23 21:48:08.173
STEP: create a configmap that should be updated by the webhook 03/15/23 21:48:08.188
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:48:08.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1741" for this suite. 03/15/23 21:48:08.271
STEP: Destroying namespace "webhook-1741-markers" for this suite. 03/15/23 21:48:08.28
------------------------------
• [SLOW TEST] [6.071 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:48:02.227
    Mar 15 21:48:02.227: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 21:48:02.228
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:02.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:02.274
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 21:48:02.29
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 21:48:03.108
    STEP: Deploying the webhook pod 03/15/23 21:48:03.118
    STEP: Wait for the deployment to be ready 03/15/23 21:48:03.13
    Mar 15 21:48:03.139: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 15 21:48:05.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 21, 48, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 21, 48, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 21, 48, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 21, 48, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/15/23 21:48:07.153
    STEP: Verifying the service has paired with the endpoint 03/15/23 21:48:07.169
    Mar 15 21:48:08.169: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/15/23 21:48:08.173
    STEP: create a configmap that should be updated by the webhook 03/15/23 21:48:08.188
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:48:08.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1741" for this suite. 03/15/23 21:48:08.271
    STEP: Destroying namespace "webhook-1741-markers" for this suite. 03/15/23 21:48:08.28
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:48:08.299
Mar 15 21:48:08.299: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename svcaccounts 03/15/23 21:48:08.3
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:08.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:08.327
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Mar 15 21:48:08.340: INFO: Got root ca configmap in namespace "svcaccounts-7054"
Mar 15 21:48:08.345: INFO: Deleted root ca configmap in namespace "svcaccounts-7054"
STEP: waiting for a new root ca configmap created 03/15/23 21:48:08.846
Mar 15 21:48:08.849: INFO: Recreated root ca configmap in namespace "svcaccounts-7054"
Mar 15 21:48:08.854: INFO: Updated root ca configmap in namespace "svcaccounts-7054"
STEP: waiting for the root ca configmap reconciled 03/15/23 21:48:09.355
Mar 15 21:48:09.364: INFO: Reconciled root ca configmap in namespace "svcaccounts-7054"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 15 21:48:09.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7054" for this suite. 03/15/23 21:48:09.374
------------------------------
• [1.082 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:48:08.299
    Mar 15 21:48:08.299: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename svcaccounts 03/15/23 21:48:08.3
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:08.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:08.327
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Mar 15 21:48:08.340: INFO: Got root ca configmap in namespace "svcaccounts-7054"
    Mar 15 21:48:08.345: INFO: Deleted root ca configmap in namespace "svcaccounts-7054"
    STEP: waiting for a new root ca configmap created 03/15/23 21:48:08.846
    Mar 15 21:48:08.849: INFO: Recreated root ca configmap in namespace "svcaccounts-7054"
    Mar 15 21:48:08.854: INFO: Updated root ca configmap in namespace "svcaccounts-7054"
    STEP: waiting for the root ca configmap reconciled 03/15/23 21:48:09.355
    Mar 15 21:48:09.364: INFO: Reconciled root ca configmap in namespace "svcaccounts-7054"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:48:09.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7054" for this suite. 03/15/23 21:48:09.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:48:09.39
Mar 15 21:48:09.390: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename containers 03/15/23 21:48:09.391
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:09.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:09.414
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 03/15/23 21:48:09.417
Mar 15 21:48:09.426: INFO: Waiting up to 5m0s for pod "client-containers-688f1b1a-fc78-4070-849c-06c838f23b75" in namespace "containers-2310" to be "Succeeded or Failed"
Mar 15 21:48:09.433: INFO: Pod "client-containers-688f1b1a-fc78-4070-849c-06c838f23b75": Phase="Pending", Reason="", readiness=false. Elapsed: 6.333112ms
Mar 15 21:48:11.440: INFO: Pod "client-containers-688f1b1a-fc78-4070-849c-06c838f23b75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013807006s
Mar 15 21:48:13.445: INFO: Pod "client-containers-688f1b1a-fc78-4070-849c-06c838f23b75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018174814s
STEP: Saw pod success 03/15/23 21:48:13.445
Mar 15 21:48:13.445: INFO: Pod "client-containers-688f1b1a-fc78-4070-849c-06c838f23b75" satisfied condition "Succeeded or Failed"
Mar 15 21:48:13.456: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod client-containers-688f1b1a-fc78-4070-849c-06c838f23b75 container agnhost-container: <nil>
STEP: delete the pod 03/15/23 21:48:13.473
Mar 15 21:48:13.516: INFO: Waiting for pod client-containers-688f1b1a-fc78-4070-849c-06c838f23b75 to disappear
Mar 15 21:48:13.527: INFO: Pod client-containers-688f1b1a-fc78-4070-849c-06c838f23b75 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 15 21:48:13.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2310" for this suite. 03/15/23 21:48:13.532
------------------------------
• [4.152 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:48:09.39
    Mar 15 21:48:09.390: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename containers 03/15/23 21:48:09.391
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:09.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:09.414
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 03/15/23 21:48:09.417
    Mar 15 21:48:09.426: INFO: Waiting up to 5m0s for pod "client-containers-688f1b1a-fc78-4070-849c-06c838f23b75" in namespace "containers-2310" to be "Succeeded or Failed"
    Mar 15 21:48:09.433: INFO: Pod "client-containers-688f1b1a-fc78-4070-849c-06c838f23b75": Phase="Pending", Reason="", readiness=false. Elapsed: 6.333112ms
    Mar 15 21:48:11.440: INFO: Pod "client-containers-688f1b1a-fc78-4070-849c-06c838f23b75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013807006s
    Mar 15 21:48:13.445: INFO: Pod "client-containers-688f1b1a-fc78-4070-849c-06c838f23b75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018174814s
    STEP: Saw pod success 03/15/23 21:48:13.445
    Mar 15 21:48:13.445: INFO: Pod "client-containers-688f1b1a-fc78-4070-849c-06c838f23b75" satisfied condition "Succeeded or Failed"
    Mar 15 21:48:13.456: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod client-containers-688f1b1a-fc78-4070-849c-06c838f23b75 container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 21:48:13.473
    Mar 15 21:48:13.516: INFO: Waiting for pod client-containers-688f1b1a-fc78-4070-849c-06c838f23b75 to disappear
    Mar 15 21:48:13.527: INFO: Pod client-containers-688f1b1a-fc78-4070-849c-06c838f23b75 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:48:13.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2310" for this suite. 03/15/23 21:48:13.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:48:13.543
Mar 15 21:48:13.543: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 21:48:13.546
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:13.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:13.565
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 03/15/23 21:48:13.57
Mar 15 21:48:13.579: INFO: Waiting up to 5m0s for pod "downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00" in namespace "downward-api-6937" to be "Succeeded or Failed"
Mar 15 21:48:13.586: INFO: Pod "downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00": Phase="Pending", Reason="", readiness=false. Elapsed: 7.546999ms
Mar 15 21:48:15.590: INFO: Pod "downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011419558s
Mar 15 21:48:17.590: INFO: Pod "downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01158878s
STEP: Saw pod success 03/15/23 21:48:17.59
Mar 15 21:48:17.590: INFO: Pod "downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00" satisfied condition "Succeeded or Failed"
Mar 15 21:48:17.599: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00 container dapi-container: <nil>
STEP: delete the pod 03/15/23 21:48:17.612
Mar 15 21:48:17.628: INFO: Waiting for pod downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00 to disappear
Mar 15 21:48:17.631: INFO: Pod downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 15 21:48:17.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6937" for this suite. 03/15/23 21:48:17.635
------------------------------
• [4.099 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:48:13.543
    Mar 15 21:48:13.543: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 21:48:13.546
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:13.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:13.565
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 03/15/23 21:48:13.57
    Mar 15 21:48:13.579: INFO: Waiting up to 5m0s for pod "downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00" in namespace "downward-api-6937" to be "Succeeded or Failed"
    Mar 15 21:48:13.586: INFO: Pod "downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00": Phase="Pending", Reason="", readiness=false. Elapsed: 7.546999ms
    Mar 15 21:48:15.590: INFO: Pod "downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011419558s
    Mar 15 21:48:17.590: INFO: Pod "downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01158878s
    STEP: Saw pod success 03/15/23 21:48:17.59
    Mar 15 21:48:17.590: INFO: Pod "downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00" satisfied condition "Succeeded or Failed"
    Mar 15 21:48:17.599: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00 container dapi-container: <nil>
    STEP: delete the pod 03/15/23 21:48:17.612
    Mar 15 21:48:17.628: INFO: Waiting for pod downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00 to disappear
    Mar 15 21:48:17.631: INFO: Pod downward-api-5b78f10e-6c3e-443b-b399-b7101ab45b00 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:48:17.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6937" for this suite. 03/15/23 21:48:17.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:48:17.646
Mar 15 21:48:17.646: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename subpath 03/15/23 21:48:17.647
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:17.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:17.667
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/15/23 21:48:17.67
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-5sf2 03/15/23 21:48:17.68
STEP: Creating a pod to test atomic-volume-subpath 03/15/23 21:48:17.68
Mar 15 21:48:17.693: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5sf2" in namespace "subpath-226" to be "Succeeded or Failed"
Mar 15 21:48:17.699: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.303285ms
Mar 15 21:48:19.703: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010230278s
Mar 15 21:48:21.705: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011886597s
Mar 15 21:48:23.704: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 6.010618081s
Mar 15 21:48:25.703: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 8.00967468s
Mar 15 21:48:27.702: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 10.009356475s
Mar 15 21:48:29.706: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 12.013560669s
Mar 15 21:48:31.704: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 14.010587617s
Mar 15 21:48:33.703: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 16.010374097s
Mar 15 21:48:35.703: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 18.010116286s
Mar 15 21:48:37.706: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 20.01280146s
Mar 15 21:48:39.703: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=false. Elapsed: 22.010066598s
Mar 15 21:48:41.703: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009735057s
STEP: Saw pod success 03/15/23 21:48:41.703
Mar 15 21:48:41.703: INFO: Pod "pod-subpath-test-projected-5sf2" satisfied condition "Succeeded or Failed"
Mar 15 21:48:41.706: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-subpath-test-projected-5sf2 container test-container-subpath-projected-5sf2: <nil>
STEP: delete the pod 03/15/23 21:48:41.713
Mar 15 21:48:41.723: INFO: Waiting for pod pod-subpath-test-projected-5sf2 to disappear
Mar 15 21:48:41.727: INFO: Pod pod-subpath-test-projected-5sf2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-5sf2 03/15/23 21:48:41.727
Mar 15 21:48:41.727: INFO: Deleting pod "pod-subpath-test-projected-5sf2" in namespace "subpath-226"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 15 21:48:41.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-226" for this suite. 03/15/23 21:48:41.734
------------------------------
• [SLOW TEST] [24.096 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:48:17.646
    Mar 15 21:48:17.646: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename subpath 03/15/23 21:48:17.647
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:17.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:17.667
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/15/23 21:48:17.67
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-5sf2 03/15/23 21:48:17.68
    STEP: Creating a pod to test atomic-volume-subpath 03/15/23 21:48:17.68
    Mar 15 21:48:17.693: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5sf2" in namespace "subpath-226" to be "Succeeded or Failed"
    Mar 15 21:48:17.699: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.303285ms
    Mar 15 21:48:19.703: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010230278s
    Mar 15 21:48:21.705: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011886597s
    Mar 15 21:48:23.704: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 6.010618081s
    Mar 15 21:48:25.703: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 8.00967468s
    Mar 15 21:48:27.702: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 10.009356475s
    Mar 15 21:48:29.706: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 12.013560669s
    Mar 15 21:48:31.704: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 14.010587617s
    Mar 15 21:48:33.703: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 16.010374097s
    Mar 15 21:48:35.703: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 18.010116286s
    Mar 15 21:48:37.706: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=true. Elapsed: 20.01280146s
    Mar 15 21:48:39.703: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Running", Reason="", readiness=false. Elapsed: 22.010066598s
    Mar 15 21:48:41.703: INFO: Pod "pod-subpath-test-projected-5sf2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009735057s
    STEP: Saw pod success 03/15/23 21:48:41.703
    Mar 15 21:48:41.703: INFO: Pod "pod-subpath-test-projected-5sf2" satisfied condition "Succeeded or Failed"
    Mar 15 21:48:41.706: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-subpath-test-projected-5sf2 container test-container-subpath-projected-5sf2: <nil>
    STEP: delete the pod 03/15/23 21:48:41.713
    Mar 15 21:48:41.723: INFO: Waiting for pod pod-subpath-test-projected-5sf2 to disappear
    Mar 15 21:48:41.727: INFO: Pod pod-subpath-test-projected-5sf2 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-5sf2 03/15/23 21:48:41.727
    Mar 15 21:48:41.727: INFO: Deleting pod "pod-subpath-test-projected-5sf2" in namespace "subpath-226"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:48:41.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-226" for this suite. 03/15/23 21:48:41.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:48:41.743
Mar 15 21:48:41.743: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 21:48:41.745
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:41.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:41.772
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 03/15/23 21:48:41.776
Mar 15 21:48:41.785: INFO: Waiting up to 5m0s for pod "annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98" in namespace "downward-api-4562" to be "running and ready"
Mar 15 21:48:41.788: INFO: Pod "annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.922057ms
Mar 15 21:48:41.788: INFO: The phase of Pod annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:48:43.791: INFO: Pod "annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98": Phase="Running", Reason="", readiness=true. Elapsed: 2.00596409s
Mar 15 21:48:43.791: INFO: The phase of Pod annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98 is Running (Ready = true)
Mar 15 21:48:43.791: INFO: Pod "annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98" satisfied condition "running and ready"
Mar 15 21:48:44.312: INFO: Successfully updated pod "annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 15 21:48:46.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4562" for this suite. 03/15/23 21:48:46.336
------------------------------
• [4.598 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:48:41.743
    Mar 15 21:48:41.743: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 21:48:41.745
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:41.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:41.772
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 03/15/23 21:48:41.776
    Mar 15 21:48:41.785: INFO: Waiting up to 5m0s for pod "annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98" in namespace "downward-api-4562" to be "running and ready"
    Mar 15 21:48:41.788: INFO: Pod "annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.922057ms
    Mar 15 21:48:41.788: INFO: The phase of Pod annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:48:43.791: INFO: Pod "annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98": Phase="Running", Reason="", readiness=true. Elapsed: 2.00596409s
    Mar 15 21:48:43.791: INFO: The phase of Pod annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98 is Running (Ready = true)
    Mar 15 21:48:43.791: INFO: Pod "annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98" satisfied condition "running and ready"
    Mar 15 21:48:44.312: INFO: Successfully updated pod "annotationupdate5bed7c55-684b-4f74-8324-b6b863a53e98"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:48:46.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4562" for this suite. 03/15/23 21:48:46.336
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:48:46.342
Mar 15 21:48:46.342: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename svcaccounts 03/15/23 21:48:46.342
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:46.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:46.358
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Mar 15 21:48:46.377: INFO: created pod pod-service-account-defaultsa
Mar 15 21:48:46.377: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 15 21:48:46.383: INFO: created pod pod-service-account-mountsa
Mar 15 21:48:46.383: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 15 21:48:46.396: INFO: created pod pod-service-account-nomountsa
Mar 15 21:48:46.396: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 15 21:48:46.403: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 15 21:48:46.403: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 15 21:48:46.416: INFO: created pod pod-service-account-mountsa-mountspec
Mar 15 21:48:46.416: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 15 21:48:46.445: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 15 21:48:46.445: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 15 21:48:46.457: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 15 21:48:46.457: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 15 21:48:46.469: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 15 21:48:46.469: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 15 21:48:46.486: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 15 21:48:46.486: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 15 21:48:46.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8458" for this suite. 03/15/23 21:48:46.496
------------------------------
• [0.162 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:48:46.342
    Mar 15 21:48:46.342: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename svcaccounts 03/15/23 21:48:46.342
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:46.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:46.358
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Mar 15 21:48:46.377: INFO: created pod pod-service-account-defaultsa
    Mar 15 21:48:46.377: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Mar 15 21:48:46.383: INFO: created pod pod-service-account-mountsa
    Mar 15 21:48:46.383: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Mar 15 21:48:46.396: INFO: created pod pod-service-account-nomountsa
    Mar 15 21:48:46.396: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Mar 15 21:48:46.403: INFO: created pod pod-service-account-defaultsa-mountspec
    Mar 15 21:48:46.403: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Mar 15 21:48:46.416: INFO: created pod pod-service-account-mountsa-mountspec
    Mar 15 21:48:46.416: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Mar 15 21:48:46.445: INFO: created pod pod-service-account-nomountsa-mountspec
    Mar 15 21:48:46.445: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Mar 15 21:48:46.457: INFO: created pod pod-service-account-defaultsa-nomountspec
    Mar 15 21:48:46.457: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Mar 15 21:48:46.469: INFO: created pod pod-service-account-mountsa-nomountspec
    Mar 15 21:48:46.469: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Mar 15 21:48:46.486: INFO: created pod pod-service-account-nomountsa-nomountspec
    Mar 15 21:48:46.486: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:48:46.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8458" for this suite. 03/15/23 21:48:46.496
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:48:46.504
Mar 15 21:48:46.504: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename resourcequota 03/15/23 21:48:46.505
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:46.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:46.544
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 03/15/23 21:48:46.55
STEP: Getting a ResourceQuota 03/15/23 21:48:46.557
STEP: Listing all ResourceQuotas with LabelSelector 03/15/23 21:48:46.56
STEP: Patching the ResourceQuota 03/15/23 21:48:46.569
STEP: Deleting a Collection of ResourceQuotas 03/15/23 21:48:46.586
STEP: Verifying the deleted ResourceQuota 03/15/23 21:48:46.605
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 15 21:48:46.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4566" for this suite. 03/15/23 21:48:46.618
------------------------------
• [0.126 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:48:46.504
    Mar 15 21:48:46.504: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename resourcequota 03/15/23 21:48:46.505
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:46.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:46.544
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 03/15/23 21:48:46.55
    STEP: Getting a ResourceQuota 03/15/23 21:48:46.557
    STEP: Listing all ResourceQuotas with LabelSelector 03/15/23 21:48:46.56
    STEP: Patching the ResourceQuota 03/15/23 21:48:46.569
    STEP: Deleting a Collection of ResourceQuotas 03/15/23 21:48:46.586
    STEP: Verifying the deleted ResourceQuota 03/15/23 21:48:46.605
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:48:46.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4566" for this suite. 03/15/23 21:48:46.618
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:48:46.663
Mar 15 21:48:46.663: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 21:48:46.671
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:46.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:46.717
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 21:48:46.735
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 21:48:47.428
STEP: Deploying the webhook pod 03/15/23 21:48:47.435
STEP: Wait for the deployment to be ready 03/15/23 21:48:47.448
Mar 15 21:48:47.462: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 21:48:49.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 21, 48, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 21, 48, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 21, 48, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 21, 48, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/15/23 21:48:51.487
STEP: Verifying the service has paired with the endpoint 03/15/23 21:48:51.504
Mar 15 21:48:52.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 03/15/23 21:48:52.572
STEP: Creating a configMap that should be mutated 03/15/23 21:48:52.584
STEP: Deleting the collection of validation webhooks 03/15/23 21:48:52.612
STEP: Creating a configMap that should not be mutated 03/15/23 21:48:52.643
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:48:52.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7698" for this suite. 03/15/23 21:48:52.712
STEP: Destroying namespace "webhook-7698-markers" for this suite. 03/15/23 21:48:52.727
------------------------------
• [SLOW TEST] [6.078 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:48:46.663
    Mar 15 21:48:46.663: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 21:48:46.671
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:46.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:46.717
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 21:48:46.735
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 21:48:47.428
    STEP: Deploying the webhook pod 03/15/23 21:48:47.435
    STEP: Wait for the deployment to be ready 03/15/23 21:48:47.448
    Mar 15 21:48:47.462: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 15 21:48:49.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 21, 48, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 21, 48, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 21, 48, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 21, 48, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/15/23 21:48:51.487
    STEP: Verifying the service has paired with the endpoint 03/15/23 21:48:51.504
    Mar 15 21:48:52.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 03/15/23 21:48:52.572
    STEP: Creating a configMap that should be mutated 03/15/23 21:48:52.584
    STEP: Deleting the collection of validation webhooks 03/15/23 21:48:52.612
    STEP: Creating a configMap that should not be mutated 03/15/23 21:48:52.643
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:48:52.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7698" for this suite. 03/15/23 21:48:52.712
    STEP: Destroying namespace "webhook-7698-markers" for this suite. 03/15/23 21:48:52.727
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:48:52.742
Mar 15 21:48:52.742: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/15/23 21:48:52.744
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:52.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:52.765
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 03/15/23 21:48:52.768
STEP: Creating hostNetwork=false pod 03/15/23 21:48:52.768
Mar 15 21:48:52.776: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7757" to be "running and ready"
Mar 15 21:48:52.779: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.494005ms
Mar 15 21:48:52.779: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:48:54.786: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009977741s
Mar 15 21:48:54.786: INFO: The phase of Pod test-pod is Running (Ready = true)
Mar 15 21:48:54.786: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 03/15/23 21:48:54.789
Mar 15 21:48:54.795: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7757" to be "running and ready"
Mar 15 21:48:54.801: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.446056ms
Mar 15 21:48:54.801: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:48:56.804: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008782485s
Mar 15 21:48:56.804: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Mar 15 21:48:56.804: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 03/15/23 21:48:56.807
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/15/23 21:48:56.807
Mar 15 21:48:56.808: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:48:56.808: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:48:56.809: INFO: ExecWithOptions: Clientset creation
Mar 15 21:48:56.809: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 15 21:48:56.961: INFO: Exec stderr: ""
Mar 15 21:48:56.961: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:48:56.961: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:48:56.962: INFO: ExecWithOptions: Clientset creation
Mar 15 21:48:56.962: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 15 21:48:57.071: INFO: Exec stderr: ""
Mar 15 21:48:57.071: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:48:57.071: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:48:57.071: INFO: ExecWithOptions: Clientset creation
Mar 15 21:48:57.071: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 15 21:48:57.160: INFO: Exec stderr: ""
Mar 15 21:48:57.160: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:48:57.160: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:48:57.161: INFO: ExecWithOptions: Clientset creation
Mar 15 21:48:57.161: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 15 21:48:57.221: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/15/23 21:48:57.221
Mar 15 21:48:57.221: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:48:57.221: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:48:57.222: INFO: ExecWithOptions: Clientset creation
Mar 15 21:48:57.222: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 15 21:48:57.282: INFO: Exec stderr: ""
Mar 15 21:48:57.282: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:48:57.282: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:48:57.283: INFO: ExecWithOptions: Clientset creation
Mar 15 21:48:57.283: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 15 21:48:57.373: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/15/23 21:48:57.373
Mar 15 21:48:57.373: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:48:57.373: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:48:57.374: INFO: ExecWithOptions: Clientset creation
Mar 15 21:48:57.374: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 15 21:48:57.450: INFO: Exec stderr: ""
Mar 15 21:48:57.451: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:48:57.451: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:48:57.451: INFO: ExecWithOptions: Clientset creation
Mar 15 21:48:57.451: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 15 21:48:57.598: INFO: Exec stderr: ""
Mar 15 21:48:57.598: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:48:57.598: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:48:57.599: INFO: ExecWithOptions: Clientset creation
Mar 15 21:48:57.599: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 15 21:48:57.681: INFO: Exec stderr: ""
Mar 15 21:48:57.681: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:48:57.681: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:48:57.682: INFO: ExecWithOptions: Clientset creation
Mar 15 21:48:57.682: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 15 21:48:57.767: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Mar 15 21:48:57.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7757" for this suite. 03/15/23 21:48:57.772
------------------------------
• [SLOW TEST] [5.040 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:48:52.742
    Mar 15 21:48:52.742: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/15/23 21:48:52.744
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:52.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:52.765
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 03/15/23 21:48:52.768
    STEP: Creating hostNetwork=false pod 03/15/23 21:48:52.768
    Mar 15 21:48:52.776: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7757" to be "running and ready"
    Mar 15 21:48:52.779: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.494005ms
    Mar 15 21:48:52.779: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:48:54.786: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009977741s
    Mar 15 21:48:54.786: INFO: The phase of Pod test-pod is Running (Ready = true)
    Mar 15 21:48:54.786: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 03/15/23 21:48:54.789
    Mar 15 21:48:54.795: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7757" to be "running and ready"
    Mar 15 21:48:54.801: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.446056ms
    Mar 15 21:48:54.801: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:48:56.804: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008782485s
    Mar 15 21:48:56.804: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Mar 15 21:48:56.804: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 03/15/23 21:48:56.807
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/15/23 21:48:56.807
    Mar 15 21:48:56.808: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:48:56.808: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:48:56.809: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:48:56.809: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 15 21:48:56.961: INFO: Exec stderr: ""
    Mar 15 21:48:56.961: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:48:56.961: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:48:56.962: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:48:56.962: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 15 21:48:57.071: INFO: Exec stderr: ""
    Mar 15 21:48:57.071: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:48:57.071: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:48:57.071: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:48:57.071: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 15 21:48:57.160: INFO: Exec stderr: ""
    Mar 15 21:48:57.160: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:48:57.160: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:48:57.161: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:48:57.161: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 15 21:48:57.221: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/15/23 21:48:57.221
    Mar 15 21:48:57.221: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:48:57.221: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:48:57.222: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:48:57.222: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 15 21:48:57.282: INFO: Exec stderr: ""
    Mar 15 21:48:57.282: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:48:57.282: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:48:57.283: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:48:57.283: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 15 21:48:57.373: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/15/23 21:48:57.373
    Mar 15 21:48:57.373: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:48:57.373: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:48:57.374: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:48:57.374: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 15 21:48:57.450: INFO: Exec stderr: ""
    Mar 15 21:48:57.451: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:48:57.451: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:48:57.451: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:48:57.451: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 15 21:48:57.598: INFO: Exec stderr: ""
    Mar 15 21:48:57.598: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:48:57.598: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:48:57.599: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:48:57.599: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 15 21:48:57.681: INFO: Exec stderr: ""
    Mar 15 21:48:57.681: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7757 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:48:57.681: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:48:57.682: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:48:57.682: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7757/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 15 21:48:57.767: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:48:57.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-7757" for this suite. 03/15/23 21:48:57.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:48:57.783
Mar 15 21:48:57.783: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename statefulset 03/15/23 21:48:57.784
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:57.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:57.801
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4302 03/15/23 21:48:57.806
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Mar 15 21:48:57.828: INFO: Found 0 stateful pods, waiting for 1
Mar 15 21:49:07.832: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 03/15/23 21:49:07.839
W0315 21:49:07.851156      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 15 21:49:07.864: INFO: Found 1 stateful pods, waiting for 2
Mar 15 21:49:17.871: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 21:49:17.871: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 03/15/23 21:49:17.877
STEP: Delete all of the StatefulSets 03/15/23 21:49:17.88
STEP: Verify that StatefulSets have been deleted 03/15/23 21:49:17.886
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 15 21:49:17.890: INFO: Deleting all statefulset in ns statefulset-4302
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 15 21:49:17.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4302" for this suite. 03/15/23 21:49:17.93
------------------------------
• [SLOW TEST] [20.164 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:48:57.783
    Mar 15 21:48:57.783: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename statefulset 03/15/23 21:48:57.784
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:48:57.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:48:57.801
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4302 03/15/23 21:48:57.806
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Mar 15 21:48:57.828: INFO: Found 0 stateful pods, waiting for 1
    Mar 15 21:49:07.832: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 03/15/23 21:49:07.839
    W0315 21:49:07.851156      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 15 21:49:07.864: INFO: Found 1 stateful pods, waiting for 2
    Mar 15 21:49:17.871: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 15 21:49:17.871: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 03/15/23 21:49:17.877
    STEP: Delete all of the StatefulSets 03/15/23 21:49:17.88
    STEP: Verify that StatefulSets have been deleted 03/15/23 21:49:17.886
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 15 21:49:17.890: INFO: Deleting all statefulset in ns statefulset-4302
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:49:17.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4302" for this suite. 03/15/23 21:49:17.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:49:17.961
Mar 15 21:49:17.961: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename resourcequota 03/15/23 21:49:17.962
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:49:17.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:49:17.998
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 03/15/23 21:49:18.007
STEP: Ensuring ResourceQuota status is calculated 03/15/23 21:49:18.014
STEP: Creating a ResourceQuota with not best effort scope 03/15/23 21:49:20.017
STEP: Ensuring ResourceQuota status is calculated 03/15/23 21:49:20.023
STEP: Creating a best-effort pod 03/15/23 21:49:22.026
STEP: Ensuring resource quota with best effort scope captures the pod usage 03/15/23 21:49:22.038
STEP: Ensuring resource quota with not best effort ignored the pod usage 03/15/23 21:49:24.042
STEP: Deleting the pod 03/15/23 21:49:26.046
STEP: Ensuring resource quota status released the pod usage 03/15/23 21:49:26.064
STEP: Creating a not best-effort pod 03/15/23 21:49:28.072
STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/15/23 21:49:28.084
STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/15/23 21:49:30.088
STEP: Deleting the pod 03/15/23 21:49:32.092
STEP: Ensuring resource quota status released the pod usage 03/15/23 21:49:32.109
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 15 21:49:34.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1256" for this suite. 03/15/23 21:49:34.117
------------------------------
• [SLOW TEST] [16.161 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:49:17.961
    Mar 15 21:49:17.961: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename resourcequota 03/15/23 21:49:17.962
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:49:17.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:49:17.998
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 03/15/23 21:49:18.007
    STEP: Ensuring ResourceQuota status is calculated 03/15/23 21:49:18.014
    STEP: Creating a ResourceQuota with not best effort scope 03/15/23 21:49:20.017
    STEP: Ensuring ResourceQuota status is calculated 03/15/23 21:49:20.023
    STEP: Creating a best-effort pod 03/15/23 21:49:22.026
    STEP: Ensuring resource quota with best effort scope captures the pod usage 03/15/23 21:49:22.038
    STEP: Ensuring resource quota with not best effort ignored the pod usage 03/15/23 21:49:24.042
    STEP: Deleting the pod 03/15/23 21:49:26.046
    STEP: Ensuring resource quota status released the pod usage 03/15/23 21:49:26.064
    STEP: Creating a not best-effort pod 03/15/23 21:49:28.072
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/15/23 21:49:28.084
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/15/23 21:49:30.088
    STEP: Deleting the pod 03/15/23 21:49:32.092
    STEP: Ensuring resource quota status released the pod usage 03/15/23 21:49:32.109
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:49:34.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1256" for this suite. 03/15/23 21:49:34.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:49:34.122
Mar 15 21:49:34.122: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename daemonsets 03/15/23 21:49:34.123
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:49:34.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:49:34.147
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 03/15/23 21:49:34.166
STEP: Check that daemon pods launch on every node of the cluster. 03/15/23 21:49:34.174
Mar 15 21:49:34.180: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 21:49:34.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 21:49:34.184: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 21:49:35.188: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 21:49:35.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 21:49:35.191: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 21:49:36.189: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 21:49:36.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 15 21:49:36.197: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
Mar 15 21:49:37.189: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 21:49:37.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 15 21:49:37.192: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
Mar 15 21:49:38.188: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 21:49:38.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 15 21:49:38.191: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
Mar 15 21:49:39.192: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 21:49:39.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 15 21:49:39.196: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 03/15/23 21:49:39.198
STEP: DeleteCollection of the DaemonSets 03/15/23 21:49:39.201
STEP: Verify that ReplicaSets have been deleted 03/15/23 21:49:39.207
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Mar 15 21:49:39.230: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5111"},"items":null}

Mar 15 21:49:39.245: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5114"},"items":[{"metadata":{"name":"daemon-set-5shpg","generateName":"daemon-set-","namespace":"daemonsets-5702","uid":"72babf5d-eeb5-41fd-a064-47f5f9e50f19","resourceVersion":"5113","creationTimestamp":"2023-03-15T21:49:34Z","deletionTimestamp":"2023-03-15T21:50:09Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"687a1563-54ce-4b32-bea0-d214b556181a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-15T21:49:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"687a1563-54ce-4b32-bea0-d214b556181a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-15T21:49:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tnd24","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tnd24","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"i-077ee0eb7ec5a02aa","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["i-077ee0eb7ec5a02aa"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:34Z"}],"hostIP":"172.20.75.105","podIP":"100.96.2.51","podIPs":[{"ip":"100.96.2.51"}],"startTime":"2023-03-15T21:49:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-15T21:49:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b4a73a0a6860eb33952efc3fd52fbaecb48a043bd862f38c1f2f0e9c62ed1aaf","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kvrmf","generateName":"daemon-set-","namespace":"daemonsets-5702","uid":"851e7a9e-9351-4e29-9068-6f33e8a76138","resourceVersion":"5111","creationTimestamp":"2023-03-15T21:49:34Z","deletionTimestamp":"2023-03-15T21:50:09Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"687a1563-54ce-4b32-bea0-d214b556181a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-15T21:49:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"687a1563-54ce-4b32-bea0-d214b556181a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-15T21:49:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-w7xqk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-w7xqk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"i-0faaf83f00b43c88c","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["i-0faaf83f00b43c88c"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:34Z"}],"hostIP":"172.20.126.23","podIP":"100.96.3.163","podIPs":[{"ip":"100.96.3.163"}],"startTime":"2023-03-15T21:49:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-15T21:49:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://05a67b52aa289d25aa91beecdd2caf362cb3603cf598e719dc32c956bca72895","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rxrhd","generateName":"daemon-set-","namespace":"daemonsets-5702","uid":"c2a17676-0ad3-4120-a352-d05e18c12dd9","resourceVersion":"5112","creationTimestamp":"2023-03-15T21:49:34Z","deletionTimestamp":"2023-03-15T21:50:09Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"687a1563-54ce-4b32-bea0-d214b556181a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-15T21:49:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"687a1563-54ce-4b32-bea0-d214b556181a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-15T21:49:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nd9hk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nd9hk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"i-0baafb3f4e7bf826e","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["i-0baafb3f4e7bf826e"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:34Z"}],"hostIP":"172.20.60.208","podIP":"100.96.1.126","podIPs":[{"ip":"100.96.1.126"}],"startTime":"2023-03-15T21:49:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-15T21:49:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://017f62f25eb152285d793d95de3d2254c52e5aeee0908544a5234b7f8b5f3745","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:49:39.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5702" for this suite. 03/15/23 21:49:39.279
------------------------------
• [SLOW TEST] [5.168 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:49:34.122
    Mar 15 21:49:34.122: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename daemonsets 03/15/23 21:49:34.123
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:49:34.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:49:34.147
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 03/15/23 21:49:34.166
    STEP: Check that daemon pods launch on every node of the cluster. 03/15/23 21:49:34.174
    Mar 15 21:49:34.180: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 21:49:34.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 21:49:34.184: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 21:49:35.188: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 21:49:35.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 21:49:35.191: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 21:49:36.189: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 21:49:36.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 15 21:49:36.197: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
    Mar 15 21:49:37.189: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 21:49:37.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 15 21:49:37.192: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
    Mar 15 21:49:38.188: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 21:49:38.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 15 21:49:38.191: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
    Mar 15 21:49:39.192: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 21:49:39.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 15 21:49:39.196: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 03/15/23 21:49:39.198
    STEP: DeleteCollection of the DaemonSets 03/15/23 21:49:39.201
    STEP: Verify that ReplicaSets have been deleted 03/15/23 21:49:39.207
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Mar 15 21:49:39.230: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5111"},"items":null}

    Mar 15 21:49:39.245: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5114"},"items":[{"metadata":{"name":"daemon-set-5shpg","generateName":"daemon-set-","namespace":"daemonsets-5702","uid":"72babf5d-eeb5-41fd-a064-47f5f9e50f19","resourceVersion":"5113","creationTimestamp":"2023-03-15T21:49:34Z","deletionTimestamp":"2023-03-15T21:50:09Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"687a1563-54ce-4b32-bea0-d214b556181a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-15T21:49:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"687a1563-54ce-4b32-bea0-d214b556181a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-15T21:49:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tnd24","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tnd24","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"i-077ee0eb7ec5a02aa","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["i-077ee0eb7ec5a02aa"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:34Z"}],"hostIP":"172.20.75.105","podIP":"100.96.2.51","podIPs":[{"ip":"100.96.2.51"}],"startTime":"2023-03-15T21:49:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-15T21:49:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b4a73a0a6860eb33952efc3fd52fbaecb48a043bd862f38c1f2f0e9c62ed1aaf","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kvrmf","generateName":"daemon-set-","namespace":"daemonsets-5702","uid":"851e7a9e-9351-4e29-9068-6f33e8a76138","resourceVersion":"5111","creationTimestamp":"2023-03-15T21:49:34Z","deletionTimestamp":"2023-03-15T21:50:09Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"687a1563-54ce-4b32-bea0-d214b556181a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-15T21:49:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"687a1563-54ce-4b32-bea0-d214b556181a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-15T21:49:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-w7xqk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-w7xqk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"i-0faaf83f00b43c88c","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["i-0faaf83f00b43c88c"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:34Z"}],"hostIP":"172.20.126.23","podIP":"100.96.3.163","podIPs":[{"ip":"100.96.3.163"}],"startTime":"2023-03-15T21:49:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-15T21:49:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://05a67b52aa289d25aa91beecdd2caf362cb3603cf598e719dc32c956bca72895","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rxrhd","generateName":"daemon-set-","namespace":"daemonsets-5702","uid":"c2a17676-0ad3-4120-a352-d05e18c12dd9","resourceVersion":"5112","creationTimestamp":"2023-03-15T21:49:34Z","deletionTimestamp":"2023-03-15T21:50:09Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"687a1563-54ce-4b32-bea0-d214b556181a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-15T21:49:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"687a1563-54ce-4b32-bea0-d214b556181a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-15T21:49:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nd9hk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nd9hk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"i-0baafb3f4e7bf826e","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["i-0baafb3f4e7bf826e"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-15T21:49:34Z"}],"hostIP":"172.20.60.208","podIP":"100.96.1.126","podIPs":[{"ip":"100.96.1.126"}],"startTime":"2023-03-15T21:49:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-15T21:49:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://017f62f25eb152285d793d95de3d2254c52e5aeee0908544a5234b7f8b5f3745","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:49:39.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5702" for this suite. 03/15/23 21:49:39.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:49:39.299
Mar 15 21:49:39.299: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename statefulset 03/15/23 21:49:39.3
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:49:39.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:49:39.328
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8123 03/15/23 21:49:39.338
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 03/15/23 21:49:39.346
STEP: Creating pod with conflicting port in namespace statefulset-8123 03/15/23 21:49:39.35
STEP: Waiting until pod test-pod will start running in namespace statefulset-8123 03/15/23 21:49:39.359
Mar 15 21:49:39.359: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8123" to be "running"
Mar 15 21:49:39.363: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.807728ms
Mar 15 21:49:41.367: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007877721s
Mar 15 21:49:41.367: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-8123 03/15/23 21:49:41.367
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8123 03/15/23 21:49:41.372
Mar 15 21:49:41.382: INFO: Observed stateful pod in namespace: statefulset-8123, name: ss-0, uid: 092149cf-489c-4800-a406-352bc191001d, status phase: Pending. Waiting for statefulset controller to delete.
Mar 15 21:49:41.396: INFO: Observed stateful pod in namespace: statefulset-8123, name: ss-0, uid: 092149cf-489c-4800-a406-352bc191001d, status phase: Failed. Waiting for statefulset controller to delete.
Mar 15 21:49:41.406: INFO: Observed stateful pod in namespace: statefulset-8123, name: ss-0, uid: 092149cf-489c-4800-a406-352bc191001d, status phase: Failed. Waiting for statefulset controller to delete.
Mar 15 21:49:41.410: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8123
STEP: Removing pod with conflicting port in namespace statefulset-8123 03/15/23 21:49:41.41
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8123 and will be in running state 03/15/23 21:49:41.44
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 15 21:49:43.450: INFO: Deleting all statefulset in ns statefulset-8123
Mar 15 21:49:43.453: INFO: Scaling statefulset ss to 0
Mar 15 21:49:53.468: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 21:49:53.471: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 15 21:49:53.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8123" for this suite. 03/15/23 21:49:53.486
------------------------------
• [SLOW TEST] [14.195 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:49:39.299
    Mar 15 21:49:39.299: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename statefulset 03/15/23 21:49:39.3
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:49:39.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:49:39.328
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8123 03/15/23 21:49:39.338
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 03/15/23 21:49:39.346
    STEP: Creating pod with conflicting port in namespace statefulset-8123 03/15/23 21:49:39.35
    STEP: Waiting until pod test-pod will start running in namespace statefulset-8123 03/15/23 21:49:39.359
    Mar 15 21:49:39.359: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8123" to be "running"
    Mar 15 21:49:39.363: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.807728ms
    Mar 15 21:49:41.367: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007877721s
    Mar 15 21:49:41.367: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-8123 03/15/23 21:49:41.367
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8123 03/15/23 21:49:41.372
    Mar 15 21:49:41.382: INFO: Observed stateful pod in namespace: statefulset-8123, name: ss-0, uid: 092149cf-489c-4800-a406-352bc191001d, status phase: Pending. Waiting for statefulset controller to delete.
    Mar 15 21:49:41.396: INFO: Observed stateful pod in namespace: statefulset-8123, name: ss-0, uid: 092149cf-489c-4800-a406-352bc191001d, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 15 21:49:41.406: INFO: Observed stateful pod in namespace: statefulset-8123, name: ss-0, uid: 092149cf-489c-4800-a406-352bc191001d, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 15 21:49:41.410: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8123
    STEP: Removing pod with conflicting port in namespace statefulset-8123 03/15/23 21:49:41.41
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8123 and will be in running state 03/15/23 21:49:41.44
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 15 21:49:43.450: INFO: Deleting all statefulset in ns statefulset-8123
    Mar 15 21:49:43.453: INFO: Scaling statefulset ss to 0
    Mar 15 21:49:53.468: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 15 21:49:53.471: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:49:53.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8123" for this suite. 03/15/23 21:49:53.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:49:53.496
Mar 15 21:49:53.496: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename replicaset 03/15/23 21:49:53.497
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:49:53.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:49:53.513
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 03/15/23 21:49:53.528
STEP: Verify that the required pods have come up. 03/15/23 21:49:53.537
Mar 15 21:49:53.546: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 15 21:49:58.551: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/15/23 21:49:58.551
STEP: Getting /status 03/15/23 21:49:58.551
Mar 15 21:49:58.558: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 03/15/23 21:49:58.558
Mar 15 21:49:58.573: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 03/15/23 21:49:58.573
Mar 15 21:49:58.576: INFO: Observed &ReplicaSet event: ADDED
Mar 15 21:49:58.577: INFO: Observed &ReplicaSet event: MODIFIED
Mar 15 21:49:58.577: INFO: Observed &ReplicaSet event: MODIFIED
Mar 15 21:49:58.578: INFO: Observed &ReplicaSet event: MODIFIED
Mar 15 21:49:58.578: INFO: Found replicaset test-rs in namespace replicaset-6067 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 15 21:49:58.578: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 03/15/23 21:49:58.578
Mar 15 21:49:58.578: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 15 21:49:58.584: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 03/15/23 21:49:58.584
Mar 15 21:49:58.587: INFO: Observed &ReplicaSet event: ADDED
Mar 15 21:49:58.587: INFO: Observed &ReplicaSet event: MODIFIED
Mar 15 21:49:58.587: INFO: Observed &ReplicaSet event: MODIFIED
Mar 15 21:49:58.587: INFO: Observed &ReplicaSet event: MODIFIED
Mar 15 21:49:58.587: INFO: Observed replicaset test-rs in namespace replicaset-6067 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 15 21:49:58.588: INFO: Observed &ReplicaSet event: MODIFIED
Mar 15 21:49:58.588: INFO: Found replicaset test-rs in namespace replicaset-6067 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar 15 21:49:58.588: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 15 21:49:58.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6067" for this suite. 03/15/23 21:49:58.592
------------------------------
• [SLOW TEST] [5.102 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:49:53.496
    Mar 15 21:49:53.496: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename replicaset 03/15/23 21:49:53.497
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:49:53.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:49:53.513
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 03/15/23 21:49:53.528
    STEP: Verify that the required pods have come up. 03/15/23 21:49:53.537
    Mar 15 21:49:53.546: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 15 21:49:58.551: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/15/23 21:49:58.551
    STEP: Getting /status 03/15/23 21:49:58.551
    Mar 15 21:49:58.558: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 03/15/23 21:49:58.558
    Mar 15 21:49:58.573: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 03/15/23 21:49:58.573
    Mar 15 21:49:58.576: INFO: Observed &ReplicaSet event: ADDED
    Mar 15 21:49:58.577: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 15 21:49:58.577: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 15 21:49:58.578: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 15 21:49:58.578: INFO: Found replicaset test-rs in namespace replicaset-6067 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 15 21:49:58.578: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 03/15/23 21:49:58.578
    Mar 15 21:49:58.578: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 15 21:49:58.584: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 03/15/23 21:49:58.584
    Mar 15 21:49:58.587: INFO: Observed &ReplicaSet event: ADDED
    Mar 15 21:49:58.587: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 15 21:49:58.587: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 15 21:49:58.587: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 15 21:49:58.587: INFO: Observed replicaset test-rs in namespace replicaset-6067 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 15 21:49:58.588: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 15 21:49:58.588: INFO: Found replicaset test-rs in namespace replicaset-6067 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Mar 15 21:49:58.588: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:49:58.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6067" for this suite. 03/15/23 21:49:58.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:49:58.599
Mar 15 21:49:58.599: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename gc 03/15/23 21:49:58.6
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:49:58.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:49:58.618
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 03/15/23 21:49:58.626
STEP: create the rc2 03/15/23 21:49:58.631
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/15/23 21:50:03.646
STEP: delete the rc simpletest-rc-to-be-deleted 03/15/23 21:50:04.544
STEP: wait for the rc to be deleted 03/15/23 21:50:04.552
Mar 15 21:50:09.587: INFO: 83 pods remaining
Mar 15 21:50:09.588: INFO: 70 pods has nil DeletionTimestamp
Mar 15 21:50:09.588: INFO: 
Mar 15 21:50:14.585: INFO: 72 pods remaining
Mar 15 21:50:14.585: INFO: 50 pods has nil DeletionTimestamp
Mar 15 21:50:14.585: INFO: 
STEP: Gathering metrics 03/15/23 21:50:19.592
Mar 15 21:50:19.623: INFO: Waiting up to 5m0s for pod "kube-controller-manager-i-00864a1c8c75a434c" in namespace "kube-system" to be "running and ready"
Mar 15 21:50:19.628: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c": Phase="Running", Reason="", readiness=true. Elapsed: 4.73199ms
Mar 15 21:50:19.628: INFO: The phase of Pod kube-controller-manager-i-00864a1c8c75a434c is Running (Ready = true)
Mar 15 21:50:19.628: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c" satisfied condition "running and ready"
Mar 15 21:50:19.700: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 15 21:50:19.700: INFO: Deleting pod "simpletest-rc-to-be-deleted-28sss" in namespace "gc-1939"
Mar 15 21:50:19.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mpj8" in namespace "gc-1939"
Mar 15 21:50:19.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zz7z" in namespace "gc-1939"
Mar 15 21:50:19.761: INFO: Deleting pod "simpletest-rc-to-be-deleted-477zb" in namespace "gc-1939"
Mar 15 21:50:19.774: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c2h7" in namespace "gc-1939"
Mar 15 21:50:19.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-575wg" in namespace "gc-1939"
Mar 15 21:50:19.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-58jq4" in namespace "gc-1939"
Mar 15 21:50:19.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-592bv" in namespace "gc-1939"
Mar 15 21:50:19.858: INFO: Deleting pod "simpletest-rc-to-be-deleted-5f8jr" in namespace "gc-1939"
Mar 15 21:50:19.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-5g2dn" in namespace "gc-1939"
Mar 15 21:50:19.897: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qcfd" in namespace "gc-1939"
Mar 15 21:50:19.919: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rhdr" in namespace "gc-1939"
Mar 15 21:50:19.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-5v2rb" in namespace "gc-1939"
Mar 15 21:50:19.955: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dhhl" in namespace "gc-1939"
Mar 15 21:50:19.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k9qb" in namespace "gc-1939"
Mar 15 21:50:20.000: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pwkt" in namespace "gc-1939"
Mar 15 21:50:20.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tlbr" in namespace "gc-1939"
Mar 15 21:50:20.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tpf5" in namespace "gc-1939"
Mar 15 21:50:20.055: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bx4m" in namespace "gc-1939"
Mar 15 21:50:20.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pc9t" in namespace "gc-1939"
Mar 15 21:50:20.109: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rp7g" in namespace "gc-1939"
Mar 15 21:50:20.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-82v26" in namespace "gc-1939"
Mar 15 21:50:20.184: INFO: Deleting pod "simpletest-rc-to-be-deleted-888cw" in namespace "gc-1939"
Mar 15 21:50:20.216: INFO: Deleting pod "simpletest-rc-to-be-deleted-8twc4" in namespace "gc-1939"
Mar 15 21:50:20.246: INFO: Deleting pod "simpletest-rc-to-be-deleted-94q7g" in namespace "gc-1939"
Mar 15 21:50:20.268: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bs49" in namespace "gc-1939"
Mar 15 21:50:20.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-b25zw" in namespace "gc-1939"
Mar 15 21:50:20.309: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9fdq" in namespace "gc-1939"
Mar 15 21:50:20.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-bh582" in namespace "gc-1939"
Mar 15 21:50:20.347: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnlr8" in namespace "gc-1939"
Mar 15 21:50:20.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq2d5" in namespace "gc-1939"
Mar 15 21:50:20.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv5tk" in namespace "gc-1939"
Mar 15 21:50:20.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx9wd" in namespace "gc-1939"
Mar 15 21:50:20.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5z85" in namespace "gc-1939"
Mar 15 21:50:20.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9mmd" in namespace "gc-1939"
Mar 15 21:50:20.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgh56" in namespace "gc-1939"
Mar 15 21:50:20.450: INFO: Deleting pod "simpletest-rc-to-be-deleted-djcnm" in namespace "gc-1939"
Mar 15 21:50:20.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-djsmm" in namespace "gc-1939"
Mar 15 21:50:20.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-dldzl" in namespace "gc-1939"
Mar 15 21:50:20.499: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmx47" in namespace "gc-1939"
Mar 15 21:50:20.512: INFO: Deleting pod "simpletest-rc-to-be-deleted-gpvgb" in namespace "gc-1939"
Mar 15 21:50:20.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgc9d" in namespace "gc-1939"
Mar 15 21:50:20.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-hlrft" in namespace "gc-1939"
Mar 15 21:50:20.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-jkdlm" in namespace "gc-1939"
Mar 15 21:50:20.556: INFO: Deleting pod "simpletest-rc-to-be-deleted-jkf6n" in namespace "gc-1939"
Mar 15 21:50:20.568: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4m44" in namespace "gc-1939"
Mar 15 21:50:20.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-k5vc9" in namespace "gc-1939"
Mar 15 21:50:20.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfb4j" in namespace "gc-1939"
Mar 15 21:50:20.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-kjh5c" in namespace "gc-1939"
Mar 15 21:50:20.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-kjltr" in namespace "gc-1939"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 15 21:50:20.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1939" for this suite. 03/15/23 21:50:20.625
------------------------------
• [SLOW TEST] [22.034 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:49:58.599
    Mar 15 21:49:58.599: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename gc 03/15/23 21:49:58.6
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:49:58.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:49:58.618
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 03/15/23 21:49:58.626
    STEP: create the rc2 03/15/23 21:49:58.631
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/15/23 21:50:03.646
    STEP: delete the rc simpletest-rc-to-be-deleted 03/15/23 21:50:04.544
    STEP: wait for the rc to be deleted 03/15/23 21:50:04.552
    Mar 15 21:50:09.587: INFO: 83 pods remaining
    Mar 15 21:50:09.588: INFO: 70 pods has nil DeletionTimestamp
    Mar 15 21:50:09.588: INFO: 
    Mar 15 21:50:14.585: INFO: 72 pods remaining
    Mar 15 21:50:14.585: INFO: 50 pods has nil DeletionTimestamp
    Mar 15 21:50:14.585: INFO: 
    STEP: Gathering metrics 03/15/23 21:50:19.592
    Mar 15 21:50:19.623: INFO: Waiting up to 5m0s for pod "kube-controller-manager-i-00864a1c8c75a434c" in namespace "kube-system" to be "running and ready"
    Mar 15 21:50:19.628: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c": Phase="Running", Reason="", readiness=true. Elapsed: 4.73199ms
    Mar 15 21:50:19.628: INFO: The phase of Pod kube-controller-manager-i-00864a1c8c75a434c is Running (Ready = true)
    Mar 15 21:50:19.628: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c" satisfied condition "running and ready"
    Mar 15 21:50:19.700: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 15 21:50:19.700: INFO: Deleting pod "simpletest-rc-to-be-deleted-28sss" in namespace "gc-1939"
    Mar 15 21:50:19.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mpj8" in namespace "gc-1939"
    Mar 15 21:50:19.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zz7z" in namespace "gc-1939"
    Mar 15 21:50:19.761: INFO: Deleting pod "simpletest-rc-to-be-deleted-477zb" in namespace "gc-1939"
    Mar 15 21:50:19.774: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c2h7" in namespace "gc-1939"
    Mar 15 21:50:19.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-575wg" in namespace "gc-1939"
    Mar 15 21:50:19.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-58jq4" in namespace "gc-1939"
    Mar 15 21:50:19.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-592bv" in namespace "gc-1939"
    Mar 15 21:50:19.858: INFO: Deleting pod "simpletest-rc-to-be-deleted-5f8jr" in namespace "gc-1939"
    Mar 15 21:50:19.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-5g2dn" in namespace "gc-1939"
    Mar 15 21:50:19.897: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qcfd" in namespace "gc-1939"
    Mar 15 21:50:19.919: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rhdr" in namespace "gc-1939"
    Mar 15 21:50:19.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-5v2rb" in namespace "gc-1939"
    Mar 15 21:50:19.955: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dhhl" in namespace "gc-1939"
    Mar 15 21:50:19.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k9qb" in namespace "gc-1939"
    Mar 15 21:50:20.000: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pwkt" in namespace "gc-1939"
    Mar 15 21:50:20.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tlbr" in namespace "gc-1939"
    Mar 15 21:50:20.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tpf5" in namespace "gc-1939"
    Mar 15 21:50:20.055: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bx4m" in namespace "gc-1939"
    Mar 15 21:50:20.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pc9t" in namespace "gc-1939"
    Mar 15 21:50:20.109: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rp7g" in namespace "gc-1939"
    Mar 15 21:50:20.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-82v26" in namespace "gc-1939"
    Mar 15 21:50:20.184: INFO: Deleting pod "simpletest-rc-to-be-deleted-888cw" in namespace "gc-1939"
    Mar 15 21:50:20.216: INFO: Deleting pod "simpletest-rc-to-be-deleted-8twc4" in namespace "gc-1939"
    Mar 15 21:50:20.246: INFO: Deleting pod "simpletest-rc-to-be-deleted-94q7g" in namespace "gc-1939"
    Mar 15 21:50:20.268: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bs49" in namespace "gc-1939"
    Mar 15 21:50:20.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-b25zw" in namespace "gc-1939"
    Mar 15 21:50:20.309: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9fdq" in namespace "gc-1939"
    Mar 15 21:50:20.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-bh582" in namespace "gc-1939"
    Mar 15 21:50:20.347: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnlr8" in namespace "gc-1939"
    Mar 15 21:50:20.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq2d5" in namespace "gc-1939"
    Mar 15 21:50:20.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv5tk" in namespace "gc-1939"
    Mar 15 21:50:20.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx9wd" in namespace "gc-1939"
    Mar 15 21:50:20.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5z85" in namespace "gc-1939"
    Mar 15 21:50:20.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9mmd" in namespace "gc-1939"
    Mar 15 21:50:20.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgh56" in namespace "gc-1939"
    Mar 15 21:50:20.450: INFO: Deleting pod "simpletest-rc-to-be-deleted-djcnm" in namespace "gc-1939"
    Mar 15 21:50:20.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-djsmm" in namespace "gc-1939"
    Mar 15 21:50:20.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-dldzl" in namespace "gc-1939"
    Mar 15 21:50:20.499: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmx47" in namespace "gc-1939"
    Mar 15 21:50:20.512: INFO: Deleting pod "simpletest-rc-to-be-deleted-gpvgb" in namespace "gc-1939"
    Mar 15 21:50:20.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgc9d" in namespace "gc-1939"
    Mar 15 21:50:20.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-hlrft" in namespace "gc-1939"
    Mar 15 21:50:20.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-jkdlm" in namespace "gc-1939"
    Mar 15 21:50:20.556: INFO: Deleting pod "simpletest-rc-to-be-deleted-jkf6n" in namespace "gc-1939"
    Mar 15 21:50:20.568: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4m44" in namespace "gc-1939"
    Mar 15 21:50:20.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-k5vc9" in namespace "gc-1939"
    Mar 15 21:50:20.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfb4j" in namespace "gc-1939"
    Mar 15 21:50:20.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-kjh5c" in namespace "gc-1939"
    Mar 15 21:50:20.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-kjltr" in namespace "gc-1939"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:50:20.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1939" for this suite. 03/15/23 21:50:20.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:50:20.633
Mar 15 21:50:20.633: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 21:50:20.635
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:50:20.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:50:20.65
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 03/15/23 21:50:20.654
Mar 15 21:50:20.655: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: mark a version not serverd 03/15/23 21:50:24.791
STEP: check the unserved version gets removed 03/15/23 21:50:24.81
STEP: check the other version is not changed 03/15/23 21:50:25.928
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:50:29.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5164" for this suite. 03/15/23 21:50:29.189
------------------------------
• [SLOW TEST] [8.562 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:50:20.633
    Mar 15 21:50:20.633: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 21:50:20.635
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:50:20.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:50:20.65
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 03/15/23 21:50:20.654
    Mar 15 21:50:20.655: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: mark a version not serverd 03/15/23 21:50:24.791
    STEP: check the unserved version gets removed 03/15/23 21:50:24.81
    STEP: check the other version is not changed 03/15/23 21:50:25.928
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:50:29.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5164" for this suite. 03/15/23 21:50:29.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:50:29.197
Mar 15 21:50:29.197: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename sched-preemption 03/15/23 21:50:29.198
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:50:29.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:50:29.212
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 15 21:50:29.227: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 15 21:51:29.270: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 03/15/23 21:51:29.276
Mar 15 21:51:29.318: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 15 21:51:29.326: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 15 21:51:29.364: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 15 21:51:29.377: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 15 21:51:29.423: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 15 21:51:29.434: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/15/23 21:51:29.434
Mar 15 21:51:29.435: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-704" to be "running"
Mar 15 21:51:29.442: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.891958ms
Mar 15 21:51:31.446: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.011072993s
Mar 15 21:51:31.446: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 15 21:51:31.446: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-704" to be "running"
Mar 15 21:51:31.455: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.472039ms
Mar 15 21:51:31.455: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 15 21:51:31.455: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-704" to be "running"
Mar 15 21:51:31.460: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.824644ms
Mar 15 21:51:33.463: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.008446718s
Mar 15 21:51:33.463: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 15 21:51:33.463: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-704" to be "running"
Mar 15 21:51:33.466: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.076928ms
Mar 15 21:51:33.467: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 15 21:51:33.467: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-704" to be "running"
Mar 15 21:51:33.470: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.992691ms
Mar 15 21:51:33.470: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 15 21:51:33.470: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-704" to be "running"
Mar 15 21:51:33.472: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.403556ms
Mar 15 21:51:33.472: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 03/15/23 21:51:33.472
Mar 15 21:51:33.482: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Mar 15 21:51:33.486: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.808335ms
Mar 15 21:51:35.501: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019178243s
Mar 15 21:51:37.489: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.0072674s
Mar 15 21:51:37.489: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:51:37.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-704" for this suite. 03/15/23 21:51:37.607
------------------------------
• [SLOW TEST] [68.417 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:50:29.197
    Mar 15 21:50:29.197: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename sched-preemption 03/15/23 21:50:29.198
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:50:29.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:50:29.212
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 15 21:50:29.227: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 15 21:51:29.270: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 03/15/23 21:51:29.276
    Mar 15 21:51:29.318: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 15 21:51:29.326: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 15 21:51:29.364: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 15 21:51:29.377: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 15 21:51:29.423: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 15 21:51:29.434: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/15/23 21:51:29.434
    Mar 15 21:51:29.435: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-704" to be "running"
    Mar 15 21:51:29.442: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.891958ms
    Mar 15 21:51:31.446: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.011072993s
    Mar 15 21:51:31.446: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 15 21:51:31.446: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-704" to be "running"
    Mar 15 21:51:31.455: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.472039ms
    Mar 15 21:51:31.455: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 15 21:51:31.455: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-704" to be "running"
    Mar 15 21:51:31.460: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.824644ms
    Mar 15 21:51:33.463: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.008446718s
    Mar 15 21:51:33.463: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 15 21:51:33.463: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-704" to be "running"
    Mar 15 21:51:33.466: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.076928ms
    Mar 15 21:51:33.467: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 15 21:51:33.467: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-704" to be "running"
    Mar 15 21:51:33.470: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.992691ms
    Mar 15 21:51:33.470: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 15 21:51:33.470: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-704" to be "running"
    Mar 15 21:51:33.472: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.403556ms
    Mar 15 21:51:33.472: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 03/15/23 21:51:33.472
    Mar 15 21:51:33.482: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Mar 15 21:51:33.486: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.808335ms
    Mar 15 21:51:35.501: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019178243s
    Mar 15 21:51:37.489: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.0072674s
    Mar 15 21:51:37.489: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:51:37.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-704" for this suite. 03/15/23 21:51:37.607
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:51:37.615
Mar 15 21:51:37.616: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename watch 03/15/23 21:51:37.617
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:51:37.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:51:37.637
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 03/15/23 21:51:37.641
STEP: creating a new configmap 03/15/23 21:51:37.642
STEP: modifying the configmap once 03/15/23 21:51:37.647
STEP: closing the watch once it receives two notifications 03/15/23 21:51:37.656
Mar 15 21:51:37.656: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3822  1c89ff5f-bcfc-4403-b951-748017ea4fe7 6627 0 2023-03-15 21:51:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-15 21:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 21:51:37.657: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3822  1c89ff5f-bcfc-4403-b951-748017ea4fe7 6628 0 2023-03-15 21:51:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-15 21:51:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 03/15/23 21:51:37.657
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/15/23 21:51:37.663
STEP: deleting the configmap 03/15/23 21:51:37.665
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/15/23 21:51:37.669
Mar 15 21:51:37.669: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3822  1c89ff5f-bcfc-4403-b951-748017ea4fe7 6629 0 2023-03-15 21:51:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-15 21:51:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 21:51:37.670: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3822  1c89ff5f-bcfc-4403-b951-748017ea4fe7 6630 0 2023-03-15 21:51:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-15 21:51:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 15 21:51:37.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3822" for this suite. 03/15/23 21:51:37.674
------------------------------
• [0.072 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:51:37.615
    Mar 15 21:51:37.616: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename watch 03/15/23 21:51:37.617
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:51:37.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:51:37.637
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 03/15/23 21:51:37.641
    STEP: creating a new configmap 03/15/23 21:51:37.642
    STEP: modifying the configmap once 03/15/23 21:51:37.647
    STEP: closing the watch once it receives two notifications 03/15/23 21:51:37.656
    Mar 15 21:51:37.656: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3822  1c89ff5f-bcfc-4403-b951-748017ea4fe7 6627 0 2023-03-15 21:51:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-15 21:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 21:51:37.657: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3822  1c89ff5f-bcfc-4403-b951-748017ea4fe7 6628 0 2023-03-15 21:51:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-15 21:51:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 03/15/23 21:51:37.657
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/15/23 21:51:37.663
    STEP: deleting the configmap 03/15/23 21:51:37.665
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/15/23 21:51:37.669
    Mar 15 21:51:37.669: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3822  1c89ff5f-bcfc-4403-b951-748017ea4fe7 6629 0 2023-03-15 21:51:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-15 21:51:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 21:51:37.670: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3822  1c89ff5f-bcfc-4403-b951-748017ea4fe7 6630 0 2023-03-15 21:51:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-15 21:51:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:51:37.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3822" for this suite. 03/15/23 21:51:37.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:51:37.691
Mar 15 21:51:37.692: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename ingress 03/15/23 21:51:37.692
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:51:37.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:51:37.707
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 03/15/23 21:51:37.711
STEP: getting /apis/networking.k8s.io 03/15/23 21:51:37.713
STEP: getting /apis/networking.k8s.iov1 03/15/23 21:51:37.715
STEP: creating 03/15/23 21:51:37.717
STEP: getting 03/15/23 21:51:37.764
STEP: listing 03/15/23 21:51:37.767
STEP: watching 03/15/23 21:51:37.771
Mar 15 21:51:37.771: INFO: starting watch
STEP: cluster-wide listing 03/15/23 21:51:37.773
STEP: cluster-wide watching 03/15/23 21:51:37.776
Mar 15 21:51:37.776: INFO: starting watch
STEP: patching 03/15/23 21:51:37.778
STEP: updating 03/15/23 21:51:37.79
Mar 15 21:51:37.801: INFO: waiting for watch events with expected annotations
Mar 15 21:51:37.801: INFO: saw patched and updated annotations
STEP: patching /status 03/15/23 21:51:37.801
STEP: updating /status 03/15/23 21:51:37.806
STEP: get /status 03/15/23 21:51:37.812
STEP: deleting 03/15/23 21:51:37.816
STEP: deleting a collection 03/15/23 21:51:37.826
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Mar 15 21:51:37.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-6711" for this suite. 03/15/23 21:51:37.852
------------------------------
• [0.174 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:51:37.691
    Mar 15 21:51:37.692: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename ingress 03/15/23 21:51:37.692
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:51:37.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:51:37.707
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 03/15/23 21:51:37.711
    STEP: getting /apis/networking.k8s.io 03/15/23 21:51:37.713
    STEP: getting /apis/networking.k8s.iov1 03/15/23 21:51:37.715
    STEP: creating 03/15/23 21:51:37.717
    STEP: getting 03/15/23 21:51:37.764
    STEP: listing 03/15/23 21:51:37.767
    STEP: watching 03/15/23 21:51:37.771
    Mar 15 21:51:37.771: INFO: starting watch
    STEP: cluster-wide listing 03/15/23 21:51:37.773
    STEP: cluster-wide watching 03/15/23 21:51:37.776
    Mar 15 21:51:37.776: INFO: starting watch
    STEP: patching 03/15/23 21:51:37.778
    STEP: updating 03/15/23 21:51:37.79
    Mar 15 21:51:37.801: INFO: waiting for watch events with expected annotations
    Mar 15 21:51:37.801: INFO: saw patched and updated annotations
    STEP: patching /status 03/15/23 21:51:37.801
    STEP: updating /status 03/15/23 21:51:37.806
    STEP: get /status 03/15/23 21:51:37.812
    STEP: deleting 03/15/23 21:51:37.816
    STEP: deleting a collection 03/15/23 21:51:37.826
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:51:37.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-6711" for this suite. 03/15/23 21:51:37.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:51:37.872
Mar 15 21:51:37.872: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 21:51:37.873
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:51:37.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:51:37.892
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 03/15/23 21:51:37.905
STEP: waiting for available Endpoint 03/15/23 21:51:37.914
STEP: listing all Endpoints 03/15/23 21:51:37.921
STEP: updating the Endpoint 03/15/23 21:51:37.931
STEP: fetching the Endpoint 03/15/23 21:51:37.946
STEP: patching the Endpoint 03/15/23 21:51:37.95
STEP: fetching the Endpoint 03/15/23 21:51:37.968
STEP: deleting the Endpoint by Collection 03/15/23 21:51:37.97
STEP: waiting for Endpoint deletion 03/15/23 21:51:37.979
STEP: fetching the Endpoint 03/15/23 21:51:37.981
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 21:51:37.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8656" for this suite. 03/15/23 21:51:37.987
------------------------------
• [0.122 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:51:37.872
    Mar 15 21:51:37.872: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 21:51:37.873
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:51:37.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:51:37.892
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 03/15/23 21:51:37.905
    STEP: waiting for available Endpoint 03/15/23 21:51:37.914
    STEP: listing all Endpoints 03/15/23 21:51:37.921
    STEP: updating the Endpoint 03/15/23 21:51:37.931
    STEP: fetching the Endpoint 03/15/23 21:51:37.946
    STEP: patching the Endpoint 03/15/23 21:51:37.95
    STEP: fetching the Endpoint 03/15/23 21:51:37.968
    STEP: deleting the Endpoint by Collection 03/15/23 21:51:37.97
    STEP: waiting for Endpoint deletion 03/15/23 21:51:37.979
    STEP: fetching the Endpoint 03/15/23 21:51:37.981
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:51:37.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8656" for this suite. 03/15/23 21:51:37.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:51:37.996
Mar 15 21:51:37.996: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 21:51:37.997
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:51:38.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:51:38.011
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 03/15/23 21:51:38.017
STEP: watching for the Service to be added 03/15/23 21:51:38.035
Mar 15 21:51:38.038: INFO: Found Service test-service-vsp62 in namespace services-4250 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar 15 21:51:38.038: INFO: Service test-service-vsp62 created
STEP: Getting /status 03/15/23 21:51:38.038
Mar 15 21:51:38.044: INFO: Service test-service-vsp62 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 03/15/23 21:51:38.044
STEP: watching for the Service to be patched 03/15/23 21:51:38.056
Mar 15 21:51:38.058: INFO: observed Service test-service-vsp62 in namespace services-4250 with annotations: map[] & LoadBalancer: {[]}
Mar 15 21:51:38.059: INFO: Found Service test-service-vsp62 in namespace services-4250 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar 15 21:51:38.059: INFO: Service test-service-vsp62 has service status patched
STEP: updating the ServiceStatus 03/15/23 21:51:38.059
Mar 15 21:51:38.088: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 03/15/23 21:51:38.088
Mar 15 21:51:38.095: INFO: Observed Service test-service-vsp62 in namespace services-4250 with annotations: map[] & Conditions: {[]}
Mar 15 21:51:38.095: INFO: Observed event: &Service{ObjectMeta:{test-service-vsp62  services-4250  cb0f3b93-3bab-43b3-986a-0ee0e26bc394 6659 0 2023-03-15 21:51:38 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-15 21:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-15 21:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.69.106.163,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.69.106.163],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar 15 21:51:38.095: INFO: Found Service test-service-vsp62 in namespace services-4250 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 15 21:51:38.096: INFO: Service test-service-vsp62 has service status updated
STEP: patching the service 03/15/23 21:51:38.096
STEP: watching for the Service to be patched 03/15/23 21:51:38.163
Mar 15 21:51:38.167: INFO: observed Service test-service-vsp62 in namespace services-4250 with labels: map[test-service-static:true]
Mar 15 21:51:38.167: INFO: observed Service test-service-vsp62 in namespace services-4250 with labels: map[test-service-static:true]
Mar 15 21:51:38.181: INFO: observed Service test-service-vsp62 in namespace services-4250 with labels: map[test-service-static:true]
Mar 15 21:51:38.181: INFO: Found Service test-service-vsp62 in namespace services-4250 with labels: map[test-service:patched test-service-static:true]
Mar 15 21:51:38.181: INFO: Service test-service-vsp62 patched
STEP: deleting the service 03/15/23 21:51:38.181
STEP: watching for the Service to be deleted 03/15/23 21:51:38.216
Mar 15 21:51:38.218: INFO: Observed event: ADDED
Mar 15 21:51:38.218: INFO: Observed event: MODIFIED
Mar 15 21:51:38.218: INFO: Observed event: MODIFIED
Mar 15 21:51:38.219: INFO: Observed event: MODIFIED
Mar 15 21:51:38.219: INFO: Found Service test-service-vsp62 in namespace services-4250 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar 15 21:51:38.219: INFO: Service test-service-vsp62 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 21:51:38.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4250" for this suite. 03/15/23 21:51:38.232
------------------------------
• [0.247 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:51:37.996
    Mar 15 21:51:37.996: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 21:51:37.997
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:51:38.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:51:38.011
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 03/15/23 21:51:38.017
    STEP: watching for the Service to be added 03/15/23 21:51:38.035
    Mar 15 21:51:38.038: INFO: Found Service test-service-vsp62 in namespace services-4250 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Mar 15 21:51:38.038: INFO: Service test-service-vsp62 created
    STEP: Getting /status 03/15/23 21:51:38.038
    Mar 15 21:51:38.044: INFO: Service test-service-vsp62 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 03/15/23 21:51:38.044
    STEP: watching for the Service to be patched 03/15/23 21:51:38.056
    Mar 15 21:51:38.058: INFO: observed Service test-service-vsp62 in namespace services-4250 with annotations: map[] & LoadBalancer: {[]}
    Mar 15 21:51:38.059: INFO: Found Service test-service-vsp62 in namespace services-4250 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Mar 15 21:51:38.059: INFO: Service test-service-vsp62 has service status patched
    STEP: updating the ServiceStatus 03/15/23 21:51:38.059
    Mar 15 21:51:38.088: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 03/15/23 21:51:38.088
    Mar 15 21:51:38.095: INFO: Observed Service test-service-vsp62 in namespace services-4250 with annotations: map[] & Conditions: {[]}
    Mar 15 21:51:38.095: INFO: Observed event: &Service{ObjectMeta:{test-service-vsp62  services-4250  cb0f3b93-3bab-43b3-986a-0ee0e26bc394 6659 0 2023-03-15 21:51:38 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-15 21:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-15 21:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.69.106.163,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.69.106.163],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Mar 15 21:51:38.095: INFO: Found Service test-service-vsp62 in namespace services-4250 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 15 21:51:38.096: INFO: Service test-service-vsp62 has service status updated
    STEP: patching the service 03/15/23 21:51:38.096
    STEP: watching for the Service to be patched 03/15/23 21:51:38.163
    Mar 15 21:51:38.167: INFO: observed Service test-service-vsp62 in namespace services-4250 with labels: map[test-service-static:true]
    Mar 15 21:51:38.167: INFO: observed Service test-service-vsp62 in namespace services-4250 with labels: map[test-service-static:true]
    Mar 15 21:51:38.181: INFO: observed Service test-service-vsp62 in namespace services-4250 with labels: map[test-service-static:true]
    Mar 15 21:51:38.181: INFO: Found Service test-service-vsp62 in namespace services-4250 with labels: map[test-service:patched test-service-static:true]
    Mar 15 21:51:38.181: INFO: Service test-service-vsp62 patched
    STEP: deleting the service 03/15/23 21:51:38.181
    STEP: watching for the Service to be deleted 03/15/23 21:51:38.216
    Mar 15 21:51:38.218: INFO: Observed event: ADDED
    Mar 15 21:51:38.218: INFO: Observed event: MODIFIED
    Mar 15 21:51:38.218: INFO: Observed event: MODIFIED
    Mar 15 21:51:38.219: INFO: Observed event: MODIFIED
    Mar 15 21:51:38.219: INFO: Found Service test-service-vsp62 in namespace services-4250 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Mar 15 21:51:38.219: INFO: Service test-service-vsp62 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:51:38.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4250" for this suite. 03/15/23 21:51:38.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:51:38.247
Mar 15 21:51:38.247: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename sched-preemption 03/15/23 21:51:38.248
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:51:38.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:51:38.273
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 15 21:51:38.308: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 15 21:52:38.354: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:52:38.358
Mar 15 21:52:38.358: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename sched-preemption-path 03/15/23 21:52:38.359
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:52:38.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:52:38.376
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 03/15/23 21:52:38.383
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/15/23 21:52:38.383
Mar 15 21:52:38.390: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2889" to be "running"
Mar 15 21:52:38.395: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.681199ms
Mar 15 21:52:40.399: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009135217s
Mar 15 21:52:40.399: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/15/23 21:52:40.404
Mar 15 21:52:40.420: INFO: found a healthy node: i-0faaf83f00b43c88c
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Mar 15 21:52:46.570: INFO: pods created so far: [1 1 1]
Mar 15 21:52:46.570: INFO: length of pods created so far: 3
Mar 15 21:52:48.582: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Mar 15 21:52:55.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:52:55.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-2889" for this suite. 03/15/23 21:52:55.678
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6717" for this suite. 03/15/23 21:52:55.684
------------------------------
• [SLOW TEST] [77.444 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:51:38.247
    Mar 15 21:51:38.247: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename sched-preemption 03/15/23 21:51:38.248
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:51:38.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:51:38.273
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 15 21:51:38.308: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 15 21:52:38.354: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:52:38.358
    Mar 15 21:52:38.358: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename sched-preemption-path 03/15/23 21:52:38.359
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:52:38.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:52:38.376
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 03/15/23 21:52:38.383
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/15/23 21:52:38.383
    Mar 15 21:52:38.390: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2889" to be "running"
    Mar 15 21:52:38.395: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.681199ms
    Mar 15 21:52:40.399: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009135217s
    Mar 15 21:52:40.399: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/15/23 21:52:40.404
    Mar 15 21:52:40.420: INFO: found a healthy node: i-0faaf83f00b43c88c
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Mar 15 21:52:46.570: INFO: pods created so far: [1 1 1]
    Mar 15 21:52:46.570: INFO: length of pods created so far: 3
    Mar 15 21:52:48.582: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:52:55.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:52:55.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-2889" for this suite. 03/15/23 21:52:55.678
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6717" for this suite. 03/15/23 21:52:55.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:52:55.691
Mar 15 21:52:55.691: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pods 03/15/23 21:52:55.695
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:52:55.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:52:55.712
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 03/15/23 21:52:55.715
Mar 15 21:52:55.723: INFO: created test-pod-1
Mar 15 21:52:55.732: INFO: created test-pod-2
Mar 15 21:52:55.747: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 03/15/23 21:52:55.747
Mar 15 21:52:55.747: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1655' to be running and ready
Mar 15 21:52:55.766: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 15 21:52:55.766: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 15 21:52:55.766: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 15 21:52:55.767: INFO: 0 / 3 pods in namespace 'pods-1655' are running and ready (0 seconds elapsed)
Mar 15 21:52:55.767: INFO: expected 0 pod replicas in namespace 'pods-1655', 0 are Running and Ready.
Mar 15 21:52:55.767: INFO: POD         NODE                 PHASE    GRACE  CONDITIONS
Mar 15 21:52:55.767: INFO: test-pod-1  i-077ee0eb7ec5a02aa  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:52:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:52:55 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:52:55 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:52:55 +0000 UTC  }]
Mar 15 21:52:55.767: INFO: test-pod-2  i-0faaf83f00b43c88c  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:52:55 +0000 UTC  }]
Mar 15 21:52:55.767: INFO: test-pod-3  i-077ee0eb7ec5a02aa  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:52:55 +0000 UTC  }]
Mar 15 21:52:55.767: INFO: 
Mar 15 21:52:57.776: INFO: 3 / 3 pods in namespace 'pods-1655' are running and ready (2 seconds elapsed)
Mar 15 21:52:57.776: INFO: expected 0 pod replicas in namespace 'pods-1655', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 03/15/23 21:52:57.818
Mar 15 21:52:57.826: INFO: Pod quantity 3 is different from expected quantity 0
Mar 15 21:52:58.829: INFO: Pod quantity 3 is different from expected quantity 0
Mar 15 21:52:59.835: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 15 21:53:00.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1655" for this suite. 03/15/23 21:53:00.834
------------------------------
• [SLOW TEST] [5.149 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:52:55.691
    Mar 15 21:52:55.691: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pods 03/15/23 21:52:55.695
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:52:55.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:52:55.712
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 03/15/23 21:52:55.715
    Mar 15 21:52:55.723: INFO: created test-pod-1
    Mar 15 21:52:55.732: INFO: created test-pod-2
    Mar 15 21:52:55.747: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 03/15/23 21:52:55.747
    Mar 15 21:52:55.747: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1655' to be running and ready
    Mar 15 21:52:55.766: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 15 21:52:55.766: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 15 21:52:55.766: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 15 21:52:55.767: INFO: 0 / 3 pods in namespace 'pods-1655' are running and ready (0 seconds elapsed)
    Mar 15 21:52:55.767: INFO: expected 0 pod replicas in namespace 'pods-1655', 0 are Running and Ready.
    Mar 15 21:52:55.767: INFO: POD         NODE                 PHASE    GRACE  CONDITIONS
    Mar 15 21:52:55.767: INFO: test-pod-1  i-077ee0eb7ec5a02aa  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:52:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:52:55 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:52:55 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:52:55 +0000 UTC  }]
    Mar 15 21:52:55.767: INFO: test-pod-2  i-0faaf83f00b43c88c  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:52:55 +0000 UTC  }]
    Mar 15 21:52:55.767: INFO: test-pod-3  i-077ee0eb7ec5a02aa  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:52:55 +0000 UTC  }]
    Mar 15 21:52:55.767: INFO: 
    Mar 15 21:52:57.776: INFO: 3 / 3 pods in namespace 'pods-1655' are running and ready (2 seconds elapsed)
    Mar 15 21:52:57.776: INFO: expected 0 pod replicas in namespace 'pods-1655', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 03/15/23 21:52:57.818
    Mar 15 21:52:57.826: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 15 21:52:58.829: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 15 21:52:59.835: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:53:00.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1655" for this suite. 03/15/23 21:53:00.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:53:00.842
Mar 15 21:53:00.842: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename podtemplate 03/15/23 21:53:00.843
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:00.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:00.875
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 03/15/23 21:53:00.879
STEP: Replace a pod template 03/15/23 21:53:00.892
Mar 15 21:53:00.902: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 15 21:53:00.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8163" for this suite. 03/15/23 21:53:00.906
------------------------------
• [0.071 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:53:00.842
    Mar 15 21:53:00.842: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename podtemplate 03/15/23 21:53:00.843
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:00.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:00.875
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 03/15/23 21:53:00.879
    STEP: Replace a pod template 03/15/23 21:53:00.892
    Mar 15 21:53:00.902: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:53:00.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8163" for this suite. 03/15/23 21:53:00.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:53:00.913
Mar 15 21:53:00.913: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename statefulset 03/15/23 21:53:00.915
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:00.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:00.953
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9860 03/15/23 21:53:00.958
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-9860 03/15/23 21:53:00.972
Mar 15 21:53:01.005: INFO: Found 0 stateful pods, waiting for 1
Mar 15 21:53:11.016: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 03/15/23 21:53:11.021
STEP: Getting /status 03/15/23 21:53:11.028
Mar 15 21:53:11.032: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 03/15/23 21:53:11.032
Mar 15 21:53:11.045: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 03/15/23 21:53:11.045
Mar 15 21:53:11.048: INFO: Observed &StatefulSet event: ADDED
Mar 15 21:53:11.048: INFO: Found Statefulset ss in namespace statefulset-9860 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 15 21:53:11.048: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 03/15/23 21:53:11.048
Mar 15 21:53:11.048: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 15 21:53:11.054: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 03/15/23 21:53:11.054
Mar 15 21:53:11.057: INFO: Observed &StatefulSet event: ADDED
Mar 15 21:53:11.057: INFO: Observed Statefulset ss in namespace statefulset-9860 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 15 21:53:11.057: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 15 21:53:11.057: INFO: Deleting all statefulset in ns statefulset-9860
Mar 15 21:53:11.060: INFO: Scaling statefulset ss to 0
Mar 15 21:53:21.079: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 21:53:21.082: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 15 21:53:21.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9860" for this suite. 03/15/23 21:53:21.105
------------------------------
• [SLOW TEST] [20.201 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:53:00.913
    Mar 15 21:53:00.913: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename statefulset 03/15/23 21:53:00.915
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:00.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:00.953
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9860 03/15/23 21:53:00.958
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-9860 03/15/23 21:53:00.972
    Mar 15 21:53:01.005: INFO: Found 0 stateful pods, waiting for 1
    Mar 15 21:53:11.016: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 03/15/23 21:53:11.021
    STEP: Getting /status 03/15/23 21:53:11.028
    Mar 15 21:53:11.032: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 03/15/23 21:53:11.032
    Mar 15 21:53:11.045: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 03/15/23 21:53:11.045
    Mar 15 21:53:11.048: INFO: Observed &StatefulSet event: ADDED
    Mar 15 21:53:11.048: INFO: Found Statefulset ss in namespace statefulset-9860 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 15 21:53:11.048: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 03/15/23 21:53:11.048
    Mar 15 21:53:11.048: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 15 21:53:11.054: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 03/15/23 21:53:11.054
    Mar 15 21:53:11.057: INFO: Observed &StatefulSet event: ADDED
    Mar 15 21:53:11.057: INFO: Observed Statefulset ss in namespace statefulset-9860 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 15 21:53:11.057: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 15 21:53:11.057: INFO: Deleting all statefulset in ns statefulset-9860
    Mar 15 21:53:11.060: INFO: Scaling statefulset ss to 0
    Mar 15 21:53:21.079: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 15 21:53:21.082: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:53:21.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9860" for this suite. 03/15/23 21:53:21.105
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:53:21.116
Mar 15 21:53:21.117: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 21:53:21.118
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:21.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:21.145
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 03/15/23 21:53:21.149
Mar 15 21:53:21.159: INFO: Waiting up to 5m0s for pod "pod-f684b65c-ca44-4220-8228-977af3ed2d88" in namespace "emptydir-6411" to be "Succeeded or Failed"
Mar 15 21:53:21.162: INFO: Pod "pod-f684b65c-ca44-4220-8228-977af3ed2d88": Phase="Pending", Reason="", readiness=false. Elapsed: 3.453205ms
Mar 15 21:53:23.166: INFO: Pod "pod-f684b65c-ca44-4220-8228-977af3ed2d88": Phase="Running", Reason="", readiness=true. Elapsed: 2.007365685s
Mar 15 21:53:25.167: INFO: Pod "pod-f684b65c-ca44-4220-8228-977af3ed2d88": Phase="Running", Reason="", readiness=false. Elapsed: 4.008442919s
Mar 15 21:53:27.166: INFO: Pod "pod-f684b65c-ca44-4220-8228-977af3ed2d88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006950502s
STEP: Saw pod success 03/15/23 21:53:27.166
Mar 15 21:53:27.166: INFO: Pod "pod-f684b65c-ca44-4220-8228-977af3ed2d88" satisfied condition "Succeeded or Failed"
Mar 15 21:53:27.169: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-f684b65c-ca44-4220-8228-977af3ed2d88 container test-container: <nil>
STEP: delete the pod 03/15/23 21:53:27.185
Mar 15 21:53:27.197: INFO: Waiting for pod pod-f684b65c-ca44-4220-8228-977af3ed2d88 to disappear
Mar 15 21:53:27.200: INFO: Pod pod-f684b65c-ca44-4220-8228-977af3ed2d88 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 21:53:27.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6411" for this suite. 03/15/23 21:53:27.204
------------------------------
• [SLOW TEST] [6.094 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:53:21.116
    Mar 15 21:53:21.117: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 21:53:21.118
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:21.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:21.145
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/15/23 21:53:21.149
    Mar 15 21:53:21.159: INFO: Waiting up to 5m0s for pod "pod-f684b65c-ca44-4220-8228-977af3ed2d88" in namespace "emptydir-6411" to be "Succeeded or Failed"
    Mar 15 21:53:21.162: INFO: Pod "pod-f684b65c-ca44-4220-8228-977af3ed2d88": Phase="Pending", Reason="", readiness=false. Elapsed: 3.453205ms
    Mar 15 21:53:23.166: INFO: Pod "pod-f684b65c-ca44-4220-8228-977af3ed2d88": Phase="Running", Reason="", readiness=true. Elapsed: 2.007365685s
    Mar 15 21:53:25.167: INFO: Pod "pod-f684b65c-ca44-4220-8228-977af3ed2d88": Phase="Running", Reason="", readiness=false. Elapsed: 4.008442919s
    Mar 15 21:53:27.166: INFO: Pod "pod-f684b65c-ca44-4220-8228-977af3ed2d88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006950502s
    STEP: Saw pod success 03/15/23 21:53:27.166
    Mar 15 21:53:27.166: INFO: Pod "pod-f684b65c-ca44-4220-8228-977af3ed2d88" satisfied condition "Succeeded or Failed"
    Mar 15 21:53:27.169: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-f684b65c-ca44-4220-8228-977af3ed2d88 container test-container: <nil>
    STEP: delete the pod 03/15/23 21:53:27.185
    Mar 15 21:53:27.197: INFO: Waiting for pod pod-f684b65c-ca44-4220-8228-977af3ed2d88 to disappear
    Mar 15 21:53:27.200: INFO: Pod pod-f684b65c-ca44-4220-8228-977af3ed2d88 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:53:27.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6411" for this suite. 03/15/23 21:53:27.204
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:53:27.212
Mar 15 21:53:27.212: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pod-network-test 03/15/23 21:53:27.213
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:27.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:27.229
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-1096 03/15/23 21:53:27.233
STEP: creating a selector 03/15/23 21:53:27.233
STEP: Creating the service pods in kubernetes 03/15/23 21:53:27.234
Mar 15 21:53:27.235: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 15 21:53:27.275: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1096" to be "running and ready"
Mar 15 21:53:27.294: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.033934ms
Mar 15 21:53:27.294: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:53:29.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023673366s
Mar 15 21:53:29.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:53:31.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.02313194s
Mar 15 21:53:31.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:53:33.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023599411s
Mar 15 21:53:33.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:53:35.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.022854459s
Mar 15 21:53:35.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:53:37.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.024048842s
Mar 15 21:53:37.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:53:39.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.023872373s
Mar 15 21:53:39.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:53:41.297: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.022284035s
Mar 15 21:53:41.297: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:53:43.300: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.024902527s
Mar 15 21:53:43.300: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:53:45.297: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.022349363s
Mar 15 21:53:45.297: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:53:47.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.024148171s
Mar 15 21:53:47.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 21:53:49.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.023914005s
Mar 15 21:53:49.299: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 15 21:53:49.299: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 15 21:53:49.302: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1096" to be "running and ready"
Mar 15 21:53:49.304: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.624925ms
Mar 15 21:53:49.304: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 15 21:53:49.304: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 15 21:53:49.307: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1096" to be "running and ready"
Mar 15 21:53:49.318: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.119196ms
Mar 15 21:53:49.319: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 15 21:53:49.319: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/15/23 21:53:49.323
Mar 15 21:53:49.330: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1096" to be "running"
Mar 15 21:53:49.334: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.985847ms
Mar 15 21:53:51.337: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007143605s
Mar 15 21:53:53.338: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007782731s
Mar 15 21:53:53.338: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 15 21:53:53.343: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 15 21:53:53.343: INFO: Breadth first check of 100.96.2.13 on host 172.20.75.105...
Mar 15 21:53:53.345: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.87:9080/dial?request=hostname&protocol=udp&host=100.96.2.13&port=8081&tries=1'] Namespace:pod-network-test-1096 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:53:53.345: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:53:53.346: INFO: ExecWithOptions: Clientset creation
Mar 15 21:53:53.346: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-1096/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.87%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.96.2.13%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 15 21:53:53.485: INFO: Waiting for responses: map[]
Mar 15 21:53:53.485: INFO: reached 100.96.2.13 after 0/1 tries
Mar 15 21:53:53.485: INFO: Breadth first check of 100.96.1.220 on host 172.20.60.208...
Mar 15 21:53:53.488: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.87:9080/dial?request=hostname&protocol=udp&host=100.96.1.220&port=8081&tries=1'] Namespace:pod-network-test-1096 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:53:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:53:53.489: INFO: ExecWithOptions: Clientset creation
Mar 15 21:53:53.489: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-1096/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.87%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.96.1.220%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 15 21:53:53.605: INFO: Waiting for responses: map[]
Mar 15 21:53:53.605: INFO: reached 100.96.1.220 after 0/1 tries
Mar 15 21:53:53.605: INFO: Breadth first check of 100.96.3.61 on host 172.20.126.23...
Mar 15 21:53:53.608: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.87:9080/dial?request=hostname&protocol=udp&host=100.96.3.61&port=8081&tries=1'] Namespace:pod-network-test-1096 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 21:53:53.608: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 21:53:53.609: INFO: ExecWithOptions: Clientset creation
Mar 15 21:53:53.609: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-1096/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.87%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.96.3.61%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 15 21:53:53.706: INFO: Waiting for responses: map[]
Mar 15 21:53:53.706: INFO: reached 100.96.3.61 after 0/1 tries
Mar 15 21:53:53.706: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 15 21:53:53.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1096" for this suite. 03/15/23 21:53:53.71
------------------------------
• [SLOW TEST] [26.504 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:53:27.212
    Mar 15 21:53:27.212: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pod-network-test 03/15/23 21:53:27.213
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:27.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:27.229
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-1096 03/15/23 21:53:27.233
    STEP: creating a selector 03/15/23 21:53:27.233
    STEP: Creating the service pods in kubernetes 03/15/23 21:53:27.234
    Mar 15 21:53:27.235: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 15 21:53:27.275: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1096" to be "running and ready"
    Mar 15 21:53:27.294: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.033934ms
    Mar 15 21:53:27.294: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:53:29.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023673366s
    Mar 15 21:53:29.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:53:31.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.02313194s
    Mar 15 21:53:31.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:53:33.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023599411s
    Mar 15 21:53:33.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:53:35.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.022854459s
    Mar 15 21:53:35.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:53:37.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.024048842s
    Mar 15 21:53:37.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:53:39.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.023872373s
    Mar 15 21:53:39.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:53:41.297: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.022284035s
    Mar 15 21:53:41.297: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:53:43.300: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.024902527s
    Mar 15 21:53:43.300: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:53:45.297: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.022349363s
    Mar 15 21:53:45.297: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:53:47.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.024148171s
    Mar 15 21:53:47.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 21:53:49.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.023914005s
    Mar 15 21:53:49.299: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 15 21:53:49.299: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 15 21:53:49.302: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1096" to be "running and ready"
    Mar 15 21:53:49.304: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.624925ms
    Mar 15 21:53:49.304: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 15 21:53:49.304: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 15 21:53:49.307: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1096" to be "running and ready"
    Mar 15 21:53:49.318: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.119196ms
    Mar 15 21:53:49.319: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 15 21:53:49.319: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/15/23 21:53:49.323
    Mar 15 21:53:49.330: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1096" to be "running"
    Mar 15 21:53:49.334: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.985847ms
    Mar 15 21:53:51.337: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007143605s
    Mar 15 21:53:53.338: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007782731s
    Mar 15 21:53:53.338: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 15 21:53:53.343: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 15 21:53:53.343: INFO: Breadth first check of 100.96.2.13 on host 172.20.75.105...
    Mar 15 21:53:53.345: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.87:9080/dial?request=hostname&protocol=udp&host=100.96.2.13&port=8081&tries=1'] Namespace:pod-network-test-1096 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:53:53.345: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:53:53.346: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:53:53.346: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-1096/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.87%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.96.2.13%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 15 21:53:53.485: INFO: Waiting for responses: map[]
    Mar 15 21:53:53.485: INFO: reached 100.96.2.13 after 0/1 tries
    Mar 15 21:53:53.485: INFO: Breadth first check of 100.96.1.220 on host 172.20.60.208...
    Mar 15 21:53:53.488: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.87:9080/dial?request=hostname&protocol=udp&host=100.96.1.220&port=8081&tries=1'] Namespace:pod-network-test-1096 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:53:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:53:53.489: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:53:53.489: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-1096/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.87%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.96.1.220%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 15 21:53:53.605: INFO: Waiting for responses: map[]
    Mar 15 21:53:53.605: INFO: reached 100.96.1.220 after 0/1 tries
    Mar 15 21:53:53.605: INFO: Breadth first check of 100.96.3.61 on host 172.20.126.23...
    Mar 15 21:53:53.608: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.87:9080/dial?request=hostname&protocol=udp&host=100.96.3.61&port=8081&tries=1'] Namespace:pod-network-test-1096 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 21:53:53.608: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 21:53:53.609: INFO: ExecWithOptions: Clientset creation
    Mar 15 21:53:53.609: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-1096/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.87%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.96.3.61%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 15 21:53:53.706: INFO: Waiting for responses: map[]
    Mar 15 21:53:53.706: INFO: reached 100.96.3.61 after 0/1 tries
    Mar 15 21:53:53.706: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:53:53.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1096" for this suite. 03/15/23 21:53:53.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:53:53.717
Mar 15 21:53:53.717: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename csiinlinevolumes 03/15/23 21:53:53.718
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:53.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:53.733
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 03/15/23 21:53:53.737
STEP: getting 03/15/23 21:53:53.756
STEP: listing 03/15/23 21:53:53.762
STEP: deleting 03/15/23 21:53:53.765
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Mar 15 21:53:53.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6683" for this suite. 03/15/23 21:53:53.805
------------------------------
• [0.098 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:53:53.717
    Mar 15 21:53:53.717: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename csiinlinevolumes 03/15/23 21:53:53.718
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:53.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:53.733
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 03/15/23 21:53:53.737
    STEP: getting 03/15/23 21:53:53.756
    STEP: listing 03/15/23 21:53:53.762
    STEP: deleting 03/15/23 21:53:53.765
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:53:53.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6683" for this suite. 03/15/23 21:53:53.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:53:53.819
Mar 15 21:53:53.819: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 21:53:53.82
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:53.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:53.843
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 03/15/23 21:53:53.846
Mar 15 21:53:53.856: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95" in namespace "downward-api-3725" to be "Succeeded or Failed"
Mar 15 21:53:53.859: INFO: Pod "downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95": Phase="Pending", Reason="", readiness=false. Elapsed: 3.689302ms
Mar 15 21:53:55.863: INFO: Pod "downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007322012s
Mar 15 21:53:57.863: INFO: Pod "downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007746074s
STEP: Saw pod success 03/15/23 21:53:57.863
Mar 15 21:53:57.864: INFO: Pod "downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95" satisfied condition "Succeeded or Failed"
Mar 15 21:53:57.868: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95 container client-container: <nil>
STEP: delete the pod 03/15/23 21:53:57.878
Mar 15 21:53:57.895: INFO: Waiting for pod downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95 to disappear
Mar 15 21:53:57.898: INFO: Pod downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 15 21:53:57.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3725" for this suite. 03/15/23 21:53:57.903
------------------------------
• [4.090 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:53:53.819
    Mar 15 21:53:53.819: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 21:53:53.82
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:53.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:53.843
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 03/15/23 21:53:53.846
    Mar 15 21:53:53.856: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95" in namespace "downward-api-3725" to be "Succeeded or Failed"
    Mar 15 21:53:53.859: INFO: Pod "downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95": Phase="Pending", Reason="", readiness=false. Elapsed: 3.689302ms
    Mar 15 21:53:55.863: INFO: Pod "downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007322012s
    Mar 15 21:53:57.863: INFO: Pod "downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007746074s
    STEP: Saw pod success 03/15/23 21:53:57.863
    Mar 15 21:53:57.864: INFO: Pod "downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95" satisfied condition "Succeeded or Failed"
    Mar 15 21:53:57.868: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95 container client-container: <nil>
    STEP: delete the pod 03/15/23 21:53:57.878
    Mar 15 21:53:57.895: INFO: Waiting for pod downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95 to disappear
    Mar 15 21:53:57.898: INFO: Pod downwardapi-volume-d33607cc-5c58-49ba-8f17-3d7ff8677f95 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:53:57.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3725" for this suite. 03/15/23 21:53:57.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:53:57.911
Mar 15 21:53:57.911: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename sched-pred 03/15/23 21:53:57.912
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:57.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:57.932
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 15 21:53:57.935: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 15 21:53:57.942: INFO: Waiting for terminating namespaces to be deleted...
Mar 15 21:53:57.953: INFO: 
Logging pods the apiserver thinks is on node i-077ee0eb7ec5a02aa before test
Mar 15 21:53:57.991: INFO: etcd1 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:57.991: INFO: 	Container etcd1 ready: true, restart count 0
Mar 15 21:53:57.991: INFO: cilium-hvkmm from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:57.991: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 15 21:53:57.991: INFO: coredns-85dfcfb87f-p4x25 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:57.991: INFO: 	Container coredns ready: true, restart count 0
Mar 15 21:53:57.991: INFO: coredns-autoscaler-7cb5c5b969-tn76l from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:57.991: INFO: 	Container autoscaler ready: true, restart count 0
Mar 15 21:53:57.991: INFO: ebs-csi-node-9zwns from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
Mar 15 21:53:57.991: INFO: 	Container ebs-plugin ready: true, restart count 0
Mar 15 21:53:57.991: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 21:53:57.991: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 21:53:57.991: INFO: kube-proxy-i-077ee0eb7ec5a02aa from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:57.991: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 15 21:53:57.991: INFO: metrics-server-79b7f8b6d9-g4qd4 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:57.991: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 21:53:57.991: INFO: netserver-0 from pod-network-test-1096 started at 2023-03-15 21:53:27 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:57.991: INFO: 	Container webserver ready: true, restart count 0
Mar 15 21:53:57.991: INFO: sonobuoy-e2e-job-cc297e458b6c49af from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 21:53:57.991: INFO: 	Container e2e ready: true, restart count 0
Mar 15 21:53:57.991: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 21:53:57.991: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-hkt9j from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 21:53:57.991: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 21:53:57.991: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 21:53:57.991: INFO: 
Logging pods the apiserver thinks is on node i-0baafb3f4e7bf826e before test
Mar 15 21:53:58.000: INFO: csi-mockplugin-85d5674684-nwzd7 from default started at 2023-03-15 21:42:36 +0000 UTC (7 container statuses recorded)
Mar 15 21:53:58.000: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 15 21:53:58.000: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 15 21:53:58.000: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 15 21:53:58.000: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 15 21:53:58.000: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 21:53:58.000: INFO: 	Container mock-driver ready: true, restart count 0
Mar 15 21:53:58.000: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 21:53:58.000: INFO: etcd2 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.000: INFO: 	Container etcd2 ready: true, restart count 0
Mar 15 21:53:58.000: INFO: web-server from default started at 2023-03-15 21:42:51 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.000: INFO: 	Container web-server ready: true, restart count 0
Mar 15 21:53:58.000: INFO: cilium-8q84b from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.000: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 15 21:53:58.000: INFO: ebs-csi-node-8vkhc from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
Mar 15 21:53:58.000: INFO: 	Container ebs-plugin ready: true, restart count 0
Mar 15 21:53:58.000: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 21:53:58.000: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 21:53:58.000: INFO: kube-proxy-i-0baafb3f4e7bf826e from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.000: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 15 21:53:58.000: INFO: metrics-server-79b7f8b6d9-dgk5h from kube-system started at 2023-03-15 21:41:14 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.000: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 21:53:58.000: INFO: netserver-1 from pod-network-test-1096 started at 2023-03-15 21:53:27 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.000: INFO: 	Container webserver ready: true, restart count 0
Mar 15 21:53:58.000: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-49vvc from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 21:53:58.000: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 21:53:58.000: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 21:53:58.000: INFO: 
Logging pods the apiserver thinks is on node i-0faaf83f00b43c88c before test
Mar 15 21:53:58.016: INFO: etcd0 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.016: INFO: 	Container etcd0 ready: true, restart count 0
Mar 15 21:53:58.016: INFO: cilium-nht5t from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.016: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 15 21:53:58.016: INFO: coredns-85dfcfb87f-b7q7b from kube-system started at 2023-03-15 21:41:25 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.016: INFO: 	Container coredns ready: true, restart count 0
Mar 15 21:53:58.016: INFO: ebs-csi-node-kkqjj from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
Mar 15 21:53:58.016: INFO: 	Container ebs-plugin ready: true, restart count 0
Mar 15 21:53:58.016: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 21:53:58.016: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 21:53:58.016: INFO: kube-proxy-i-0faaf83f00b43c88c from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.016: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 15 21:53:58.016: INFO: netserver-2 from pod-network-test-1096 started at 2023-03-15 21:53:27 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.016: INFO: 	Container webserver ready: true, restart count 0
Mar 15 21:53:58.016: INFO: test-container-pod from pod-network-test-1096 started at 2023-03-15 21:53:49 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.016: INFO: 	Container webserver ready: true, restart count 0
Mar 15 21:53:58.016: INFO: sonobuoy from sonobuoy started at 2023-03-15 21:43:05 +0000 UTC (1 container statuses recorded)
Mar 15 21:53:58.016: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 15 21:53:58.016: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-dc6rq from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 21:53:58.016: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 21:53:58.016: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/15/23 21:53:58.017
Mar 15 21:53:58.025: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6919" to be "running"
Mar 15 21:53:58.028: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.281349ms
Mar 15 21:54:00.032: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006788348s
Mar 15 21:54:00.032: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/15/23 21:54:00.04
STEP: Trying to apply a random label on the found node. 03/15/23 21:54:00.062
STEP: verifying the node has the label kubernetes.io/e2e-87bafd6b-7379-43a7-bca0-e6de2ca6ea10 42 03/15/23 21:54:00.089
STEP: Trying to relaunch the pod, now with labels. 03/15/23 21:54:00.103
Mar 15 21:54:00.114: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6919" to be "not pending"
Mar 15 21:54:00.119: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.663473ms
Mar 15 21:54:02.134: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.019190263s
Mar 15 21:54:02.134: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-87bafd6b-7379-43a7-bca0-e6de2ca6ea10 off the node i-0faaf83f00b43c88c 03/15/23 21:54:02.139
STEP: verifying the node doesn't have the label kubernetes.io/e2e-87bafd6b-7379-43a7-bca0-e6de2ca6ea10 03/15/23 21:54:02.17
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:54:02.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6919" for this suite. 03/15/23 21:54:02.198
------------------------------
• [4.303 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:53:57.911
    Mar 15 21:53:57.911: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename sched-pred 03/15/23 21:53:57.912
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:53:57.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:53:57.932
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 15 21:53:57.935: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 15 21:53:57.942: INFO: Waiting for terminating namespaces to be deleted...
    Mar 15 21:53:57.953: INFO: 
    Logging pods the apiserver thinks is on node i-077ee0eb7ec5a02aa before test
    Mar 15 21:53:57.991: INFO: etcd1 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:57.991: INFO: 	Container etcd1 ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: cilium-hvkmm from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:57.991: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: coredns-85dfcfb87f-p4x25 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:57.991: INFO: 	Container coredns ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: coredns-autoscaler-7cb5c5b969-tn76l from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:57.991: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: ebs-csi-node-9zwns from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
    Mar 15 21:53:57.991: INFO: 	Container ebs-plugin ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: kube-proxy-i-077ee0eb7ec5a02aa from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:57.991: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: metrics-server-79b7f8b6d9-g4qd4 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:57.991: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: netserver-0 from pod-network-test-1096 started at 2023-03-15 21:53:27 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:57.991: INFO: 	Container webserver ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: sonobuoy-e2e-job-cc297e458b6c49af from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 21:53:57.991: INFO: 	Container e2e ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-hkt9j from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 21:53:57.991: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 15 21:53:57.991: INFO: 
    Logging pods the apiserver thinks is on node i-0baafb3f4e7bf826e before test
    Mar 15 21:53:58.000: INFO: csi-mockplugin-85d5674684-nwzd7 from default started at 2023-03-15 21:42:36 +0000 UTC (7 container statuses recorded)
    Mar 15 21:53:58.000: INFO: 	Container csi-attacher ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: 	Container csi-resizer ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: 	Container mock-driver ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: etcd2 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.000: INFO: 	Container etcd2 ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: web-server from default started at 2023-03-15 21:42:51 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.000: INFO: 	Container web-server ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: cilium-8q84b from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.000: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: ebs-csi-node-8vkhc from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
    Mar 15 21:53:58.000: INFO: 	Container ebs-plugin ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: kube-proxy-i-0baafb3f4e7bf826e from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.000: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: metrics-server-79b7f8b6d9-dgk5h from kube-system started at 2023-03-15 21:41:14 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.000: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: netserver-1 from pod-network-test-1096 started at 2023-03-15 21:53:27 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.000: INFO: 	Container webserver ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-49vvc from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 21:53:58.000: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 15 21:53:58.000: INFO: 
    Logging pods the apiserver thinks is on node i-0faaf83f00b43c88c before test
    Mar 15 21:53:58.016: INFO: etcd0 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.016: INFO: 	Container etcd0 ready: true, restart count 0
    Mar 15 21:53:58.016: INFO: cilium-nht5t from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.016: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 15 21:53:58.016: INFO: coredns-85dfcfb87f-b7q7b from kube-system started at 2023-03-15 21:41:25 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.016: INFO: 	Container coredns ready: true, restart count 0
    Mar 15 21:53:58.016: INFO: ebs-csi-node-kkqjj from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
    Mar 15 21:53:58.016: INFO: 	Container ebs-plugin ready: true, restart count 0
    Mar 15 21:53:58.016: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 21:53:58.016: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 21:53:58.016: INFO: kube-proxy-i-0faaf83f00b43c88c from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.016: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 15 21:53:58.016: INFO: netserver-2 from pod-network-test-1096 started at 2023-03-15 21:53:27 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.016: INFO: 	Container webserver ready: true, restart count 0
    Mar 15 21:53:58.016: INFO: test-container-pod from pod-network-test-1096 started at 2023-03-15 21:53:49 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.016: INFO: 	Container webserver ready: true, restart count 0
    Mar 15 21:53:58.016: INFO: sonobuoy from sonobuoy started at 2023-03-15 21:43:05 +0000 UTC (1 container statuses recorded)
    Mar 15 21:53:58.016: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 15 21:53:58.016: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-dc6rq from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 21:53:58.016: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 21:53:58.016: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/15/23 21:53:58.017
    Mar 15 21:53:58.025: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6919" to be "running"
    Mar 15 21:53:58.028: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.281349ms
    Mar 15 21:54:00.032: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006788348s
    Mar 15 21:54:00.032: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/15/23 21:54:00.04
    STEP: Trying to apply a random label on the found node. 03/15/23 21:54:00.062
    STEP: verifying the node has the label kubernetes.io/e2e-87bafd6b-7379-43a7-bca0-e6de2ca6ea10 42 03/15/23 21:54:00.089
    STEP: Trying to relaunch the pod, now with labels. 03/15/23 21:54:00.103
    Mar 15 21:54:00.114: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6919" to be "not pending"
    Mar 15 21:54:00.119: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.663473ms
    Mar 15 21:54:02.134: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.019190263s
    Mar 15 21:54:02.134: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-87bafd6b-7379-43a7-bca0-e6de2ca6ea10 off the node i-0faaf83f00b43c88c 03/15/23 21:54:02.139
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-87bafd6b-7379-43a7-bca0-e6de2ca6ea10 03/15/23 21:54:02.17
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:54:02.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6919" for this suite. 03/15/23 21:54:02.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:54:02.215
Mar 15 21:54:02.215: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename prestop 03/15/23 21:54:02.216
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:02.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:02.242
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-7907 03/15/23 21:54:02.249
STEP: Waiting for pods to come up. 03/15/23 21:54:02.266
Mar 15 21:54:02.266: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-7907" to be "running"
Mar 15 21:54:02.272: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.277193ms
Mar 15 21:54:04.278: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.011194786s
Mar 15 21:54:04.278: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-7907 03/15/23 21:54:04.281
Mar 15 21:54:04.286: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-7907" to be "running"
Mar 15 21:54:04.293: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 7.518361ms
Mar 15 21:54:06.299: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01279102s
Mar 15 21:54:08.298: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.011870041s
Mar 15 21:54:08.298: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 03/15/23 21:54:08.298
Mar 15 21:54:13.308: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 03/15/23 21:54:13.308
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Mar 15 21:54:13.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-7907" for this suite. 03/15/23 21:54:13.341
------------------------------
• [SLOW TEST] [11.149 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:54:02.215
    Mar 15 21:54:02.215: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename prestop 03/15/23 21:54:02.216
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:02.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:02.242
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-7907 03/15/23 21:54:02.249
    STEP: Waiting for pods to come up. 03/15/23 21:54:02.266
    Mar 15 21:54:02.266: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-7907" to be "running"
    Mar 15 21:54:02.272: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.277193ms
    Mar 15 21:54:04.278: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.011194786s
    Mar 15 21:54:04.278: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-7907 03/15/23 21:54:04.281
    Mar 15 21:54:04.286: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-7907" to be "running"
    Mar 15 21:54:04.293: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 7.518361ms
    Mar 15 21:54:06.299: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01279102s
    Mar 15 21:54:08.298: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.011870041s
    Mar 15 21:54:08.298: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 03/15/23 21:54:08.298
    Mar 15 21:54:13.308: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 03/15/23 21:54:13.308
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:54:13.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-7907" for this suite. 03/15/23 21:54:13.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:54:13.367
Mar 15 21:54:13.367: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename var-expansion 03/15/23 21:54:13.368
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:13.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:13.386
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Mar 15 21:54:13.396: INFO: Waiting up to 2m0s for pod "var-expansion-f2423099-5c63-4bfe-98ae-628b51341734" in namespace "var-expansion-7114" to be "container 0 failed with reason CreateContainerConfigError"
Mar 15 21:54:13.402: INFO: Pod "var-expansion-f2423099-5c63-4bfe-98ae-628b51341734": Phase="Pending", Reason="", readiness=false. Elapsed: 5.895948ms
Mar 15 21:54:15.406: INFO: Pod "var-expansion-f2423099-5c63-4bfe-98ae-628b51341734": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009733518s
Mar 15 21:54:15.406: INFO: Pod "var-expansion-f2423099-5c63-4bfe-98ae-628b51341734" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 15 21:54:15.406: INFO: Deleting pod "var-expansion-f2423099-5c63-4bfe-98ae-628b51341734" in namespace "var-expansion-7114"
Mar 15 21:54:15.413: INFO: Wait up to 5m0s for pod "var-expansion-f2423099-5c63-4bfe-98ae-628b51341734" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 15 21:54:17.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7114" for this suite. 03/15/23 21:54:17.425
------------------------------
• [4.063 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:54:13.367
    Mar 15 21:54:13.367: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename var-expansion 03/15/23 21:54:13.368
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:13.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:13.386
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Mar 15 21:54:13.396: INFO: Waiting up to 2m0s for pod "var-expansion-f2423099-5c63-4bfe-98ae-628b51341734" in namespace "var-expansion-7114" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 15 21:54:13.402: INFO: Pod "var-expansion-f2423099-5c63-4bfe-98ae-628b51341734": Phase="Pending", Reason="", readiness=false. Elapsed: 5.895948ms
    Mar 15 21:54:15.406: INFO: Pod "var-expansion-f2423099-5c63-4bfe-98ae-628b51341734": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009733518s
    Mar 15 21:54:15.406: INFO: Pod "var-expansion-f2423099-5c63-4bfe-98ae-628b51341734" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 15 21:54:15.406: INFO: Deleting pod "var-expansion-f2423099-5c63-4bfe-98ae-628b51341734" in namespace "var-expansion-7114"
    Mar 15 21:54:15.413: INFO: Wait up to 5m0s for pod "var-expansion-f2423099-5c63-4bfe-98ae-628b51341734" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:54:17.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7114" for this suite. 03/15/23 21:54:17.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:54:17.431
Mar 15 21:54:17.431: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 21:54:17.432
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:17.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:17.449
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 03/15/23 21:54:17.452
Mar 15 21:54:17.459: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75" in namespace "projected-374" to be "Succeeded or Failed"
Mar 15 21:54:17.464: INFO: Pod "downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75": Phase="Pending", Reason="", readiness=false. Elapsed: 4.69691ms
Mar 15 21:54:19.467: INFO: Pod "downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75": Phase="Running", Reason="", readiness=true. Elapsed: 2.007685493s
Mar 15 21:54:21.467: INFO: Pod "downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75": Phase="Running", Reason="", readiness=false. Elapsed: 4.007780976s
Mar 15 21:54:23.467: INFO: Pod "downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007912497s
STEP: Saw pod success 03/15/23 21:54:23.467
Mar 15 21:54:23.467: INFO: Pod "downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75" satisfied condition "Succeeded or Failed"
Mar 15 21:54:23.470: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75 container client-container: <nil>
STEP: delete the pod 03/15/23 21:54:23.477
Mar 15 21:54:23.490: INFO: Waiting for pod downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75 to disappear
Mar 15 21:54:23.493: INFO: Pod downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 15 21:54:23.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-374" for this suite. 03/15/23 21:54:23.496
------------------------------
• [SLOW TEST] [6.070 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:54:17.431
    Mar 15 21:54:17.431: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 21:54:17.432
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:17.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:17.449
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 03/15/23 21:54:17.452
    Mar 15 21:54:17.459: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75" in namespace "projected-374" to be "Succeeded or Failed"
    Mar 15 21:54:17.464: INFO: Pod "downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75": Phase="Pending", Reason="", readiness=false. Elapsed: 4.69691ms
    Mar 15 21:54:19.467: INFO: Pod "downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75": Phase="Running", Reason="", readiness=true. Elapsed: 2.007685493s
    Mar 15 21:54:21.467: INFO: Pod "downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75": Phase="Running", Reason="", readiness=false. Elapsed: 4.007780976s
    Mar 15 21:54:23.467: INFO: Pod "downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007912497s
    STEP: Saw pod success 03/15/23 21:54:23.467
    Mar 15 21:54:23.467: INFO: Pod "downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75" satisfied condition "Succeeded or Failed"
    Mar 15 21:54:23.470: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75 container client-container: <nil>
    STEP: delete the pod 03/15/23 21:54:23.477
    Mar 15 21:54:23.490: INFO: Waiting for pod downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75 to disappear
    Mar 15 21:54:23.493: INFO: Pod downwardapi-volume-6fae5324-f352-48e6-9051-f288c0d4ae75 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:54:23.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-374" for this suite. 03/15/23 21:54:23.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:54:23.502
Mar 15 21:54:23.502: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 21:54:23.503
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:23.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:23.516
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-4509420f-e8c2-48e2-bbd1-4cfc35c5f6df 03/15/23 21:54:23.519
STEP: Creating a pod to test consume secrets 03/15/23 21:54:23.524
Mar 15 21:54:23.533: INFO: Waiting up to 5m0s for pod "pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81" in namespace "secrets-2800" to be "Succeeded or Failed"
Mar 15 21:54:23.536: INFO: Pod "pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81": Phase="Pending", Reason="", readiness=false. Elapsed: 3.279734ms
Mar 15 21:54:25.540: INFO: Pod "pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007576929s
Mar 15 21:54:27.540: INFO: Pod "pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007476035s
STEP: Saw pod success 03/15/23 21:54:27.54
Mar 15 21:54:27.541: INFO: Pod "pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81" satisfied condition "Succeeded or Failed"
Mar 15 21:54:27.543: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81 container secret-volume-test: <nil>
STEP: delete the pod 03/15/23 21:54:27.549
Mar 15 21:54:27.560: INFO: Waiting for pod pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81 to disappear
Mar 15 21:54:27.562: INFO: Pod pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 21:54:27.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2800" for this suite. 03/15/23 21:54:27.566
------------------------------
• [4.070 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:54:23.502
    Mar 15 21:54:23.502: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 21:54:23.503
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:23.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:23.516
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-4509420f-e8c2-48e2-bbd1-4cfc35c5f6df 03/15/23 21:54:23.519
    STEP: Creating a pod to test consume secrets 03/15/23 21:54:23.524
    Mar 15 21:54:23.533: INFO: Waiting up to 5m0s for pod "pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81" in namespace "secrets-2800" to be "Succeeded or Failed"
    Mar 15 21:54:23.536: INFO: Pod "pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81": Phase="Pending", Reason="", readiness=false. Elapsed: 3.279734ms
    Mar 15 21:54:25.540: INFO: Pod "pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007576929s
    Mar 15 21:54:27.540: INFO: Pod "pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007476035s
    STEP: Saw pod success 03/15/23 21:54:27.54
    Mar 15 21:54:27.541: INFO: Pod "pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81" satisfied condition "Succeeded or Failed"
    Mar 15 21:54:27.543: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81 container secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 21:54:27.549
    Mar 15 21:54:27.560: INFO: Waiting for pod pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81 to disappear
    Mar 15 21:54:27.562: INFO: Pod pod-secrets-d51deada-d116-4886-af21-59f2fffe8d81 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:54:27.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2800" for this suite. 03/15/23 21:54:27.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:54:27.576
Mar 15 21:54:27.576: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename namespaces 03/15/23 21:54:27.577
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:27.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:27.593
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 03/15/23 21:54:27.596
Mar 15 21:54:27.598: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 03/15/23 21:54:27.599
Mar 15 21:54:27.603: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 03/15/23 21:54:27.603
Mar 15 21:54:27.612: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:54:27.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9203" for this suite. 03/15/23 21:54:27.616
------------------------------
• [0.046 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:54:27.576
    Mar 15 21:54:27.576: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename namespaces 03/15/23 21:54:27.577
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:27.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:27.593
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 03/15/23 21:54:27.596
    Mar 15 21:54:27.598: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 03/15/23 21:54:27.599
    Mar 15 21:54:27.603: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 03/15/23 21:54:27.603
    Mar 15 21:54:27.612: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:54:27.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9203" for this suite. 03/15/23 21:54:27.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:54:27.623
Mar 15 21:54:27.623: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 21:54:27.624
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:27.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:27.637
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 03/15/23 21:54:27.64
Mar 15 21:54:27.648: INFO: Waiting up to 5m0s for pod "labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44" in namespace "projected-6395" to be "running and ready"
Mar 15 21:54:27.655: INFO: Pod "labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44": Phase="Pending", Reason="", readiness=false. Elapsed: 6.864783ms
Mar 15 21:54:27.655: INFO: The phase of Pod labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:54:29.659: INFO: Pod "labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44": Phase="Running", Reason="", readiness=true. Elapsed: 2.01073572s
Mar 15 21:54:29.659: INFO: The phase of Pod labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44 is Running (Ready = true)
Mar 15 21:54:29.659: INFO: Pod "labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44" satisfied condition "running and ready"
Mar 15 21:54:30.181: INFO: Successfully updated pod "labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 15 21:54:34.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6395" for this suite. 03/15/23 21:54:34.22
------------------------------
• [SLOW TEST] [6.604 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:54:27.623
    Mar 15 21:54:27.623: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 21:54:27.624
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:27.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:27.637
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 03/15/23 21:54:27.64
    Mar 15 21:54:27.648: INFO: Waiting up to 5m0s for pod "labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44" in namespace "projected-6395" to be "running and ready"
    Mar 15 21:54:27.655: INFO: Pod "labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44": Phase="Pending", Reason="", readiness=false. Elapsed: 6.864783ms
    Mar 15 21:54:27.655: INFO: The phase of Pod labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:54:29.659: INFO: Pod "labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44": Phase="Running", Reason="", readiness=true. Elapsed: 2.01073572s
    Mar 15 21:54:29.659: INFO: The phase of Pod labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44 is Running (Ready = true)
    Mar 15 21:54:29.659: INFO: Pod "labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44" satisfied condition "running and ready"
    Mar 15 21:54:30.181: INFO: Successfully updated pod "labelsupdate04acf37a-7aa3-4385-b3ff-7ce566fffa44"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:54:34.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6395" for this suite. 03/15/23 21:54:34.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:54:34.23
Mar 15 21:54:34.230: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename replication-controller 03/15/23 21:54:34.231
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:34.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:34.251
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-mbqmm" 03/15/23 21:54:34.254
Mar 15 21:54:34.260: INFO: Get Replication Controller "e2e-rc-mbqmm" to confirm replicas
Mar 15 21:54:35.267: INFO: Get Replication Controller "e2e-rc-mbqmm" to confirm replicas
Mar 15 21:54:35.270: INFO: Found 1 replicas for "e2e-rc-mbqmm" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-mbqmm" 03/15/23 21:54:35.27
STEP: Updating a scale subresource 03/15/23 21:54:35.272
STEP: Verifying replicas where modified for replication controller "e2e-rc-mbqmm" 03/15/23 21:54:35.278
Mar 15 21:54:35.279: INFO: Get Replication Controller "e2e-rc-mbqmm" to confirm replicas
Mar 15 21:54:36.281: INFO: Get Replication Controller "e2e-rc-mbqmm" to confirm replicas
Mar 15 21:54:36.285: INFO: Found 2 replicas for "e2e-rc-mbqmm" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 15 21:54:36.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5320" for this suite. 03/15/23 21:54:36.289
------------------------------
• [2.064 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:54:34.23
    Mar 15 21:54:34.230: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename replication-controller 03/15/23 21:54:34.231
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:34.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:34.251
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-mbqmm" 03/15/23 21:54:34.254
    Mar 15 21:54:34.260: INFO: Get Replication Controller "e2e-rc-mbqmm" to confirm replicas
    Mar 15 21:54:35.267: INFO: Get Replication Controller "e2e-rc-mbqmm" to confirm replicas
    Mar 15 21:54:35.270: INFO: Found 1 replicas for "e2e-rc-mbqmm" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-mbqmm" 03/15/23 21:54:35.27
    STEP: Updating a scale subresource 03/15/23 21:54:35.272
    STEP: Verifying replicas where modified for replication controller "e2e-rc-mbqmm" 03/15/23 21:54:35.278
    Mar 15 21:54:35.279: INFO: Get Replication Controller "e2e-rc-mbqmm" to confirm replicas
    Mar 15 21:54:36.281: INFO: Get Replication Controller "e2e-rc-mbqmm" to confirm replicas
    Mar 15 21:54:36.285: INFO: Found 2 replicas for "e2e-rc-mbqmm" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:54:36.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5320" for this suite. 03/15/23 21:54:36.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:54:36.294
Mar 15 21:54:36.294: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 21:54:36.297
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:36.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:36.318
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-cebb5fce-3be8-4906-971d-b5378a85382e 03/15/23 21:54:36.325
STEP: Creating secret with name s-test-opt-upd-ef050d24-0a86-4a3e-afa7-529fb8ee2b57 03/15/23 21:54:36.329
STEP: Creating the pod 03/15/23 21:54:36.334
Mar 15 21:54:36.345: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ba1d987f-de43-4938-99d8-6c94d4e82ce8" in namespace "projected-3250" to be "running and ready"
Mar 15 21:54:36.355: INFO: Pod "pod-projected-secrets-ba1d987f-de43-4938-99d8-6c94d4e82ce8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.273272ms
Mar 15 21:54:36.355: INFO: The phase of Pod pod-projected-secrets-ba1d987f-de43-4938-99d8-6c94d4e82ce8 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:54:38.359: INFO: Pod "pod-projected-secrets-ba1d987f-de43-4938-99d8-6c94d4e82ce8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013277843s
Mar 15 21:54:38.359: INFO: The phase of Pod pod-projected-secrets-ba1d987f-de43-4938-99d8-6c94d4e82ce8 is Running (Ready = true)
Mar 15 21:54:38.359: INFO: Pod "pod-projected-secrets-ba1d987f-de43-4938-99d8-6c94d4e82ce8" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-cebb5fce-3be8-4906-971d-b5378a85382e 03/15/23 21:54:38.382
STEP: Updating secret s-test-opt-upd-ef050d24-0a86-4a3e-afa7-529fb8ee2b57 03/15/23 21:54:38.387
STEP: Creating secret with name s-test-opt-create-c112131d-b30a-4404-a8e8-4384901107c7 03/15/23 21:54:38.399
STEP: waiting to observe update in volume 03/15/23 21:54:38.404
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 15 21:54:40.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3250" for this suite. 03/15/23 21:54:40.438
------------------------------
• [4.150 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:54:36.294
    Mar 15 21:54:36.294: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 21:54:36.297
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:36.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:36.318
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-cebb5fce-3be8-4906-971d-b5378a85382e 03/15/23 21:54:36.325
    STEP: Creating secret with name s-test-opt-upd-ef050d24-0a86-4a3e-afa7-529fb8ee2b57 03/15/23 21:54:36.329
    STEP: Creating the pod 03/15/23 21:54:36.334
    Mar 15 21:54:36.345: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ba1d987f-de43-4938-99d8-6c94d4e82ce8" in namespace "projected-3250" to be "running and ready"
    Mar 15 21:54:36.355: INFO: Pod "pod-projected-secrets-ba1d987f-de43-4938-99d8-6c94d4e82ce8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.273272ms
    Mar 15 21:54:36.355: INFO: The phase of Pod pod-projected-secrets-ba1d987f-de43-4938-99d8-6c94d4e82ce8 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:54:38.359: INFO: Pod "pod-projected-secrets-ba1d987f-de43-4938-99d8-6c94d4e82ce8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013277843s
    Mar 15 21:54:38.359: INFO: The phase of Pod pod-projected-secrets-ba1d987f-de43-4938-99d8-6c94d4e82ce8 is Running (Ready = true)
    Mar 15 21:54:38.359: INFO: Pod "pod-projected-secrets-ba1d987f-de43-4938-99d8-6c94d4e82ce8" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-cebb5fce-3be8-4906-971d-b5378a85382e 03/15/23 21:54:38.382
    STEP: Updating secret s-test-opt-upd-ef050d24-0a86-4a3e-afa7-529fb8ee2b57 03/15/23 21:54:38.387
    STEP: Creating secret with name s-test-opt-create-c112131d-b30a-4404-a8e8-4384901107c7 03/15/23 21:54:38.399
    STEP: waiting to observe update in volume 03/15/23 21:54:38.404
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:54:40.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3250" for this suite. 03/15/23 21:54:40.438
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:54:40.445
Mar 15 21:54:40.445: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 21:54:40.446
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:40.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:40.465
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Mar 15 21:54:40.470: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/15/23 21:54:42.066
Mar 15 21:54:42.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-9140 --namespace=crd-publish-openapi-9140 create -f -'
Mar 15 21:54:42.653: INFO: stderr: ""
Mar 15 21:54:42.653: INFO: stdout: "e2e-test-crd-publish-openapi-9290-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 15 21:54:42.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-9140 --namespace=crd-publish-openapi-9140 delete e2e-test-crd-publish-openapi-9290-crds test-cr'
Mar 15 21:54:42.729: INFO: stderr: ""
Mar 15 21:54:42.730: INFO: stdout: "e2e-test-crd-publish-openapi-9290-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 15 21:54:42.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-9140 --namespace=crd-publish-openapi-9140 apply -f -'
Mar 15 21:54:42.966: INFO: stderr: ""
Mar 15 21:54:42.966: INFO: stdout: "e2e-test-crd-publish-openapi-9290-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 15 21:54:42.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-9140 --namespace=crd-publish-openapi-9140 delete e2e-test-crd-publish-openapi-9290-crds test-cr'
Mar 15 21:54:43.040: INFO: stderr: ""
Mar 15 21:54:43.040: INFO: stdout: "e2e-test-crd-publish-openapi-9290-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/15/23 21:54:43.04
Mar 15 21:54:43.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-9140 explain e2e-test-crd-publish-openapi-9290-crds'
Mar 15 21:54:43.250: INFO: stderr: ""
Mar 15 21:54:43.250: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9290-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:54:44.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9140" for this suite. 03/15/23 21:54:44.807
------------------------------
• [4.368 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:54:40.445
    Mar 15 21:54:40.445: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 21:54:40.446
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:40.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:40.465
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Mar 15 21:54:40.470: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/15/23 21:54:42.066
    Mar 15 21:54:42.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-9140 --namespace=crd-publish-openapi-9140 create -f -'
    Mar 15 21:54:42.653: INFO: stderr: ""
    Mar 15 21:54:42.653: INFO: stdout: "e2e-test-crd-publish-openapi-9290-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 15 21:54:42.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-9140 --namespace=crd-publish-openapi-9140 delete e2e-test-crd-publish-openapi-9290-crds test-cr'
    Mar 15 21:54:42.729: INFO: stderr: ""
    Mar 15 21:54:42.730: INFO: stdout: "e2e-test-crd-publish-openapi-9290-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Mar 15 21:54:42.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-9140 --namespace=crd-publish-openapi-9140 apply -f -'
    Mar 15 21:54:42.966: INFO: stderr: ""
    Mar 15 21:54:42.966: INFO: stdout: "e2e-test-crd-publish-openapi-9290-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 15 21:54:42.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-9140 --namespace=crd-publish-openapi-9140 delete e2e-test-crd-publish-openapi-9290-crds test-cr'
    Mar 15 21:54:43.040: INFO: stderr: ""
    Mar 15 21:54:43.040: INFO: stdout: "e2e-test-crd-publish-openapi-9290-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/15/23 21:54:43.04
    Mar 15 21:54:43.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-9140 explain e2e-test-crd-publish-openapi-9290-crds'
    Mar 15 21:54:43.250: INFO: stderr: ""
    Mar 15 21:54:43.250: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9290-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:54:44.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9140" for this suite. 03/15/23 21:54:44.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:54:44.815
Mar 15 21:54:44.815: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 21:54:44.816
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:44.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:44.834
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 03/15/23 21:54:44.837
Mar 15 21:54:44.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1" in namespace "downward-api-9573" to be "Succeeded or Failed"
Mar 15 21:54:44.850: INFO: Pod "downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027336ms
Mar 15 21:54:46.853: INFO: Pod "downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007629492s
Mar 15 21:54:48.858: INFO: Pod "downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012053302s
STEP: Saw pod success 03/15/23 21:54:48.858
Mar 15 21:54:48.858: INFO: Pod "downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1" satisfied condition "Succeeded or Failed"
Mar 15 21:54:48.861: INFO: Trying to get logs from node i-077ee0eb7ec5a02aa pod downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1 container client-container: <nil>
STEP: delete the pod 03/15/23 21:54:48.875
Mar 15 21:54:48.889: INFO: Waiting for pod downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1 to disappear
Mar 15 21:54:48.893: INFO: Pod downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 15 21:54:48.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9573" for this suite. 03/15/23 21:54:48.901
------------------------------
• [4.095 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:54:44.815
    Mar 15 21:54:44.815: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 21:54:44.816
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:44.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:44.834
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 03/15/23 21:54:44.837
    Mar 15 21:54:44.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1" in namespace "downward-api-9573" to be "Succeeded or Failed"
    Mar 15 21:54:44.850: INFO: Pod "downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027336ms
    Mar 15 21:54:46.853: INFO: Pod "downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007629492s
    Mar 15 21:54:48.858: INFO: Pod "downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012053302s
    STEP: Saw pod success 03/15/23 21:54:48.858
    Mar 15 21:54:48.858: INFO: Pod "downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1" satisfied condition "Succeeded or Failed"
    Mar 15 21:54:48.861: INFO: Trying to get logs from node i-077ee0eb7ec5a02aa pod downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1 container client-container: <nil>
    STEP: delete the pod 03/15/23 21:54:48.875
    Mar 15 21:54:48.889: INFO: Waiting for pod downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1 to disappear
    Mar 15 21:54:48.893: INFO: Pod downwardapi-volume-c857c61a-e3fc-4803-a398-ea40a79773e1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:54:48.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9573" for this suite. 03/15/23 21:54:48.901
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:54:48.912
Mar 15 21:54:48.913: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 21:54:48.913
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:48.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:48.937
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-bc02e4b5-0efa-463e-89b8-5e5db10cef3c 03/15/23 21:54:48.941
STEP: Creating a pod to test consume secrets 03/15/23 21:54:48.945
Mar 15 21:54:48.954: INFO: Waiting up to 5m0s for pod "pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67" in namespace "secrets-3282" to be "Succeeded or Failed"
Mar 15 21:54:48.958: INFO: Pod "pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.400258ms
Mar 15 21:54:50.962: INFO: Pod "pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008359578s
Mar 15 21:54:52.961: INFO: Pod "pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007523249s
STEP: Saw pod success 03/15/23 21:54:52.961
Mar 15 21:54:52.962: INFO: Pod "pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67" satisfied condition "Succeeded or Failed"
Mar 15 21:54:52.964: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67 container secret-volume-test: <nil>
STEP: delete the pod 03/15/23 21:54:52.971
Mar 15 21:54:52.985: INFO: Waiting for pod pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67 to disappear
Mar 15 21:54:52.988: INFO: Pod pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 21:54:52.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3282" for this suite. 03/15/23 21:54:52.993
------------------------------
• [4.085 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:54:48.912
    Mar 15 21:54:48.913: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 21:54:48.913
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:48.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:48.937
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-bc02e4b5-0efa-463e-89b8-5e5db10cef3c 03/15/23 21:54:48.941
    STEP: Creating a pod to test consume secrets 03/15/23 21:54:48.945
    Mar 15 21:54:48.954: INFO: Waiting up to 5m0s for pod "pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67" in namespace "secrets-3282" to be "Succeeded or Failed"
    Mar 15 21:54:48.958: INFO: Pod "pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.400258ms
    Mar 15 21:54:50.962: INFO: Pod "pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008359578s
    Mar 15 21:54:52.961: INFO: Pod "pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007523249s
    STEP: Saw pod success 03/15/23 21:54:52.961
    Mar 15 21:54:52.962: INFO: Pod "pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67" satisfied condition "Succeeded or Failed"
    Mar 15 21:54:52.964: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67 container secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 21:54:52.971
    Mar 15 21:54:52.985: INFO: Waiting for pod pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67 to disappear
    Mar 15 21:54:52.988: INFO: Pod pod-secrets-36f45fcf-3372-42a8-91a0-f5138bff8e67 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:54:52.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3282" for this suite. 03/15/23 21:54:52.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:54:53.002
Mar 15 21:54:53.002: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-probe 03/15/23 21:54:53.003
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:53.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:53.023
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274 in namespace container-probe-2794 03/15/23 21:54:53.033
Mar 15 21:54:53.049: INFO: Waiting up to 5m0s for pod "busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274" in namespace "container-probe-2794" to be "not pending"
Mar 15 21:54:53.053: INFO: Pod "busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061403ms
Mar 15 21:54:55.057: INFO: Pod "busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274": Phase="Running", Reason="", readiness=true. Elapsed: 2.007549315s
Mar 15 21:54:55.057: INFO: Pod "busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274" satisfied condition "not pending"
Mar 15 21:54:55.057: INFO: Started pod busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274 in namespace container-probe-2794
STEP: checking the pod's current state and verifying that restartCount is present 03/15/23 21:54:55.057
Mar 15 21:54:55.060: INFO: Initial restart count of pod busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274 is 0
STEP: deleting the pod 03/15/23 21:58:55.596
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 15 21:58:55.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2794" for this suite. 03/15/23 21:58:55.615
------------------------------
• [SLOW TEST] [242.629 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:54:53.002
    Mar 15 21:54:53.002: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-probe 03/15/23 21:54:53.003
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:54:53.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:54:53.023
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274 in namespace container-probe-2794 03/15/23 21:54:53.033
    Mar 15 21:54:53.049: INFO: Waiting up to 5m0s for pod "busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274" in namespace "container-probe-2794" to be "not pending"
    Mar 15 21:54:53.053: INFO: Pod "busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061403ms
    Mar 15 21:54:55.057: INFO: Pod "busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274": Phase="Running", Reason="", readiness=true. Elapsed: 2.007549315s
    Mar 15 21:54:55.057: INFO: Pod "busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274" satisfied condition "not pending"
    Mar 15 21:54:55.057: INFO: Started pod busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274 in namespace container-probe-2794
    STEP: checking the pod's current state and verifying that restartCount is present 03/15/23 21:54:55.057
    Mar 15 21:54:55.060: INFO: Initial restart count of pod busybox-e6fe385b-f8cd-4ee2-b218-aa36b3c13274 is 0
    STEP: deleting the pod 03/15/23 21:58:55.596
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:58:55.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2794" for this suite. 03/15/23 21:58:55.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:58:55.632
Mar 15 21:58:55.632: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 21:58:55.633
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:58:55.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:58:55.658
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-1a92f5c0-4125-4d69-a8d1-73460bf03100 03/15/23 21:58:55.664
STEP: Creating a pod to test consume secrets 03/15/23 21:58:55.67
Mar 15 21:58:55.676: INFO: Waiting up to 5m0s for pod "pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177" in namespace "secrets-5547" to be "Succeeded or Failed"
Mar 15 21:58:55.679: INFO: Pod "pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177": Phase="Pending", Reason="", readiness=false. Elapsed: 2.68555ms
Mar 15 21:58:57.682: INFO: Pod "pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006076002s
Mar 15 21:58:59.686: INFO: Pod "pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009762522s
STEP: Saw pod success 03/15/23 21:58:59.686
Mar 15 21:58:59.686: INFO: Pod "pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177" satisfied condition "Succeeded or Failed"
Mar 15 21:58:59.693: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177 container secret-volume-test: <nil>
STEP: delete the pod 03/15/23 21:58:59.716
Mar 15 21:58:59.731: INFO: Waiting for pod pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177 to disappear
Mar 15 21:58:59.734: INFO: Pod pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 21:58:59.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5547" for this suite. 03/15/23 21:58:59.741
------------------------------
• [4.116 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:58:55.632
    Mar 15 21:58:55.632: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 21:58:55.633
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:58:55.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:58:55.658
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-1a92f5c0-4125-4d69-a8d1-73460bf03100 03/15/23 21:58:55.664
    STEP: Creating a pod to test consume secrets 03/15/23 21:58:55.67
    Mar 15 21:58:55.676: INFO: Waiting up to 5m0s for pod "pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177" in namespace "secrets-5547" to be "Succeeded or Failed"
    Mar 15 21:58:55.679: INFO: Pod "pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177": Phase="Pending", Reason="", readiness=false. Elapsed: 2.68555ms
    Mar 15 21:58:57.682: INFO: Pod "pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006076002s
    Mar 15 21:58:59.686: INFO: Pod "pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009762522s
    STEP: Saw pod success 03/15/23 21:58:59.686
    Mar 15 21:58:59.686: INFO: Pod "pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177" satisfied condition "Succeeded or Failed"
    Mar 15 21:58:59.693: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177 container secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 21:58:59.716
    Mar 15 21:58:59.731: INFO: Waiting for pod pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177 to disappear
    Mar 15 21:58:59.734: INFO: Pod pod-secrets-fa621213-2b6d-4a0a-af04-aac61cad7177 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:58:59.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5547" for this suite. 03/15/23 21:58:59.741
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:58:59.748
Mar 15 21:58:59.748: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 21:58:59.749
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:58:59.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:58:59.772
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-3e1809ab-a7dd-4e80-a7f3-af58b22af519 03/15/23 21:58:59.779
STEP: Creating secret with name s-test-opt-upd-85cb90d5-a76b-4329-b404-70c477d71d3b 03/15/23 21:58:59.786
STEP: Creating the pod 03/15/23 21:58:59.801
Mar 15 21:58:59.811: INFO: Waiting up to 5m0s for pod "pod-secrets-56094cea-8a56-425b-a6c3-af8e8faa5fce" in namespace "secrets-5390" to be "running and ready"
Mar 15 21:58:59.816: INFO: Pod "pod-secrets-56094cea-8a56-425b-a6c3-af8e8faa5fce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056751ms
Mar 15 21:58:59.816: INFO: The phase of Pod pod-secrets-56094cea-8a56-425b-a6c3-af8e8faa5fce is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:59:01.820: INFO: Pod "pod-secrets-56094cea-8a56-425b-a6c3-af8e8faa5fce": Phase="Running", Reason="", readiness=true. Elapsed: 2.008115598s
Mar 15 21:59:01.820: INFO: The phase of Pod pod-secrets-56094cea-8a56-425b-a6c3-af8e8faa5fce is Running (Ready = true)
Mar 15 21:59:01.820: INFO: Pod "pod-secrets-56094cea-8a56-425b-a6c3-af8e8faa5fce" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-3e1809ab-a7dd-4e80-a7f3-af58b22af519 03/15/23 21:59:01.847
STEP: Updating secret s-test-opt-upd-85cb90d5-a76b-4329-b404-70c477d71d3b 03/15/23 21:59:01.852
STEP: Creating secret with name s-test-opt-create-86fa33e8-51c7-4080-a580-220b740b39e0 03/15/23 21:59:01.857
STEP: waiting to observe update in volume 03/15/23 21:59:01.861
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 21:59:03.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5390" for this suite. 03/15/23 21:59:03.907
------------------------------
• [4.167 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:58:59.748
    Mar 15 21:58:59.748: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 21:58:59.749
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:58:59.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:58:59.772
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-3e1809ab-a7dd-4e80-a7f3-af58b22af519 03/15/23 21:58:59.779
    STEP: Creating secret with name s-test-opt-upd-85cb90d5-a76b-4329-b404-70c477d71d3b 03/15/23 21:58:59.786
    STEP: Creating the pod 03/15/23 21:58:59.801
    Mar 15 21:58:59.811: INFO: Waiting up to 5m0s for pod "pod-secrets-56094cea-8a56-425b-a6c3-af8e8faa5fce" in namespace "secrets-5390" to be "running and ready"
    Mar 15 21:58:59.816: INFO: Pod "pod-secrets-56094cea-8a56-425b-a6c3-af8e8faa5fce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056751ms
    Mar 15 21:58:59.816: INFO: The phase of Pod pod-secrets-56094cea-8a56-425b-a6c3-af8e8faa5fce is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:59:01.820: INFO: Pod "pod-secrets-56094cea-8a56-425b-a6c3-af8e8faa5fce": Phase="Running", Reason="", readiness=true. Elapsed: 2.008115598s
    Mar 15 21:59:01.820: INFO: The phase of Pod pod-secrets-56094cea-8a56-425b-a6c3-af8e8faa5fce is Running (Ready = true)
    Mar 15 21:59:01.820: INFO: Pod "pod-secrets-56094cea-8a56-425b-a6c3-af8e8faa5fce" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-3e1809ab-a7dd-4e80-a7f3-af58b22af519 03/15/23 21:59:01.847
    STEP: Updating secret s-test-opt-upd-85cb90d5-a76b-4329-b404-70c477d71d3b 03/15/23 21:59:01.852
    STEP: Creating secret with name s-test-opt-create-86fa33e8-51c7-4080-a580-220b740b39e0 03/15/23 21:59:01.857
    STEP: waiting to observe update in volume 03/15/23 21:59:01.861
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:59:03.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5390" for this suite. 03/15/23 21:59:03.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:59:03.916
Mar 15 21:59:03.917: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename job 03/15/23 21:59:03.917
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:03.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:03.953
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 03/15/23 21:59:03.959
STEP: Ensuring job reaches completions 03/15/23 21:59:04.013
STEP: Ensuring pods with index for job exist 03/15/23 21:59:16.016
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 15 21:59:16.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9818" for this suite. 03/15/23 21:59:16.034
------------------------------
• [SLOW TEST] [12.130 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:59:03.916
    Mar 15 21:59:03.917: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename job 03/15/23 21:59:03.917
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:03.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:03.953
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 03/15/23 21:59:03.959
    STEP: Ensuring job reaches completions 03/15/23 21:59:04.013
    STEP: Ensuring pods with index for job exist 03/15/23 21:59:16.016
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:59:16.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9818" for this suite. 03/15/23 21:59:16.034
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:59:16.047
Mar 15 21:59:16.047: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pods 03/15/23 21:59:16.048
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:16.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:16.075
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 03/15/23 21:59:16.086
STEP: watching for Pod to be ready 03/15/23 21:59:16.093
Mar 15 21:59:16.095: INFO: observed Pod pod-test in namespace pods-7042 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar 15 21:59:16.102: INFO: observed Pod pod-test in namespace pods-7042 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC  }]
Mar 15 21:59:16.113: INFO: observed Pod pod-test in namespace pods-7042 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC  }]
Mar 15 21:59:18.367: INFO: Found Pod pod-test in namespace pods-7042 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 03/15/23 21:59:18.371
STEP: getting the Pod and ensuring that it's patched 03/15/23 21:59:18.379
STEP: replacing the Pod's status Ready condition to False 03/15/23 21:59:18.392
STEP: check the Pod again to ensure its Ready conditions are False 03/15/23 21:59:18.407
STEP: deleting the Pod via a Collection with a LabelSelector 03/15/23 21:59:18.408
STEP: watching for the Pod to be deleted 03/15/23 21:59:18.417
Mar 15 21:59:18.421: INFO: observed event type MODIFIED
Mar 15 21:59:20.370: INFO: observed event type MODIFIED
Mar 15 21:59:21.394: INFO: observed event type MODIFIED
Mar 15 21:59:21.412: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 15 21:59:21.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7042" for this suite. 03/15/23 21:59:21.447
------------------------------
• [SLOW TEST] [5.417 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:59:16.047
    Mar 15 21:59:16.047: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pods 03/15/23 21:59:16.048
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:16.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:16.075
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 03/15/23 21:59:16.086
    STEP: watching for Pod to be ready 03/15/23 21:59:16.093
    Mar 15 21:59:16.095: INFO: observed Pod pod-test in namespace pods-7042 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Mar 15 21:59:16.102: INFO: observed Pod pod-test in namespace pods-7042 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC  }]
    Mar 15 21:59:16.113: INFO: observed Pod pod-test in namespace pods-7042 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC  }]
    Mar 15 21:59:18.367: INFO: Found Pod pod-test in namespace pods-7042 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 21:59:16 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 03/15/23 21:59:18.371
    STEP: getting the Pod and ensuring that it's patched 03/15/23 21:59:18.379
    STEP: replacing the Pod's status Ready condition to False 03/15/23 21:59:18.392
    STEP: check the Pod again to ensure its Ready conditions are False 03/15/23 21:59:18.407
    STEP: deleting the Pod via a Collection with a LabelSelector 03/15/23 21:59:18.408
    STEP: watching for the Pod to be deleted 03/15/23 21:59:18.417
    Mar 15 21:59:18.421: INFO: observed event type MODIFIED
    Mar 15 21:59:20.370: INFO: observed event type MODIFIED
    Mar 15 21:59:21.394: INFO: observed event type MODIFIED
    Mar 15 21:59:21.412: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:59:21.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7042" for this suite. 03/15/23 21:59:21.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:59:21.468
Mar 15 21:59:21.469: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename disruption 03/15/23 21:59:21.469
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:21.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:21.488
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 03/15/23 21:59:21.492
STEP: Waiting for the pdb to be processed 03/15/23 21:59:21.498
STEP: First trying to evict a pod which shouldn't be evictable 03/15/23 21:59:23.514
STEP: Waiting for all pods to be running 03/15/23 21:59:23.514
Mar 15 21:59:23.518: INFO: pods: 0 < 3
STEP: locating a running pod 03/15/23 21:59:25.522
STEP: Updating the pdb to allow a pod to be evicted 03/15/23 21:59:25.53
STEP: Waiting for the pdb to be processed 03/15/23 21:59:25.536
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/15/23 21:59:27.544
STEP: Waiting for all pods to be running 03/15/23 21:59:27.544
STEP: Waiting for the pdb to observed all healthy pods 03/15/23 21:59:27.547
STEP: Patching the pdb to disallow a pod to be evicted 03/15/23 21:59:27.578
STEP: Waiting for the pdb to be processed 03/15/23 21:59:27.605
STEP: Waiting for all pods to be running 03/15/23 21:59:29.614
STEP: locating a running pod 03/15/23 21:59:29.617
STEP: Deleting the pdb to allow a pod to be evicted 03/15/23 21:59:29.631
STEP: Waiting for the pdb to be deleted 03/15/23 21:59:29.639
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/15/23 21:59:29.644
STEP: Waiting for all pods to be running 03/15/23 21:59:29.644
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 15 21:59:29.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2621" for this suite. 03/15/23 21:59:29.672
------------------------------
• [SLOW TEST] [8.231 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:59:21.468
    Mar 15 21:59:21.469: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename disruption 03/15/23 21:59:21.469
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:21.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:21.488
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 03/15/23 21:59:21.492
    STEP: Waiting for the pdb to be processed 03/15/23 21:59:21.498
    STEP: First trying to evict a pod which shouldn't be evictable 03/15/23 21:59:23.514
    STEP: Waiting for all pods to be running 03/15/23 21:59:23.514
    Mar 15 21:59:23.518: INFO: pods: 0 < 3
    STEP: locating a running pod 03/15/23 21:59:25.522
    STEP: Updating the pdb to allow a pod to be evicted 03/15/23 21:59:25.53
    STEP: Waiting for the pdb to be processed 03/15/23 21:59:25.536
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/15/23 21:59:27.544
    STEP: Waiting for all pods to be running 03/15/23 21:59:27.544
    STEP: Waiting for the pdb to observed all healthy pods 03/15/23 21:59:27.547
    STEP: Patching the pdb to disallow a pod to be evicted 03/15/23 21:59:27.578
    STEP: Waiting for the pdb to be processed 03/15/23 21:59:27.605
    STEP: Waiting for all pods to be running 03/15/23 21:59:29.614
    STEP: locating a running pod 03/15/23 21:59:29.617
    STEP: Deleting the pdb to allow a pod to be evicted 03/15/23 21:59:29.631
    STEP: Waiting for the pdb to be deleted 03/15/23 21:59:29.639
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/15/23 21:59:29.644
    STEP: Waiting for all pods to be running 03/15/23 21:59:29.644
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:59:29.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2621" for this suite. 03/15/23 21:59:29.672
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:59:29.701
Mar 15 21:59:29.701: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 21:59:29.702
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:29.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:29.728
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-859aa135-5e3e-49f8-bfa1-721e4deca2fc 03/15/23 21:59:29.731
STEP: Creating secret with name secret-projected-all-test-volume-8fc3851a-1033-4041-9f4a-0148ea449964 03/15/23 21:59:29.735
STEP: Creating a pod to test Check all projections for projected volume plugin 03/15/23 21:59:29.739
Mar 15 21:59:29.749: INFO: Waiting up to 5m0s for pod "projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed" in namespace "projected-1388" to be "Succeeded or Failed"
Mar 15 21:59:29.752: INFO: Pod "projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed": Phase="Pending", Reason="", readiness=false. Elapsed: 3.790701ms
Mar 15 21:59:31.756: INFO: Pod "projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007840492s
Mar 15 21:59:33.758: INFO: Pod "projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009368955s
STEP: Saw pod success 03/15/23 21:59:33.758
Mar 15 21:59:33.758: INFO: Pod "projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed" satisfied condition "Succeeded or Failed"
Mar 15 21:59:33.762: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed container projected-all-volume-test: <nil>
STEP: delete the pod 03/15/23 21:59:33.768
Mar 15 21:59:33.779: INFO: Waiting for pod projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed to disappear
Mar 15 21:59:33.783: INFO: Pod projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Mar 15 21:59:33.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1388" for this suite. 03/15/23 21:59:33.787
------------------------------
• [4.093 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:59:29.701
    Mar 15 21:59:29.701: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 21:59:29.702
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:29.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:29.728
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-859aa135-5e3e-49f8-bfa1-721e4deca2fc 03/15/23 21:59:29.731
    STEP: Creating secret with name secret-projected-all-test-volume-8fc3851a-1033-4041-9f4a-0148ea449964 03/15/23 21:59:29.735
    STEP: Creating a pod to test Check all projections for projected volume plugin 03/15/23 21:59:29.739
    Mar 15 21:59:29.749: INFO: Waiting up to 5m0s for pod "projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed" in namespace "projected-1388" to be "Succeeded or Failed"
    Mar 15 21:59:29.752: INFO: Pod "projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed": Phase="Pending", Reason="", readiness=false. Elapsed: 3.790701ms
    Mar 15 21:59:31.756: INFO: Pod "projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007840492s
    Mar 15 21:59:33.758: INFO: Pod "projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009368955s
    STEP: Saw pod success 03/15/23 21:59:33.758
    Mar 15 21:59:33.758: INFO: Pod "projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed" satisfied condition "Succeeded or Failed"
    Mar 15 21:59:33.762: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed container projected-all-volume-test: <nil>
    STEP: delete the pod 03/15/23 21:59:33.768
    Mar 15 21:59:33.779: INFO: Waiting for pod projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed to disappear
    Mar 15 21:59:33.783: INFO: Pod projected-volume-7a553d21-f03e-4212-a800-44c6d211f0ed no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:59:33.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1388" for this suite. 03/15/23 21:59:33.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:59:33.798
Mar 15 21:59:33.798: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename daemonsets 03/15/23 21:59:33.799
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:33.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:33.833
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Mar 15 21:59:33.858: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 03/15/23 21:59:33.862
Mar 15 21:59:33.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 21:59:33.866: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 03/15/23 21:59:33.866
Mar 15 21:59:33.908: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 21:59:33.910: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 21:59:34.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 21:59:34.913: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 21:59:35.914: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 15 21:59:35.914: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 03/15/23 21:59:35.916
Mar 15 21:59:35.953: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 21:59:35.953: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/15/23 21:59:35.953
Mar 15 21:59:35.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 21:59:35.970: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 21:59:36.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 21:59:36.974: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 21:59:37.973: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 21:59:37.973: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 21:59:38.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 21:59:38.991: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 21:59:39.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 15 21:59:39.974: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/15/23 21:59:39.979
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-170, will wait for the garbage collector to delete the pods 03/15/23 21:59:39.979
Mar 15 21:59:40.038: INFO: Deleting DaemonSet.extensions daemon-set took: 5.594951ms
Mar 15 21:59:40.139: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.023739ms
Mar 15 21:59:42.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 21:59:42.443: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 15 21:59:42.448: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9318"},"items":null}

Mar 15 21:59:42.451: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9318"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 21:59:42.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-170" for this suite. 03/15/23 21:59:42.509
------------------------------
• [SLOW TEST] [8.721 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:59:33.798
    Mar 15 21:59:33.798: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename daemonsets 03/15/23 21:59:33.799
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:33.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:33.833
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Mar 15 21:59:33.858: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 03/15/23 21:59:33.862
    Mar 15 21:59:33.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 21:59:33.866: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 03/15/23 21:59:33.866
    Mar 15 21:59:33.908: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 21:59:33.910: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 21:59:34.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 21:59:34.913: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 21:59:35.914: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 15 21:59:35.914: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 03/15/23 21:59:35.916
    Mar 15 21:59:35.953: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 21:59:35.953: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/15/23 21:59:35.953
    Mar 15 21:59:35.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 21:59:35.970: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 21:59:36.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 21:59:36.974: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 21:59:37.973: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 21:59:37.973: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 21:59:38.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 21:59:38.991: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 21:59:39.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 15 21:59:39.974: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/15/23 21:59:39.979
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-170, will wait for the garbage collector to delete the pods 03/15/23 21:59:39.979
    Mar 15 21:59:40.038: INFO: Deleting DaemonSet.extensions daemon-set took: 5.594951ms
    Mar 15 21:59:40.139: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.023739ms
    Mar 15 21:59:42.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 21:59:42.443: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 15 21:59:42.448: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9318"},"items":null}

    Mar 15 21:59:42.451: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9318"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:59:42.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-170" for this suite. 03/15/23 21:59:42.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:59:42.543
Mar 15 21:59:42.543: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 21:59:42.544
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:42.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:42.561
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2595 03/15/23 21:59:42.565
STEP: changing the ExternalName service to type=ClusterIP 03/15/23 21:59:42.577
STEP: creating replication controller externalname-service in namespace services-2595 03/15/23 21:59:42.599
I0315 21:59:42.609015      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2595, replica count: 2
I0315 21:59:45.664023      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 21:59:45.664: INFO: Creating new exec pod
Mar 15 21:59:45.671: INFO: Waiting up to 5m0s for pod "execpodmppll" in namespace "services-2595" to be "running"
Mar 15 21:59:45.681: INFO: Pod "execpodmppll": Phase="Pending", Reason="", readiness=false. Elapsed: 9.363837ms
Mar 15 21:59:47.684: INFO: Pod "execpodmppll": Phase="Running", Reason="", readiness=true. Elapsed: 2.013047514s
Mar 15 21:59:47.684: INFO: Pod "execpodmppll" satisfied condition "running"
Mar 15 21:59:48.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2595 exec execpodmppll -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Mar 15 21:59:48.935: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 15 21:59:48.935: INFO: stdout: ""
Mar 15 21:59:48.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2595 exec execpodmppll -- /bin/sh -x -c nc -v -z -w 2 100.71.84.64 80'
Mar 15 21:59:49.132: INFO: stderr: "+ nc -v -z -w 2 100.71.84.64 80\nConnection to 100.71.84.64 80 port [tcp/http] succeeded!\n"
Mar 15 21:59:49.132: INFO: stdout: ""
Mar 15 21:59:49.132: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 21:59:49.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2595" for this suite. 03/15/23 21:59:49.204
------------------------------
• [SLOW TEST] [6.670 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:59:42.543
    Mar 15 21:59:42.543: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 21:59:42.544
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:42.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:42.561
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2595 03/15/23 21:59:42.565
    STEP: changing the ExternalName service to type=ClusterIP 03/15/23 21:59:42.577
    STEP: creating replication controller externalname-service in namespace services-2595 03/15/23 21:59:42.599
    I0315 21:59:42.609015      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2595, replica count: 2
    I0315 21:59:45.664023      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 15 21:59:45.664: INFO: Creating new exec pod
    Mar 15 21:59:45.671: INFO: Waiting up to 5m0s for pod "execpodmppll" in namespace "services-2595" to be "running"
    Mar 15 21:59:45.681: INFO: Pod "execpodmppll": Phase="Pending", Reason="", readiness=false. Elapsed: 9.363837ms
    Mar 15 21:59:47.684: INFO: Pod "execpodmppll": Phase="Running", Reason="", readiness=true. Elapsed: 2.013047514s
    Mar 15 21:59:47.684: INFO: Pod "execpodmppll" satisfied condition "running"
    Mar 15 21:59:48.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2595 exec execpodmppll -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Mar 15 21:59:48.935: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 15 21:59:48.935: INFO: stdout: ""
    Mar 15 21:59:48.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2595 exec execpodmppll -- /bin/sh -x -c nc -v -z -w 2 100.71.84.64 80'
    Mar 15 21:59:49.132: INFO: stderr: "+ nc -v -z -w 2 100.71.84.64 80\nConnection to 100.71.84.64 80 port [tcp/http] succeeded!\n"
    Mar 15 21:59:49.132: INFO: stdout: ""
    Mar 15 21:59:49.132: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:59:49.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2595" for this suite. 03/15/23 21:59:49.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:59:49.215
Mar 15 21:59:49.215: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename events 03/15/23 21:59:49.219
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:49.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:49.238
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 03/15/23 21:59:49.245
STEP: listing events in all namespaces 03/15/23 21:59:49.252
STEP: listing events in test namespace 03/15/23 21:59:49.269
STEP: listing events with field selection filtering on source 03/15/23 21:59:49.281
STEP: listing events with field selection filtering on reportingController 03/15/23 21:59:49.294
STEP: getting the test event 03/15/23 21:59:49.306
STEP: patching the test event 03/15/23 21:59:49.309
STEP: getting the test event 03/15/23 21:59:49.324
STEP: updating the test event 03/15/23 21:59:49.329
STEP: getting the test event 03/15/23 21:59:49.336
STEP: deleting the test event 03/15/23 21:59:49.34
STEP: listing events in all namespaces 03/15/23 21:59:49.346
STEP: listing events in test namespace 03/15/23 21:59:49.354
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Mar 15 21:59:49.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7557" for this suite. 03/15/23 21:59:49.361
------------------------------
• [0.151 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:59:49.215
    Mar 15 21:59:49.215: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename events 03/15/23 21:59:49.219
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:49.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:49.238
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 03/15/23 21:59:49.245
    STEP: listing events in all namespaces 03/15/23 21:59:49.252
    STEP: listing events in test namespace 03/15/23 21:59:49.269
    STEP: listing events with field selection filtering on source 03/15/23 21:59:49.281
    STEP: listing events with field selection filtering on reportingController 03/15/23 21:59:49.294
    STEP: getting the test event 03/15/23 21:59:49.306
    STEP: patching the test event 03/15/23 21:59:49.309
    STEP: getting the test event 03/15/23 21:59:49.324
    STEP: updating the test event 03/15/23 21:59:49.329
    STEP: getting the test event 03/15/23 21:59:49.336
    STEP: deleting the test event 03/15/23 21:59:49.34
    STEP: listing events in all namespaces 03/15/23 21:59:49.346
    STEP: listing events in test namespace 03/15/23 21:59:49.354
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:59:49.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7557" for this suite. 03/15/23 21:59:49.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:59:49.368
Mar 15 21:59:49.368: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 21:59:49.369
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:49.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:49.385
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 21:59:49.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4077" for this suite. 03/15/23 21:59:49.394
------------------------------
• [0.033 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:59:49.368
    Mar 15 21:59:49.368: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 21:59:49.369
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:49.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:49.385
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:59:49.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4077" for this suite. 03/15/23 21:59:49.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:59:49.416
Mar 15 21:59:49.416: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 21:59:49.417
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:49.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:49.434
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 03/15/23 21:59:49.437
Mar 15 21:59:49.446: INFO: Waiting up to 5m0s for pod "annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f" in namespace "projected-316" to be "running and ready"
Mar 15 21:59:49.450: INFO: Pod "annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.645986ms
Mar 15 21:59:49.450: INFO: The phase of Pod annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:59:51.454: INFO: Pod "annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008103014s
Mar 15 21:59:51.454: INFO: The phase of Pod annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f is Pending, waiting for it to be Running (with Ready = true)
Mar 15 21:59:53.454: INFO: Pod "annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f": Phase="Running", Reason="", readiness=true. Elapsed: 4.008168745s
Mar 15 21:59:53.454: INFO: The phase of Pod annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f is Running (Ready = true)
Mar 15 21:59:53.454: INFO: Pod "annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f" satisfied condition "running and ready"
Mar 15 21:59:53.982: INFO: Successfully updated pod "annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 15 21:59:55.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-316" for this suite. 03/15/23 21:59:56.005
------------------------------
• [SLOW TEST] [6.595 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:59:49.416
    Mar 15 21:59:49.416: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 21:59:49.417
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:49.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:49.434
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 03/15/23 21:59:49.437
    Mar 15 21:59:49.446: INFO: Waiting up to 5m0s for pod "annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f" in namespace "projected-316" to be "running and ready"
    Mar 15 21:59:49.450: INFO: Pod "annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.645986ms
    Mar 15 21:59:49.450: INFO: The phase of Pod annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:59:51.454: INFO: Pod "annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008103014s
    Mar 15 21:59:51.454: INFO: The phase of Pod annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 21:59:53.454: INFO: Pod "annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f": Phase="Running", Reason="", readiness=true. Elapsed: 4.008168745s
    Mar 15 21:59:53.454: INFO: The phase of Pod annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f is Running (Ready = true)
    Mar 15 21:59:53.454: INFO: Pod "annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f" satisfied condition "running and ready"
    Mar 15 21:59:53.982: INFO: Successfully updated pod "annotationupdate60c66da4-46a2-46d0-ab09-e0240e16b64f"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 15 21:59:55.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-316" for this suite. 03/15/23 21:59:56.005
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 21:59:56.011
Mar 15 21:59:56.012: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 21:59:56.013
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:56.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:56.03
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Mar 15 21:59:56.034: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/15/23 21:59:57.798
Mar 15 21:59:57.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 create -f -'
Mar 15 21:59:58.455: INFO: stderr: ""
Mar 15 21:59:58.455: INFO: stdout: "e2e-test-crd-publish-openapi-921-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 15 21:59:58.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 delete e2e-test-crd-publish-openapi-921-crds test-foo'
Mar 15 21:59:58.525: INFO: stderr: ""
Mar 15 21:59:58.525: INFO: stdout: "e2e-test-crd-publish-openapi-921-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 15 21:59:58.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 apply -f -'
Mar 15 21:59:58.752: INFO: stderr: ""
Mar 15 21:59:58.752: INFO: stdout: "e2e-test-crd-publish-openapi-921-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 15 21:59:58.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 delete e2e-test-crd-publish-openapi-921-crds test-foo'
Mar 15 21:59:58.828: INFO: stderr: ""
Mar 15 21:59:58.828: INFO: stdout: "e2e-test-crd-publish-openapi-921-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/15/23 21:59:58.828
Mar 15 21:59:58.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 create -f -'
Mar 15 21:59:59.054: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/15/23 21:59:59.054
Mar 15 21:59:59.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 create -f -'
Mar 15 21:59:59.264: INFO: rc: 1
Mar 15 21:59:59.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 apply -f -'
Mar 15 21:59:59.514: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/15/23 21:59:59.514
Mar 15 21:59:59.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 create -f -'
Mar 15 21:59:59.723: INFO: rc: 1
Mar 15 21:59:59.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 apply -f -'
Mar 15 21:59:59.972: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 03/15/23 21:59:59.972
Mar 15 21:59:59.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 explain e2e-test-crd-publish-openapi-921-crds'
Mar 15 22:00:00.271: INFO: stderr: ""
Mar 15 22:00:00.271: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-921-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 03/15/23 22:00:00.271
Mar 15 22:00:00.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 explain e2e-test-crd-publish-openapi-921-crds.metadata'
Mar 15 22:00:00.497: INFO: stderr: ""
Mar 15 22:00:00.497: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-921-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 15 22:00:00.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 explain e2e-test-crd-publish-openapi-921-crds.spec'
Mar 15 22:00:00.729: INFO: stderr: ""
Mar 15 22:00:00.729: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-921-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 15 22:00:00.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 explain e2e-test-crd-publish-openapi-921-crds.spec.bars'
Mar 15 22:00:00.940: INFO: stderr: ""
Mar 15 22:00:00.940: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-921-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/15/23 22:00:00.94
Mar 15 22:00:00.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 explain e2e-test-crd-publish-openapi-921-crds.spec.bars2'
Mar 15 22:00:01.222: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:00:04.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1122" for this suite. 03/15/23 22:00:04.331
------------------------------
• [SLOW TEST] [8.343 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 21:59:56.011
    Mar 15 21:59:56.012: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 21:59:56.013
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 21:59:56.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 21:59:56.03
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Mar 15 21:59:56.034: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/15/23 21:59:57.798
    Mar 15 21:59:57.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 create -f -'
    Mar 15 21:59:58.455: INFO: stderr: ""
    Mar 15 21:59:58.455: INFO: stdout: "e2e-test-crd-publish-openapi-921-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 15 21:59:58.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 delete e2e-test-crd-publish-openapi-921-crds test-foo'
    Mar 15 21:59:58.525: INFO: stderr: ""
    Mar 15 21:59:58.525: INFO: stdout: "e2e-test-crd-publish-openapi-921-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Mar 15 21:59:58.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 apply -f -'
    Mar 15 21:59:58.752: INFO: stderr: ""
    Mar 15 21:59:58.752: INFO: stdout: "e2e-test-crd-publish-openapi-921-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 15 21:59:58.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 delete e2e-test-crd-publish-openapi-921-crds test-foo'
    Mar 15 21:59:58.828: INFO: stderr: ""
    Mar 15 21:59:58.828: INFO: stdout: "e2e-test-crd-publish-openapi-921-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/15/23 21:59:58.828
    Mar 15 21:59:58.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 create -f -'
    Mar 15 21:59:59.054: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/15/23 21:59:59.054
    Mar 15 21:59:59.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 create -f -'
    Mar 15 21:59:59.264: INFO: rc: 1
    Mar 15 21:59:59.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 apply -f -'
    Mar 15 21:59:59.514: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/15/23 21:59:59.514
    Mar 15 21:59:59.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 create -f -'
    Mar 15 21:59:59.723: INFO: rc: 1
    Mar 15 21:59:59.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 --namespace=crd-publish-openapi-1122 apply -f -'
    Mar 15 21:59:59.972: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 03/15/23 21:59:59.972
    Mar 15 21:59:59.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 explain e2e-test-crd-publish-openapi-921-crds'
    Mar 15 22:00:00.271: INFO: stderr: ""
    Mar 15 22:00:00.271: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-921-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 03/15/23 22:00:00.271
    Mar 15 22:00:00.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 explain e2e-test-crd-publish-openapi-921-crds.metadata'
    Mar 15 22:00:00.497: INFO: stderr: ""
    Mar 15 22:00:00.497: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-921-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Mar 15 22:00:00.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 explain e2e-test-crd-publish-openapi-921-crds.spec'
    Mar 15 22:00:00.729: INFO: stderr: ""
    Mar 15 22:00:00.729: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-921-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Mar 15 22:00:00.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 explain e2e-test-crd-publish-openapi-921-crds.spec.bars'
    Mar 15 22:00:00.940: INFO: stderr: ""
    Mar 15 22:00:00.940: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-921-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/15/23 22:00:00.94
    Mar 15 22:00:00.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-1122 explain e2e-test-crd-publish-openapi-921-crds.spec.bars2'
    Mar 15 22:00:01.222: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:00:04.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1122" for this suite. 03/15/23 22:00:04.331
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:00:04.355
Mar 15 22:00:04.355: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename sched-pred 03/15/23 22:00:04.356
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:04.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:04.411
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 15 22:00:04.424: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 15 22:00:04.446: INFO: Waiting for terminating namespaces to be deleted...
Mar 15 22:00:04.468: INFO: 
Logging pods the apiserver thinks is on node i-077ee0eb7ec5a02aa before test
Mar 15 22:00:04.489: INFO: etcd1 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.489: INFO: 	Container etcd1 ready: true, restart count 0
Mar 15 22:00:04.489: INFO: cilium-hvkmm from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.489: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 15 22:00:04.489: INFO: coredns-85dfcfb87f-p4x25 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.489: INFO: 	Container coredns ready: true, restart count 0
Mar 15 22:00:04.489: INFO: coredns-autoscaler-7cb5c5b969-tn76l from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.489: INFO: 	Container autoscaler ready: true, restart count 0
Mar 15 22:00:04.489: INFO: ebs-csi-node-9zwns from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
Mar 15 22:00:04.489: INFO: 	Container ebs-plugin ready: true, restart count 0
Mar 15 22:00:04.489: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 22:00:04.489: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 22:00:04.489: INFO: kube-proxy-i-077ee0eb7ec5a02aa from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.489: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 15 22:00:04.489: INFO: metrics-server-79b7f8b6d9-g4qd4 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.490: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 22:00:04.490: INFO: sonobuoy-e2e-job-cc297e458b6c49af from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 22:00:04.490: INFO: 	Container e2e ready: true, restart count 0
Mar 15 22:00:04.490: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 22:00:04.490: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-hkt9j from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 22:00:04.490: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 22:00:04.490: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 22:00:04.490: INFO: 
Logging pods the apiserver thinks is on node i-0baafb3f4e7bf826e before test
Mar 15 22:00:04.504: INFO: csi-mockplugin-85d5674684-nwzd7 from default started at 2023-03-15 21:42:36 +0000 UTC (7 container statuses recorded)
Mar 15 22:00:04.504: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 15 22:00:04.504: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 15 22:00:04.504: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 15 22:00:04.504: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 15 22:00:04.504: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 22:00:04.504: INFO: 	Container mock-driver ready: true, restart count 0
Mar 15 22:00:04.504: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 22:00:04.504: INFO: etcd2 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.504: INFO: 	Container etcd2 ready: true, restart count 0
Mar 15 22:00:04.504: INFO: web-server from default started at 2023-03-15 21:42:51 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.504: INFO: 	Container web-server ready: true, restart count 0
Mar 15 22:00:04.504: INFO: cilium-8q84b from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.504: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 15 22:00:04.504: INFO: ebs-csi-node-8vkhc from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
Mar 15 22:00:04.504: INFO: 	Container ebs-plugin ready: true, restart count 0
Mar 15 22:00:04.504: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 22:00:04.504: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 22:00:04.504: INFO: kube-proxy-i-0baafb3f4e7bf826e from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.504: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 15 22:00:04.504: INFO: metrics-server-79b7f8b6d9-dgk5h from kube-system started at 2023-03-15 21:41:14 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.504: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 22:00:04.504: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-49vvc from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 22:00:04.505: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 22:00:04.505: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 22:00:04.505: INFO: 
Logging pods the apiserver thinks is on node i-0faaf83f00b43c88c before test
Mar 15 22:00:04.529: INFO: etcd0 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.529: INFO: 	Container etcd0 ready: true, restart count 0
Mar 15 22:00:04.529: INFO: cilium-nht5t from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.529: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 15 22:00:04.529: INFO: coredns-85dfcfb87f-b7q7b from kube-system started at 2023-03-15 21:41:25 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.529: INFO: 	Container coredns ready: true, restart count 0
Mar 15 22:00:04.529: INFO: ebs-csi-node-kkqjj from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
Mar 15 22:00:04.529: INFO: 	Container ebs-plugin ready: true, restart count 0
Mar 15 22:00:04.529: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 22:00:04.529: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 22:00:04.529: INFO: kube-proxy-i-0faaf83f00b43c88c from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.529: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 15 22:00:04.529: INFO: sonobuoy from sonobuoy started at 2023-03-15 21:43:05 +0000 UTC (1 container statuses recorded)
Mar 15 22:00:04.529: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 15 22:00:04.529: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-dc6rq from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 22:00:04.529: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 22:00:04.529: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node i-077ee0eb7ec5a02aa 03/15/23 22:00:04.596
STEP: verifying the node has the label node i-0baafb3f4e7bf826e 03/15/23 22:00:04.675
STEP: verifying the node has the label node i-0faaf83f00b43c88c 03/15/23 22:00:04.776
Mar 15 22:00:04.844: INFO: Pod csi-mockplugin-85d5674684-nwzd7 requesting resource cpu=700m on Node i-0baafb3f4e7bf826e
Mar 15 22:00:04.844: INFO: Pod etcd0 requesting resource cpu=100m on Node i-0faaf83f00b43c88c
Mar 15 22:00:04.845: INFO: Pod etcd1 requesting resource cpu=100m on Node i-077ee0eb7ec5a02aa
Mar 15 22:00:04.845: INFO: Pod etcd2 requesting resource cpu=100m on Node i-0baafb3f4e7bf826e
Mar 15 22:00:04.845: INFO: Pod web-server requesting resource cpu=100m on Node i-0baafb3f4e7bf826e
Mar 15 22:00:04.847: INFO: Pod cilium-8q84b requesting resource cpu=25m on Node i-0baafb3f4e7bf826e
Mar 15 22:00:04.848: INFO: Pod cilium-hvkmm requesting resource cpu=25m on Node i-077ee0eb7ec5a02aa
Mar 15 22:00:04.848: INFO: Pod cilium-nht5t requesting resource cpu=25m on Node i-0faaf83f00b43c88c
Mar 15 22:00:04.849: INFO: Pod coredns-85dfcfb87f-b7q7b requesting resource cpu=100m on Node i-0faaf83f00b43c88c
Mar 15 22:00:04.850: INFO: Pod coredns-85dfcfb87f-p4x25 requesting resource cpu=100m on Node i-077ee0eb7ec5a02aa
Mar 15 22:00:04.850: INFO: Pod coredns-autoscaler-7cb5c5b969-tn76l requesting resource cpu=20m on Node i-077ee0eb7ec5a02aa
Mar 15 22:00:04.850: INFO: Pod ebs-csi-node-8vkhc requesting resource cpu=0m on Node i-0baafb3f4e7bf826e
Mar 15 22:00:04.850: INFO: Pod ebs-csi-node-9zwns requesting resource cpu=0m on Node i-077ee0eb7ec5a02aa
Mar 15 22:00:04.851: INFO: Pod ebs-csi-node-kkqjj requesting resource cpu=0m on Node i-0faaf83f00b43c88c
Mar 15 22:00:04.851: INFO: Pod kube-proxy-i-077ee0eb7ec5a02aa requesting resource cpu=100m on Node i-077ee0eb7ec5a02aa
Mar 15 22:00:04.851: INFO: Pod kube-proxy-i-0baafb3f4e7bf826e requesting resource cpu=100m on Node i-0baafb3f4e7bf826e
Mar 15 22:00:04.851: INFO: Pod kube-proxy-i-0faaf83f00b43c88c requesting resource cpu=100m on Node i-0faaf83f00b43c88c
Mar 15 22:00:04.852: INFO: Pod metrics-server-79b7f8b6d9-dgk5h requesting resource cpu=50m on Node i-0baafb3f4e7bf826e
Mar 15 22:00:04.853: INFO: Pod metrics-server-79b7f8b6d9-g4qd4 requesting resource cpu=50m on Node i-077ee0eb7ec5a02aa
Mar 15 22:00:04.853: INFO: Pod sonobuoy requesting resource cpu=0m on Node i-0faaf83f00b43c88c
Mar 15 22:00:04.853: INFO: Pod sonobuoy-e2e-job-cc297e458b6c49af requesting resource cpu=0m on Node i-077ee0eb7ec5a02aa
Mar 15 22:00:04.853: INFO: Pod sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-49vvc requesting resource cpu=0m on Node i-0baafb3f4e7bf826e
Mar 15 22:00:04.853: INFO: Pod sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-dc6rq requesting resource cpu=0m on Node i-0faaf83f00b43c88c
Mar 15 22:00:04.854: INFO: Pod sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-hkt9j requesting resource cpu=0m on Node i-077ee0eb7ec5a02aa
STEP: Starting Pods to consume most of the cluster CPU. 03/15/23 22:00:04.854
Mar 15 22:00:04.856: INFO: Creating a pod which consumes cpu=1123m on Node i-077ee0eb7ec5a02aa
Mar 15 22:00:04.875: INFO: Creating a pod which consumes cpu=647m on Node i-0baafb3f4e7bf826e
Mar 15 22:00:04.895: INFO: Creating a pod which consumes cpu=1172m on Node i-0faaf83f00b43c88c
Mar 15 22:00:04.932: INFO: Waiting up to 5m0s for pod "filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae" in namespace "sched-pred-9443" to be "running"
Mar 15 22:00:04.974: INFO: Pod "filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae": Phase="Pending", Reason="", readiness=false. Elapsed: 41.386424ms
Mar 15 22:00:06.984: INFO: Pod "filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae": Phase="Running", Reason="", readiness=true. Elapsed: 2.051787125s
Mar 15 22:00:06.984: INFO: Pod "filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae" satisfied condition "running"
Mar 15 22:00:06.984: INFO: Waiting up to 5m0s for pod "filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e" in namespace "sched-pred-9443" to be "running"
Mar 15 22:00:06.994: INFO: Pod "filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.05715ms
Mar 15 22:00:08.998: INFO: Pod "filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013543415s
Mar 15 22:00:08.998: INFO: Pod "filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e" satisfied condition "running"
Mar 15 22:00:08.998: INFO: Waiting up to 5m0s for pod "filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d" in namespace "sched-pred-9443" to be "running"
Mar 15 22:00:09.009: INFO: Pod "filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d": Phase="Running", Reason="", readiness=true. Elapsed: 10.645432ms
Mar 15 22:00:09.009: INFO: Pod "filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 03/15/23 22:00:09.009
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e.174cb6aa9bb01f57], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9443/filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e to i-0baafb3f4e7bf826e] 03/15/23 22:00:09.023
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e.174cb6aadc3c38b4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/15/23 22:00:09.024
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e.174cb6aadede85d2], Reason = [Created], Message = [Created container filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e] 03/15/23 22:00:09.024
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e.174cb6aae681d1f9], Reason = [Started], Message = [Started container filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e] 03/15/23 22:00:09.024
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d.174cb6aa9bb0b967], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9443/filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d to i-0faaf83f00b43c88c] 03/15/23 22:00:09.024
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d.174cb6aada88ecee], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/15/23 22:00:09.024
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d.174cb6aadf5362dc], Reason = [Created], Message = [Created container filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d] 03/15/23 22:00:09.024
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d.174cb6aae7a5421e], Reason = [Started], Message = [Started container filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d] 03/15/23 22:00:09.024
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae.174cb6aa96dc0b1c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9443/filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae to i-077ee0eb7ec5a02aa] 03/15/23 22:00:09.025
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae.174cb6aae4839f84], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/15/23 22:00:09.025
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae.174cb6aae598afa1], Reason = [Created], Message = [Created container filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae] 03/15/23 22:00:09.025
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae.174cb6aaec39302b], Reason = [Started], Message = [Started container filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae] 03/15/23 22:00:09.025
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.174cb6ab8e205cbc], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/4 nodes are available: 1 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] 03/15/23 22:00:09.069
STEP: removing the label node off the node i-077ee0eb7ec5a02aa 03/15/23 22:00:10.05
STEP: verifying the node doesn't have the label node 03/15/23 22:00:10.068
STEP: removing the label node off the node i-0baafb3f4e7bf826e 03/15/23 22:00:10.138
STEP: verifying the node doesn't have the label node 03/15/23 22:00:10.232
STEP: removing the label node off the node i-0faaf83f00b43c88c 03/15/23 22:00:10.242
STEP: verifying the node doesn't have the label node 03/15/23 22:00:10.275
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:00:10.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9443" for this suite. 03/15/23 22:00:10.332
------------------------------
• [SLOW TEST] [6.009 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:00:04.355
    Mar 15 22:00:04.355: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename sched-pred 03/15/23 22:00:04.356
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:04.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:04.411
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 15 22:00:04.424: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 15 22:00:04.446: INFO: Waiting for terminating namespaces to be deleted...
    Mar 15 22:00:04.468: INFO: 
    Logging pods the apiserver thinks is on node i-077ee0eb7ec5a02aa before test
    Mar 15 22:00:04.489: INFO: etcd1 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.489: INFO: 	Container etcd1 ready: true, restart count 0
    Mar 15 22:00:04.489: INFO: cilium-hvkmm from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.489: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 15 22:00:04.489: INFO: coredns-85dfcfb87f-p4x25 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.489: INFO: 	Container coredns ready: true, restart count 0
    Mar 15 22:00:04.489: INFO: coredns-autoscaler-7cb5c5b969-tn76l from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.489: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 15 22:00:04.489: INFO: ebs-csi-node-9zwns from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
    Mar 15 22:00:04.489: INFO: 	Container ebs-plugin ready: true, restart count 0
    Mar 15 22:00:04.489: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 22:00:04.489: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 22:00:04.489: INFO: kube-proxy-i-077ee0eb7ec5a02aa from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.489: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 15 22:00:04.489: INFO: metrics-server-79b7f8b6d9-g4qd4 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.490: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 15 22:00:04.490: INFO: sonobuoy-e2e-job-cc297e458b6c49af from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 22:00:04.490: INFO: 	Container e2e ready: true, restart count 0
    Mar 15 22:00:04.490: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 22:00:04.490: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-hkt9j from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 22:00:04.490: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 22:00:04.490: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 15 22:00:04.490: INFO: 
    Logging pods the apiserver thinks is on node i-0baafb3f4e7bf826e before test
    Mar 15 22:00:04.504: INFO: csi-mockplugin-85d5674684-nwzd7 from default started at 2023-03-15 21:42:36 +0000 UTC (7 container statuses recorded)
    Mar 15 22:00:04.504: INFO: 	Container csi-attacher ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: 	Container csi-resizer ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: 	Container mock-driver ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: etcd2 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.504: INFO: 	Container etcd2 ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: web-server from default started at 2023-03-15 21:42:51 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.504: INFO: 	Container web-server ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: cilium-8q84b from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.504: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: ebs-csi-node-8vkhc from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
    Mar 15 22:00:04.504: INFO: 	Container ebs-plugin ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: kube-proxy-i-0baafb3f4e7bf826e from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.504: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: metrics-server-79b7f8b6d9-dgk5h from kube-system started at 2023-03-15 21:41:14 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.504: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 15 22:00:04.504: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-49vvc from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 22:00:04.505: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 22:00:04.505: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 15 22:00:04.505: INFO: 
    Logging pods the apiserver thinks is on node i-0faaf83f00b43c88c before test
    Mar 15 22:00:04.529: INFO: etcd0 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.529: INFO: 	Container etcd0 ready: true, restart count 0
    Mar 15 22:00:04.529: INFO: cilium-nht5t from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.529: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 15 22:00:04.529: INFO: coredns-85dfcfb87f-b7q7b from kube-system started at 2023-03-15 21:41:25 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.529: INFO: 	Container coredns ready: true, restart count 0
    Mar 15 22:00:04.529: INFO: ebs-csi-node-kkqjj from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
    Mar 15 22:00:04.529: INFO: 	Container ebs-plugin ready: true, restart count 0
    Mar 15 22:00:04.529: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 22:00:04.529: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 22:00:04.529: INFO: kube-proxy-i-0faaf83f00b43c88c from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.529: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 15 22:00:04.529: INFO: sonobuoy from sonobuoy started at 2023-03-15 21:43:05 +0000 UTC (1 container statuses recorded)
    Mar 15 22:00:04.529: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 15 22:00:04.529: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-dc6rq from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 22:00:04.529: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 22:00:04.529: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node i-077ee0eb7ec5a02aa 03/15/23 22:00:04.596
    STEP: verifying the node has the label node i-0baafb3f4e7bf826e 03/15/23 22:00:04.675
    STEP: verifying the node has the label node i-0faaf83f00b43c88c 03/15/23 22:00:04.776
    Mar 15 22:00:04.844: INFO: Pod csi-mockplugin-85d5674684-nwzd7 requesting resource cpu=700m on Node i-0baafb3f4e7bf826e
    Mar 15 22:00:04.844: INFO: Pod etcd0 requesting resource cpu=100m on Node i-0faaf83f00b43c88c
    Mar 15 22:00:04.845: INFO: Pod etcd1 requesting resource cpu=100m on Node i-077ee0eb7ec5a02aa
    Mar 15 22:00:04.845: INFO: Pod etcd2 requesting resource cpu=100m on Node i-0baafb3f4e7bf826e
    Mar 15 22:00:04.845: INFO: Pod web-server requesting resource cpu=100m on Node i-0baafb3f4e7bf826e
    Mar 15 22:00:04.847: INFO: Pod cilium-8q84b requesting resource cpu=25m on Node i-0baafb3f4e7bf826e
    Mar 15 22:00:04.848: INFO: Pod cilium-hvkmm requesting resource cpu=25m on Node i-077ee0eb7ec5a02aa
    Mar 15 22:00:04.848: INFO: Pod cilium-nht5t requesting resource cpu=25m on Node i-0faaf83f00b43c88c
    Mar 15 22:00:04.849: INFO: Pod coredns-85dfcfb87f-b7q7b requesting resource cpu=100m on Node i-0faaf83f00b43c88c
    Mar 15 22:00:04.850: INFO: Pod coredns-85dfcfb87f-p4x25 requesting resource cpu=100m on Node i-077ee0eb7ec5a02aa
    Mar 15 22:00:04.850: INFO: Pod coredns-autoscaler-7cb5c5b969-tn76l requesting resource cpu=20m on Node i-077ee0eb7ec5a02aa
    Mar 15 22:00:04.850: INFO: Pod ebs-csi-node-8vkhc requesting resource cpu=0m on Node i-0baafb3f4e7bf826e
    Mar 15 22:00:04.850: INFO: Pod ebs-csi-node-9zwns requesting resource cpu=0m on Node i-077ee0eb7ec5a02aa
    Mar 15 22:00:04.851: INFO: Pod ebs-csi-node-kkqjj requesting resource cpu=0m on Node i-0faaf83f00b43c88c
    Mar 15 22:00:04.851: INFO: Pod kube-proxy-i-077ee0eb7ec5a02aa requesting resource cpu=100m on Node i-077ee0eb7ec5a02aa
    Mar 15 22:00:04.851: INFO: Pod kube-proxy-i-0baafb3f4e7bf826e requesting resource cpu=100m on Node i-0baafb3f4e7bf826e
    Mar 15 22:00:04.851: INFO: Pod kube-proxy-i-0faaf83f00b43c88c requesting resource cpu=100m on Node i-0faaf83f00b43c88c
    Mar 15 22:00:04.852: INFO: Pod metrics-server-79b7f8b6d9-dgk5h requesting resource cpu=50m on Node i-0baafb3f4e7bf826e
    Mar 15 22:00:04.853: INFO: Pod metrics-server-79b7f8b6d9-g4qd4 requesting resource cpu=50m on Node i-077ee0eb7ec5a02aa
    Mar 15 22:00:04.853: INFO: Pod sonobuoy requesting resource cpu=0m on Node i-0faaf83f00b43c88c
    Mar 15 22:00:04.853: INFO: Pod sonobuoy-e2e-job-cc297e458b6c49af requesting resource cpu=0m on Node i-077ee0eb7ec5a02aa
    Mar 15 22:00:04.853: INFO: Pod sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-49vvc requesting resource cpu=0m on Node i-0baafb3f4e7bf826e
    Mar 15 22:00:04.853: INFO: Pod sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-dc6rq requesting resource cpu=0m on Node i-0faaf83f00b43c88c
    Mar 15 22:00:04.854: INFO: Pod sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-hkt9j requesting resource cpu=0m on Node i-077ee0eb7ec5a02aa
    STEP: Starting Pods to consume most of the cluster CPU. 03/15/23 22:00:04.854
    Mar 15 22:00:04.856: INFO: Creating a pod which consumes cpu=1123m on Node i-077ee0eb7ec5a02aa
    Mar 15 22:00:04.875: INFO: Creating a pod which consumes cpu=647m on Node i-0baafb3f4e7bf826e
    Mar 15 22:00:04.895: INFO: Creating a pod which consumes cpu=1172m on Node i-0faaf83f00b43c88c
    Mar 15 22:00:04.932: INFO: Waiting up to 5m0s for pod "filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae" in namespace "sched-pred-9443" to be "running"
    Mar 15 22:00:04.974: INFO: Pod "filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae": Phase="Pending", Reason="", readiness=false. Elapsed: 41.386424ms
    Mar 15 22:00:06.984: INFO: Pod "filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae": Phase="Running", Reason="", readiness=true. Elapsed: 2.051787125s
    Mar 15 22:00:06.984: INFO: Pod "filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae" satisfied condition "running"
    Mar 15 22:00:06.984: INFO: Waiting up to 5m0s for pod "filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e" in namespace "sched-pred-9443" to be "running"
    Mar 15 22:00:06.994: INFO: Pod "filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.05715ms
    Mar 15 22:00:08.998: INFO: Pod "filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013543415s
    Mar 15 22:00:08.998: INFO: Pod "filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e" satisfied condition "running"
    Mar 15 22:00:08.998: INFO: Waiting up to 5m0s for pod "filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d" in namespace "sched-pred-9443" to be "running"
    Mar 15 22:00:09.009: INFO: Pod "filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d": Phase="Running", Reason="", readiness=true. Elapsed: 10.645432ms
    Mar 15 22:00:09.009: INFO: Pod "filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 03/15/23 22:00:09.009
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e.174cb6aa9bb01f57], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9443/filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e to i-0baafb3f4e7bf826e] 03/15/23 22:00:09.023
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e.174cb6aadc3c38b4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/15/23 22:00:09.024
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e.174cb6aadede85d2], Reason = [Created], Message = [Created container filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e] 03/15/23 22:00:09.024
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e.174cb6aae681d1f9], Reason = [Started], Message = [Started container filler-pod-257959b4-b0b3-478d-89b3-0c0115eb479e] 03/15/23 22:00:09.024
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d.174cb6aa9bb0b967], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9443/filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d to i-0faaf83f00b43c88c] 03/15/23 22:00:09.024
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d.174cb6aada88ecee], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/15/23 22:00:09.024
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d.174cb6aadf5362dc], Reason = [Created], Message = [Created container filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d] 03/15/23 22:00:09.024
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d.174cb6aae7a5421e], Reason = [Started], Message = [Started container filler-pod-7c804a9d-90ed-4c3e-b45a-6a43703c3c8d] 03/15/23 22:00:09.024
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae.174cb6aa96dc0b1c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9443/filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae to i-077ee0eb7ec5a02aa] 03/15/23 22:00:09.025
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae.174cb6aae4839f84], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/15/23 22:00:09.025
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae.174cb6aae598afa1], Reason = [Created], Message = [Created container filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae] 03/15/23 22:00:09.025
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae.174cb6aaec39302b], Reason = [Started], Message = [Started container filler-pod-8431ac5c-232e-473a-9ab1-5e76d7be2eae] 03/15/23 22:00:09.025
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.174cb6ab8e205cbc], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/4 nodes are available: 1 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] 03/15/23 22:00:09.069
    STEP: removing the label node off the node i-077ee0eb7ec5a02aa 03/15/23 22:00:10.05
    STEP: verifying the node doesn't have the label node 03/15/23 22:00:10.068
    STEP: removing the label node off the node i-0baafb3f4e7bf826e 03/15/23 22:00:10.138
    STEP: verifying the node doesn't have the label node 03/15/23 22:00:10.232
    STEP: removing the label node off the node i-0faaf83f00b43c88c 03/15/23 22:00:10.242
    STEP: verifying the node doesn't have the label node 03/15/23 22:00:10.275
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:00:10.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9443" for this suite. 03/15/23 22:00:10.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:00:10.388
Mar 15 22:00:10.388: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename init-container 03/15/23 22:00:10.393
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:10.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:10.432
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 03/15/23 22:00:10.44
Mar 15 22:00:10.440: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:00:13.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7564" for this suite. 03/15/23 22:00:14
------------------------------
• [3.621 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:00:10.388
    Mar 15 22:00:10.388: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename init-container 03/15/23 22:00:10.393
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:10.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:10.432
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 03/15/23 22:00:10.44
    Mar 15 22:00:10.440: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:00:13.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7564" for this suite. 03/15/23 22:00:14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:00:14.009
Mar 15 22:00:14.009: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename resourcequota 03/15/23 22:00:14.01
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:14.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:14.029
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 03/15/23 22:00:14.032
STEP: Creating a ResourceQuota 03/15/23 22:00:19.038
STEP: Ensuring resource quota status is calculated 03/15/23 22:00:19.053
STEP: Creating a ReplicaSet 03/15/23 22:00:21.058
STEP: Ensuring resource quota status captures replicaset creation 03/15/23 22:00:21.075
STEP: Deleting a ReplicaSet 03/15/23 22:00:23.078
STEP: Ensuring resource quota status released usage 03/15/23 22:00:23.084
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 15 22:00:25.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1432" for this suite. 03/15/23 22:00:25.093
------------------------------
• [SLOW TEST] [11.090 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:00:14.009
    Mar 15 22:00:14.009: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename resourcequota 03/15/23 22:00:14.01
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:14.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:14.029
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 03/15/23 22:00:14.032
    STEP: Creating a ResourceQuota 03/15/23 22:00:19.038
    STEP: Ensuring resource quota status is calculated 03/15/23 22:00:19.053
    STEP: Creating a ReplicaSet 03/15/23 22:00:21.058
    STEP: Ensuring resource quota status captures replicaset creation 03/15/23 22:00:21.075
    STEP: Deleting a ReplicaSet 03/15/23 22:00:23.078
    STEP: Ensuring resource quota status released usage 03/15/23 22:00:23.084
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:00:25.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1432" for this suite. 03/15/23 22:00:25.093
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:00:25.101
Mar 15 22:00:25.101: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename controllerrevisions 03/15/23 22:00:25.102
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:25.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:25.116
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-lmb4s-daemon-set" 03/15/23 22:00:25.139
STEP: Check that daemon pods launch on every node of the cluster. 03/15/23 22:00:25.147
Mar 15 22:00:25.153: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:00:25.157: INFO: Number of nodes with available pods controlled by daemonset e2e-lmb4s-daemon-set: 0
Mar 15 22:00:25.157: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:00:26.162: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:00:26.165: INFO: Number of nodes with available pods controlled by daemonset e2e-lmb4s-daemon-set: 0
Mar 15 22:00:26.165: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:00:27.166: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:00:27.173: INFO: Number of nodes with available pods controlled by daemonset e2e-lmb4s-daemon-set: 3
Mar 15 22:00:27.173: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-lmb4s-daemon-set
STEP: Confirm DaemonSet "e2e-lmb4s-daemon-set" successfully created with "daemonset-name=e2e-lmb4s-daemon-set" label 03/15/23 22:00:27.18
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-lmb4s-daemon-set" 03/15/23 22:00:27.186
Mar 15 22:00:27.189: INFO: Located ControllerRevision: "e2e-lmb4s-daemon-set-558876b5d7"
STEP: Patching ControllerRevision "e2e-lmb4s-daemon-set-558876b5d7" 03/15/23 22:00:27.192
Mar 15 22:00:27.204: INFO: e2e-lmb4s-daemon-set-558876b5d7 has been patched
STEP: Create a new ControllerRevision 03/15/23 22:00:27.204
Mar 15 22:00:27.210: INFO: Created ControllerRevision: e2e-lmb4s-daemon-set-59f7c48b4c
STEP: Confirm that there are two ControllerRevisions 03/15/23 22:00:27.211
Mar 15 22:00:27.211: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 15 22:00:27.214: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-lmb4s-daemon-set-558876b5d7" 03/15/23 22:00:27.214
STEP: Confirm that there is only one ControllerRevision 03/15/23 22:00:27.218
Mar 15 22:00:27.218: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 15 22:00:27.221: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-lmb4s-daemon-set-59f7c48b4c" 03/15/23 22:00:27.223
Mar 15 22:00:27.230: INFO: e2e-lmb4s-daemon-set-59f7c48b4c has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 03/15/23 22:00:27.23
W0315 22:00:27.237614      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 03/15/23 22:00:27.237
Mar 15 22:00:27.238: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 15 22:00:28.243: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 15 22:00:28.249: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-lmb4s-daemon-set-59f7c48b4c=updated" 03/15/23 22:00:28.249
STEP: Confirm that there is only one ControllerRevision 03/15/23 22:00:28.255
Mar 15 22:00:28.255: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 15 22:00:28.268: INFO: Found 1 ControllerRevisions
Mar 15 22:00:28.274: INFO: ControllerRevision "e2e-lmb4s-daemon-set-9d88fbc78" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-lmb4s-daemon-set" 03/15/23 22:00:28.278
STEP: deleting DaemonSet.extensions e2e-lmb4s-daemon-set in namespace controllerrevisions-1377, will wait for the garbage collector to delete the pods 03/15/23 22:00:28.278
Mar 15 22:00:28.338: INFO: Deleting DaemonSet.extensions e2e-lmb4s-daemon-set took: 5.470789ms
Mar 15 22:00:28.438: INFO: Terminating DaemonSet.extensions e2e-lmb4s-daemon-set pods took: 100.516386ms
Mar 15 22:00:30.145: INFO: Number of nodes with available pods controlled by daemonset e2e-lmb4s-daemon-set: 0
Mar 15 22:00:30.145: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-lmb4s-daemon-set
Mar 15 22:00:30.148: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9758"},"items":null}

Mar 15 22:00:30.151: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9758"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:00:30.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-1377" for this suite. 03/15/23 22:00:30.169
------------------------------
• [SLOW TEST] [5.077 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:00:25.101
    Mar 15 22:00:25.101: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename controllerrevisions 03/15/23 22:00:25.102
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:25.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:25.116
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-lmb4s-daemon-set" 03/15/23 22:00:25.139
    STEP: Check that daemon pods launch on every node of the cluster. 03/15/23 22:00:25.147
    Mar 15 22:00:25.153: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:00:25.157: INFO: Number of nodes with available pods controlled by daemonset e2e-lmb4s-daemon-set: 0
    Mar 15 22:00:25.157: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:00:26.162: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:00:26.165: INFO: Number of nodes with available pods controlled by daemonset e2e-lmb4s-daemon-set: 0
    Mar 15 22:00:26.165: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:00:27.166: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:00:27.173: INFO: Number of nodes with available pods controlled by daemonset e2e-lmb4s-daemon-set: 3
    Mar 15 22:00:27.173: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-lmb4s-daemon-set
    STEP: Confirm DaemonSet "e2e-lmb4s-daemon-set" successfully created with "daemonset-name=e2e-lmb4s-daemon-set" label 03/15/23 22:00:27.18
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-lmb4s-daemon-set" 03/15/23 22:00:27.186
    Mar 15 22:00:27.189: INFO: Located ControllerRevision: "e2e-lmb4s-daemon-set-558876b5d7"
    STEP: Patching ControllerRevision "e2e-lmb4s-daemon-set-558876b5d7" 03/15/23 22:00:27.192
    Mar 15 22:00:27.204: INFO: e2e-lmb4s-daemon-set-558876b5d7 has been patched
    STEP: Create a new ControllerRevision 03/15/23 22:00:27.204
    Mar 15 22:00:27.210: INFO: Created ControllerRevision: e2e-lmb4s-daemon-set-59f7c48b4c
    STEP: Confirm that there are two ControllerRevisions 03/15/23 22:00:27.211
    Mar 15 22:00:27.211: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 15 22:00:27.214: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-lmb4s-daemon-set-558876b5d7" 03/15/23 22:00:27.214
    STEP: Confirm that there is only one ControllerRevision 03/15/23 22:00:27.218
    Mar 15 22:00:27.218: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 15 22:00:27.221: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-lmb4s-daemon-set-59f7c48b4c" 03/15/23 22:00:27.223
    Mar 15 22:00:27.230: INFO: e2e-lmb4s-daemon-set-59f7c48b4c has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 03/15/23 22:00:27.23
    W0315 22:00:27.237614      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 03/15/23 22:00:27.237
    Mar 15 22:00:27.238: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 15 22:00:28.243: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 15 22:00:28.249: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-lmb4s-daemon-set-59f7c48b4c=updated" 03/15/23 22:00:28.249
    STEP: Confirm that there is only one ControllerRevision 03/15/23 22:00:28.255
    Mar 15 22:00:28.255: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 15 22:00:28.268: INFO: Found 1 ControllerRevisions
    Mar 15 22:00:28.274: INFO: ControllerRevision "e2e-lmb4s-daemon-set-9d88fbc78" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-lmb4s-daemon-set" 03/15/23 22:00:28.278
    STEP: deleting DaemonSet.extensions e2e-lmb4s-daemon-set in namespace controllerrevisions-1377, will wait for the garbage collector to delete the pods 03/15/23 22:00:28.278
    Mar 15 22:00:28.338: INFO: Deleting DaemonSet.extensions e2e-lmb4s-daemon-set took: 5.470789ms
    Mar 15 22:00:28.438: INFO: Terminating DaemonSet.extensions e2e-lmb4s-daemon-set pods took: 100.516386ms
    Mar 15 22:00:30.145: INFO: Number of nodes with available pods controlled by daemonset e2e-lmb4s-daemon-set: 0
    Mar 15 22:00:30.145: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-lmb4s-daemon-set
    Mar 15 22:00:30.148: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9758"},"items":null}

    Mar 15 22:00:30.151: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9758"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:00:30.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-1377" for this suite. 03/15/23 22:00:30.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:00:30.18
Mar 15 22:00:30.180: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubelet-test 03/15/23 22:00:30.18
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:30.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:30.203
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:00:30.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6915" for this suite. 03/15/23 22:00:30.261
------------------------------
• [0.101 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:00:30.18
    Mar 15 22:00:30.180: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubelet-test 03/15/23 22:00:30.18
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:30.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:30.203
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:00:30.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6915" for this suite. 03/15/23 22:00:30.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:00:30.288
Mar 15 22:00:30.288: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 22:00:30.289
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:30.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:30.336
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 22:00:30.384
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:00:30.948
STEP: Deploying the webhook pod 03/15/23 22:00:30.954
STEP: Wait for the deployment to be ready 03/15/23 22:00:30.964
Mar 15 22:00:30.979: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 22:00:32.987
STEP: Verifying the service has paired with the endpoint 03/15/23 22:00:33.01
Mar 15 22:00:34.013: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 03/15/23 22:00:34.016
STEP: create a pod that should be denied by the webhook 03/15/23 22:00:34.033
STEP: create a pod that causes the webhook to hang 03/15/23 22:00:34.046
STEP: create a configmap that should be denied by the webhook 03/15/23 22:00:44.079
STEP: create a configmap that should be admitted by the webhook 03/15/23 22:00:44.101
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/15/23 22:00:44.111
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/15/23 22:00:44.119
STEP: create a namespace that bypass the webhook 03/15/23 22:00:44.124
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/15/23 22:00:44.131
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:00:44.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1499" for this suite. 03/15/23 22:00:44.236
STEP: Destroying namespace "webhook-1499-markers" for this suite. 03/15/23 22:00:44.247
------------------------------
• [SLOW TEST] [13.978 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:00:30.288
    Mar 15 22:00:30.288: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 22:00:30.289
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:30.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:30.336
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 22:00:30.384
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:00:30.948
    STEP: Deploying the webhook pod 03/15/23 22:00:30.954
    STEP: Wait for the deployment to be ready 03/15/23 22:00:30.964
    Mar 15 22:00:30.979: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 22:00:32.987
    STEP: Verifying the service has paired with the endpoint 03/15/23 22:00:33.01
    Mar 15 22:00:34.013: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 03/15/23 22:00:34.016
    STEP: create a pod that should be denied by the webhook 03/15/23 22:00:34.033
    STEP: create a pod that causes the webhook to hang 03/15/23 22:00:34.046
    STEP: create a configmap that should be denied by the webhook 03/15/23 22:00:44.079
    STEP: create a configmap that should be admitted by the webhook 03/15/23 22:00:44.101
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/15/23 22:00:44.111
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/15/23 22:00:44.119
    STEP: create a namespace that bypass the webhook 03/15/23 22:00:44.124
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/15/23 22:00:44.131
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:00:44.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1499" for this suite. 03/15/23 22:00:44.236
    STEP: Destroying namespace "webhook-1499-markers" for this suite. 03/15/23 22:00:44.247
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:00:44.276
Mar 15 22:00:44.277: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pods 03/15/23 22:00:44.28
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:44.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:44.312
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 03/15/23 22:00:44.316
STEP: submitting the pod to kubernetes 03/15/23 22:00:44.316
STEP: verifying QOS class is set on the pod 03/15/23 22:00:44.328
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Mar 15 22:00:44.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3702" for this suite. 03/15/23 22:00:44.343
------------------------------
• [0.086 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:00:44.276
    Mar 15 22:00:44.277: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pods 03/15/23 22:00:44.28
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:44.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:44.312
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 03/15/23 22:00:44.316
    STEP: submitting the pod to kubernetes 03/15/23 22:00:44.316
    STEP: verifying QOS class is set on the pod 03/15/23 22:00:44.328
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:00:44.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3702" for this suite. 03/15/23 22:00:44.343
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:00:44.364
Mar 15 22:00:44.365: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 22:00:44.366
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:44.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:44.398
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-769 03/15/23 22:00:44.403
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-769 to expose endpoints map[] 03/15/23 22:00:44.42
Mar 15 22:00:44.431: INFO: successfully validated that service endpoint-test2 in namespace services-769 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-769 03/15/23 22:00:44.431
Mar 15 22:00:44.441: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-769" to be "running and ready"
Mar 15 22:00:44.444: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.173108ms
Mar 15 22:00:44.444: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:00:46.449: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007809781s
Mar 15 22:00:46.449: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 15 22:00:46.449: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-769 to expose endpoints map[pod1:[80]] 03/15/23 22:00:46.452
Mar 15 22:00:46.464: INFO: successfully validated that service endpoint-test2 in namespace services-769 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 03/15/23 22:00:46.465
Mar 15 22:00:46.465: INFO: Creating new exec pod
Mar 15 22:00:46.470: INFO: Waiting up to 5m0s for pod "execpod9969w" in namespace "services-769" to be "running"
Mar 15 22:00:46.473: INFO: Pod "execpod9969w": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48998ms
Mar 15 22:00:48.477: INFO: Pod "execpod9969w": Phase="Running", Reason="", readiness=true. Elapsed: 2.007120146s
Mar 15 22:00:48.477: INFO: Pod "execpod9969w" satisfied condition "running"
Mar 15 22:00:49.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-769 exec execpod9969w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 15 22:00:49.690: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 15 22:00:49.690: INFO: stdout: ""
Mar 15 22:00:49.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-769 exec execpod9969w -- /bin/sh -x -c nc -v -z -w 2 100.67.48.214 80'
Mar 15 22:00:49.850: INFO: stderr: "+ nc -v -z -w 2 100.67.48.214 80\nConnection to 100.67.48.214 80 port [tcp/http] succeeded!\n"
Mar 15 22:00:49.850: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-769 03/15/23 22:00:49.85
Mar 15 22:00:49.856: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-769" to be "running and ready"
Mar 15 22:00:49.860: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.415105ms
Mar 15 22:00:49.860: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:00:51.864: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007836969s
Mar 15 22:00:51.864: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 15 22:00:51.864: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-769 to expose endpoints map[pod1:[80] pod2:[80]] 03/15/23 22:00:51.867
Mar 15 22:00:51.876: INFO: successfully validated that service endpoint-test2 in namespace services-769 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 03/15/23 22:00:51.876
Mar 15 22:00:52.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-769 exec execpod9969w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 15 22:00:53.039: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 15 22:00:53.039: INFO: stdout: ""
Mar 15 22:00:53.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-769 exec execpod9969w -- /bin/sh -x -c nc -v -z -w 2 100.67.48.214 80'
Mar 15 22:00:53.211: INFO: stderr: "+ nc -v -z -w 2 100.67.48.214 80\nConnection to 100.67.48.214 80 port [tcp/http] succeeded!\n"
Mar 15 22:00:53.211: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-769 03/15/23 22:00:53.211
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-769 to expose endpoints map[pod2:[80]] 03/15/23 22:00:53.222
Mar 15 22:00:54.274: INFO: successfully validated that service endpoint-test2 in namespace services-769 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 03/15/23 22:00:54.275
Mar 15 22:00:55.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-769 exec execpod9969w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 15 22:00:55.633: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 15 22:00:55.633: INFO: stdout: ""
Mar 15 22:00:55.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-769 exec execpod9969w -- /bin/sh -x -c nc -v -z -w 2 100.67.48.214 80'
Mar 15 22:00:55.954: INFO: stderr: "+ nc -v -z -w 2 100.67.48.214 80\nConnection to 100.67.48.214 80 port [tcp/http] succeeded!\n"
Mar 15 22:00:55.954: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-769 03/15/23 22:00:55.954
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-769 to expose endpoints map[] 03/15/23 22:00:55.979
Mar 15 22:00:56.022: INFO: successfully validated that service endpoint-test2 in namespace services-769 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 22:00:56.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-769" for this suite. 03/15/23 22:00:56.077
------------------------------
• [SLOW TEST] [11.720 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:00:44.364
    Mar 15 22:00:44.365: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 22:00:44.366
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:44.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:44.398
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-769 03/15/23 22:00:44.403
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-769 to expose endpoints map[] 03/15/23 22:00:44.42
    Mar 15 22:00:44.431: INFO: successfully validated that service endpoint-test2 in namespace services-769 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-769 03/15/23 22:00:44.431
    Mar 15 22:00:44.441: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-769" to be "running and ready"
    Mar 15 22:00:44.444: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.173108ms
    Mar 15 22:00:44.444: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:00:46.449: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007809781s
    Mar 15 22:00:46.449: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 15 22:00:46.449: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-769 to expose endpoints map[pod1:[80]] 03/15/23 22:00:46.452
    Mar 15 22:00:46.464: INFO: successfully validated that service endpoint-test2 in namespace services-769 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 03/15/23 22:00:46.465
    Mar 15 22:00:46.465: INFO: Creating new exec pod
    Mar 15 22:00:46.470: INFO: Waiting up to 5m0s for pod "execpod9969w" in namespace "services-769" to be "running"
    Mar 15 22:00:46.473: INFO: Pod "execpod9969w": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48998ms
    Mar 15 22:00:48.477: INFO: Pod "execpod9969w": Phase="Running", Reason="", readiness=true. Elapsed: 2.007120146s
    Mar 15 22:00:48.477: INFO: Pod "execpod9969w" satisfied condition "running"
    Mar 15 22:00:49.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-769 exec execpod9969w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 15 22:00:49.690: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 15 22:00:49.690: INFO: stdout: ""
    Mar 15 22:00:49.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-769 exec execpod9969w -- /bin/sh -x -c nc -v -z -w 2 100.67.48.214 80'
    Mar 15 22:00:49.850: INFO: stderr: "+ nc -v -z -w 2 100.67.48.214 80\nConnection to 100.67.48.214 80 port [tcp/http] succeeded!\n"
    Mar 15 22:00:49.850: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-769 03/15/23 22:00:49.85
    Mar 15 22:00:49.856: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-769" to be "running and ready"
    Mar 15 22:00:49.860: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.415105ms
    Mar 15 22:00:49.860: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:00:51.864: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007836969s
    Mar 15 22:00:51.864: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 15 22:00:51.864: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-769 to expose endpoints map[pod1:[80] pod2:[80]] 03/15/23 22:00:51.867
    Mar 15 22:00:51.876: INFO: successfully validated that service endpoint-test2 in namespace services-769 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 03/15/23 22:00:51.876
    Mar 15 22:00:52.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-769 exec execpod9969w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 15 22:00:53.039: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 15 22:00:53.039: INFO: stdout: ""
    Mar 15 22:00:53.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-769 exec execpod9969w -- /bin/sh -x -c nc -v -z -w 2 100.67.48.214 80'
    Mar 15 22:00:53.211: INFO: stderr: "+ nc -v -z -w 2 100.67.48.214 80\nConnection to 100.67.48.214 80 port [tcp/http] succeeded!\n"
    Mar 15 22:00:53.211: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-769 03/15/23 22:00:53.211
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-769 to expose endpoints map[pod2:[80]] 03/15/23 22:00:53.222
    Mar 15 22:00:54.274: INFO: successfully validated that service endpoint-test2 in namespace services-769 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 03/15/23 22:00:54.275
    Mar 15 22:00:55.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-769 exec execpod9969w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 15 22:00:55.633: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 15 22:00:55.633: INFO: stdout: ""
    Mar 15 22:00:55.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-769 exec execpod9969w -- /bin/sh -x -c nc -v -z -w 2 100.67.48.214 80'
    Mar 15 22:00:55.954: INFO: stderr: "+ nc -v -z -w 2 100.67.48.214 80\nConnection to 100.67.48.214 80 port [tcp/http] succeeded!\n"
    Mar 15 22:00:55.954: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-769 03/15/23 22:00:55.954
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-769 to expose endpoints map[] 03/15/23 22:00:55.979
    Mar 15 22:00:56.022: INFO: successfully validated that service endpoint-test2 in namespace services-769 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:00:56.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-769" for this suite. 03/15/23 22:00:56.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:00:56.103
Mar 15 22:00:56.103: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-lifecycle-hook 03/15/23 22:00:56.105
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:56.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:56.153
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/15/23 22:00:56.163
Mar 15 22:00:56.173: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9529" to be "running and ready"
Mar 15 22:00:56.177: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.30278ms
Mar 15 22:00:56.177: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:00:58.181: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007632655s
Mar 15 22:00:58.181: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 15 22:00:58.181: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 03/15/23 22:00:58.184
Mar 15 22:00:58.193: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9529" to be "running and ready"
Mar 15 22:00:58.205: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.581431ms
Mar 15 22:00:58.205: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:01:00.228: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.035688561s
Mar 15 22:01:00.229: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Mar 15 22:01:00.229: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/15/23 22:01:00.258
STEP: delete the pod with lifecycle hook 03/15/23 22:01:00.311
Mar 15 22:01:00.336: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 15 22:01:00.340: INFO: Pod pod-with-poststart-http-hook still exists
Mar 15 22:01:02.341: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 15 22:01:02.345: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 15 22:01:02.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9529" for this suite. 03/15/23 22:01:02.349
------------------------------
• [SLOW TEST] [6.254 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:00:56.103
    Mar 15 22:00:56.103: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/15/23 22:00:56.105
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:00:56.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:00:56.153
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/15/23 22:00:56.163
    Mar 15 22:00:56.173: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9529" to be "running and ready"
    Mar 15 22:00:56.177: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.30278ms
    Mar 15 22:00:56.177: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:00:58.181: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007632655s
    Mar 15 22:00:58.181: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 15 22:00:58.181: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 03/15/23 22:00:58.184
    Mar 15 22:00:58.193: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9529" to be "running and ready"
    Mar 15 22:00:58.205: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.581431ms
    Mar 15 22:00:58.205: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:01:00.228: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.035688561s
    Mar 15 22:01:00.229: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Mar 15 22:01:00.229: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/15/23 22:01:00.258
    STEP: delete the pod with lifecycle hook 03/15/23 22:01:00.311
    Mar 15 22:01:00.336: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 15 22:01:00.340: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 15 22:01:02.341: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 15 22:01:02.345: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:01:02.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9529" for this suite. 03/15/23 22:01:02.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:01:02.358
Mar 15 22:01:02.359: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:01:02.359
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:01:02.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:01:02.378
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-ab2b6b08-18f0-4c75-ab46-f90a747366c3 03/15/23 22:01:02.384
STEP: Creating a pod to test consume configMaps 03/15/23 22:01:02.393
Mar 15 22:01:02.412: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525" in namespace "configmap-4906" to be "Succeeded or Failed"
Mar 15 22:01:02.419: INFO: Pod "pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525": Phase="Pending", Reason="", readiness=false. Elapsed: 6.392003ms
Mar 15 22:01:04.422: INFO: Pod "pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009784032s
Mar 15 22:01:06.422: INFO: Pod "pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009604768s
STEP: Saw pod success 03/15/23 22:01:06.422
Mar 15 22:01:06.423: INFO: Pod "pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525" satisfied condition "Succeeded or Failed"
Mar 15 22:01:06.425: INFO: Trying to get logs from node i-077ee0eb7ec5a02aa pod pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525 container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:01:06.446
Mar 15 22:01:06.471: INFO: Waiting for pod pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525 to disappear
Mar 15 22:01:06.476: INFO: Pod pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:01:06.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4906" for this suite. 03/15/23 22:01:06.481
------------------------------
• [4.133 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:01:02.358
    Mar 15 22:01:02.359: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:01:02.359
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:01:02.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:01:02.378
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-ab2b6b08-18f0-4c75-ab46-f90a747366c3 03/15/23 22:01:02.384
    STEP: Creating a pod to test consume configMaps 03/15/23 22:01:02.393
    Mar 15 22:01:02.412: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525" in namespace "configmap-4906" to be "Succeeded or Failed"
    Mar 15 22:01:02.419: INFO: Pod "pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525": Phase="Pending", Reason="", readiness=false. Elapsed: 6.392003ms
    Mar 15 22:01:04.422: INFO: Pod "pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009784032s
    Mar 15 22:01:06.422: INFO: Pod "pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009604768s
    STEP: Saw pod success 03/15/23 22:01:06.422
    Mar 15 22:01:06.423: INFO: Pod "pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525" satisfied condition "Succeeded or Failed"
    Mar 15 22:01:06.425: INFO: Trying to get logs from node i-077ee0eb7ec5a02aa pod pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525 container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:01:06.446
    Mar 15 22:01:06.471: INFO: Waiting for pod pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525 to disappear
    Mar 15 22:01:06.476: INFO: Pod pod-configmaps-ee0abd2a-4939-4514-9586-8b7859819525 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:01:06.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4906" for this suite. 03/15/23 22:01:06.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:01:06.496
Mar 15 22:01:06.497: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-probe 03/15/23 22:01:06.497
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:01:06.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:01:06.536
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064 in namespace container-probe-6069 03/15/23 22:01:06.545
Mar 15 22:01:06.556: INFO: Waiting up to 5m0s for pod "busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064" in namespace "container-probe-6069" to be "not pending"
Mar 15 22:01:06.564: INFO: Pod "busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064": Phase="Pending", Reason="", readiness=false. Elapsed: 7.569674ms
Mar 15 22:01:08.568: INFO: Pod "busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064": Phase="Running", Reason="", readiness=true. Elapsed: 2.011844291s
Mar 15 22:01:08.568: INFO: Pod "busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064" satisfied condition "not pending"
Mar 15 22:01:08.568: INFO: Started pod busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064 in namespace container-probe-6069
STEP: checking the pod's current state and verifying that restartCount is present 03/15/23 22:01:08.568
Mar 15 22:01:08.572: INFO: Initial restart count of pod busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064 is 0
Mar 15 22:01:58.698: INFO: Restart count of pod container-probe-6069/busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064 is now 1 (50.125782152s elapsed)
STEP: deleting the pod 03/15/23 22:01:58.698
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 15 22:01:58.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6069" for this suite. 03/15/23 22:01:58.734
------------------------------
• [SLOW TEST] [52.256 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:01:06.496
    Mar 15 22:01:06.497: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-probe 03/15/23 22:01:06.497
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:01:06.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:01:06.536
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064 in namespace container-probe-6069 03/15/23 22:01:06.545
    Mar 15 22:01:06.556: INFO: Waiting up to 5m0s for pod "busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064" in namespace "container-probe-6069" to be "not pending"
    Mar 15 22:01:06.564: INFO: Pod "busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064": Phase="Pending", Reason="", readiness=false. Elapsed: 7.569674ms
    Mar 15 22:01:08.568: INFO: Pod "busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064": Phase="Running", Reason="", readiness=true. Elapsed: 2.011844291s
    Mar 15 22:01:08.568: INFO: Pod "busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064" satisfied condition "not pending"
    Mar 15 22:01:08.568: INFO: Started pod busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064 in namespace container-probe-6069
    STEP: checking the pod's current state and verifying that restartCount is present 03/15/23 22:01:08.568
    Mar 15 22:01:08.572: INFO: Initial restart count of pod busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064 is 0
    Mar 15 22:01:58.698: INFO: Restart count of pod container-probe-6069/busybox-bbea1f5f-c59c-4d1f-9f5a-3720d76d0064 is now 1 (50.125782152s elapsed)
    STEP: deleting the pod 03/15/23 22:01:58.698
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:01:58.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6069" for this suite. 03/15/23 22:01:58.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:01:58.754
Mar 15 22:01:58.754: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubelet-test 03/15/23 22:01:58.755
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:01:58.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:01:58.781
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 03/15/23 22:01:58.792
Mar 15 22:01:58.792: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesc00a3b34-2db2-4ea7-93a1-120a6e1eecce" in namespace "kubelet-test-4176" to be "completed"
Mar 15 22:01:58.796: INFO: Pod "agnhost-host-aliasesc00a3b34-2db2-4ea7-93a1-120a6e1eecce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.979511ms
Mar 15 22:02:00.802: INFO: Pod "agnhost-host-aliasesc00a3b34-2db2-4ea7-93a1-120a6e1eecce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009977238s
Mar 15 22:02:02.799: INFO: Pod "agnhost-host-aliasesc00a3b34-2db2-4ea7-93a1-120a6e1eecce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007601288s
Mar 15 22:02:02.799: INFO: Pod "agnhost-host-aliasesc00a3b34-2db2-4ea7-93a1-120a6e1eecce" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:02:02.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4176" for this suite. 03/15/23 22:02:02.813
------------------------------
• [4.073 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:01:58.754
    Mar 15 22:01:58.754: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubelet-test 03/15/23 22:01:58.755
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:01:58.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:01:58.781
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 03/15/23 22:01:58.792
    Mar 15 22:01:58.792: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesc00a3b34-2db2-4ea7-93a1-120a6e1eecce" in namespace "kubelet-test-4176" to be "completed"
    Mar 15 22:01:58.796: INFO: Pod "agnhost-host-aliasesc00a3b34-2db2-4ea7-93a1-120a6e1eecce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.979511ms
    Mar 15 22:02:00.802: INFO: Pod "agnhost-host-aliasesc00a3b34-2db2-4ea7-93a1-120a6e1eecce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009977238s
    Mar 15 22:02:02.799: INFO: Pod "agnhost-host-aliasesc00a3b34-2db2-4ea7-93a1-120a6e1eecce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007601288s
    Mar 15 22:02:02.799: INFO: Pod "agnhost-host-aliasesc00a3b34-2db2-4ea7-93a1-120a6e1eecce" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:02:02.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4176" for this suite. 03/15/23 22:02:02.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:02:02.834
Mar 15 22:02:02.834: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename svcaccounts 03/15/23 22:02:02.835
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:02:02.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:02:02.859
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-p6cpf"  03/15/23 22:02:02.872
Mar 15 22:02:02.878: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-p6cpf"  03/15/23 22:02:02.878
Mar 15 22:02:02.886: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 15 22:02:02.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1625" for this suite. 03/15/23 22:02:02.89
------------------------------
• [0.064 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:02:02.834
    Mar 15 22:02:02.834: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename svcaccounts 03/15/23 22:02:02.835
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:02:02.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:02:02.859
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-p6cpf"  03/15/23 22:02:02.872
    Mar 15 22:02:02.878: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-p6cpf"  03/15/23 22:02:02.878
    Mar 15 22:02:02.886: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:02:02.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1625" for this suite. 03/15/23 22:02:02.89
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:02:02.9
Mar 15 22:02:02.900: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 22:02:02.901
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:02:02.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:02:02.927
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/15/23 22:02:02.936
Mar 15 22:02:02.957: INFO: Waiting up to 5m0s for pod "pod-765a2db3-fbe2-460d-9d68-c7c2a6070750" in namespace "emptydir-4721" to be "Succeeded or Failed"
Mar 15 22:02:02.960: INFO: Pod "pod-765a2db3-fbe2-460d-9d68-c7c2a6070750": Phase="Pending", Reason="", readiness=false. Elapsed: 3.682076ms
Mar 15 22:02:04.964: INFO: Pod "pod-765a2db3-fbe2-460d-9d68-c7c2a6070750": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006912781s
Mar 15 22:02:06.967: INFO: Pod "pod-765a2db3-fbe2-460d-9d68-c7c2a6070750": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01063797s
STEP: Saw pod success 03/15/23 22:02:06.967
Mar 15 22:02:06.968: INFO: Pod "pod-765a2db3-fbe2-460d-9d68-c7c2a6070750" satisfied condition "Succeeded or Failed"
Mar 15 22:02:06.971: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-765a2db3-fbe2-460d-9d68-c7c2a6070750 container test-container: <nil>
STEP: delete the pod 03/15/23 22:02:06.979
Mar 15 22:02:06.993: INFO: Waiting for pod pod-765a2db3-fbe2-460d-9d68-c7c2a6070750 to disappear
Mar 15 22:02:06.996: INFO: Pod pod-765a2db3-fbe2-460d-9d68-c7c2a6070750 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:02:06.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4721" for this suite. 03/15/23 22:02:07.002
------------------------------
• [4.108 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:02:02.9
    Mar 15 22:02:02.900: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 22:02:02.901
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:02:02.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:02:02.927
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/15/23 22:02:02.936
    Mar 15 22:02:02.957: INFO: Waiting up to 5m0s for pod "pod-765a2db3-fbe2-460d-9d68-c7c2a6070750" in namespace "emptydir-4721" to be "Succeeded or Failed"
    Mar 15 22:02:02.960: INFO: Pod "pod-765a2db3-fbe2-460d-9d68-c7c2a6070750": Phase="Pending", Reason="", readiness=false. Elapsed: 3.682076ms
    Mar 15 22:02:04.964: INFO: Pod "pod-765a2db3-fbe2-460d-9d68-c7c2a6070750": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006912781s
    Mar 15 22:02:06.967: INFO: Pod "pod-765a2db3-fbe2-460d-9d68-c7c2a6070750": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01063797s
    STEP: Saw pod success 03/15/23 22:02:06.967
    Mar 15 22:02:06.968: INFO: Pod "pod-765a2db3-fbe2-460d-9d68-c7c2a6070750" satisfied condition "Succeeded or Failed"
    Mar 15 22:02:06.971: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-765a2db3-fbe2-460d-9d68-c7c2a6070750 container test-container: <nil>
    STEP: delete the pod 03/15/23 22:02:06.979
    Mar 15 22:02:06.993: INFO: Waiting for pod pod-765a2db3-fbe2-460d-9d68-c7c2a6070750 to disappear
    Mar 15 22:02:06.996: INFO: Pod pod-765a2db3-fbe2-460d-9d68-c7c2a6070750 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:02:06.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4721" for this suite. 03/15/23 22:02:07.002
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:02:07.009
Mar 15 22:02:07.009: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 22:02:07.01
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:02:07.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:02:07.03
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 22:02:07.048
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:02:07.571
STEP: Deploying the webhook pod 03/15/23 22:02:07.582
STEP: Wait for the deployment to be ready 03/15/23 22:02:07.612
Mar 15 22:02:07.623: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 22:02:09.643
STEP: Verifying the service has paired with the endpoint 03/15/23 22:02:09.701
Mar 15 22:02:10.704: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/15/23 22:02:10.72
STEP: create a namespace for the webhook 03/15/23 22:02:10.744
STEP: create a configmap should be unconditionally rejected by the webhook 03/15/23 22:02:10.752
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:02:10.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9450" for this suite. 03/15/23 22:02:10.875
STEP: Destroying namespace "webhook-9450-markers" for this suite. 03/15/23 22:02:10.884
------------------------------
• [3.890 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:02:07.009
    Mar 15 22:02:07.009: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 22:02:07.01
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:02:07.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:02:07.03
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 22:02:07.048
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:02:07.571
    STEP: Deploying the webhook pod 03/15/23 22:02:07.582
    STEP: Wait for the deployment to be ready 03/15/23 22:02:07.612
    Mar 15 22:02:07.623: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 22:02:09.643
    STEP: Verifying the service has paired with the endpoint 03/15/23 22:02:09.701
    Mar 15 22:02:10.704: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/15/23 22:02:10.72
    STEP: create a namespace for the webhook 03/15/23 22:02:10.744
    STEP: create a configmap should be unconditionally rejected by the webhook 03/15/23 22:02:10.752
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:02:10.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9450" for this suite. 03/15/23 22:02:10.875
    STEP: Destroying namespace "webhook-9450-markers" for this suite. 03/15/23 22:02:10.884
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:02:10.899
Mar 15 22:02:10.899: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename custom-resource-definition 03/15/23 22:02:10.901
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:02:10.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:02:10.939
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Mar 15 22:02:10.942: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:02:14.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2515" for this suite. 03/15/23 22:02:14.193
------------------------------
• [3.300 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:02:10.899
    Mar 15 22:02:10.899: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename custom-resource-definition 03/15/23 22:02:10.901
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:02:10.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:02:10.939
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Mar 15 22:02:10.942: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:02:14.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2515" for this suite. 03/15/23 22:02:14.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:02:14.202
Mar 15 22:02:14.203: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename endpointslicemirroring 03/15/23 22:02:14.203
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:02:14.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:02:14.222
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 03/15/23 22:02:14.242
Mar 15 22:02:14.260: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 03/15/23 22:02:16.264
STEP: mirroring deletion of a custom Endpoint 03/15/23 22:02:16.275
Mar 15 22:02:16.300: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Mar 15 22:02:18.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-3580" for this suite. 03/15/23 22:02:18.31
------------------------------
• [4.115 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:02:14.202
    Mar 15 22:02:14.203: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename endpointslicemirroring 03/15/23 22:02:14.203
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:02:14.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:02:14.222
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 03/15/23 22:02:14.242
    Mar 15 22:02:14.260: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 03/15/23 22:02:16.264
    STEP: mirroring deletion of a custom Endpoint 03/15/23 22:02:16.275
    Mar 15 22:02:16.300: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:02:18.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-3580" for this suite. 03/15/23 22:02:18.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:02:18.32
Mar 15 22:02:18.320: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename cronjob 03/15/23 22:02:18.321
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:02:18.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:02:18.358
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 03/15/23 22:02:18.362
STEP: Ensuring no jobs are scheduled 03/15/23 22:02:18.37
STEP: Ensuring no job exists by listing jobs explicitly 03/15/23 22:07:18.377
STEP: Removing cronjob 03/15/23 22:07:18.38
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 15 22:07:18.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3515" for this suite. 03/15/23 22:07:18.39
------------------------------
• [SLOW TEST] [300.077 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:02:18.32
    Mar 15 22:02:18.320: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename cronjob 03/15/23 22:02:18.321
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:02:18.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:02:18.358
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 03/15/23 22:02:18.362
    STEP: Ensuring no jobs are scheduled 03/15/23 22:02:18.37
    STEP: Ensuring no job exists by listing jobs explicitly 03/15/23 22:07:18.377
    STEP: Removing cronjob 03/15/23 22:07:18.38
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:07:18.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3515" for this suite. 03/15/23 22:07:18.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:07:18.397
Mar 15 22:07:18.397: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename disruption 03/15/23 22:07:18.399
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:18.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:18.428
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:07:18.433
Mar 15 22:07:18.433: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename disruption-2 03/15/23 22:07:18.434
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:18.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:18.46
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 03/15/23 22:07:18.48
STEP: Waiting for the pdb to be processed 03/15/23 22:07:18.506
STEP: Waiting for the pdb to be processed 03/15/23 22:07:18.53
STEP: listing a collection of PDBs across all namespaces 03/15/23 22:07:18.535
STEP: listing a collection of PDBs in namespace disruption-7469 03/15/23 22:07:18.545
STEP: deleting a collection of PDBs 03/15/23 22:07:18.55
STEP: Waiting for the PDB collection to be deleted 03/15/23 22:07:18.56
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Mar 15 22:07:18.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 15 22:07:18.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-6307" for this suite. 03/15/23 22:07:18.571
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7469" for this suite. 03/15/23 22:07:18.58
------------------------------
• [0.191 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:07:18.397
    Mar 15 22:07:18.397: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename disruption 03/15/23 22:07:18.399
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:18.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:18.428
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:07:18.433
    Mar 15 22:07:18.433: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename disruption-2 03/15/23 22:07:18.434
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:18.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:18.46
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 03/15/23 22:07:18.48
    STEP: Waiting for the pdb to be processed 03/15/23 22:07:18.506
    STEP: Waiting for the pdb to be processed 03/15/23 22:07:18.53
    STEP: listing a collection of PDBs across all namespaces 03/15/23 22:07:18.535
    STEP: listing a collection of PDBs in namespace disruption-7469 03/15/23 22:07:18.545
    STEP: deleting a collection of PDBs 03/15/23 22:07:18.55
    STEP: Waiting for the PDB collection to be deleted 03/15/23 22:07:18.56
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:07:18.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:07:18.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-6307" for this suite. 03/15/23 22:07:18.571
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7469" for this suite. 03/15/23 22:07:18.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:07:18.59
Mar 15 22:07:18.590: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 22:07:18.591
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:18.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:18.605
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 22:07:18.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4911" for this suite. 03/15/23 22:07:18.66
------------------------------
• [0.076 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:07:18.59
    Mar 15 22:07:18.590: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 22:07:18.591
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:18.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:18.605
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:07:18.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4911" for this suite. 03/15/23 22:07:18.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:07:18.672
Mar 15 22:07:18.672: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename security-context-test 03/15/23 22:07:18.673
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:18.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:18.689
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Mar 15 22:07:18.700: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-2239fc18-2361-4001-b279-2e0b85dba85b" in namespace "security-context-test-428" to be "Succeeded or Failed"
Mar 15 22:07:18.703: INFO: Pod "busybox-privileged-false-2239fc18-2361-4001-b279-2e0b85dba85b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.023255ms
Mar 15 22:07:20.710: INFO: Pod "busybox-privileged-false-2239fc18-2361-4001-b279-2e0b85dba85b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009875388s
Mar 15 22:07:22.708: INFO: Pod "busybox-privileged-false-2239fc18-2361-4001-b279-2e0b85dba85b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0079841s
Mar 15 22:07:22.708: INFO: Pod "busybox-privileged-false-2239fc18-2361-4001-b279-2e0b85dba85b" satisfied condition "Succeeded or Failed"
Mar 15 22:07:22.721: INFO: Got logs for pod "busybox-privileged-false-2239fc18-2361-4001-b279-2e0b85dba85b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 15 22:07:22.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-428" for this suite. 03/15/23 22:07:22.728
------------------------------
• [4.065 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:07:18.672
    Mar 15 22:07:18.672: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename security-context-test 03/15/23 22:07:18.673
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:18.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:18.689
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Mar 15 22:07:18.700: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-2239fc18-2361-4001-b279-2e0b85dba85b" in namespace "security-context-test-428" to be "Succeeded or Failed"
    Mar 15 22:07:18.703: INFO: Pod "busybox-privileged-false-2239fc18-2361-4001-b279-2e0b85dba85b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.023255ms
    Mar 15 22:07:20.710: INFO: Pod "busybox-privileged-false-2239fc18-2361-4001-b279-2e0b85dba85b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009875388s
    Mar 15 22:07:22.708: INFO: Pod "busybox-privileged-false-2239fc18-2361-4001-b279-2e0b85dba85b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0079841s
    Mar 15 22:07:22.708: INFO: Pod "busybox-privileged-false-2239fc18-2361-4001-b279-2e0b85dba85b" satisfied condition "Succeeded or Failed"
    Mar 15 22:07:22.721: INFO: Got logs for pod "busybox-privileged-false-2239fc18-2361-4001-b279-2e0b85dba85b": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:07:22.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-428" for this suite. 03/15/23 22:07:22.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:07:22.743
Mar 15 22:07:22.743: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename replicaset 03/15/23 22:07:22.745
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:22.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:22.767
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 03/15/23 22:07:22.77
STEP: Verify that the required pods have come up 03/15/23 22:07:22.775
Mar 15 22:07:22.780: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar 15 22:07:27.784: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 03/15/23 22:07:27.784
Mar 15 22:07:27.788: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 03/15/23 22:07:27.788
STEP: DeleteCollection of the ReplicaSets 03/15/23 22:07:27.792
STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/15/23 22:07:27.798
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:07:27.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-102" for this suite. 03/15/23 22:07:27.826
------------------------------
• [SLOW TEST] [5.098 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:07:22.743
    Mar 15 22:07:22.743: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename replicaset 03/15/23 22:07:22.745
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:22.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:22.767
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 03/15/23 22:07:22.77
    STEP: Verify that the required pods have come up 03/15/23 22:07:22.775
    Mar 15 22:07:22.780: INFO: Pod name sample-pod: Found 0 pods out of 3
    Mar 15 22:07:27.784: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 03/15/23 22:07:27.784
    Mar 15 22:07:27.788: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 03/15/23 22:07:27.788
    STEP: DeleteCollection of the ReplicaSets 03/15/23 22:07:27.792
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/15/23 22:07:27.798
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:07:27.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-102" for this suite. 03/15/23 22:07:27.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:07:27.841
Mar 15 22:07:27.842: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename certificates 03/15/23 22:07:27.843
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:27.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:27.927
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 03/15/23 22:07:28.414
STEP: getting /apis/certificates.k8s.io 03/15/23 22:07:28.417
STEP: getting /apis/certificates.k8s.io/v1 03/15/23 22:07:28.419
STEP: creating 03/15/23 22:07:28.42
STEP: getting 03/15/23 22:07:28.44
STEP: listing 03/15/23 22:07:28.443
STEP: watching 03/15/23 22:07:28.445
Mar 15 22:07:28.445: INFO: starting watch
STEP: patching 03/15/23 22:07:28.447
STEP: updating 03/15/23 22:07:28.452
Mar 15 22:07:28.458: INFO: waiting for watch events with expected annotations
Mar 15 22:07:28.458: INFO: saw patched and updated annotations
STEP: getting /approval 03/15/23 22:07:28.458
STEP: patching /approval 03/15/23 22:07:28.461
STEP: updating /approval 03/15/23 22:07:28.466
STEP: getting /status 03/15/23 22:07:28.473
STEP: patching /status 03/15/23 22:07:28.476
STEP: updating /status 03/15/23 22:07:28.483
STEP: deleting 03/15/23 22:07:28.49
STEP: deleting a collection 03/15/23 22:07:28.503
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:07:28.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-3045" for this suite. 03/15/23 22:07:28.525
------------------------------
• [0.689 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:07:27.841
    Mar 15 22:07:27.842: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename certificates 03/15/23 22:07:27.843
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:27.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:27.927
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 03/15/23 22:07:28.414
    STEP: getting /apis/certificates.k8s.io 03/15/23 22:07:28.417
    STEP: getting /apis/certificates.k8s.io/v1 03/15/23 22:07:28.419
    STEP: creating 03/15/23 22:07:28.42
    STEP: getting 03/15/23 22:07:28.44
    STEP: listing 03/15/23 22:07:28.443
    STEP: watching 03/15/23 22:07:28.445
    Mar 15 22:07:28.445: INFO: starting watch
    STEP: patching 03/15/23 22:07:28.447
    STEP: updating 03/15/23 22:07:28.452
    Mar 15 22:07:28.458: INFO: waiting for watch events with expected annotations
    Mar 15 22:07:28.458: INFO: saw patched and updated annotations
    STEP: getting /approval 03/15/23 22:07:28.458
    STEP: patching /approval 03/15/23 22:07:28.461
    STEP: updating /approval 03/15/23 22:07:28.466
    STEP: getting /status 03/15/23 22:07:28.473
    STEP: patching /status 03/15/23 22:07:28.476
    STEP: updating /status 03/15/23 22:07:28.483
    STEP: deleting 03/15/23 22:07:28.49
    STEP: deleting a collection 03/15/23 22:07:28.503
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:07:28.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-3045" for this suite. 03/15/23 22:07:28.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:07:28.537
Mar 15 22:07:28.537: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename endpointslice 03/15/23 22:07:28.539
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:28.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:28.581
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Mar 15 22:07:28.591: INFO: Endpoints addresses: [172.20.56.193] , ports: [443]
Mar 15 22:07:28.591: INFO: EndpointSlices addresses: [172.20.56.193] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 15 22:07:28.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7183" for this suite. 03/15/23 22:07:28.596
------------------------------
• [0.064 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:07:28.537
    Mar 15 22:07:28.537: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename endpointslice 03/15/23 22:07:28.539
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:28.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:28.581
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Mar 15 22:07:28.591: INFO: Endpoints addresses: [172.20.56.193] , ports: [443]
    Mar 15 22:07:28.591: INFO: EndpointSlices addresses: [172.20.56.193] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:07:28.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7183" for this suite. 03/15/23 22:07:28.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:07:28.607
Mar 15 22:07:28.607: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename events 03/15/23 22:07:28.608
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:28.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:28.625
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 03/15/23 22:07:28.628
Mar 15 22:07:28.632: INFO: created test-event-1
Mar 15 22:07:28.636: INFO: created test-event-2
Mar 15 22:07:28.640: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 03/15/23 22:07:28.64
STEP: delete collection of events 03/15/23 22:07:28.644
Mar 15 22:07:28.644: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/15/23 22:07:28.656
Mar 15 22:07:28.656: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Mar 15 22:07:28.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4492" for this suite. 03/15/23 22:07:28.662
------------------------------
• [0.074 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:07:28.607
    Mar 15 22:07:28.607: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename events 03/15/23 22:07:28.608
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:28.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:28.625
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 03/15/23 22:07:28.628
    Mar 15 22:07:28.632: INFO: created test-event-1
    Mar 15 22:07:28.636: INFO: created test-event-2
    Mar 15 22:07:28.640: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 03/15/23 22:07:28.64
    STEP: delete collection of events 03/15/23 22:07:28.644
    Mar 15 22:07:28.644: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/15/23 22:07:28.656
    Mar 15 22:07:28.656: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:07:28.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4492" for this suite. 03/15/23 22:07:28.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:07:28.684
Mar 15 22:07:28.685: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename subpath 03/15/23 22:07:28.686
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:28.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:28.74
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/15/23 22:07:28.744
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-mj58 03/15/23 22:07:28.754
STEP: Creating a pod to test atomic-volume-subpath 03/15/23 22:07:28.754
Mar 15 22:07:28.761: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-mj58" in namespace "subpath-8869" to be "Succeeded or Failed"
Mar 15 22:07:28.767: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Pending", Reason="", readiness=false. Elapsed: 5.349817ms
Mar 15 22:07:30.770: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 2.008423491s
Mar 15 22:07:32.771: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 4.009703774s
Mar 15 22:07:34.771: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 6.009706932s
Mar 15 22:07:36.770: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 8.009342461s
Mar 15 22:07:38.771: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 10.009976766s
Mar 15 22:07:40.771: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 12.009637892s
Mar 15 22:07:42.770: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 14.009324497s
Mar 15 22:07:44.771: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 16.009743777s
Mar 15 22:07:46.770: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 18.009035175s
Mar 15 22:07:48.772: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 20.011007498s
Mar 15 22:07:50.771: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=false. Elapsed: 22.009760975s
Mar 15 22:07:52.770: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009081461s
STEP: Saw pod success 03/15/23 22:07:52.77
Mar 15 22:07:52.771: INFO: Pod "pod-subpath-test-secret-mj58" satisfied condition "Succeeded or Failed"
Mar 15 22:07:52.773: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-subpath-test-secret-mj58 container test-container-subpath-secret-mj58: <nil>
STEP: delete the pod 03/15/23 22:07:52.779
Mar 15 22:07:52.791: INFO: Waiting for pod pod-subpath-test-secret-mj58 to disappear
Mar 15 22:07:52.793: INFO: Pod pod-subpath-test-secret-mj58 no longer exists
STEP: Deleting pod pod-subpath-test-secret-mj58 03/15/23 22:07:52.793
Mar 15 22:07:52.793: INFO: Deleting pod "pod-subpath-test-secret-mj58" in namespace "subpath-8869"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 15 22:07:52.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8869" for this suite. 03/15/23 22:07:52.799
------------------------------
• [SLOW TEST] [24.122 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:07:28.684
    Mar 15 22:07:28.685: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename subpath 03/15/23 22:07:28.686
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:28.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:28.74
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/15/23 22:07:28.744
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-mj58 03/15/23 22:07:28.754
    STEP: Creating a pod to test atomic-volume-subpath 03/15/23 22:07:28.754
    Mar 15 22:07:28.761: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-mj58" in namespace "subpath-8869" to be "Succeeded or Failed"
    Mar 15 22:07:28.767: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Pending", Reason="", readiness=false. Elapsed: 5.349817ms
    Mar 15 22:07:30.770: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 2.008423491s
    Mar 15 22:07:32.771: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 4.009703774s
    Mar 15 22:07:34.771: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 6.009706932s
    Mar 15 22:07:36.770: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 8.009342461s
    Mar 15 22:07:38.771: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 10.009976766s
    Mar 15 22:07:40.771: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 12.009637892s
    Mar 15 22:07:42.770: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 14.009324497s
    Mar 15 22:07:44.771: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 16.009743777s
    Mar 15 22:07:46.770: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 18.009035175s
    Mar 15 22:07:48.772: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=true. Elapsed: 20.011007498s
    Mar 15 22:07:50.771: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Running", Reason="", readiness=false. Elapsed: 22.009760975s
    Mar 15 22:07:52.770: INFO: Pod "pod-subpath-test-secret-mj58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009081461s
    STEP: Saw pod success 03/15/23 22:07:52.77
    Mar 15 22:07:52.771: INFO: Pod "pod-subpath-test-secret-mj58" satisfied condition "Succeeded or Failed"
    Mar 15 22:07:52.773: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-subpath-test-secret-mj58 container test-container-subpath-secret-mj58: <nil>
    STEP: delete the pod 03/15/23 22:07:52.779
    Mar 15 22:07:52.791: INFO: Waiting for pod pod-subpath-test-secret-mj58 to disappear
    Mar 15 22:07:52.793: INFO: Pod pod-subpath-test-secret-mj58 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-mj58 03/15/23 22:07:52.793
    Mar 15 22:07:52.793: INFO: Deleting pod "pod-subpath-test-secret-mj58" in namespace "subpath-8869"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:07:52.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8869" for this suite. 03/15/23 22:07:52.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:07:52.814
Mar 15 22:07:52.814: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename proxy 03/15/23 22:07:52.815
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:52.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:52.828
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Mar 15 22:07:52.833: INFO: Creating pod...
Mar 15 22:07:52.844: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5492" to be "running"
Mar 15 22:07:52.849: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.869569ms
Mar 15 22:07:54.853: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00864322s
Mar 15 22:07:54.853: INFO: Pod "agnhost" satisfied condition "running"
Mar 15 22:07:54.853: INFO: Creating service...
Mar 15 22:07:54.863: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=DELETE
Mar 15 22:07:54.874: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 15 22:07:54.874: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=OPTIONS
Mar 15 22:07:54.877: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 15 22:07:54.878: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=PATCH
Mar 15 22:07:54.882: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 15 22:07:54.882: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=POST
Mar 15 22:07:54.886: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 15 22:07:54.886: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=PUT
Mar 15 22:07:54.891: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 15 22:07:54.891: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=DELETE
Mar 15 22:07:54.897: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 15 22:07:54.897: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar 15 22:07:54.905: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 15 22:07:54.905: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=PATCH
Mar 15 22:07:54.912: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 15 22:07:54.912: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=POST
Mar 15 22:07:54.916: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 15 22:07:54.916: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=PUT
Mar 15 22:07:54.923: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 15 22:07:54.923: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=GET
Mar 15 22:07:54.929: INFO: http.Client request:GET StatusCode:301
Mar 15 22:07:54.929: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=GET
Mar 15 22:07:54.934: INFO: http.Client request:GET StatusCode:301
Mar 15 22:07:54.935: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=HEAD
Mar 15 22:07:54.938: INFO: http.Client request:HEAD StatusCode:301
Mar 15 22:07:54.938: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=HEAD
Mar 15 22:07:54.944: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 15 22:07:54.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5492" for this suite. 03/15/23 22:07:54.954
------------------------------
• [2.148 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:07:52.814
    Mar 15 22:07:52.814: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename proxy 03/15/23 22:07:52.815
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:52.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:52.828
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Mar 15 22:07:52.833: INFO: Creating pod...
    Mar 15 22:07:52.844: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5492" to be "running"
    Mar 15 22:07:52.849: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.869569ms
    Mar 15 22:07:54.853: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00864322s
    Mar 15 22:07:54.853: INFO: Pod "agnhost" satisfied condition "running"
    Mar 15 22:07:54.853: INFO: Creating service...
    Mar 15 22:07:54.863: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=DELETE
    Mar 15 22:07:54.874: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 15 22:07:54.874: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=OPTIONS
    Mar 15 22:07:54.877: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 15 22:07:54.878: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=PATCH
    Mar 15 22:07:54.882: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 15 22:07:54.882: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=POST
    Mar 15 22:07:54.886: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 15 22:07:54.886: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=PUT
    Mar 15 22:07:54.891: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 15 22:07:54.891: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=DELETE
    Mar 15 22:07:54.897: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 15 22:07:54.897: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Mar 15 22:07:54.905: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 15 22:07:54.905: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=PATCH
    Mar 15 22:07:54.912: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 15 22:07:54.912: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=POST
    Mar 15 22:07:54.916: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 15 22:07:54.916: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=PUT
    Mar 15 22:07:54.923: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 15 22:07:54.923: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=GET
    Mar 15 22:07:54.929: INFO: http.Client request:GET StatusCode:301
    Mar 15 22:07:54.929: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=GET
    Mar 15 22:07:54.934: INFO: http.Client request:GET StatusCode:301
    Mar 15 22:07:54.935: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/pods/agnhost/proxy?method=HEAD
    Mar 15 22:07:54.938: INFO: http.Client request:HEAD StatusCode:301
    Mar 15 22:07:54.938: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-5492/services/e2e-proxy-test-service/proxy?method=HEAD
    Mar 15 22:07:54.944: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:07:54.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5492" for this suite. 03/15/23 22:07:54.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:07:54.975
Mar 15 22:07:54.976: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:07:54.977
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:54.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:54.995
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 03/15/23 22:07:54.998
Mar 15 22:07:54.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 create -f -'
Mar 15 22:07:56.023: INFO: stderr: ""
Mar 15 22:07:56.023: INFO: stdout: "pod/pause created\n"
Mar 15 22:07:56.023: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 15 22:07:56.023: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-661" to be "running and ready"
Mar 15 22:07:56.027: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.618702ms
Mar 15 22:07:56.027: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'i-0faaf83f00b43c88c' to be 'Running' but was 'Pending'
Mar 15 22:07:58.030: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006943413s
Mar 15 22:07:58.030: INFO: Pod "pause" satisfied condition "running and ready"
Mar 15 22:07:58.030: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 03/15/23 22:07:58.031
Mar 15 22:07:58.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 label pods pause testing-label=testing-label-value'
Mar 15 22:07:58.173: INFO: stderr: ""
Mar 15 22:07:58.173: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 03/15/23 22:07:58.173
Mar 15 22:07:58.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 get pod pause -L testing-label'
Mar 15 22:07:58.260: INFO: stderr: ""
Mar 15 22:07:58.260: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 03/15/23 22:07:58.26
Mar 15 22:07:58.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 label pods pause testing-label-'
Mar 15 22:07:58.361: INFO: stderr: ""
Mar 15 22:07:58.361: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 03/15/23 22:07:58.361
Mar 15 22:07:58.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 get pod pause -L testing-label'
Mar 15 22:07:58.438: INFO: stderr: ""
Mar 15 22:07:58.438: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 03/15/23 22:07:58.438
Mar 15 22:07:58.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 delete --grace-period=0 --force -f -'
Mar 15 22:07:58.537: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 22:07:58.537: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 15 22:07:58.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 get rc,svc -l name=pause --no-headers'
Mar 15 22:07:58.630: INFO: stderr: "No resources found in kubectl-661 namespace.\n"
Mar 15 22:07:58.630: INFO: stdout: ""
Mar 15 22:07:58.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 15 22:07:58.710: INFO: stderr: ""
Mar 15 22:07:58.710: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:07:58.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-661" for this suite. 03/15/23 22:07:58.723
------------------------------
• [3.763 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:07:54.975
    Mar 15 22:07:54.976: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:07:54.977
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:54.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:54.995
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 03/15/23 22:07:54.998
    Mar 15 22:07:54.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 create -f -'
    Mar 15 22:07:56.023: INFO: stderr: ""
    Mar 15 22:07:56.023: INFO: stdout: "pod/pause created\n"
    Mar 15 22:07:56.023: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Mar 15 22:07:56.023: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-661" to be "running and ready"
    Mar 15 22:07:56.027: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.618702ms
    Mar 15 22:07:56.027: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'i-0faaf83f00b43c88c' to be 'Running' but was 'Pending'
    Mar 15 22:07:58.030: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006943413s
    Mar 15 22:07:58.030: INFO: Pod "pause" satisfied condition "running and ready"
    Mar 15 22:07:58.030: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 03/15/23 22:07:58.031
    Mar 15 22:07:58.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 label pods pause testing-label=testing-label-value'
    Mar 15 22:07:58.173: INFO: stderr: ""
    Mar 15 22:07:58.173: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 03/15/23 22:07:58.173
    Mar 15 22:07:58.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 get pod pause -L testing-label'
    Mar 15 22:07:58.260: INFO: stderr: ""
    Mar 15 22:07:58.260: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 03/15/23 22:07:58.26
    Mar 15 22:07:58.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 label pods pause testing-label-'
    Mar 15 22:07:58.361: INFO: stderr: ""
    Mar 15 22:07:58.361: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 03/15/23 22:07:58.361
    Mar 15 22:07:58.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 get pod pause -L testing-label'
    Mar 15 22:07:58.438: INFO: stderr: ""
    Mar 15 22:07:58.438: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 03/15/23 22:07:58.438
    Mar 15 22:07:58.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 delete --grace-period=0 --force -f -'
    Mar 15 22:07:58.537: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 15 22:07:58.537: INFO: stdout: "pod \"pause\" force deleted\n"
    Mar 15 22:07:58.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 get rc,svc -l name=pause --no-headers'
    Mar 15 22:07:58.630: INFO: stderr: "No resources found in kubectl-661 namespace.\n"
    Mar 15 22:07:58.630: INFO: stdout: ""
    Mar 15 22:07:58.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-661 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 15 22:07:58.710: INFO: stderr: ""
    Mar 15 22:07:58.710: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:07:58.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-661" for this suite. 03/15/23 22:07:58.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:07:58.74
Mar 15 22:07:58.740: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename resourcequota 03/15/23 22:07:58.741
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:58.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:58.768
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 03/15/23 22:07:58.772
STEP: Ensuring ResourceQuota status is calculated 03/15/23 22:07:58.777
STEP: Creating a ResourceQuota with not terminating scope 03/15/23 22:08:00.782
STEP: Ensuring ResourceQuota status is calculated 03/15/23 22:08:00.788
STEP: Creating a long running pod 03/15/23 22:08:02.792
STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/15/23 22:08:02.831
STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/15/23 22:08:04.836
STEP: Deleting the pod 03/15/23 22:08:06.84
STEP: Ensuring resource quota status released the pod usage 03/15/23 22:08:06.854
STEP: Creating a terminating pod 03/15/23 22:08:08.859
STEP: Ensuring resource quota with terminating scope captures the pod usage 03/15/23 22:08:08.871
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/15/23 22:08:10.877
STEP: Deleting the pod 03/15/23 22:08:12.881
STEP: Ensuring resource quota status released the pod usage 03/15/23 22:08:12.896
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 15 22:08:14.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6075" for this suite. 03/15/23 22:08:14.903
------------------------------
• [SLOW TEST] [16.170 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:07:58.74
    Mar 15 22:07:58.740: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename resourcequota 03/15/23 22:07:58.741
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:07:58.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:07:58.768
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 03/15/23 22:07:58.772
    STEP: Ensuring ResourceQuota status is calculated 03/15/23 22:07:58.777
    STEP: Creating a ResourceQuota with not terminating scope 03/15/23 22:08:00.782
    STEP: Ensuring ResourceQuota status is calculated 03/15/23 22:08:00.788
    STEP: Creating a long running pod 03/15/23 22:08:02.792
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/15/23 22:08:02.831
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/15/23 22:08:04.836
    STEP: Deleting the pod 03/15/23 22:08:06.84
    STEP: Ensuring resource quota status released the pod usage 03/15/23 22:08:06.854
    STEP: Creating a terminating pod 03/15/23 22:08:08.859
    STEP: Ensuring resource quota with terminating scope captures the pod usage 03/15/23 22:08:08.871
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/15/23 22:08:10.877
    STEP: Deleting the pod 03/15/23 22:08:12.881
    STEP: Ensuring resource quota status released the pod usage 03/15/23 22:08:12.896
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:08:14.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6075" for this suite. 03/15/23 22:08:14.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:08:14.931
Mar 15 22:08:14.931: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename cronjob 03/15/23 22:08:14.931
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:08:14.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:08:14.954
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 03/15/23 22:08:14.958
STEP: Ensuring more than one job is running at a time 03/15/23 22:08:14.964
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/15/23 22:10:00.968
STEP: Removing cronjob 03/15/23 22:10:00.971
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:00.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1884" for this suite. 03/15/23 22:10:00.989
------------------------------
• [SLOW TEST] [106.074 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:08:14.931
    Mar 15 22:08:14.931: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename cronjob 03/15/23 22:08:14.931
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:08:14.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:08:14.954
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 03/15/23 22:08:14.958
    STEP: Ensuring more than one job is running at a time 03/15/23 22:08:14.964
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/15/23 22:10:00.968
    STEP: Removing cronjob 03/15/23 22:10:00.971
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:00.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1884" for this suite. 03/15/23 22:10:00.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:01.007
Mar 15 22:10:01.007: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename watch 03/15/23 22:10:01.008
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:01.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:01.064
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 03/15/23 22:10:01.068
STEP: starting a background goroutine to produce watch events 03/15/23 22:10:01.074
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/15/23 22:10:01.074
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:03.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9944" for this suite. 03/15/23 22:10:03.863
------------------------------
• [2.907 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:01.007
    Mar 15 22:10:01.007: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename watch 03/15/23 22:10:01.008
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:01.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:01.064
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 03/15/23 22:10:01.068
    STEP: starting a background goroutine to produce watch events 03/15/23 22:10:01.074
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/15/23 22:10:01.074
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:03.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9944" for this suite. 03/15/23 22:10:03.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:03.918
Mar 15 22:10:03.921: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pods 03/15/23 22:10:03.922
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:03.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:03.94
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 03/15/23 22:10:03.944
STEP: setting up watch 03/15/23 22:10:03.944
STEP: submitting the pod to kubernetes 03/15/23 22:10:04.048
STEP: verifying the pod is in kubernetes 03/15/23 22:10:04.055
STEP: verifying pod creation was observed 03/15/23 22:10:04.06
Mar 15 22:10:04.061: INFO: Waiting up to 5m0s for pod "pod-submit-remove-ffa029db-8126-4920-8385-2480b20c52b5" in namespace "pods-5250" to be "running"
Mar 15 22:10:04.066: INFO: Pod "pod-submit-remove-ffa029db-8126-4920-8385-2480b20c52b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.867323ms
Mar 15 22:10:06.073: INFO: Pod "pod-submit-remove-ffa029db-8126-4920-8385-2480b20c52b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012674883s
Mar 15 22:10:06.073: INFO: Pod "pod-submit-remove-ffa029db-8126-4920-8385-2480b20c52b5" satisfied condition "running"
STEP: deleting the pod gracefully 03/15/23 22:10:06.076
STEP: verifying pod deletion was observed 03/15/23 22:10:06.097
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:08.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5250" for this suite. 03/15/23 22:10:08.901
------------------------------
• [4.991 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:03.918
    Mar 15 22:10:03.921: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pods 03/15/23 22:10:03.922
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:03.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:03.94
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 03/15/23 22:10:03.944
    STEP: setting up watch 03/15/23 22:10:03.944
    STEP: submitting the pod to kubernetes 03/15/23 22:10:04.048
    STEP: verifying the pod is in kubernetes 03/15/23 22:10:04.055
    STEP: verifying pod creation was observed 03/15/23 22:10:04.06
    Mar 15 22:10:04.061: INFO: Waiting up to 5m0s for pod "pod-submit-remove-ffa029db-8126-4920-8385-2480b20c52b5" in namespace "pods-5250" to be "running"
    Mar 15 22:10:04.066: INFO: Pod "pod-submit-remove-ffa029db-8126-4920-8385-2480b20c52b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.867323ms
    Mar 15 22:10:06.073: INFO: Pod "pod-submit-remove-ffa029db-8126-4920-8385-2480b20c52b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012674883s
    Mar 15 22:10:06.073: INFO: Pod "pod-submit-remove-ffa029db-8126-4920-8385-2480b20c52b5" satisfied condition "running"
    STEP: deleting the pod gracefully 03/15/23 22:10:06.076
    STEP: verifying pod deletion was observed 03/15/23 22:10:06.097
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:08.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5250" for this suite. 03/15/23 22:10:08.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:08.91
Mar 15 22:10:08.910: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename gc 03/15/23 22:10:08.91
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:08.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:08.933
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 03/15/23 22:10:08.937
STEP: Wait for the Deployment to create new ReplicaSet 03/15/23 22:10:08.944
STEP: delete the deployment 03/15/23 22:10:09.457
STEP: wait for all rs to be garbage collected 03/15/23 22:10:09.463
STEP: expected 0 pods, got 2 pods 03/15/23 22:10:09.497
STEP: Gathering metrics 03/15/23 22:10:10.02
Mar 15 22:10:10.062: INFO: Waiting up to 5m0s for pod "kube-controller-manager-i-00864a1c8c75a434c" in namespace "kube-system" to be "running and ready"
Mar 15 22:10:10.077: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c": Phase="Running", Reason="", readiness=true. Elapsed: 14.709885ms
Mar 15 22:10:10.077: INFO: The phase of Pod kube-controller-manager-i-00864a1c8c75a434c is Running (Ready = true)
Mar 15 22:10:10.077: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c" satisfied condition "running and ready"
Mar 15 22:10:10.229: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:10.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7547" for this suite. 03/15/23 22:10:10.237
------------------------------
• [1.337 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:08.91
    Mar 15 22:10:08.910: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename gc 03/15/23 22:10:08.91
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:08.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:08.933
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 03/15/23 22:10:08.937
    STEP: Wait for the Deployment to create new ReplicaSet 03/15/23 22:10:08.944
    STEP: delete the deployment 03/15/23 22:10:09.457
    STEP: wait for all rs to be garbage collected 03/15/23 22:10:09.463
    STEP: expected 0 pods, got 2 pods 03/15/23 22:10:09.497
    STEP: Gathering metrics 03/15/23 22:10:10.02
    Mar 15 22:10:10.062: INFO: Waiting up to 5m0s for pod "kube-controller-manager-i-00864a1c8c75a434c" in namespace "kube-system" to be "running and ready"
    Mar 15 22:10:10.077: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c": Phase="Running", Reason="", readiness=true. Elapsed: 14.709885ms
    Mar 15 22:10:10.077: INFO: The phase of Pod kube-controller-manager-i-00864a1c8c75a434c is Running (Ready = true)
    Mar 15 22:10:10.077: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c" satisfied condition "running and ready"
    Mar 15 22:10:10.229: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:10.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7547" for this suite. 03/15/23 22:10:10.237
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:10.248
Mar 15 22:10:10.248: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 22:10:10.249
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:10.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:10.27
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/15/23 22:10:10.277
Mar 15 22:10:10.293: INFO: Waiting up to 5m0s for pod "pod-07ebb9e4-17a8-4d24-be34-42ad543349d3" in namespace "emptydir-3227" to be "Succeeded or Failed"
Mar 15 22:10:10.308: INFO: Pod "pod-07ebb9e4-17a8-4d24-be34-42ad543349d3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.724062ms
Mar 15 22:10:12.312: INFO: Pod "pod-07ebb9e4-17a8-4d24-be34-42ad543349d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019648344s
Mar 15 22:10:14.311: INFO: Pod "pod-07ebb9e4-17a8-4d24-be34-42ad543349d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018671787s
STEP: Saw pod success 03/15/23 22:10:14.311
Mar 15 22:10:14.312: INFO: Pod "pod-07ebb9e4-17a8-4d24-be34-42ad543349d3" satisfied condition "Succeeded or Failed"
Mar 15 22:10:14.314: INFO: Trying to get logs from node i-077ee0eb7ec5a02aa pod pod-07ebb9e4-17a8-4d24-be34-42ad543349d3 container test-container: <nil>
STEP: delete the pod 03/15/23 22:10:14.329
Mar 15 22:10:14.341: INFO: Waiting for pod pod-07ebb9e4-17a8-4d24-be34-42ad543349d3 to disappear
Mar 15 22:10:14.343: INFO: Pod pod-07ebb9e4-17a8-4d24-be34-42ad543349d3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:14.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3227" for this suite. 03/15/23 22:10:14.347
------------------------------
• [4.110 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:10.248
    Mar 15 22:10:10.248: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 22:10:10.249
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:10.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:10.27
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/15/23 22:10:10.277
    Mar 15 22:10:10.293: INFO: Waiting up to 5m0s for pod "pod-07ebb9e4-17a8-4d24-be34-42ad543349d3" in namespace "emptydir-3227" to be "Succeeded or Failed"
    Mar 15 22:10:10.308: INFO: Pod "pod-07ebb9e4-17a8-4d24-be34-42ad543349d3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.724062ms
    Mar 15 22:10:12.312: INFO: Pod "pod-07ebb9e4-17a8-4d24-be34-42ad543349d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019648344s
    Mar 15 22:10:14.311: INFO: Pod "pod-07ebb9e4-17a8-4d24-be34-42ad543349d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018671787s
    STEP: Saw pod success 03/15/23 22:10:14.311
    Mar 15 22:10:14.312: INFO: Pod "pod-07ebb9e4-17a8-4d24-be34-42ad543349d3" satisfied condition "Succeeded or Failed"
    Mar 15 22:10:14.314: INFO: Trying to get logs from node i-077ee0eb7ec5a02aa pod pod-07ebb9e4-17a8-4d24-be34-42ad543349d3 container test-container: <nil>
    STEP: delete the pod 03/15/23 22:10:14.329
    Mar 15 22:10:14.341: INFO: Waiting for pod pod-07ebb9e4-17a8-4d24-be34-42ad543349d3 to disappear
    Mar 15 22:10:14.343: INFO: Pod pod-07ebb9e4-17a8-4d24-be34-42ad543349d3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:14.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3227" for this suite. 03/15/23 22:10:14.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:14.359
Mar 15 22:10:14.359: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename csistoragecapacity 03/15/23 22:10:14.36
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:14.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:14.377
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 03/15/23 22:10:14.381
STEP: getting /apis/storage.k8s.io 03/15/23 22:10:14.384
STEP: getting /apis/storage.k8s.io/v1 03/15/23 22:10:14.385
STEP: creating 03/15/23 22:10:14.386
STEP: watching 03/15/23 22:10:14.399
Mar 15 22:10:14.399: INFO: starting watch
STEP: getting 03/15/23 22:10:14.406
STEP: listing in namespace 03/15/23 22:10:14.408
STEP: listing across namespaces 03/15/23 22:10:14.411
STEP: patching 03/15/23 22:10:14.413
STEP: updating 03/15/23 22:10:14.417
Mar 15 22:10:14.426: INFO: waiting for watch events with expected annotations in namespace
Mar 15 22:10:14.426: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 03/15/23 22:10:14.426
STEP: deleting a collection 03/15/23 22:10:14.435
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:14.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-9171" for this suite. 03/15/23 22:10:14.475
------------------------------
• [0.121 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:14.359
    Mar 15 22:10:14.359: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename csistoragecapacity 03/15/23 22:10:14.36
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:14.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:14.377
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 03/15/23 22:10:14.381
    STEP: getting /apis/storage.k8s.io 03/15/23 22:10:14.384
    STEP: getting /apis/storage.k8s.io/v1 03/15/23 22:10:14.385
    STEP: creating 03/15/23 22:10:14.386
    STEP: watching 03/15/23 22:10:14.399
    Mar 15 22:10:14.399: INFO: starting watch
    STEP: getting 03/15/23 22:10:14.406
    STEP: listing in namespace 03/15/23 22:10:14.408
    STEP: listing across namespaces 03/15/23 22:10:14.411
    STEP: patching 03/15/23 22:10:14.413
    STEP: updating 03/15/23 22:10:14.417
    Mar 15 22:10:14.426: INFO: waiting for watch events with expected annotations in namespace
    Mar 15 22:10:14.426: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 03/15/23 22:10:14.426
    STEP: deleting a collection 03/15/23 22:10:14.435
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:14.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-9171" for this suite. 03/15/23 22:10:14.475
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:14.481
Mar 15 22:10:14.481: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 22:10:14.482
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:14.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:14.508
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 03/15/23 22:10:14.511
STEP: listing secrets in all namespaces to ensure that there are more than zero 03/15/23 22:10:14.515
STEP: patching the secret 03/15/23 22:10:14.519
STEP: deleting the secret using a LabelSelector 03/15/23 22:10:14.533
STEP: listing secrets in all namespaces, searching for label name and value in patch 03/15/23 22:10:14.544
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:14.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3895" for this suite. 03/15/23 22:10:14.558
------------------------------
• [0.088 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:14.481
    Mar 15 22:10:14.481: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 22:10:14.482
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:14.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:14.508
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 03/15/23 22:10:14.511
    STEP: listing secrets in all namespaces to ensure that there are more than zero 03/15/23 22:10:14.515
    STEP: patching the secret 03/15/23 22:10:14.519
    STEP: deleting the secret using a LabelSelector 03/15/23 22:10:14.533
    STEP: listing secrets in all namespaces, searching for label name and value in patch 03/15/23 22:10:14.544
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:14.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3895" for this suite. 03/15/23 22:10:14.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:14.573
Mar 15 22:10:14.573: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:10:14.574
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:14.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:14.59
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/15/23 22:10:14.593
Mar 15 22:10:14.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-476 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 15 22:10:14.676: INFO: stderr: ""
Mar 15 22:10:14.676: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 03/15/23 22:10:14.676
Mar 15 22:10:14.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-476 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Mar 15 22:10:14.920: INFO: stderr: ""
Mar 15 22:10:14.920: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/15/23 22:10:14.92
Mar 15 22:10:14.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-476 delete pods e2e-test-httpd-pod'
Mar 15 22:10:16.954: INFO: stderr: ""
Mar 15 22:10:16.954: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:16.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-476" for this suite. 03/15/23 22:10:16.967
------------------------------
• [2.404 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:14.573
    Mar 15 22:10:14.573: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:10:14.574
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:14.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:14.59
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/15/23 22:10:14.593
    Mar 15 22:10:14.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-476 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 15 22:10:14.676: INFO: stderr: ""
    Mar 15 22:10:14.676: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 03/15/23 22:10:14.676
    Mar 15 22:10:14.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-476 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Mar 15 22:10:14.920: INFO: stderr: ""
    Mar 15 22:10:14.920: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/15/23 22:10:14.92
    Mar 15 22:10:14.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-476 delete pods e2e-test-httpd-pod'
    Mar 15 22:10:16.954: INFO: stderr: ""
    Mar 15 22:10:16.954: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:16.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-476" for this suite. 03/15/23 22:10:16.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:16.982
Mar 15 22:10:16.982: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:10:16.983
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:16.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:17.003
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 03/15/23 22:10:17.007
Mar 15 22:10:17.018: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8" in namespace "projected-2197" to be "Succeeded or Failed"
Mar 15 22:10:17.024: INFO: Pod "downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.391786ms
Mar 15 22:10:19.028: INFO: Pod "downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8": Phase="Running", Reason="", readiness=false. Elapsed: 2.00992216s
Mar 15 22:10:21.028: INFO: Pod "downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009645705s
STEP: Saw pod success 03/15/23 22:10:21.028
Mar 15 22:10:21.028: INFO: Pod "downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8" satisfied condition "Succeeded or Failed"
Mar 15 22:10:21.031: INFO: Trying to get logs from node i-077ee0eb7ec5a02aa pod downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8 container client-container: <nil>
STEP: delete the pod 03/15/23 22:10:21.036
Mar 15 22:10:21.046: INFO: Waiting for pod downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8 to disappear
Mar 15 22:10:21.050: INFO: Pod downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:21.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2197" for this suite. 03/15/23 22:10:21.054
------------------------------
• [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:16.982
    Mar 15 22:10:16.982: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:10:16.983
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:16.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:17.003
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 03/15/23 22:10:17.007
    Mar 15 22:10:17.018: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8" in namespace "projected-2197" to be "Succeeded or Failed"
    Mar 15 22:10:17.024: INFO: Pod "downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.391786ms
    Mar 15 22:10:19.028: INFO: Pod "downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8": Phase="Running", Reason="", readiness=false. Elapsed: 2.00992216s
    Mar 15 22:10:21.028: INFO: Pod "downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009645705s
    STEP: Saw pod success 03/15/23 22:10:21.028
    Mar 15 22:10:21.028: INFO: Pod "downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8" satisfied condition "Succeeded or Failed"
    Mar 15 22:10:21.031: INFO: Trying to get logs from node i-077ee0eb7ec5a02aa pod downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8 container client-container: <nil>
    STEP: delete the pod 03/15/23 22:10:21.036
    Mar 15 22:10:21.046: INFO: Waiting for pod downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8 to disappear
    Mar 15 22:10:21.050: INFO: Pod downwardapi-volume-e2246c0d-bb71-4e55-9581-b008496651e8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:21.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2197" for this suite. 03/15/23 22:10:21.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:21.063
Mar 15 22:10:21.063: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename tables 03/15/23 22:10:21.064
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:21.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:21.083
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:21.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-2738" for this suite. 03/15/23 22:10:21.092
------------------------------
• [0.037 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:21.063
    Mar 15 22:10:21.063: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename tables 03/15/23 22:10:21.064
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:21.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:21.083
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:21.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-2738" for this suite. 03/15/23 22:10:21.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:21.101
Mar 15 22:10:21.101: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-probe 03/15/23 22:10:21.102
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:21.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:21.12
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Mar 15 22:10:21.136: INFO: Waiting up to 5m0s for pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c" in namespace "container-probe-887" to be "running and ready"
Mar 15 22:10:21.140: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.961897ms
Mar 15 22:10:21.140: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:10:23.143: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 2.007271365s
Mar 15 22:10:23.143: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
Mar 15 22:10:25.149: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 4.013558856s
Mar 15 22:10:25.149: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
Mar 15 22:10:27.143: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 6.007445748s
Mar 15 22:10:27.143: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
Mar 15 22:10:29.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 8.007606027s
Mar 15 22:10:29.144: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
Mar 15 22:10:31.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 10.008230719s
Mar 15 22:10:31.144: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
Mar 15 22:10:33.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 12.008319879s
Mar 15 22:10:33.144: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
Mar 15 22:10:35.143: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 14.007227074s
Mar 15 22:10:35.143: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
Mar 15 22:10:37.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 16.00794963s
Mar 15 22:10:37.144: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
Mar 15 22:10:39.151: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 18.015131009s
Mar 15 22:10:39.151: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
Mar 15 22:10:41.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 20.008241867s
Mar 15 22:10:41.144: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
Mar 15 22:10:43.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=true. Elapsed: 22.00826112s
Mar 15 22:10:43.144: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = true)
Mar 15 22:10:43.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c" satisfied condition "running and ready"
Mar 15 22:10:43.147: INFO: Container started at 2023-03-15 22:10:21 +0000 UTC, pod became ready at 2023-03-15 22:10:41 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:43.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-887" for this suite. 03/15/23 22:10:43.151
------------------------------
• [SLOW TEST] [22.055 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:21.101
    Mar 15 22:10:21.101: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-probe 03/15/23 22:10:21.102
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:21.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:21.12
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Mar 15 22:10:21.136: INFO: Waiting up to 5m0s for pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c" in namespace "container-probe-887" to be "running and ready"
    Mar 15 22:10:21.140: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.961897ms
    Mar 15 22:10:21.140: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:10:23.143: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 2.007271365s
    Mar 15 22:10:23.143: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
    Mar 15 22:10:25.149: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 4.013558856s
    Mar 15 22:10:25.149: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
    Mar 15 22:10:27.143: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 6.007445748s
    Mar 15 22:10:27.143: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
    Mar 15 22:10:29.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 8.007606027s
    Mar 15 22:10:29.144: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
    Mar 15 22:10:31.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 10.008230719s
    Mar 15 22:10:31.144: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
    Mar 15 22:10:33.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 12.008319879s
    Mar 15 22:10:33.144: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
    Mar 15 22:10:35.143: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 14.007227074s
    Mar 15 22:10:35.143: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
    Mar 15 22:10:37.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 16.00794963s
    Mar 15 22:10:37.144: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
    Mar 15 22:10:39.151: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 18.015131009s
    Mar 15 22:10:39.151: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
    Mar 15 22:10:41.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=false. Elapsed: 20.008241867s
    Mar 15 22:10:41.144: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = false)
    Mar 15 22:10:43.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c": Phase="Running", Reason="", readiness=true. Elapsed: 22.00826112s
    Mar 15 22:10:43.144: INFO: The phase of Pod test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c is Running (Ready = true)
    Mar 15 22:10:43.144: INFO: Pod "test-webserver-14b45e47-439f-4d08-8354-dd4e9c9da38c" satisfied condition "running and ready"
    Mar 15 22:10:43.147: INFO: Container started at 2023-03-15 22:10:21 +0000 UTC, pod became ready at 2023-03-15 22:10:41 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:43.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-887" for this suite. 03/15/23 22:10:43.151
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:43.157
Mar 15 22:10:43.158: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:10:43.159
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:43.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:43.173
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-215c95fa-1588-42c0-9170-35f6070c22f6 03/15/23 22:10:43.177
STEP: Creating a pod to test consume configMaps 03/15/23 22:10:43.181
Mar 15 22:10:43.187: INFO: Waiting up to 5m0s for pod "pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0" in namespace "configmap-4189" to be "Succeeded or Failed"
Mar 15 22:10:43.192: INFO: Pod "pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.926281ms
Mar 15 22:10:45.196: INFO: Pod "pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008686397s
Mar 15 22:10:47.198: INFO: Pod "pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010393032s
STEP: Saw pod success 03/15/23 22:10:47.198
Mar 15 22:10:47.198: INFO: Pod "pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0" satisfied condition "Succeeded or Failed"
Mar 15 22:10:47.201: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0 container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:10:47.212
Mar 15 22:10:47.225: INFO: Waiting for pod pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0 to disappear
Mar 15 22:10:47.228: INFO: Pod pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:47.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4189" for this suite. 03/15/23 22:10:47.232
------------------------------
• [4.082 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:43.157
    Mar 15 22:10:43.158: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:10:43.159
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:43.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:43.173
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-215c95fa-1588-42c0-9170-35f6070c22f6 03/15/23 22:10:43.177
    STEP: Creating a pod to test consume configMaps 03/15/23 22:10:43.181
    Mar 15 22:10:43.187: INFO: Waiting up to 5m0s for pod "pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0" in namespace "configmap-4189" to be "Succeeded or Failed"
    Mar 15 22:10:43.192: INFO: Pod "pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.926281ms
    Mar 15 22:10:45.196: INFO: Pod "pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008686397s
    Mar 15 22:10:47.198: INFO: Pod "pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010393032s
    STEP: Saw pod success 03/15/23 22:10:47.198
    Mar 15 22:10:47.198: INFO: Pod "pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0" satisfied condition "Succeeded or Failed"
    Mar 15 22:10:47.201: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0 container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:10:47.212
    Mar 15 22:10:47.225: INFO: Waiting for pod pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0 to disappear
    Mar 15 22:10:47.228: INFO: Pod pod-configmaps-317b0406-7fcb-41f2-a1d2-ee18e91665a0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:47.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4189" for this suite. 03/15/23 22:10:47.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:47.242
Mar 15 22:10:47.242: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename security-context 03/15/23 22:10:47.243
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:47.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:47.259
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/15/23 22:10:47.262
Mar 15 22:10:47.272: INFO: Waiting up to 5m0s for pod "security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99" in namespace "security-context-8742" to be "Succeeded or Failed"
Mar 15 22:10:47.278: INFO: Pod "security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99": Phase="Pending", Reason="", readiness=false. Elapsed: 5.81023ms
Mar 15 22:10:49.281: INFO: Pod "security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009303579s
Mar 15 22:10:51.281: INFO: Pod "security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009211398s
STEP: Saw pod success 03/15/23 22:10:51.281
Mar 15 22:10:51.282: INFO: Pod "security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99" satisfied condition "Succeeded or Failed"
Mar 15 22:10:51.284: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99 container test-container: <nil>
STEP: delete the pod 03/15/23 22:10:51.3
Mar 15 22:10:51.312: INFO: Waiting for pod security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99 to disappear
Mar 15 22:10:51.315: INFO: Pod security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:51.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-8742" for this suite. 03/15/23 22:10:51.319
------------------------------
• [4.083 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:47.242
    Mar 15 22:10:47.242: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename security-context 03/15/23 22:10:47.243
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:47.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:47.259
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/15/23 22:10:47.262
    Mar 15 22:10:47.272: INFO: Waiting up to 5m0s for pod "security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99" in namespace "security-context-8742" to be "Succeeded or Failed"
    Mar 15 22:10:47.278: INFO: Pod "security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99": Phase="Pending", Reason="", readiness=false. Elapsed: 5.81023ms
    Mar 15 22:10:49.281: INFO: Pod "security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009303579s
    Mar 15 22:10:51.281: INFO: Pod "security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009211398s
    STEP: Saw pod success 03/15/23 22:10:51.281
    Mar 15 22:10:51.282: INFO: Pod "security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99" satisfied condition "Succeeded or Failed"
    Mar 15 22:10:51.284: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99 container test-container: <nil>
    STEP: delete the pod 03/15/23 22:10:51.3
    Mar 15 22:10:51.312: INFO: Waiting for pod security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99 to disappear
    Mar 15 22:10:51.315: INFO: Pod security-context-91ed02be-7dc3-45cb-8d57-18dff4906a99 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:51.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-8742" for this suite. 03/15/23 22:10:51.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:51.326
Mar 15 22:10:51.326: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 22:10:51.327
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:51.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:51.355
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 03/15/23 22:10:51.36
Mar 15 22:10:51.372: INFO: Waiting up to 5m0s for pod "pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916" in namespace "emptydir-6154" to be "Succeeded or Failed"
Mar 15 22:10:51.376: INFO: Pod "pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916": Phase="Pending", Reason="", readiness=false. Elapsed: 4.594622ms
Mar 15 22:10:53.380: INFO: Pod "pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007892331s
Mar 15 22:10:55.381: INFO: Pod "pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009701398s
STEP: Saw pod success 03/15/23 22:10:55.381
Mar 15 22:10:55.382: INFO: Pod "pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916" satisfied condition "Succeeded or Failed"
Mar 15 22:10:55.385: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916 container test-container: <nil>
STEP: delete the pod 03/15/23 22:10:55.391
Mar 15 22:10:55.402: INFO: Waiting for pod pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916 to disappear
Mar 15 22:10:55.410: INFO: Pod pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:10:55.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6154" for this suite. 03/15/23 22:10:55.416
------------------------------
• [4.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:51.326
    Mar 15 22:10:51.326: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 22:10:51.327
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:51.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:51.355
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/15/23 22:10:51.36
    Mar 15 22:10:51.372: INFO: Waiting up to 5m0s for pod "pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916" in namespace "emptydir-6154" to be "Succeeded or Failed"
    Mar 15 22:10:51.376: INFO: Pod "pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916": Phase="Pending", Reason="", readiness=false. Elapsed: 4.594622ms
    Mar 15 22:10:53.380: INFO: Pod "pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007892331s
    Mar 15 22:10:55.381: INFO: Pod "pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009701398s
    STEP: Saw pod success 03/15/23 22:10:55.381
    Mar 15 22:10:55.382: INFO: Pod "pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916" satisfied condition "Succeeded or Failed"
    Mar 15 22:10:55.385: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916 container test-container: <nil>
    STEP: delete the pod 03/15/23 22:10:55.391
    Mar 15 22:10:55.402: INFO: Waiting for pod pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916 to disappear
    Mar 15 22:10:55.410: INFO: Pod pod-0552c7ef-3d3d-476c-b6a5-3c16b00b3916 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:10:55.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6154" for this suite. 03/15/23 22:10:55.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:10:55.455
Mar 15 22:10:55.455: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename aggregator 03/15/23 22:10:55.456
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:55.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:55.472
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Mar 15 22:10:55.475: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 03/15/23 22:10:55.475
Mar 15 22:10:56.419: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 15 22:10:58.492: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:11:00.510: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:11:02.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:11:04.495: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:11:06.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:11:08.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:11:10.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:11:12.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:11:14.495: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:11:16.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:11:18.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:11:20.641: INFO: Waited 131.526328ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 03/15/23 22:11:20.721
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/15/23 22:11:20.726
STEP: List APIServices 03/15/23 22:11:20.738
Mar 15 22:11:20.746: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Mar 15 22:11:21.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-5365" for this suite. 03/15/23 22:11:21.028
------------------------------
• [SLOW TEST] [25.597 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:10:55.455
    Mar 15 22:10:55.455: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename aggregator 03/15/23 22:10:55.456
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:10:55.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:10:55.472
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Mar 15 22:10:55.475: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 03/15/23 22:10:55.475
    Mar 15 22:10:56.419: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Mar 15 22:10:58.492: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:11:00.510: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:11:02.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:11:04.495: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:11:06.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:11:08.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:11:10.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:11:12.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:11:14.495: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:11:16.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:11:18.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 10, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:11:20.641: INFO: Waited 131.526328ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 03/15/23 22:11:20.721
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/15/23 22:11:20.726
    STEP: List APIServices 03/15/23 22:11:20.738
    Mar 15 22:11:20.746: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:11:21.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-5365" for this suite. 03/15/23 22:11:21.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:11:21.053
Mar 15 22:11:21.053: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename conformance-tests 03/15/23 22:11:21.054
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:21.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:21.071
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 03/15/23 22:11:21.074
Mar 15 22:11:21.075: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Mar 15 22:11:21.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-3707" for this suite. 03/15/23 22:11:21.084
------------------------------
• [0.039 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:11:21.053
    Mar 15 22:11:21.053: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename conformance-tests 03/15/23 22:11:21.054
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:21.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:21.071
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 03/15/23 22:11:21.074
    Mar 15 22:11:21.075: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:11:21.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-3707" for this suite. 03/15/23 22:11:21.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:11:21.103
Mar 15 22:11:21.103: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:11:21.104
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:21.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:21.119
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 03/15/23 22:11:21.122
Mar 15 22:11:21.130: INFO: Waiting up to 5m0s for pod "downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4" in namespace "projected-3194" to be "Succeeded or Failed"
Mar 15 22:11:21.133: INFO: Pod "downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.157187ms
Mar 15 22:11:23.137: INFO: Pod "downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006642983s
Mar 15 22:11:25.137: INFO: Pod "downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006956559s
STEP: Saw pod success 03/15/23 22:11:25.137
Mar 15 22:11:25.137: INFO: Pod "downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4" satisfied condition "Succeeded or Failed"
Mar 15 22:11:25.140: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4 container client-container: <nil>
STEP: delete the pod 03/15/23 22:11:25.146
Mar 15 22:11:25.161: INFO: Waiting for pod downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4 to disappear
Mar 15 22:11:25.165: INFO: Pod downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 15 22:11:25.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3194" for this suite. 03/15/23 22:11:25.169
------------------------------
• [4.072 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:11:21.103
    Mar 15 22:11:21.103: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:11:21.104
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:21.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:21.119
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 03/15/23 22:11:21.122
    Mar 15 22:11:21.130: INFO: Waiting up to 5m0s for pod "downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4" in namespace "projected-3194" to be "Succeeded or Failed"
    Mar 15 22:11:21.133: INFO: Pod "downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.157187ms
    Mar 15 22:11:23.137: INFO: Pod "downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006642983s
    Mar 15 22:11:25.137: INFO: Pod "downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006956559s
    STEP: Saw pod success 03/15/23 22:11:25.137
    Mar 15 22:11:25.137: INFO: Pod "downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4" satisfied condition "Succeeded or Failed"
    Mar 15 22:11:25.140: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4 container client-container: <nil>
    STEP: delete the pod 03/15/23 22:11:25.146
    Mar 15 22:11:25.161: INFO: Waiting for pod downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4 to disappear
    Mar 15 22:11:25.165: INFO: Pod downwardapi-volume-205679a5-5b07-4e78-993a-1a1731290bb4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:11:25.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3194" for this suite. 03/15/23 22:11:25.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:11:25.178
Mar 15 22:11:25.178: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 22:11:25.179
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:25.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:25.203
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-4023 03/15/23 22:11:25.207
STEP: creating service affinity-nodeport in namespace services-4023 03/15/23 22:11:25.207
STEP: creating replication controller affinity-nodeport in namespace services-4023 03/15/23 22:11:25.228
I0315 22:11:25.257343      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4023, replica count: 3
I0315 22:11:28.310040      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 22:11:28.322: INFO: Creating new exec pod
Mar 15 22:11:28.334: INFO: Waiting up to 5m0s for pod "execpod-affinity6d9lg" in namespace "services-4023" to be "running"
Mar 15 22:11:28.338: INFO: Pod "execpod-affinity6d9lg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076881ms
Mar 15 22:11:30.341: INFO: Pod "execpod-affinity6d9lg": Phase="Running", Reason="", readiness=true. Elapsed: 2.006774547s
Mar 15 22:11:30.341: INFO: Pod "execpod-affinity6d9lg" satisfied condition "running"
Mar 15 22:11:31.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-4023 exec execpod-affinity6d9lg -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Mar 15 22:11:31.539: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 15 22:11:31.539: INFO: stdout: ""
Mar 15 22:11:31.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-4023 exec execpod-affinity6d9lg -- /bin/sh -x -c nc -v -z -w 2 100.65.238.223 80'
Mar 15 22:11:31.718: INFO: stderr: "+ nc -v -z -w 2 100.65.238.223 80\nConnection to 100.65.238.223 80 port [tcp/http] succeeded!\n"
Mar 15 22:11:31.718: INFO: stdout: ""
Mar 15 22:11:31.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-4023 exec execpod-affinity6d9lg -- /bin/sh -x -c nc -v -z -w 2 172.20.126.23 31792'
Mar 15 22:11:31.885: INFO: stderr: "+ nc -v -z -w 2 172.20.126.23 31792\nConnection to 172.20.126.23 31792 port [tcp/*] succeeded!\n"
Mar 15 22:11:31.885: INFO: stdout: ""
Mar 15 22:11:31.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-4023 exec execpod-affinity6d9lg -- /bin/sh -x -c nc -v -z -w 2 172.20.75.105 31792'
Mar 15 22:11:32.074: INFO: stderr: "+ nc -v -z -w 2 172.20.75.105 31792\nConnection to 172.20.75.105 31792 port [tcp/*] succeeded!\n"
Mar 15 22:11:32.074: INFO: stdout: ""
Mar 15 22:11:32.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-4023 exec execpod-affinity6d9lg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.75.105:31792/ ; done'
Mar 15 22:11:32.335: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n"
Mar 15 22:11:32.335: INFO: stdout: "\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq"
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
Mar 15 22:11:32.335: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4023, will wait for the garbage collector to delete the pods 03/15/23 22:11:32.348
Mar 15 22:11:32.410: INFO: Deleting ReplicationController affinity-nodeport took: 5.064685ms
Mar 15 22:11:32.512: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.836734ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 22:11:34.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4023" for this suite. 03/15/23 22:11:34.24
------------------------------
• [SLOW TEST] [9.070 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:11:25.178
    Mar 15 22:11:25.178: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 22:11:25.179
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:25.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:25.203
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-4023 03/15/23 22:11:25.207
    STEP: creating service affinity-nodeport in namespace services-4023 03/15/23 22:11:25.207
    STEP: creating replication controller affinity-nodeport in namespace services-4023 03/15/23 22:11:25.228
    I0315 22:11:25.257343      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4023, replica count: 3
    I0315 22:11:28.310040      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 15 22:11:28.322: INFO: Creating new exec pod
    Mar 15 22:11:28.334: INFO: Waiting up to 5m0s for pod "execpod-affinity6d9lg" in namespace "services-4023" to be "running"
    Mar 15 22:11:28.338: INFO: Pod "execpod-affinity6d9lg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076881ms
    Mar 15 22:11:30.341: INFO: Pod "execpod-affinity6d9lg": Phase="Running", Reason="", readiness=true. Elapsed: 2.006774547s
    Mar 15 22:11:30.341: INFO: Pod "execpod-affinity6d9lg" satisfied condition "running"
    Mar 15 22:11:31.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-4023 exec execpod-affinity6d9lg -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Mar 15 22:11:31.539: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Mar 15 22:11:31.539: INFO: stdout: ""
    Mar 15 22:11:31.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-4023 exec execpod-affinity6d9lg -- /bin/sh -x -c nc -v -z -w 2 100.65.238.223 80'
    Mar 15 22:11:31.718: INFO: stderr: "+ nc -v -z -w 2 100.65.238.223 80\nConnection to 100.65.238.223 80 port [tcp/http] succeeded!\n"
    Mar 15 22:11:31.718: INFO: stdout: ""
    Mar 15 22:11:31.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-4023 exec execpod-affinity6d9lg -- /bin/sh -x -c nc -v -z -w 2 172.20.126.23 31792'
    Mar 15 22:11:31.885: INFO: stderr: "+ nc -v -z -w 2 172.20.126.23 31792\nConnection to 172.20.126.23 31792 port [tcp/*] succeeded!\n"
    Mar 15 22:11:31.885: INFO: stdout: ""
    Mar 15 22:11:31.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-4023 exec execpod-affinity6d9lg -- /bin/sh -x -c nc -v -z -w 2 172.20.75.105 31792'
    Mar 15 22:11:32.074: INFO: stderr: "+ nc -v -z -w 2 172.20.75.105 31792\nConnection to 172.20.75.105 31792 port [tcp/*] succeeded!\n"
    Mar 15 22:11:32.074: INFO: stdout: ""
    Mar 15 22:11:32.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-4023 exec execpod-affinity6d9lg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.75.105:31792/ ; done'
    Mar 15 22:11:32.335: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:31792/\n"
    Mar 15 22:11:32.335: INFO: stdout: "\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq\naffinity-nodeport-7ntlq"
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Received response from host: affinity-nodeport-7ntlq
    Mar 15 22:11:32.335: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-4023, will wait for the garbage collector to delete the pods 03/15/23 22:11:32.348
    Mar 15 22:11:32.410: INFO: Deleting ReplicationController affinity-nodeport took: 5.064685ms
    Mar 15 22:11:32.512: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.836734ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:11:34.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4023" for this suite. 03/15/23 22:11:34.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:11:34.248
Mar 15 22:11:34.248: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename resourcequota 03/15/23 22:11:34.251
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:34.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:34.274
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 03/15/23 22:11:34.278
STEP: Creating a ResourceQuota 03/15/23 22:11:39.281
STEP: Ensuring resource quota status is calculated 03/15/23 22:11:39.286
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 15 22:11:41.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9290" for this suite. 03/15/23 22:11:41.293
------------------------------
• [SLOW TEST] [7.064 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:11:34.248
    Mar 15 22:11:34.248: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename resourcequota 03/15/23 22:11:34.251
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:34.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:34.274
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 03/15/23 22:11:34.278
    STEP: Creating a ResourceQuota 03/15/23 22:11:39.281
    STEP: Ensuring resource quota status is calculated 03/15/23 22:11:39.286
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:11:41.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9290" for this suite. 03/15/23 22:11:41.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:11:41.318
Mar 15 22:11:41.318: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-runtime 03/15/23 22:11:41.319
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:41.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:41.338
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 03/15/23 22:11:41.344
STEP: wait for the container to reach Succeeded 03/15/23 22:11:41.35
STEP: get the container status 03/15/23 22:11:45.375
STEP: the container should be terminated 03/15/23 22:11:45.379
STEP: the termination message should be set 03/15/23 22:11:45.379
Mar 15 22:11:45.379: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 03/15/23 22:11:45.379
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 15 22:11:45.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5513" for this suite. 03/15/23 22:11:45.399
------------------------------
• [4.087 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:11:41.318
    Mar 15 22:11:41.318: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-runtime 03/15/23 22:11:41.319
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:41.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:41.338
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 03/15/23 22:11:41.344
    STEP: wait for the container to reach Succeeded 03/15/23 22:11:41.35
    STEP: get the container status 03/15/23 22:11:45.375
    STEP: the container should be terminated 03/15/23 22:11:45.379
    STEP: the termination message should be set 03/15/23 22:11:45.379
    Mar 15 22:11:45.379: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 03/15/23 22:11:45.379
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:11:45.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5513" for this suite. 03/15/23 22:11:45.399
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:11:45.405
Mar 15 22:11:45.406: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-lifecycle-hook 03/15/23 22:11:45.407
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:45.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:45.424
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/15/23 22:11:45.431
Mar 15 22:11:45.442: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3089" to be "running and ready"
Mar 15 22:11:45.446: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.609752ms
Mar 15 22:11:45.446: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:11:47.449: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006795798s
Mar 15 22:11:47.449: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 15 22:11:47.449: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 03/15/23 22:11:47.452
Mar 15 22:11:47.457: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-3089" to be "running and ready"
Mar 15 22:11:47.460: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.844622ms
Mar 15 22:11:47.460: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:11:49.466: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00961016s
Mar 15 22:11:49.466: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Mar 15 22:11:49.466: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/15/23 22:11:49.469
STEP: delete the pod with lifecycle hook 03/15/23 22:11:49.488
Mar 15 22:11:49.496: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 15 22:11:49.504: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 15 22:11:51.505: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 15 22:11:51.514: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 15 22:11:53.506: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 15 22:11:53.511: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 15 22:11:53.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3089" for this suite. 03/15/23 22:11:53.515
------------------------------
• [SLOW TEST] [8.116 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:11:45.405
    Mar 15 22:11:45.406: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/15/23 22:11:45.407
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:45.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:45.424
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/15/23 22:11:45.431
    Mar 15 22:11:45.442: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3089" to be "running and ready"
    Mar 15 22:11:45.446: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.609752ms
    Mar 15 22:11:45.446: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:11:47.449: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006795798s
    Mar 15 22:11:47.449: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 15 22:11:47.449: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 03/15/23 22:11:47.452
    Mar 15 22:11:47.457: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-3089" to be "running and ready"
    Mar 15 22:11:47.460: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.844622ms
    Mar 15 22:11:47.460: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:11:49.466: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00961016s
    Mar 15 22:11:49.466: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Mar 15 22:11:49.466: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/15/23 22:11:49.469
    STEP: delete the pod with lifecycle hook 03/15/23 22:11:49.488
    Mar 15 22:11:49.496: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 15 22:11:49.504: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 15 22:11:51.505: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 15 22:11:51.514: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 15 22:11:53.506: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 15 22:11:53.511: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:11:53.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3089" for this suite. 03/15/23 22:11:53.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:11:53.528
Mar 15 22:11:53.528: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename init-container 03/15/23 22:11:53.529
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:53.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:53.545
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 03/15/23 22:11:53.549
Mar 15 22:11:53.549: INFO: PodSpec: initContainers in spec.initContainers
Mar 15 22:12:33.881: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1f285e66-a990-4992-865f-d0215dca4b83", GenerateName:"", Namespace:"init-container-9565", SelfLink:"", UID:"fa849bc8-e6fd-4ee4-af32-760a9519f867", ResourceVersion:"13527", Generation:0, CreationTimestamp:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"549348128"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00109a120), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 15, 22, 12, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00109a198), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-9wgr5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005bcdee0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9wgr5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9wgr5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9wgr5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0050c0d28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"i-0faaf83f00b43c88c", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004ffe7e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0050c0da0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0050c0dc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0050c0dc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0050c0dcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000cf31b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.20.126.23", PodIP:"100.96.3.225", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.96.3.225"}}, StartTime:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004ffe8c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004ffe930)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://6f4e82455b9e7e2964de1351ad1539fe44319767aaedea21c9768cab1ee09236", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005bcdf60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005bcdf40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0050c0e4f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:12:33.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9565" for this suite. 03/15/23 22:12:33.888
------------------------------
• [SLOW TEST] [40.375 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:11:53.528
    Mar 15 22:11:53.528: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename init-container 03/15/23 22:11:53.529
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:11:53.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:11:53.545
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 03/15/23 22:11:53.549
    Mar 15 22:11:53.549: INFO: PodSpec: initContainers in spec.initContainers
    Mar 15 22:12:33.881: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1f285e66-a990-4992-865f-d0215dca4b83", GenerateName:"", Namespace:"init-container-9565", SelfLink:"", UID:"fa849bc8-e6fd-4ee4-af32-760a9519f867", ResourceVersion:"13527", Generation:0, CreationTimestamp:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"549348128"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00109a120), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 15, 22, 12, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00109a198), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-9wgr5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005bcdee0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9wgr5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9wgr5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9wgr5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0050c0d28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"i-0faaf83f00b43c88c", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004ffe7e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0050c0da0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0050c0dc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0050c0dc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0050c0dcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000cf31b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.20.126.23", PodIP:"100.96.3.225", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.96.3.225"}}, StartTime:time.Date(2023, time.March, 15, 22, 11, 53, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004ffe8c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004ffe930)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://6f4e82455b9e7e2964de1351ad1539fe44319767aaedea21c9768cab1ee09236", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005bcdf60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005bcdf40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0050c0e4f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:12:33.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9565" for this suite. 03/15/23 22:12:33.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:12:33.91
Mar 15 22:12:33.910: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 22:12:33.911
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:12:33.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:12:33.933
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-3163 03/15/23 22:12:33.937
STEP: creating replication controller nodeport-test in namespace services-3163 03/15/23 22:12:33.949
I0315 22:12:33.958701      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3163, replica count: 2
I0315 22:12:37.010085      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 22:12:37.010: INFO: Creating new exec pod
Mar 15 22:12:37.017: INFO: Waiting up to 5m0s for pod "execpod928st" in namespace "services-3163" to be "running"
Mar 15 22:12:37.020: INFO: Pod "execpod928st": Phase="Pending", Reason="", readiness=false. Elapsed: 3.600821ms
Mar 15 22:12:39.024: INFO: Pod "execpod928st": Phase="Running", Reason="", readiness=true. Elapsed: 2.007307929s
Mar 15 22:12:39.024: INFO: Pod "execpod928st" satisfied condition "running"
Mar 15 22:12:40.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-3163 exec execpod928st -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Mar 15 22:12:40.180: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 15 22:12:40.180: INFO: stdout: ""
Mar 15 22:12:40.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-3163 exec execpod928st -- /bin/sh -x -c nc -v -z -w 2 100.69.118.173 80'
Mar 15 22:12:40.334: INFO: stderr: "+ nc -v -z -w 2 100.69.118.173 80\nConnection to 100.69.118.173 80 port [tcp/http] succeeded!\n"
Mar 15 22:12:40.335: INFO: stdout: ""
Mar 15 22:12:40.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-3163 exec execpod928st -- /bin/sh -x -c nc -v -z -w 2 172.20.75.105 31164'
Mar 15 22:12:40.546: INFO: stderr: "+ nc -v -z -w 2 172.20.75.105 31164\nConnection to 172.20.75.105 31164 port [tcp/*] succeeded!\n"
Mar 15 22:12:40.546: INFO: stdout: ""
Mar 15 22:12:40.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-3163 exec execpod928st -- /bin/sh -x -c nc -v -z -w 2 172.20.126.23 31164'
Mar 15 22:12:40.698: INFO: stderr: "+ nc -v -z -w 2 172.20.126.23 31164\nConnection to 172.20.126.23 31164 port [tcp/*] succeeded!\n"
Mar 15 22:12:40.698: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 22:12:40.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3163" for this suite. 03/15/23 22:12:40.703
------------------------------
• [SLOW TEST] [6.799 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:12:33.91
    Mar 15 22:12:33.910: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 22:12:33.911
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:12:33.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:12:33.933
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-3163 03/15/23 22:12:33.937
    STEP: creating replication controller nodeport-test in namespace services-3163 03/15/23 22:12:33.949
    I0315 22:12:33.958701      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3163, replica count: 2
    I0315 22:12:37.010085      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 15 22:12:37.010: INFO: Creating new exec pod
    Mar 15 22:12:37.017: INFO: Waiting up to 5m0s for pod "execpod928st" in namespace "services-3163" to be "running"
    Mar 15 22:12:37.020: INFO: Pod "execpod928st": Phase="Pending", Reason="", readiness=false. Elapsed: 3.600821ms
    Mar 15 22:12:39.024: INFO: Pod "execpod928st": Phase="Running", Reason="", readiness=true. Elapsed: 2.007307929s
    Mar 15 22:12:39.024: INFO: Pod "execpod928st" satisfied condition "running"
    Mar 15 22:12:40.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-3163 exec execpod928st -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Mar 15 22:12:40.180: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar 15 22:12:40.180: INFO: stdout: ""
    Mar 15 22:12:40.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-3163 exec execpod928st -- /bin/sh -x -c nc -v -z -w 2 100.69.118.173 80'
    Mar 15 22:12:40.334: INFO: stderr: "+ nc -v -z -w 2 100.69.118.173 80\nConnection to 100.69.118.173 80 port [tcp/http] succeeded!\n"
    Mar 15 22:12:40.335: INFO: stdout: ""
    Mar 15 22:12:40.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-3163 exec execpod928st -- /bin/sh -x -c nc -v -z -w 2 172.20.75.105 31164'
    Mar 15 22:12:40.546: INFO: stderr: "+ nc -v -z -w 2 172.20.75.105 31164\nConnection to 172.20.75.105 31164 port [tcp/*] succeeded!\n"
    Mar 15 22:12:40.546: INFO: stdout: ""
    Mar 15 22:12:40.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-3163 exec execpod928st -- /bin/sh -x -c nc -v -z -w 2 172.20.126.23 31164'
    Mar 15 22:12:40.698: INFO: stderr: "+ nc -v -z -w 2 172.20.126.23 31164\nConnection to 172.20.126.23 31164 port [tcp/*] succeeded!\n"
    Mar 15 22:12:40.698: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:12:40.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3163" for this suite. 03/15/23 22:12:40.703
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:12:40.709
Mar 15 22:12:40.709: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:12:40.71
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:12:40.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:12:40.726
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-1143eb66-a1d7-4e3a-9359-f1d9642bf5e1 03/15/23 22:12:40.729
STEP: Creating a pod to test consume secrets 03/15/23 22:12:40.734
Mar 15 22:12:40.740: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88" in namespace "projected-7479" to be "Succeeded or Failed"
Mar 15 22:12:40.744: INFO: Pod "pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487938ms
Mar 15 22:12:42.748: INFO: Pod "pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007219737s
Mar 15 22:12:44.748: INFO: Pod "pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007893057s
STEP: Saw pod success 03/15/23 22:12:44.748
Mar 15 22:12:44.749: INFO: Pod "pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88" satisfied condition "Succeeded or Failed"
Mar 15 22:12:44.752: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/15/23 22:12:44.761
Mar 15 22:12:44.774: INFO: Waiting for pod pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88 to disappear
Mar 15 22:12:44.778: INFO: Pod pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 15 22:12:44.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7479" for this suite. 03/15/23 22:12:44.782
------------------------------
• [4.096 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:12:40.709
    Mar 15 22:12:40.709: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:12:40.71
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:12:40.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:12:40.726
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-1143eb66-a1d7-4e3a-9359-f1d9642bf5e1 03/15/23 22:12:40.729
    STEP: Creating a pod to test consume secrets 03/15/23 22:12:40.734
    Mar 15 22:12:40.740: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88" in namespace "projected-7479" to be "Succeeded or Failed"
    Mar 15 22:12:40.744: INFO: Pod "pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487938ms
    Mar 15 22:12:42.748: INFO: Pod "pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007219737s
    Mar 15 22:12:44.748: INFO: Pod "pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007893057s
    STEP: Saw pod success 03/15/23 22:12:44.748
    Mar 15 22:12:44.749: INFO: Pod "pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88" satisfied condition "Succeeded or Failed"
    Mar 15 22:12:44.752: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 22:12:44.761
    Mar 15 22:12:44.774: INFO: Waiting for pod pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88 to disappear
    Mar 15 22:12:44.778: INFO: Pod pod-projected-secrets-9d81fc32-f640-434a-b117-444710f1ee88 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:12:44.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7479" for this suite. 03/15/23 22:12:44.782
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:12:44.808
Mar 15 22:12:44.808: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename crd-webhook 03/15/23 22:12:44.809
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:12:44.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:12:44.879
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/15/23 22:12:44.882
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/15/23 22:12:45.254
STEP: Deploying the custom resource conversion webhook pod 03/15/23 22:12:45.267
STEP: Wait for the deployment to be ready 03/15/23 22:12:45.279
Mar 15 22:12:45.297: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 22:12:47.305
STEP: Verifying the service has paired with the endpoint 03/15/23 22:12:47.318
Mar 15 22:12:48.318: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Mar 15 22:12:48.321: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Creating a v1 custom resource 03/15/23 22:12:50.871
STEP: Create a v2 custom resource 03/15/23 22:12:50.892
STEP: List CRs in v1 03/15/23 22:12:50.935
STEP: List CRs in v2 03/15/23 22:12:50.946
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:12:51.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4191" for this suite. 03/15/23 22:12:51.531
------------------------------
• [SLOW TEST] [6.738 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:12:44.808
    Mar 15 22:12:44.808: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename crd-webhook 03/15/23 22:12:44.809
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:12:44.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:12:44.879
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/15/23 22:12:44.882
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/15/23 22:12:45.254
    STEP: Deploying the custom resource conversion webhook pod 03/15/23 22:12:45.267
    STEP: Wait for the deployment to be ready 03/15/23 22:12:45.279
    Mar 15 22:12:45.297: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 22:12:47.305
    STEP: Verifying the service has paired with the endpoint 03/15/23 22:12:47.318
    Mar 15 22:12:48.318: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Mar 15 22:12:48.321: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Creating a v1 custom resource 03/15/23 22:12:50.871
    STEP: Create a v2 custom resource 03/15/23 22:12:50.892
    STEP: List CRs in v1 03/15/23 22:12:50.935
    STEP: List CRs in v2 03/15/23 22:12:50.946
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:12:51.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4191" for this suite. 03/15/23 22:12:51.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:12:51.552
Mar 15 22:12:51.556: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 22:12:51.558
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:12:51.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:12:51.588
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 03/15/23 22:12:51.598
Mar 15 22:12:51.606: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-2d96bc87-51b7-4dcc-a632-ff57a578a1ab" in namespace "emptydir-3446" to be "running"
Mar 15 22:12:51.615: INFO: Pod "pod-sharedvolume-2d96bc87-51b7-4dcc-a632-ff57a578a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.502608ms
Mar 15 22:12:53.624: INFO: Pod "pod-sharedvolume-2d96bc87-51b7-4dcc-a632-ff57a578a1ab": Phase="Running", Reason="", readiness=false. Elapsed: 2.017617597s
Mar 15 22:12:53.624: INFO: Pod "pod-sharedvolume-2d96bc87-51b7-4dcc-a632-ff57a578a1ab" satisfied condition "running"
STEP: Reading file content from the nginx-container 03/15/23 22:12:53.624
Mar 15 22:12:53.624: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3446 PodName:pod-sharedvolume-2d96bc87-51b7-4dcc-a632-ff57a578a1ab ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:12:53.624: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:12:53.625: INFO: ExecWithOptions: Clientset creation
Mar 15 22:12:53.625: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/emptydir-3446/pods/pod-sharedvolume-2d96bc87-51b7-4dcc-a632-ff57a578a1ab/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar 15 22:12:53.722: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:12:53.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3446" for this suite. 03/15/23 22:12:53.727
------------------------------
• [2.183 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:12:51.552
    Mar 15 22:12:51.556: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 22:12:51.558
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:12:51.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:12:51.588
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 03/15/23 22:12:51.598
    Mar 15 22:12:51.606: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-2d96bc87-51b7-4dcc-a632-ff57a578a1ab" in namespace "emptydir-3446" to be "running"
    Mar 15 22:12:51.615: INFO: Pod "pod-sharedvolume-2d96bc87-51b7-4dcc-a632-ff57a578a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.502608ms
    Mar 15 22:12:53.624: INFO: Pod "pod-sharedvolume-2d96bc87-51b7-4dcc-a632-ff57a578a1ab": Phase="Running", Reason="", readiness=false. Elapsed: 2.017617597s
    Mar 15 22:12:53.624: INFO: Pod "pod-sharedvolume-2d96bc87-51b7-4dcc-a632-ff57a578a1ab" satisfied condition "running"
    STEP: Reading file content from the nginx-container 03/15/23 22:12:53.624
    Mar 15 22:12:53.624: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3446 PodName:pod-sharedvolume-2d96bc87-51b7-4dcc-a632-ff57a578a1ab ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:12:53.624: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:12:53.625: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:12:53.625: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/emptydir-3446/pods/pod-sharedvolume-2d96bc87-51b7-4dcc-a632-ff57a578a1ab/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Mar 15 22:12:53.722: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:12:53.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3446" for this suite. 03/15/23 22:12:53.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:12:53.737
Mar 15 22:12:53.737: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename dns 03/15/23 22:12:53.739
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:12:53.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:12:53.756
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/15/23 22:12:53.759
Mar 15 22:12:53.767: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3477  9a9c1db5-baff-46b9-b7e7-489ef2387a80 13765 0 2023-03-15 22:12:53 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-15 22:12:53 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9zr9f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zr9f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:12:53.767: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3477" to be "running and ready"
Mar 15 22:12:53.773: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 6.292426ms
Mar 15 22:12:53.773: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:12:55.778: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.010739227s
Mar 15 22:12:55.778: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Mar 15 22:12:55.778: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 03/15/23 22:12:55.778
Mar 15 22:12:55.778: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3477 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:12:55.778: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:12:55.779: INFO: ExecWithOptions: Clientset creation
Mar 15 22:12:55.779: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/dns-3477/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 03/15/23 22:12:55.861
Mar 15 22:12:55.861: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3477 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:12:55.861: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:12:55.862: INFO: ExecWithOptions: Clientset creation
Mar 15 22:12:55.862: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/dns-3477/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 15 22:12:55.946: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 15 22:12:55.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3477" for this suite. 03/15/23 22:12:55.965
------------------------------
• [2.243 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:12:53.737
    Mar 15 22:12:53.737: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename dns 03/15/23 22:12:53.739
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:12:53.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:12:53.756
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/15/23 22:12:53.759
    Mar 15 22:12:53.767: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3477  9a9c1db5-baff-46b9-b7e7-489ef2387a80 13765 0 2023-03-15 22:12:53 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-15 22:12:53 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9zr9f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zr9f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:12:53.767: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3477" to be "running and ready"
    Mar 15 22:12:53.773: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 6.292426ms
    Mar 15 22:12:53.773: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:12:55.778: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.010739227s
    Mar 15 22:12:55.778: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Mar 15 22:12:55.778: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 03/15/23 22:12:55.778
    Mar 15 22:12:55.778: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3477 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:12:55.778: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:12:55.779: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:12:55.779: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/dns-3477/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 03/15/23 22:12:55.861
    Mar 15 22:12:55.861: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3477 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:12:55.861: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:12:55.862: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:12:55.862: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/dns-3477/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 15 22:12:55.946: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:12:55.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3477" for this suite. 03/15/23 22:12:55.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:12:55.981
Mar 15 22:12:55.981: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename gc 03/15/23 22:12:55.982
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:12:55.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:12:55.998
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Mar 15 22:12:56.054: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8c26b766-e26c-4308-bb1e-6e2a95ef1a1a", Controller:(*bool)(0xc004b6e776), BlockOwnerDeletion:(*bool)(0xc004b6e777)}}
Mar 15 22:12:56.072: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"06a7bd20-5057-4463-a3c0-3adcad35b09d", Controller:(*bool)(0xc004b6e9f6), BlockOwnerDeletion:(*bool)(0xc004b6e9f7)}}
Mar 15 22:12:56.082: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"40340444-3540-43be-8f48-3ffc47c1e6f8", Controller:(*bool)(0xc004b6ec76), BlockOwnerDeletion:(*bool)(0xc004b6ec77)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:01.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9650" for this suite. 03/15/23 22:13:01.101
------------------------------
• [SLOW TEST] [5.129 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:12:55.981
    Mar 15 22:12:55.981: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename gc 03/15/23 22:12:55.982
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:12:55.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:12:55.998
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Mar 15 22:12:56.054: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8c26b766-e26c-4308-bb1e-6e2a95ef1a1a", Controller:(*bool)(0xc004b6e776), BlockOwnerDeletion:(*bool)(0xc004b6e777)}}
    Mar 15 22:12:56.072: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"06a7bd20-5057-4463-a3c0-3adcad35b09d", Controller:(*bool)(0xc004b6e9f6), BlockOwnerDeletion:(*bool)(0xc004b6e9f7)}}
    Mar 15 22:12:56.082: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"40340444-3540-43be-8f48-3ffc47c1e6f8", Controller:(*bool)(0xc004b6ec76), BlockOwnerDeletion:(*bool)(0xc004b6ec77)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:01.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9650" for this suite. 03/15/23 22:13:01.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:01.112
Mar 15 22:13:01.112: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:13:01.113
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:01.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:01.131
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 03/15/23 22:13:01.135
Mar 15 22:13:01.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e" in namespace "projected-9102" to be "Succeeded or Failed"
Mar 15 22:13:01.161: INFO: Pod "downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.044307ms
Mar 15 22:13:03.165: INFO: Pod "downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016803848s
Mar 15 22:13:05.166: INFO: Pod "downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017470423s
STEP: Saw pod success 03/15/23 22:13:05.166
Mar 15 22:13:05.166: INFO: Pod "downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e" satisfied condition "Succeeded or Failed"
Mar 15 22:13:05.169: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e container client-container: <nil>
STEP: delete the pod 03/15/23 22:13:05.174
Mar 15 22:13:05.191: INFO: Waiting for pod downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e to disappear
Mar 15 22:13:05.196: INFO: Pod downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:05.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9102" for this suite. 03/15/23 22:13:05.203
------------------------------
• [4.097 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:01.112
    Mar 15 22:13:01.112: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:13:01.113
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:01.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:01.131
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 03/15/23 22:13:01.135
    Mar 15 22:13:01.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e" in namespace "projected-9102" to be "Succeeded or Failed"
    Mar 15 22:13:01.161: INFO: Pod "downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.044307ms
    Mar 15 22:13:03.165: INFO: Pod "downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016803848s
    Mar 15 22:13:05.166: INFO: Pod "downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017470423s
    STEP: Saw pod success 03/15/23 22:13:05.166
    Mar 15 22:13:05.166: INFO: Pod "downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e" satisfied condition "Succeeded or Failed"
    Mar 15 22:13:05.169: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e container client-container: <nil>
    STEP: delete the pod 03/15/23 22:13:05.174
    Mar 15 22:13:05.191: INFO: Waiting for pod downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e to disappear
    Mar 15 22:13:05.196: INFO: Pod downwardapi-volume-4da25702-6149-42a5-ab11-8ef99bf76d9e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:05.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9102" for this suite. 03/15/23 22:13:05.203
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:05.209
Mar 15 22:13:05.209: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 22:13:05.21
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:05.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:05.241
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 03/15/23 22:13:05.246
Mar 15 22:13:05.259: INFO: Waiting up to 5m0s for pod "downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e" in namespace "downward-api-8798" to be "Succeeded or Failed"
Mar 15 22:13:05.270: INFO: Pod "downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.638435ms
Mar 15 22:13:07.274: INFO: Pod "downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015022593s
Mar 15 22:13:09.275: INFO: Pod "downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015983442s
STEP: Saw pod success 03/15/23 22:13:09.275
Mar 15 22:13:09.275: INFO: Pod "downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e" satisfied condition "Succeeded or Failed"
Mar 15 22:13:09.277: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e container dapi-container: <nil>
STEP: delete the pod 03/15/23 22:13:09.288
Mar 15 22:13:09.305: INFO: Waiting for pod downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e to disappear
Mar 15 22:13:09.314: INFO: Pod downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:09.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8798" for this suite. 03/15/23 22:13:09.321
------------------------------
• [4.124 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:05.209
    Mar 15 22:13:05.209: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 22:13:05.21
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:05.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:05.241
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 03/15/23 22:13:05.246
    Mar 15 22:13:05.259: INFO: Waiting up to 5m0s for pod "downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e" in namespace "downward-api-8798" to be "Succeeded or Failed"
    Mar 15 22:13:05.270: INFO: Pod "downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.638435ms
    Mar 15 22:13:07.274: INFO: Pod "downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015022593s
    Mar 15 22:13:09.275: INFO: Pod "downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015983442s
    STEP: Saw pod success 03/15/23 22:13:09.275
    Mar 15 22:13:09.275: INFO: Pod "downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e" satisfied condition "Succeeded or Failed"
    Mar 15 22:13:09.277: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e container dapi-container: <nil>
    STEP: delete the pod 03/15/23 22:13:09.288
    Mar 15 22:13:09.305: INFO: Waiting for pod downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e to disappear
    Mar 15 22:13:09.314: INFO: Pod downward-api-afaaf5ea-3633-46d9-91cc-6348c911da5e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:09.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8798" for this suite. 03/15/23 22:13:09.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:09.335
Mar 15 22:13:09.335: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:13:09.336
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:09.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:09.369
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-2125504f-52ab-432e-a5c6-eea471cdc5d1 03/15/23 22:13:09.372
STEP: Creating a pod to test consume configMaps 03/15/23 22:13:09.378
Mar 15 22:13:09.385: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7" in namespace "projected-9748" to be "Succeeded or Failed"
Mar 15 22:13:09.388: INFO: Pod "pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.157783ms
Mar 15 22:13:11.392: INFO: Pod "pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007297643s
Mar 15 22:13:13.392: INFO: Pod "pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007200957s
STEP: Saw pod success 03/15/23 22:13:13.392
Mar 15 22:13:13.393: INFO: Pod "pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7" satisfied condition "Succeeded or Failed"
Mar 15 22:13:13.396: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7 container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:13:13.402
Mar 15 22:13:13.438: INFO: Waiting for pod pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7 to disappear
Mar 15 22:13:13.444: INFO: Pod pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:13.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9748" for this suite. 03/15/23 22:13:13.451
------------------------------
• [4.124 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:09.335
    Mar 15 22:13:09.335: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:13:09.336
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:09.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:09.369
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-2125504f-52ab-432e-a5c6-eea471cdc5d1 03/15/23 22:13:09.372
    STEP: Creating a pod to test consume configMaps 03/15/23 22:13:09.378
    Mar 15 22:13:09.385: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7" in namespace "projected-9748" to be "Succeeded or Failed"
    Mar 15 22:13:09.388: INFO: Pod "pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.157783ms
    Mar 15 22:13:11.392: INFO: Pod "pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007297643s
    Mar 15 22:13:13.392: INFO: Pod "pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007200957s
    STEP: Saw pod success 03/15/23 22:13:13.392
    Mar 15 22:13:13.393: INFO: Pod "pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7" satisfied condition "Succeeded or Failed"
    Mar 15 22:13:13.396: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7 container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:13:13.402
    Mar 15 22:13:13.438: INFO: Waiting for pod pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7 to disappear
    Mar 15 22:13:13.444: INFO: Pod pod-projected-configmaps-e71064cd-571c-4013-89b6-2c0ae293c8e7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:13.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9748" for this suite. 03/15/23 22:13:13.451
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:13.46
Mar 15 22:13:13.461: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:13:13.463
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:13.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:13.503
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 03/15/23 22:13:13.509
Mar 15 22:13:13.509: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3347 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 03/15/23 22:13:13.591
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:13.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3347" for this suite. 03/15/23 22:13:13.606
------------------------------
• [0.162 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:13.46
    Mar 15 22:13:13.461: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:13:13.463
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:13.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:13.503
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 03/15/23 22:13:13.509
    Mar 15 22:13:13.509: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3347 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 03/15/23 22:13:13.591
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:13.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3347" for this suite. 03/15/23 22:13:13.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:13.627
Mar 15 22:13:13.627: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:13:13.629
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:13.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:13.647
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 03/15/23 22:13:13.653
Mar 15 22:13:13.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 create -f -'
Mar 15 22:13:14.885: INFO: stderr: ""
Mar 15 22:13:14.885: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/15/23 22:13:14.885
Mar 15 22:13:14.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 22:13:15.034: INFO: stderr: ""
Mar 15 22:13:15.034: INFO: stdout: "update-demo-nautilus-phk42 update-demo-nautilus-zpckd "
Mar 15 22:13:15.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods update-demo-nautilus-phk42 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 22:13:15.156: INFO: stderr: ""
Mar 15 22:13:15.156: INFO: stdout: ""
Mar 15 22:13:15.157: INFO: update-demo-nautilus-phk42 is created but not running
Mar 15 22:13:20.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 22:13:20.239: INFO: stderr: ""
Mar 15 22:13:20.239: INFO: stdout: "update-demo-nautilus-phk42 update-demo-nautilus-zpckd "
Mar 15 22:13:20.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods update-demo-nautilus-phk42 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 22:13:20.344: INFO: stderr: ""
Mar 15 22:13:20.344: INFO: stdout: "true"
Mar 15 22:13:20.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods update-demo-nautilus-phk42 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 22:13:20.426: INFO: stderr: ""
Mar 15 22:13:20.426: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 15 22:13:20.426: INFO: validating pod update-demo-nautilus-phk42
Mar 15 22:13:20.434: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 22:13:20.434: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 22:13:20.434: INFO: update-demo-nautilus-phk42 is verified up and running
Mar 15 22:13:20.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods update-demo-nautilus-zpckd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 22:13:20.520: INFO: stderr: ""
Mar 15 22:13:20.520: INFO: stdout: "true"
Mar 15 22:13:20.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods update-demo-nautilus-zpckd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 22:13:20.603: INFO: stderr: ""
Mar 15 22:13:20.603: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 15 22:13:20.603: INFO: validating pod update-demo-nautilus-zpckd
Mar 15 22:13:20.608: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 22:13:20.608: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 22:13:20.608: INFO: update-demo-nautilus-zpckd is verified up and running
STEP: using delete to clean up resources 03/15/23 22:13:20.608
Mar 15 22:13:20.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 delete --grace-period=0 --force -f -'
Mar 15 22:13:20.696: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 22:13:20.696: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 15 22:13:20.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get rc,svc -l name=update-demo --no-headers'
Mar 15 22:13:20.825: INFO: stderr: "No resources found in kubectl-4044 namespace.\n"
Mar 15 22:13:20.825: INFO: stdout: ""
Mar 15 22:13:20.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 15 22:13:20.949: INFO: stderr: ""
Mar 15 22:13:20.950: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:20.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4044" for this suite. 03/15/23 22:13:20.954
------------------------------
• [SLOW TEST] [7.335 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:13.627
    Mar 15 22:13:13.627: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:13:13.629
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:13.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:13.647
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 03/15/23 22:13:13.653
    Mar 15 22:13:13.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 create -f -'
    Mar 15 22:13:14.885: INFO: stderr: ""
    Mar 15 22:13:14.885: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/15/23 22:13:14.885
    Mar 15 22:13:14.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 15 22:13:15.034: INFO: stderr: ""
    Mar 15 22:13:15.034: INFO: stdout: "update-demo-nautilus-phk42 update-demo-nautilus-zpckd "
    Mar 15 22:13:15.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods update-demo-nautilus-phk42 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 15 22:13:15.156: INFO: stderr: ""
    Mar 15 22:13:15.156: INFO: stdout: ""
    Mar 15 22:13:15.157: INFO: update-demo-nautilus-phk42 is created but not running
    Mar 15 22:13:20.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 15 22:13:20.239: INFO: stderr: ""
    Mar 15 22:13:20.239: INFO: stdout: "update-demo-nautilus-phk42 update-demo-nautilus-zpckd "
    Mar 15 22:13:20.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods update-demo-nautilus-phk42 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 15 22:13:20.344: INFO: stderr: ""
    Mar 15 22:13:20.344: INFO: stdout: "true"
    Mar 15 22:13:20.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods update-demo-nautilus-phk42 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 15 22:13:20.426: INFO: stderr: ""
    Mar 15 22:13:20.426: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 15 22:13:20.426: INFO: validating pod update-demo-nautilus-phk42
    Mar 15 22:13:20.434: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 15 22:13:20.434: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 15 22:13:20.434: INFO: update-demo-nautilus-phk42 is verified up and running
    Mar 15 22:13:20.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods update-demo-nautilus-zpckd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 15 22:13:20.520: INFO: stderr: ""
    Mar 15 22:13:20.520: INFO: stdout: "true"
    Mar 15 22:13:20.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods update-demo-nautilus-zpckd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 15 22:13:20.603: INFO: stderr: ""
    Mar 15 22:13:20.603: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 15 22:13:20.603: INFO: validating pod update-demo-nautilus-zpckd
    Mar 15 22:13:20.608: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 15 22:13:20.608: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 15 22:13:20.608: INFO: update-demo-nautilus-zpckd is verified up and running
    STEP: using delete to clean up resources 03/15/23 22:13:20.608
    Mar 15 22:13:20.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 delete --grace-period=0 --force -f -'
    Mar 15 22:13:20.696: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 15 22:13:20.696: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 15 22:13:20.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get rc,svc -l name=update-demo --no-headers'
    Mar 15 22:13:20.825: INFO: stderr: "No resources found in kubectl-4044 namespace.\n"
    Mar 15 22:13:20.825: INFO: stdout: ""
    Mar 15 22:13:20.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4044 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 15 22:13:20.949: INFO: stderr: ""
    Mar 15 22:13:20.950: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:20.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4044" for this suite. 03/15/23 22:13:20.954
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:20.963
Mar 15 22:13:20.963: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 22:13:20.964
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:20.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:20.979
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 22:13:20.994
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:13:21.459
STEP: Deploying the webhook pod 03/15/23 22:13:21.465
STEP: Wait for the deployment to be ready 03/15/23 22:13:21.478
Mar 15 22:13:21.485: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 22:13:23.494
STEP: Verifying the service has paired with the endpoint 03/15/23 22:13:23.51
Mar 15 22:13:24.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/15/23 22:13:24.517
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/15/23 22:13:24.534
STEP: Creating a dummy validating-webhook-configuration object 03/15/23 22:13:24.55
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/15/23 22:13:24.563
STEP: Creating a dummy mutating-webhook-configuration object 03/15/23 22:13:24.57
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/15/23 22:13:24.579
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:24.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-435" for this suite. 03/15/23 22:13:24.783
STEP: Destroying namespace "webhook-435-markers" for this suite. 03/15/23 22:13:24.813
------------------------------
• [3.869 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:20.963
    Mar 15 22:13:20.963: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 22:13:20.964
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:20.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:20.979
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 22:13:20.994
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:13:21.459
    STEP: Deploying the webhook pod 03/15/23 22:13:21.465
    STEP: Wait for the deployment to be ready 03/15/23 22:13:21.478
    Mar 15 22:13:21.485: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 22:13:23.494
    STEP: Verifying the service has paired with the endpoint 03/15/23 22:13:23.51
    Mar 15 22:13:24.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/15/23 22:13:24.517
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/15/23 22:13:24.534
    STEP: Creating a dummy validating-webhook-configuration object 03/15/23 22:13:24.55
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/15/23 22:13:24.563
    STEP: Creating a dummy mutating-webhook-configuration object 03/15/23 22:13:24.57
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/15/23 22:13:24.579
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:24.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-435" for this suite. 03/15/23 22:13:24.783
    STEP: Destroying namespace "webhook-435-markers" for this suite. 03/15/23 22:13:24.813
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:24.831
Mar 15 22:13:24.831: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:13:24.832
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:24.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:24.905
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 03/15/23 22:13:24.909
Mar 15 22:13:24.940: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09" in namespace "projected-3248" to be "Succeeded or Failed"
Mar 15 22:13:24.965: INFO: Pod "downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09": Phase="Pending", Reason="", readiness=false. Elapsed: 25.076005ms
Mar 15 22:13:26.968: INFO: Pod "downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028720046s
Mar 15 22:13:28.968: INFO: Pod "downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028132272s
STEP: Saw pod success 03/15/23 22:13:28.968
Mar 15 22:13:28.968: INFO: Pod "downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09" satisfied condition "Succeeded or Failed"
Mar 15 22:13:28.971: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09 container client-container: <nil>
STEP: delete the pod 03/15/23 22:13:28.976
Mar 15 22:13:28.990: INFO: Waiting for pod downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09 to disappear
Mar 15 22:13:28.993: INFO: Pod downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:28.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3248" for this suite. 03/15/23 22:13:28.999
------------------------------
• [4.173 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:24.831
    Mar 15 22:13:24.831: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:13:24.832
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:24.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:24.905
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 03/15/23 22:13:24.909
    Mar 15 22:13:24.940: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09" in namespace "projected-3248" to be "Succeeded or Failed"
    Mar 15 22:13:24.965: INFO: Pod "downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09": Phase="Pending", Reason="", readiness=false. Elapsed: 25.076005ms
    Mar 15 22:13:26.968: INFO: Pod "downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028720046s
    Mar 15 22:13:28.968: INFO: Pod "downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028132272s
    STEP: Saw pod success 03/15/23 22:13:28.968
    Mar 15 22:13:28.968: INFO: Pod "downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09" satisfied condition "Succeeded or Failed"
    Mar 15 22:13:28.971: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09 container client-container: <nil>
    STEP: delete the pod 03/15/23 22:13:28.976
    Mar 15 22:13:28.990: INFO: Waiting for pod downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09 to disappear
    Mar 15 22:13:28.993: INFO: Pod downwardapi-volume-cbc67e8d-1cdf-45d3-b9ca-44d201d36d09 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:28.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3248" for this suite. 03/15/23 22:13:28.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:29.005
Mar 15 22:13:29.005: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:13:29.006
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:29.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:29.022
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-54b151d6-69a2-4905-99eb-b2449b16cffe 03/15/23 22:13:29.025
STEP: Creating a pod to test consume configMaps 03/15/23 22:13:29.029
Mar 15 22:13:29.043: INFO: Waiting up to 5m0s for pod "pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73" in namespace "configmap-4163" to be "Succeeded or Failed"
Mar 15 22:13:29.057: INFO: Pod "pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73": Phase="Pending", Reason="", readiness=false. Elapsed: 14.154505ms
Mar 15 22:13:31.061: INFO: Pod "pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018201334s
Mar 15 22:13:33.062: INFO: Pod "pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018399214s
STEP: Saw pod success 03/15/23 22:13:33.062
Mar 15 22:13:33.062: INFO: Pod "pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73" satisfied condition "Succeeded or Failed"
Mar 15 22:13:33.064: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73 container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:13:33.069
Mar 15 22:13:33.092: INFO: Waiting for pod pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73 to disappear
Mar 15 22:13:33.095: INFO: Pod pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:33.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4163" for this suite. 03/15/23 22:13:33.099
------------------------------
• [4.100 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:29.005
    Mar 15 22:13:29.005: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:13:29.006
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:29.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:29.022
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-54b151d6-69a2-4905-99eb-b2449b16cffe 03/15/23 22:13:29.025
    STEP: Creating a pod to test consume configMaps 03/15/23 22:13:29.029
    Mar 15 22:13:29.043: INFO: Waiting up to 5m0s for pod "pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73" in namespace "configmap-4163" to be "Succeeded or Failed"
    Mar 15 22:13:29.057: INFO: Pod "pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73": Phase="Pending", Reason="", readiness=false. Elapsed: 14.154505ms
    Mar 15 22:13:31.061: INFO: Pod "pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018201334s
    Mar 15 22:13:33.062: INFO: Pod "pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018399214s
    STEP: Saw pod success 03/15/23 22:13:33.062
    Mar 15 22:13:33.062: INFO: Pod "pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73" satisfied condition "Succeeded or Failed"
    Mar 15 22:13:33.064: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73 container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:13:33.069
    Mar 15 22:13:33.092: INFO: Waiting for pod pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73 to disappear
    Mar 15 22:13:33.095: INFO: Pod pod-configmaps-823425ce-dbaf-48b7-8cae-b8ada02c2f73 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:33.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4163" for this suite. 03/15/23 22:13:33.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:33.106
Mar 15 22:13:33.106: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 22:13:33.107
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:33.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:33.124
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 03/15/23 22:13:33.127
Mar 15 22:13:33.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607" in namespace "downward-api-8938" to be "Succeeded or Failed"
Mar 15 22:13:33.147: INFO: Pod "downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607": Phase="Pending", Reason="", readiness=false. Elapsed: 8.812463ms
Mar 15 22:13:35.151: INFO: Pod "downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012744448s
Mar 15 22:13:37.151: INFO: Pod "downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013204181s
STEP: Saw pod success 03/15/23 22:13:37.151
Mar 15 22:13:37.151: INFO: Pod "downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607" satisfied condition "Succeeded or Failed"
Mar 15 22:13:37.154: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607 container client-container: <nil>
STEP: delete the pod 03/15/23 22:13:37.159
Mar 15 22:13:37.172: INFO: Waiting for pod downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607 to disappear
Mar 15 22:13:37.175: INFO: Pod downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:37.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8938" for this suite. 03/15/23 22:13:37.179
------------------------------
• [4.078 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:33.106
    Mar 15 22:13:33.106: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 22:13:33.107
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:33.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:33.124
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 03/15/23 22:13:33.127
    Mar 15 22:13:33.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607" in namespace "downward-api-8938" to be "Succeeded or Failed"
    Mar 15 22:13:33.147: INFO: Pod "downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607": Phase="Pending", Reason="", readiness=false. Elapsed: 8.812463ms
    Mar 15 22:13:35.151: INFO: Pod "downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012744448s
    Mar 15 22:13:37.151: INFO: Pod "downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013204181s
    STEP: Saw pod success 03/15/23 22:13:37.151
    Mar 15 22:13:37.151: INFO: Pod "downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607" satisfied condition "Succeeded or Failed"
    Mar 15 22:13:37.154: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607 container client-container: <nil>
    STEP: delete the pod 03/15/23 22:13:37.159
    Mar 15 22:13:37.172: INFO: Waiting for pod downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607 to disappear
    Mar 15 22:13:37.175: INFO: Pod downwardapi-volume-0d00a2be-d3ab-47cb-91fd-65ede2b38607 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:37.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8938" for this suite. 03/15/23 22:13:37.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:37.19
Mar 15 22:13:37.190: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename init-container 03/15/23 22:13:37.191
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:37.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:37.206
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 03/15/23 22:13:37.21
Mar 15 22:13:37.210: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:43.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7237" for this suite. 03/15/23 22:13:43.197
------------------------------
• [SLOW TEST] [6.015 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:37.19
    Mar 15 22:13:37.190: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename init-container 03/15/23 22:13:37.191
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:37.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:37.206
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 03/15/23 22:13:37.21
    Mar 15 22:13:37.210: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:43.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7237" for this suite. 03/15/23 22:13:43.197
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:43.206
Mar 15 22:13:43.206: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename csiinlinevolumes 03/15/23 22:13:43.209
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:43.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:43.228
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 03/15/23 22:13:43.231
STEP: getting 03/15/23 22:13:43.253
STEP: listing in namespace 03/15/23 22:13:43.259
STEP: patching 03/15/23 22:13:43.272
STEP: deleting 03/15/23 22:13:43.284
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:43.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2482" for this suite. 03/15/23 22:13:43.299
------------------------------
• [0.098 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:43.206
    Mar 15 22:13:43.206: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename csiinlinevolumes 03/15/23 22:13:43.209
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:43.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:43.228
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 03/15/23 22:13:43.231
    STEP: getting 03/15/23 22:13:43.253
    STEP: listing in namespace 03/15/23 22:13:43.259
    STEP: patching 03/15/23 22:13:43.272
    STEP: deleting 03/15/23 22:13:43.284
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:43.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2482" for this suite. 03/15/23 22:13:43.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:43.306
Mar 15 22:13:43.306: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 22:13:43.307
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:43.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:43.322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 22:13:43.339
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:13:43.818
STEP: Deploying the webhook pod 03/15/23 22:13:43.823
STEP: Wait for the deployment to be ready 03/15/23 22:13:43.834
Mar 15 22:13:43.839: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 22:13:45.848
STEP: Verifying the service has paired with the endpoint 03/15/23 22:13:45.859
Mar 15 22:13:46.860: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Mar 15 22:13:46.864: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8070-crds.webhook.example.com via the AdmissionRegistration API 03/15/23 22:13:47.383
STEP: Creating a custom resource that should be mutated by the webhook 03/15/23 22:13:47.412
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:50.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2960" for this suite. 03/15/23 22:13:50.096
STEP: Destroying namespace "webhook-2960-markers" for this suite. 03/15/23 22:13:50.106
------------------------------
• [SLOW TEST] [6.825 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:43.306
    Mar 15 22:13:43.306: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 22:13:43.307
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:43.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:43.322
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 22:13:43.339
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:13:43.818
    STEP: Deploying the webhook pod 03/15/23 22:13:43.823
    STEP: Wait for the deployment to be ready 03/15/23 22:13:43.834
    Mar 15 22:13:43.839: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 22:13:45.848
    STEP: Verifying the service has paired with the endpoint 03/15/23 22:13:45.859
    Mar 15 22:13:46.860: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Mar 15 22:13:46.864: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8070-crds.webhook.example.com via the AdmissionRegistration API 03/15/23 22:13:47.383
    STEP: Creating a custom resource that should be mutated by the webhook 03/15/23 22:13:47.412
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:50.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2960" for this suite. 03/15/23 22:13:50.096
    STEP: Destroying namespace "webhook-2960-markers" for this suite. 03/15/23 22:13:50.106
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:50.136
Mar 15 22:13:50.136: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 22:13:50.138
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:50.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:50.17
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-8c2d73b6-4b62-4f84-bff4-2cc4f555ca71 03/15/23 22:13:50.205
STEP: Creating a pod to test consume secrets 03/15/23 22:13:50.21
Mar 15 22:13:50.220: INFO: Waiting up to 5m0s for pod "pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf" in namespace "secrets-920" to be "Succeeded or Failed"
Mar 15 22:13:50.224: INFO: Pod "pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.647328ms
Mar 15 22:13:52.227: INFO: Pod "pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006504618s
Mar 15 22:13:54.227: INFO: Pod "pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf": Phase="Running", Reason="", readiness=false. Elapsed: 4.007180246s
Mar 15 22:13:56.228: INFO: Pod "pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007463186s
STEP: Saw pod success 03/15/23 22:13:56.228
Mar 15 22:13:56.228: INFO: Pod "pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf" satisfied condition "Succeeded or Failed"
Mar 15 22:13:56.230: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf container secret-volume-test: <nil>
STEP: delete the pod 03/15/23 22:13:56.236
Mar 15 22:13:56.248: INFO: Waiting for pod pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf to disappear
Mar 15 22:13:56.251: INFO: Pod pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 22:13:56.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-920" for this suite. 03/15/23 22:13:56.256
STEP: Destroying namespace "secret-namespace-2817" for this suite. 03/15/23 22:13:56.262
------------------------------
• [SLOW TEST] [6.130 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:50.136
    Mar 15 22:13:50.136: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 22:13:50.138
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:50.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:50.17
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-8c2d73b6-4b62-4f84-bff4-2cc4f555ca71 03/15/23 22:13:50.205
    STEP: Creating a pod to test consume secrets 03/15/23 22:13:50.21
    Mar 15 22:13:50.220: INFO: Waiting up to 5m0s for pod "pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf" in namespace "secrets-920" to be "Succeeded or Failed"
    Mar 15 22:13:50.224: INFO: Pod "pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.647328ms
    Mar 15 22:13:52.227: INFO: Pod "pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006504618s
    Mar 15 22:13:54.227: INFO: Pod "pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf": Phase="Running", Reason="", readiness=false. Elapsed: 4.007180246s
    Mar 15 22:13:56.228: INFO: Pod "pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007463186s
    STEP: Saw pod success 03/15/23 22:13:56.228
    Mar 15 22:13:56.228: INFO: Pod "pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf" satisfied condition "Succeeded or Failed"
    Mar 15 22:13:56.230: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf container secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 22:13:56.236
    Mar 15 22:13:56.248: INFO: Waiting for pod pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf to disappear
    Mar 15 22:13:56.251: INFO: Pod pod-secrets-f8df91a5-b713-4d88-8880-37db742c31cf no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:13:56.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-920" for this suite. 03/15/23 22:13:56.256
    STEP: Destroying namespace "secret-namespace-2817" for this suite. 03/15/23 22:13:56.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:13:56.27
Mar 15 22:13:56.270: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:13:56.271
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:56.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:56.288
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-3a757252-c853-45ad-a805-116c936b89d6 03/15/23 22:13:56.291
STEP: Creating a pod to test consume configMaps 03/15/23 22:13:56.295
Mar 15 22:13:56.300: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07" in namespace "projected-693" to be "Succeeded or Failed"
Mar 15 22:13:56.303: INFO: Pod "pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.298357ms
Mar 15 22:13:58.306: INFO: Pod "pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07": Phase="Running", Reason="", readiness=false. Elapsed: 2.006397533s
Mar 15 22:14:00.317: INFO: Pod "pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017023374s
STEP: Saw pod success 03/15/23 22:14:00.317
Mar 15 22:14:00.317: INFO: Pod "pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07" satisfied condition "Succeeded or Failed"
Mar 15 22:14:00.320: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07 container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:14:00.332
Mar 15 22:14:00.347: INFO: Waiting for pod pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07 to disappear
Mar 15 22:14:00.354: INFO: Pod pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:14:00.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-693" for this suite. 03/15/23 22:14:00.363
------------------------------
• [4.100 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:13:56.27
    Mar 15 22:13:56.270: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:13:56.271
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:13:56.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:13:56.288
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-3a757252-c853-45ad-a805-116c936b89d6 03/15/23 22:13:56.291
    STEP: Creating a pod to test consume configMaps 03/15/23 22:13:56.295
    Mar 15 22:13:56.300: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07" in namespace "projected-693" to be "Succeeded or Failed"
    Mar 15 22:13:56.303: INFO: Pod "pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.298357ms
    Mar 15 22:13:58.306: INFO: Pod "pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07": Phase="Running", Reason="", readiness=false. Elapsed: 2.006397533s
    Mar 15 22:14:00.317: INFO: Pod "pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017023374s
    STEP: Saw pod success 03/15/23 22:14:00.317
    Mar 15 22:14:00.317: INFO: Pod "pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07" satisfied condition "Succeeded or Failed"
    Mar 15 22:14:00.320: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07 container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:14:00.332
    Mar 15 22:14:00.347: INFO: Waiting for pod pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07 to disappear
    Mar 15 22:14:00.354: INFO: Pod pod-projected-configmaps-bb763db6-012d-4ef5-87a6-55863871db07 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:14:00.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-693" for this suite. 03/15/23 22:14:00.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:14:00.375
Mar 15 22:14:00.375: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pod-network-test 03/15/23 22:14:00.376
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:14:00.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:14:00.417
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-3654 03/15/23 22:14:00.422
STEP: creating a selector 03/15/23 22:14:00.422
STEP: Creating the service pods in kubernetes 03/15/23 22:14:00.422
Mar 15 22:14:00.422: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 15 22:14:00.503: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3654" to be "running and ready"
Mar 15 22:14:00.515: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.688042ms
Mar 15 22:14:00.515: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:14:02.518: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015097188s
Mar 15 22:14:02.518: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:14:04.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015868425s
Mar 15 22:14:04.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:14:06.520: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017060636s
Mar 15 22:14:06.520: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:14:08.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01610024s
Mar 15 22:14:08.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:14:10.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015467324s
Mar 15 22:14:10.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:14:12.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015622337s
Mar 15 22:14:12.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:14:14.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01558816s
Mar 15 22:14:14.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:14:16.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016022703s
Mar 15 22:14:16.520: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:14:18.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015879574s
Mar 15 22:14:18.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:14:20.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.0157985s
Mar 15 22:14:20.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:14:22.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015754208s
Mar 15 22:14:22.519: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 15 22:14:22.519: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 15 22:14:22.523: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3654" to be "running and ready"
Mar 15 22:14:22.526: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.359588ms
Mar 15 22:14:22.526: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 15 22:14:22.526: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 15 22:14:22.529: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3654" to be "running and ready"
Mar 15 22:14:22.531: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.33377ms
Mar 15 22:14:22.531: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 15 22:14:22.531: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/15/23 22:14:22.535
Mar 15 22:14:22.551: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3654" to be "running"
Mar 15 22:14:22.555: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065109ms
Mar 15 22:14:24.558: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007513184s
Mar 15 22:14:24.558: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 15 22:14:24.561: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3654" to be "running"
Mar 15 22:14:24.564: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.817743ms
Mar 15 22:14:24.564: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 15 22:14:24.566: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 15 22:14:24.566: INFO: Going to poll 100.96.2.134 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 15 22:14:24.568: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.2.134 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3654 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:14:24.568: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:14:24.569: INFO: ExecWithOptions: Clientset creation
Mar 15 22:14:24.569: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-3654/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.96.2.134+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 15 22:14:25.662: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 15 22:14:25.662: INFO: Going to poll 100.96.1.177 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 15 22:14:25.665: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.177 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3654 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:14:25.665: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:14:25.666: INFO: ExecWithOptions: Clientset creation
Mar 15 22:14:25.666: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-3654/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.96.1.177+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 15 22:14:26.756: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 15 22:14:26.756: INFO: Going to poll 100.96.3.4 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 15 22:14:26.759: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.3.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3654 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:14:26.759: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:14:26.760: INFO: ExecWithOptions: Clientset creation
Mar 15 22:14:26.760: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-3654/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.96.3.4+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 15 22:14:27.883: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 15 22:14:27.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3654" for this suite. 03/15/23 22:14:27.888
------------------------------
• [SLOW TEST] [27.518 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:14:00.375
    Mar 15 22:14:00.375: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pod-network-test 03/15/23 22:14:00.376
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:14:00.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:14:00.417
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-3654 03/15/23 22:14:00.422
    STEP: creating a selector 03/15/23 22:14:00.422
    STEP: Creating the service pods in kubernetes 03/15/23 22:14:00.422
    Mar 15 22:14:00.422: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 15 22:14:00.503: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3654" to be "running and ready"
    Mar 15 22:14:00.515: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.688042ms
    Mar 15 22:14:00.515: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:14:02.518: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015097188s
    Mar 15 22:14:02.518: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:14:04.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015868425s
    Mar 15 22:14:04.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:14:06.520: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017060636s
    Mar 15 22:14:06.520: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:14:08.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01610024s
    Mar 15 22:14:08.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:14:10.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015467324s
    Mar 15 22:14:10.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:14:12.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015622337s
    Mar 15 22:14:12.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:14:14.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01558816s
    Mar 15 22:14:14.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:14:16.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016022703s
    Mar 15 22:14:16.520: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:14:18.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015879574s
    Mar 15 22:14:18.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:14:20.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.0157985s
    Mar 15 22:14:20.519: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:14:22.519: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015754208s
    Mar 15 22:14:22.519: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 15 22:14:22.519: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 15 22:14:22.523: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3654" to be "running and ready"
    Mar 15 22:14:22.526: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.359588ms
    Mar 15 22:14:22.526: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 15 22:14:22.526: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 15 22:14:22.529: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3654" to be "running and ready"
    Mar 15 22:14:22.531: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.33377ms
    Mar 15 22:14:22.531: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 15 22:14:22.531: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/15/23 22:14:22.535
    Mar 15 22:14:22.551: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3654" to be "running"
    Mar 15 22:14:22.555: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065109ms
    Mar 15 22:14:24.558: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007513184s
    Mar 15 22:14:24.558: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 15 22:14:24.561: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3654" to be "running"
    Mar 15 22:14:24.564: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.817743ms
    Mar 15 22:14:24.564: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 15 22:14:24.566: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 15 22:14:24.566: INFO: Going to poll 100.96.2.134 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 15 22:14:24.568: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.2.134 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3654 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:14:24.568: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:14:24.569: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:14:24.569: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-3654/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.96.2.134+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 15 22:14:25.662: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 15 22:14:25.662: INFO: Going to poll 100.96.1.177 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 15 22:14:25.665: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.177 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3654 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:14:25.665: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:14:25.666: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:14:25.666: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-3654/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.96.1.177+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 15 22:14:26.756: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 15 22:14:26.756: INFO: Going to poll 100.96.3.4 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 15 22:14:26.759: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.3.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3654 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:14:26.759: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:14:26.760: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:14:26.760: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-3654/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.96.3.4+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 15 22:14:27.883: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:14:27.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3654" for this suite. 03/15/23 22:14:27.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:14:27.893
Mar 15 22:14:27.893: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 22:14:27.894
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:14:27.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:14:27.913
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 22:14:27.927
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:14:28.612
STEP: Deploying the webhook pod 03/15/23 22:14:28.618
STEP: Wait for the deployment to be ready 03/15/23 22:14:28.63
Mar 15 22:14:28.638: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 22:14:30.647
STEP: Verifying the service has paired with the endpoint 03/15/23 22:14:30.664
Mar 15 22:14:31.665: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 03/15/23 22:14:31.668
STEP: create a pod 03/15/23 22:14:31.699
Mar 15 22:14:31.704: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1037" to be "running"
Mar 15 22:14:31.708: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115337ms
Mar 15 22:14:33.711: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007261237s
Mar 15 22:14:33.711: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 03/15/23 22:14:33.711
Mar 15 22:14:33.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=webhook-1037 attach --namespace=webhook-1037 to-be-attached-pod -i -c=container1'
Mar 15 22:14:33.790: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:14:33.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1037" for this suite. 03/15/23 22:14:33.855
STEP: Destroying namespace "webhook-1037-markers" for this suite. 03/15/23 22:14:33.871
------------------------------
• [SLOW TEST] [5.991 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:14:27.893
    Mar 15 22:14:27.893: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 22:14:27.894
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:14:27.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:14:27.913
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 22:14:27.927
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:14:28.612
    STEP: Deploying the webhook pod 03/15/23 22:14:28.618
    STEP: Wait for the deployment to be ready 03/15/23 22:14:28.63
    Mar 15 22:14:28.638: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 22:14:30.647
    STEP: Verifying the service has paired with the endpoint 03/15/23 22:14:30.664
    Mar 15 22:14:31.665: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 03/15/23 22:14:31.668
    STEP: create a pod 03/15/23 22:14:31.699
    Mar 15 22:14:31.704: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1037" to be "running"
    Mar 15 22:14:31.708: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115337ms
    Mar 15 22:14:33.711: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007261237s
    Mar 15 22:14:33.711: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 03/15/23 22:14:33.711
    Mar 15 22:14:33.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=webhook-1037 attach --namespace=webhook-1037 to-be-attached-pod -i -c=container1'
    Mar 15 22:14:33.790: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:14:33.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1037" for this suite. 03/15/23 22:14:33.855
    STEP: Destroying namespace "webhook-1037-markers" for this suite. 03/15/23 22:14:33.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:14:33.886
Mar 15 22:14:33.886: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-runtime 03/15/23 22:14:33.887
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:14:33.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:14:33.91
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 03/15/23 22:14:33.914
STEP: wait for the container to reach Succeeded 03/15/23 22:14:33.922
STEP: get the container status 03/15/23 22:14:37.945
STEP: the container should be terminated 03/15/23 22:14:37.951
STEP: the termination message should be set 03/15/23 22:14:37.951
Mar 15 22:14:37.951: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 03/15/23 22:14:37.951
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 15 22:14:37.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2666" for this suite. 03/15/23 22:14:37.98
------------------------------
• [4.108 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:14:33.886
    Mar 15 22:14:33.886: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-runtime 03/15/23 22:14:33.887
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:14:33.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:14:33.91
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 03/15/23 22:14:33.914
    STEP: wait for the container to reach Succeeded 03/15/23 22:14:33.922
    STEP: get the container status 03/15/23 22:14:37.945
    STEP: the container should be terminated 03/15/23 22:14:37.951
    STEP: the termination message should be set 03/15/23 22:14:37.951
    Mar 15 22:14:37.951: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 03/15/23 22:14:37.951
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:14:37.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2666" for this suite. 03/15/23 22:14:37.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:14:37.996
Mar 15 22:14:37.996: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 22:14:37.997
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:14:38.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:14:38.044
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 03/15/23 22:14:38.049
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 22:14:38.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-554" for this suite. 03/15/23 22:14:38.055
------------------------------
• [0.065 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:14:37.996
    Mar 15 22:14:37.996: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 22:14:37.997
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:14:38.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:14:38.044
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 03/15/23 22:14:38.049
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:14:38.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-554" for this suite. 03/15/23 22:14:38.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:14:38.062
Mar 15 22:14:38.062: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename statefulset 03/15/23 22:14:38.063
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:14:38.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:14:38.101
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3037 03/15/23 22:14:38.107
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 03/15/23 22:14:38.113
STEP: Creating stateful set ss in namespace statefulset-3037 03/15/23 22:14:38.132
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3037 03/15/23 22:14:38.15
Mar 15 22:14:38.154: INFO: Found 0 stateful pods, waiting for 1
Mar 15 22:14:48.158: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/15/23 22:14:48.158
Mar 15 22:14:48.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 22:14:48.359: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 22:14:48.359: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 22:14:48.360: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 22:14:48.363: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 15 22:14:58.367: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 22:14:58.367: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 22:14:58.383: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999639s
Mar 15 22:14:59.386: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995404239s
Mar 15 22:15:00.390: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992283878s
Mar 15 22:15:01.394: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988033344s
Mar 15 22:15:02.398: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984324367s
Mar 15 22:15:03.401: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.980603664s
Mar 15 22:15:04.407: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976089801s
Mar 15 22:15:05.410: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.971601139s
Mar 15 22:15:06.414: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.967959277s
Mar 15 22:15:07.417: INFO: Verifying statefulset ss doesn't scale past 1 for another 964.79525ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3037 03/15/23 22:15:08.417
Mar 15 22:15:08.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 22:15:08.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 22:15:08.609: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 22:15:08.609: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 22:15:08.612: INFO: Found 1 stateful pods, waiting for 3
Mar 15 22:15:18.617: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 22:15:18.617: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 22:15:18.617: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 03/15/23 22:15:18.617
STEP: Scale down will halt with unhealthy stateful pod 03/15/23 22:15:18.617
Mar 15 22:15:18.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 22:15:18.832: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 22:15:18.832: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 22:15:18.832: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 22:15:18.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 22:15:19.025: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 22:15:19.025: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 22:15:19.025: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 22:15:19.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 22:15:19.263: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 22:15:19.263: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 22:15:19.263: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 22:15:19.263: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 22:15:19.266: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 15 22:15:29.275: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 22:15:29.275: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 22:15:29.275: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 22:15:29.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999788s
Mar 15 22:15:30.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995741544s
Mar 15 22:15:31.294: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992157907s
Mar 15 22:15:32.298: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988655587s
Mar 15 22:15:33.302: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984148447s
Mar 15 22:15:34.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980793283s
Mar 15 22:15:35.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976845709s
Mar 15 22:15:36.314: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972427656s
Mar 15 22:15:37.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96839107s
Mar 15 22:15:38.321: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.188992ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3037 03/15/23 22:15:39.322
Mar 15 22:15:39.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 22:15:39.565: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 22:15:39.565: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 22:15:39.565: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 22:15:39.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 22:15:39.838: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 22:15:39.838: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 22:15:39.838: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 22:15:39.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 22:15:40.067: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 22:15:40.067: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 22:15:40.067: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 22:15:40.067: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 03/15/23 22:15:50.091
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 15 22:15:50.091: INFO: Deleting all statefulset in ns statefulset-3037
Mar 15 22:15:50.095: INFO: Scaling statefulset ss to 0
Mar 15 22:15:50.110: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 22:15:50.113: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:15:50.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3037" for this suite. 03/15/23 22:15:50.138
------------------------------
• [SLOW TEST] [72.083 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:14:38.062
    Mar 15 22:14:38.062: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename statefulset 03/15/23 22:14:38.063
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:14:38.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:14:38.101
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3037 03/15/23 22:14:38.107
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 03/15/23 22:14:38.113
    STEP: Creating stateful set ss in namespace statefulset-3037 03/15/23 22:14:38.132
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3037 03/15/23 22:14:38.15
    Mar 15 22:14:38.154: INFO: Found 0 stateful pods, waiting for 1
    Mar 15 22:14:48.158: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/15/23 22:14:48.158
    Mar 15 22:14:48.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 15 22:14:48.359: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 15 22:14:48.359: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 15 22:14:48.360: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 15 22:14:48.363: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 15 22:14:58.367: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 15 22:14:58.367: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 15 22:14:58.383: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999639s
    Mar 15 22:14:59.386: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995404239s
    Mar 15 22:15:00.390: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992283878s
    Mar 15 22:15:01.394: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988033344s
    Mar 15 22:15:02.398: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984324367s
    Mar 15 22:15:03.401: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.980603664s
    Mar 15 22:15:04.407: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976089801s
    Mar 15 22:15:05.410: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.971601139s
    Mar 15 22:15:06.414: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.967959277s
    Mar 15 22:15:07.417: INFO: Verifying statefulset ss doesn't scale past 1 for another 964.79525ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3037 03/15/23 22:15:08.417
    Mar 15 22:15:08.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 15 22:15:08.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 15 22:15:08.609: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 15 22:15:08.609: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 15 22:15:08.612: INFO: Found 1 stateful pods, waiting for 3
    Mar 15 22:15:18.617: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 15 22:15:18.617: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 15 22:15:18.617: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 03/15/23 22:15:18.617
    STEP: Scale down will halt with unhealthy stateful pod 03/15/23 22:15:18.617
    Mar 15 22:15:18.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 15 22:15:18.832: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 15 22:15:18.832: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 15 22:15:18.832: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 15 22:15:18.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 15 22:15:19.025: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 15 22:15:19.025: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 15 22:15:19.025: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 15 22:15:19.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 15 22:15:19.263: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 15 22:15:19.263: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 15 22:15:19.263: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 15 22:15:19.263: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 15 22:15:19.266: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Mar 15 22:15:29.275: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 15 22:15:29.275: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 15 22:15:29.275: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 15 22:15:29.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999788s
    Mar 15 22:15:30.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995741544s
    Mar 15 22:15:31.294: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992157907s
    Mar 15 22:15:32.298: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988655587s
    Mar 15 22:15:33.302: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984148447s
    Mar 15 22:15:34.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980793283s
    Mar 15 22:15:35.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976845709s
    Mar 15 22:15:36.314: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972427656s
    Mar 15 22:15:37.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96839107s
    Mar 15 22:15:38.321: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.188992ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3037 03/15/23 22:15:39.322
    Mar 15 22:15:39.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 15 22:15:39.565: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 15 22:15:39.565: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 15 22:15:39.565: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 15 22:15:39.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 15 22:15:39.838: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 15 22:15:39.838: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 15 22:15:39.838: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 15 22:15:39.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-3037 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 15 22:15:40.067: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 15 22:15:40.067: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 15 22:15:40.067: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 15 22:15:40.067: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 03/15/23 22:15:50.091
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 15 22:15:50.091: INFO: Deleting all statefulset in ns statefulset-3037
    Mar 15 22:15:50.095: INFO: Scaling statefulset ss to 0
    Mar 15 22:15:50.110: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 15 22:15:50.113: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:15:50.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3037" for this suite. 03/15/23 22:15:50.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:15:50.148
Mar 15 22:15:50.148: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:15:50.149
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:15:50.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:15:50.163
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-38ddc50d-db6d-4dde-b205-f8174760a192 03/15/23 22:15:50.166
STEP: Creating a pod to test consume configMaps 03/15/23 22:15:50.17
Mar 15 22:15:50.180: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d" in namespace "projected-4229" to be "Succeeded or Failed"
Mar 15 22:15:50.183: INFO: Pod "pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.837489ms
Mar 15 22:15:52.188: INFO: Pod "pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007529015s
Mar 15 22:15:54.188: INFO: Pod "pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007789561s
STEP: Saw pod success 03/15/23 22:15:54.188
Mar 15 22:15:54.188: INFO: Pod "pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d" satisfied condition "Succeeded or Failed"
Mar 15 22:15:54.191: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d container projected-configmap-volume-test: <nil>
STEP: delete the pod 03/15/23 22:15:54.203
Mar 15 22:15:54.216: INFO: Waiting for pod pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d to disappear
Mar 15 22:15:54.219: INFO: Pod pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:15:54.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4229" for this suite. 03/15/23 22:15:54.224
------------------------------
• [4.081 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:15:50.148
    Mar 15 22:15:50.148: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:15:50.149
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:15:50.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:15:50.163
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-38ddc50d-db6d-4dde-b205-f8174760a192 03/15/23 22:15:50.166
    STEP: Creating a pod to test consume configMaps 03/15/23 22:15:50.17
    Mar 15 22:15:50.180: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d" in namespace "projected-4229" to be "Succeeded or Failed"
    Mar 15 22:15:50.183: INFO: Pod "pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.837489ms
    Mar 15 22:15:52.188: INFO: Pod "pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007529015s
    Mar 15 22:15:54.188: INFO: Pod "pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007789561s
    STEP: Saw pod success 03/15/23 22:15:54.188
    Mar 15 22:15:54.188: INFO: Pod "pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d" satisfied condition "Succeeded or Failed"
    Mar 15 22:15:54.191: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d container projected-configmap-volume-test: <nil>
    STEP: delete the pod 03/15/23 22:15:54.203
    Mar 15 22:15:54.216: INFO: Waiting for pod pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d to disappear
    Mar 15 22:15:54.219: INFO: Pod pod-projected-configmaps-c8b27c07-049e-418d-8095-c59ae1861e5d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:15:54.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4229" for this suite. 03/15/23 22:15:54.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:15:54.231
Mar 15 22:15:54.231: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename subpath 03/15/23 22:15:54.232
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:15:54.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:15:54.258
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/15/23 22:15:54.265
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-5s82 03/15/23 22:15:54.279
STEP: Creating a pod to test atomic-volume-subpath 03/15/23 22:15:54.279
Mar 15 22:15:54.290: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5s82" in namespace "subpath-6469" to be "Succeeded or Failed"
Mar 15 22:15:54.294: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.663159ms
Mar 15 22:15:56.298: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 2.007942615s
Mar 15 22:15:58.299: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 4.009726512s
Mar 15 22:16:00.302: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 6.012366656s
Mar 15 22:16:02.298: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 8.008070432s
Mar 15 22:16:04.306: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 10.016417649s
Mar 15 22:16:06.300: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 12.010595565s
Mar 15 22:16:08.298: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 14.008693098s
Mar 15 22:16:10.301: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 16.011172433s
Mar 15 22:16:12.298: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 18.008175241s
Mar 15 22:16:14.298: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 20.008001812s
Mar 15 22:16:16.299: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=false. Elapsed: 22.009888743s
Mar 15 22:16:18.299: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009042524s
STEP: Saw pod success 03/15/23 22:16:18.299
Mar 15 22:16:18.299: INFO: Pod "pod-subpath-test-configmap-5s82" satisfied condition "Succeeded or Failed"
Mar 15 22:16:18.302: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-subpath-test-configmap-5s82 container test-container-subpath-configmap-5s82: <nil>
STEP: delete the pod 03/15/23 22:16:18.309
Mar 15 22:16:18.320: INFO: Waiting for pod pod-subpath-test-configmap-5s82 to disappear
Mar 15 22:16:18.324: INFO: Pod pod-subpath-test-configmap-5s82 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5s82 03/15/23 22:16:18.324
Mar 15 22:16:18.325: INFO: Deleting pod "pod-subpath-test-configmap-5s82" in namespace "subpath-6469"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 15 22:16:18.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6469" for this suite. 03/15/23 22:16:18.349
------------------------------
• [SLOW TEST] [24.133 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:15:54.231
    Mar 15 22:15:54.231: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename subpath 03/15/23 22:15:54.232
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:15:54.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:15:54.258
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/15/23 22:15:54.265
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-5s82 03/15/23 22:15:54.279
    STEP: Creating a pod to test atomic-volume-subpath 03/15/23 22:15:54.279
    Mar 15 22:15:54.290: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5s82" in namespace "subpath-6469" to be "Succeeded or Failed"
    Mar 15 22:15:54.294: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.663159ms
    Mar 15 22:15:56.298: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 2.007942615s
    Mar 15 22:15:58.299: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 4.009726512s
    Mar 15 22:16:00.302: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 6.012366656s
    Mar 15 22:16:02.298: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 8.008070432s
    Mar 15 22:16:04.306: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 10.016417649s
    Mar 15 22:16:06.300: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 12.010595565s
    Mar 15 22:16:08.298: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 14.008693098s
    Mar 15 22:16:10.301: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 16.011172433s
    Mar 15 22:16:12.298: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 18.008175241s
    Mar 15 22:16:14.298: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=true. Elapsed: 20.008001812s
    Mar 15 22:16:16.299: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Running", Reason="", readiness=false. Elapsed: 22.009888743s
    Mar 15 22:16:18.299: INFO: Pod "pod-subpath-test-configmap-5s82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009042524s
    STEP: Saw pod success 03/15/23 22:16:18.299
    Mar 15 22:16:18.299: INFO: Pod "pod-subpath-test-configmap-5s82" satisfied condition "Succeeded or Failed"
    Mar 15 22:16:18.302: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-subpath-test-configmap-5s82 container test-container-subpath-configmap-5s82: <nil>
    STEP: delete the pod 03/15/23 22:16:18.309
    Mar 15 22:16:18.320: INFO: Waiting for pod pod-subpath-test-configmap-5s82 to disappear
    Mar 15 22:16:18.324: INFO: Pod pod-subpath-test-configmap-5s82 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-5s82 03/15/23 22:16:18.324
    Mar 15 22:16:18.325: INFO: Deleting pod "pod-subpath-test-configmap-5s82" in namespace "subpath-6469"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:16:18.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6469" for this suite. 03/15/23 22:16:18.349
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:16:18.364
Mar 15 22:16:18.364: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 22:16:18.365
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:18.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:18.399
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/15/23 22:16:18.403
Mar 15 22:16:18.413: INFO: Waiting up to 5m0s for pod "pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c" in namespace "emptydir-2328" to be "Succeeded or Failed"
Mar 15 22:16:18.426: INFO: Pod "pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.839953ms
Mar 15 22:16:20.431: INFO: Pod "pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017193318s
Mar 15 22:16:22.430: INFO: Pod "pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016655557s
STEP: Saw pod success 03/15/23 22:16:22.43
Mar 15 22:16:22.430: INFO: Pod "pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c" satisfied condition "Succeeded or Failed"
Mar 15 22:16:22.433: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c container test-container: <nil>
STEP: delete the pod 03/15/23 22:16:22.444
Mar 15 22:16:22.456: INFO: Waiting for pod pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c to disappear
Mar 15 22:16:22.458: INFO: Pod pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:16:22.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2328" for this suite. 03/15/23 22:16:22.465
------------------------------
• [4.108 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:16:18.364
    Mar 15 22:16:18.364: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 22:16:18.365
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:18.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:18.399
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/15/23 22:16:18.403
    Mar 15 22:16:18.413: INFO: Waiting up to 5m0s for pod "pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c" in namespace "emptydir-2328" to be "Succeeded or Failed"
    Mar 15 22:16:18.426: INFO: Pod "pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.839953ms
    Mar 15 22:16:20.431: INFO: Pod "pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017193318s
    Mar 15 22:16:22.430: INFO: Pod "pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016655557s
    STEP: Saw pod success 03/15/23 22:16:22.43
    Mar 15 22:16:22.430: INFO: Pod "pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c" satisfied condition "Succeeded or Failed"
    Mar 15 22:16:22.433: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c container test-container: <nil>
    STEP: delete the pod 03/15/23 22:16:22.444
    Mar 15 22:16:22.456: INFO: Waiting for pod pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c to disappear
    Mar 15 22:16:22.458: INFO: Pod pod-4ada6e68-444a-431d-a570-4d9dc3a8da3c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:16:22.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2328" for this suite. 03/15/23 22:16:22.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:16:22.474
Mar 15 22:16:22.474: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 22:16:22.475
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:22.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:22.495
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-d97ed73b-5f96-4995-a9d2-b88ea618d5c3 03/15/23 22:16:22.499
STEP: Creating a pod to test consume secrets 03/15/23 22:16:22.503
Mar 15 22:16:22.514: INFO: Waiting up to 5m0s for pod "pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c" in namespace "secrets-4342" to be "Succeeded or Failed"
Mar 15 22:16:22.519: INFO: Pod "pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.393357ms
Mar 15 22:16:24.523: INFO: Pod "pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008781612s
Mar 15 22:16:26.523: INFO: Pod "pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009017427s
STEP: Saw pod success 03/15/23 22:16:26.523
Mar 15 22:16:26.523: INFO: Pod "pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c" satisfied condition "Succeeded or Failed"
Mar 15 22:16:26.527: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c container secret-env-test: <nil>
STEP: delete the pod 03/15/23 22:16:26.539
Mar 15 22:16:26.563: INFO: Waiting for pod pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c to disappear
Mar 15 22:16:26.570: INFO: Pod pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 22:16:26.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4342" for this suite. 03/15/23 22:16:26.585
------------------------------
• [4.126 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:16:22.474
    Mar 15 22:16:22.474: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 22:16:22.475
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:22.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:22.495
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-d97ed73b-5f96-4995-a9d2-b88ea618d5c3 03/15/23 22:16:22.499
    STEP: Creating a pod to test consume secrets 03/15/23 22:16:22.503
    Mar 15 22:16:22.514: INFO: Waiting up to 5m0s for pod "pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c" in namespace "secrets-4342" to be "Succeeded or Failed"
    Mar 15 22:16:22.519: INFO: Pod "pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.393357ms
    Mar 15 22:16:24.523: INFO: Pod "pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008781612s
    Mar 15 22:16:26.523: INFO: Pod "pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009017427s
    STEP: Saw pod success 03/15/23 22:16:26.523
    Mar 15 22:16:26.523: INFO: Pod "pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c" satisfied condition "Succeeded or Failed"
    Mar 15 22:16:26.527: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c container secret-env-test: <nil>
    STEP: delete the pod 03/15/23 22:16:26.539
    Mar 15 22:16:26.563: INFO: Waiting for pod pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c to disappear
    Mar 15 22:16:26.570: INFO: Pod pod-secrets-1b36027b-6c09-4ae2-9b10-87455704b82c no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:16:26.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4342" for this suite. 03/15/23 22:16:26.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:16:26.603
Mar 15 22:16:26.603: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename daemonsets 03/15/23 22:16:26.604
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:26.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:26.62
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Mar 15 22:16:26.646: INFO: Create a RollingUpdate DaemonSet
Mar 15 22:16:26.653: INFO: Check that daemon pods launch on every node of the cluster
Mar 15 22:16:26.659: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:16:26.661: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:16:26.661: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:16:27.665: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:16:27.668: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:16:27.668: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:16:28.665: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:16:28.668: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 15 22:16:28.668: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Mar 15 22:16:28.668: INFO: Update the DaemonSet to trigger a rollout
Mar 15 22:16:28.675: INFO: Updating DaemonSet daemon-set
Mar 15 22:16:31.688: INFO: Roll back the DaemonSet before rollout is complete
Mar 15 22:16:31.698: INFO: Updating DaemonSet daemon-set
Mar 15 22:16:31.698: INFO: Make sure DaemonSet rollback is complete
Mar 15 22:16:31.705: INFO: Wrong image for pod: daemon-set-gpkmb. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Mar 15 22:16:31.705: INFO: Pod daemon-set-gpkmb is not available
Mar 15 22:16:31.710: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:16:32.718: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:16:33.717: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:16:34.713: INFO: Pod daemon-set-7d9st is not available
Mar 15 22:16:34.717: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/15/23 22:16:34.724
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6185, will wait for the garbage collector to delete the pods 03/15/23 22:16:34.724
Mar 15 22:16:34.785: INFO: Deleting DaemonSet.extensions daemon-set took: 8.183664ms
Mar 15 22:16:34.886: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.891558ms
Mar 15 22:16:37.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:16:37.889: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 15 22:16:37.892: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15410"},"items":null}

Mar 15 22:16:37.894: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15410"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:16:37.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6185" for this suite. 03/15/23 22:16:37.91
------------------------------
• [SLOW TEST] [11.313 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:16:26.603
    Mar 15 22:16:26.603: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename daemonsets 03/15/23 22:16:26.604
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:26.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:26.62
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Mar 15 22:16:26.646: INFO: Create a RollingUpdate DaemonSet
    Mar 15 22:16:26.653: INFO: Check that daemon pods launch on every node of the cluster
    Mar 15 22:16:26.659: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:16:26.661: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:16:26.661: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:16:27.665: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:16:27.668: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:16:27.668: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:16:28.665: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:16:28.668: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 15 22:16:28.668: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Mar 15 22:16:28.668: INFO: Update the DaemonSet to trigger a rollout
    Mar 15 22:16:28.675: INFO: Updating DaemonSet daemon-set
    Mar 15 22:16:31.688: INFO: Roll back the DaemonSet before rollout is complete
    Mar 15 22:16:31.698: INFO: Updating DaemonSet daemon-set
    Mar 15 22:16:31.698: INFO: Make sure DaemonSet rollback is complete
    Mar 15 22:16:31.705: INFO: Wrong image for pod: daemon-set-gpkmb. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Mar 15 22:16:31.705: INFO: Pod daemon-set-gpkmb is not available
    Mar 15 22:16:31.710: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:16:32.718: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:16:33.717: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:16:34.713: INFO: Pod daemon-set-7d9st is not available
    Mar 15 22:16:34.717: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/15/23 22:16:34.724
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6185, will wait for the garbage collector to delete the pods 03/15/23 22:16:34.724
    Mar 15 22:16:34.785: INFO: Deleting DaemonSet.extensions daemon-set took: 8.183664ms
    Mar 15 22:16:34.886: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.891558ms
    Mar 15 22:16:37.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:16:37.889: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 15 22:16:37.892: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15410"},"items":null}

    Mar 15 22:16:37.894: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15410"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:16:37.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6185" for this suite. 03/15/23 22:16:37.91
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:16:37.917
Mar 15 22:16:37.918: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubelet-test 03/15/23 22:16:37.918
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:37.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:37.937
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Mar 15 22:16:37.949: INFO: Waiting up to 5m0s for pod "busybox-scheduling-c1d9015b-7a30-4e95-98c2-bbe5855ae49c" in namespace "kubelet-test-1357" to be "running and ready"
Mar 15 22:16:37.954: INFO: Pod "busybox-scheduling-c1d9015b-7a30-4e95-98c2-bbe5855ae49c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.5751ms
Mar 15 22:16:37.954: INFO: The phase of Pod busybox-scheduling-c1d9015b-7a30-4e95-98c2-bbe5855ae49c is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:16:39.957: INFO: Pod "busybox-scheduling-c1d9015b-7a30-4e95-98c2-bbe5855ae49c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007817519s
Mar 15 22:16:39.957: INFO: The phase of Pod busybox-scheduling-c1d9015b-7a30-4e95-98c2-bbe5855ae49c is Running (Ready = true)
Mar 15 22:16:39.957: INFO: Pod "busybox-scheduling-c1d9015b-7a30-4e95-98c2-bbe5855ae49c" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:16:39.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1357" for this suite. 03/15/23 22:16:39.979
------------------------------
• [2.069 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:16:37.917
    Mar 15 22:16:37.918: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubelet-test 03/15/23 22:16:37.918
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:37.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:37.937
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Mar 15 22:16:37.949: INFO: Waiting up to 5m0s for pod "busybox-scheduling-c1d9015b-7a30-4e95-98c2-bbe5855ae49c" in namespace "kubelet-test-1357" to be "running and ready"
    Mar 15 22:16:37.954: INFO: Pod "busybox-scheduling-c1d9015b-7a30-4e95-98c2-bbe5855ae49c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.5751ms
    Mar 15 22:16:37.954: INFO: The phase of Pod busybox-scheduling-c1d9015b-7a30-4e95-98c2-bbe5855ae49c is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:16:39.957: INFO: Pod "busybox-scheduling-c1d9015b-7a30-4e95-98c2-bbe5855ae49c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007817519s
    Mar 15 22:16:39.957: INFO: The phase of Pod busybox-scheduling-c1d9015b-7a30-4e95-98c2-bbe5855ae49c is Running (Ready = true)
    Mar 15 22:16:39.957: INFO: Pod "busybox-scheduling-c1d9015b-7a30-4e95-98c2-bbe5855ae49c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:16:39.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1357" for this suite. 03/15/23 22:16:39.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:16:39.988
Mar 15 22:16:39.988: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename disruption 03/15/23 22:16:39.989
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:40.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:40.01
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 03/15/23 22:16:40.031
STEP: Updating PodDisruptionBudget status 03/15/23 22:16:42.039
STEP: Waiting for all pods to be running 03/15/23 22:16:42.047
Mar 15 22:16:42.053: INFO: running pods: 0 < 1
STEP: locating a running pod 03/15/23 22:16:44.057
STEP: Waiting for the pdb to be processed 03/15/23 22:16:44.069
STEP: Patching PodDisruptionBudget status 03/15/23 22:16:44.076
STEP: Waiting for the pdb to be processed 03/15/23 22:16:44.084
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 15 22:16:44.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2341" for this suite. 03/15/23 22:16:44.091
------------------------------
• [4.110 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:16:39.988
    Mar 15 22:16:39.988: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename disruption 03/15/23 22:16:39.989
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:40.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:40.01
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 03/15/23 22:16:40.031
    STEP: Updating PodDisruptionBudget status 03/15/23 22:16:42.039
    STEP: Waiting for all pods to be running 03/15/23 22:16:42.047
    Mar 15 22:16:42.053: INFO: running pods: 0 < 1
    STEP: locating a running pod 03/15/23 22:16:44.057
    STEP: Waiting for the pdb to be processed 03/15/23 22:16:44.069
    STEP: Patching PodDisruptionBudget status 03/15/23 22:16:44.076
    STEP: Waiting for the pdb to be processed 03/15/23 22:16:44.084
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:16:44.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2341" for this suite. 03/15/23 22:16:44.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:16:44.099
Mar 15 22:16:44.099: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename disruption 03/15/23 22:16:44.101
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:44.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:44.117
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 03/15/23 22:16:44.127
STEP: Waiting for all pods to be running 03/15/23 22:16:44.21
Mar 15 22:16:44.228: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 15 22:16:46.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6983" for this suite. 03/15/23 22:16:46.238
------------------------------
• [2.145 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:16:44.099
    Mar 15 22:16:44.099: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename disruption 03/15/23 22:16:44.101
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:44.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:44.117
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 03/15/23 22:16:44.127
    STEP: Waiting for all pods to be running 03/15/23 22:16:44.21
    Mar 15 22:16:44.228: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:16:46.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6983" for this suite. 03/15/23 22:16:46.238
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:16:46.245
Mar 15 22:16:46.246: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pods 03/15/23 22:16:46.247
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:46.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:46.268
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 03/15/23 22:16:46.272
Mar 15 22:16:46.281: INFO: Waiting up to 5m0s for pod "pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721" in namespace "pods-8394" to be "running and ready"
Mar 15 22:16:46.286: INFO: Pod "pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721": Phase="Pending", Reason="", readiness=false. Elapsed: 5.217715ms
Mar 15 22:16:46.286: INFO: The phase of Pod pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:16:48.289: INFO: Pod "pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721": Phase="Running", Reason="", readiness=true. Elapsed: 2.008472584s
Mar 15 22:16:48.289: INFO: The phase of Pod pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721 is Running (Ready = true)
Mar 15 22:16:48.289: INFO: Pod "pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721" satisfied condition "running and ready"
Mar 15 22:16:48.294: INFO: Pod pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721 has hostIP: 172.20.126.23
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 15 22:16:48.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8394" for this suite. 03/15/23 22:16:48.298
------------------------------
• [2.057 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:16:46.245
    Mar 15 22:16:46.246: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pods 03/15/23 22:16:46.247
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:46.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:46.268
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 03/15/23 22:16:46.272
    Mar 15 22:16:46.281: INFO: Waiting up to 5m0s for pod "pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721" in namespace "pods-8394" to be "running and ready"
    Mar 15 22:16:46.286: INFO: Pod "pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721": Phase="Pending", Reason="", readiness=false. Elapsed: 5.217715ms
    Mar 15 22:16:46.286: INFO: The phase of Pod pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:16:48.289: INFO: Pod "pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721": Phase="Running", Reason="", readiness=true. Elapsed: 2.008472584s
    Mar 15 22:16:48.289: INFO: The phase of Pod pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721 is Running (Ready = true)
    Mar 15 22:16:48.289: INFO: Pod "pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721" satisfied condition "running and ready"
    Mar 15 22:16:48.294: INFO: Pod pod-hostip-8f2e386b-0b6d-4aaa-8082-b48f1003e721 has hostIP: 172.20.126.23
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:16:48.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8394" for this suite. 03/15/23 22:16:48.298
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:16:48.305
Mar 15 22:16:48.305: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename deployment 03/15/23 22:16:48.306
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:48.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:48.322
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 03/15/23 22:16:48.329
Mar 15 22:16:48.329: INFO: Creating simple deployment test-deployment-5xqvf
Mar 15 22:16:48.347: INFO: deployment "test-deployment-5xqvf" doesn't have the required revision set
STEP: Getting /status 03/15/23 22:16:50.358
Mar 15 22:16:50.361: INFO: Deployment test-deployment-5xqvf has Conditions: [{Available True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xqvf-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 03/15/23 22:16:50.362
Mar 15 22:16:50.376: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 16, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 16, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 16, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 16, 48, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-5xqvf-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 03/15/23 22:16:50.376
Mar 15 22:16:50.379: INFO: Observed &Deployment event: ADDED
Mar 15 22:16:50.379: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5xqvf-54bc444df"}
Mar 15 22:16:50.379: INFO: Observed &Deployment event: MODIFIED
Mar 15 22:16:50.379: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5xqvf-54bc444df"}
Mar 15 22:16:50.379: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 15 22:16:50.379: INFO: Observed &Deployment event: MODIFIED
Mar 15 22:16:50.380: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 15 22:16:50.380: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5xqvf-54bc444df" is progressing.}
Mar 15 22:16:50.380: INFO: Observed &Deployment event: MODIFIED
Mar 15 22:16:50.380: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 15 22:16:50.380: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xqvf-54bc444df" has successfully progressed.}
Mar 15 22:16:50.380: INFO: Observed &Deployment event: MODIFIED
Mar 15 22:16:50.380: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 15 22:16:50.380: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xqvf-54bc444df" has successfully progressed.}
Mar 15 22:16:50.380: INFO: Found Deployment test-deployment-5xqvf in namespace deployment-5851 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 15 22:16:50.380: INFO: Deployment test-deployment-5xqvf has an updated status
STEP: patching the Statefulset Status 03/15/23 22:16:50.38
Mar 15 22:16:50.381: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 15 22:16:50.389: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 03/15/23 22:16:50.389
Mar 15 22:16:50.393: INFO: Observed &Deployment event: ADDED
Mar 15 22:16:50.393: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5xqvf-54bc444df"}
Mar 15 22:16:50.393: INFO: Observed &Deployment event: MODIFIED
Mar 15 22:16:50.393: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5xqvf-54bc444df"}
Mar 15 22:16:50.394: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 15 22:16:50.394: INFO: Observed &Deployment event: MODIFIED
Mar 15 22:16:50.394: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 15 22:16:50.394: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5xqvf-54bc444df" is progressing.}
Mar 15 22:16:50.395: INFO: Observed &Deployment event: MODIFIED
Mar 15 22:16:50.395: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 15 22:16:50.395: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xqvf-54bc444df" has successfully progressed.}
Mar 15 22:16:50.395: INFO: Observed &Deployment event: MODIFIED
Mar 15 22:16:50.395: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 15 22:16:50.395: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xqvf-54bc444df" has successfully progressed.}
Mar 15 22:16:50.395: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 15 22:16:50.395: INFO: Observed &Deployment event: MODIFIED
Mar 15 22:16:50.395: INFO: Found deployment test-deployment-5xqvf in namespace deployment-5851 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar 15 22:16:50.396: INFO: Deployment test-deployment-5xqvf has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 15 22:16:50.401: INFO: Deployment "test-deployment-5xqvf":
&Deployment{ObjectMeta:{test-deployment-5xqvf  deployment-5851  9184f288-9b47-4bb3-9792-0ee345ca37da 15561 1 2023-03-15 22:16:48 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-15 22:16:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-15 22:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-15 22:16:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059f2a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-5xqvf-54bc444df",LastUpdateTime:2023-03-15 22:16:50 +0000 UTC,LastTransitionTime:2023-03-15 22:16:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 15 22:16:50.406: INFO: New ReplicaSet "test-deployment-5xqvf-54bc444df" of Deployment "test-deployment-5xqvf":
&ReplicaSet{ObjectMeta:{test-deployment-5xqvf-54bc444df  deployment-5851  84a76336-8244-4539-ae7c-4bdb099392ad 15557 1 2023-03-15 22:16:48 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-5xqvf 9184f288-9b47-4bb3-9792-0ee345ca37da 0xc002989990 0xc002989991}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:16:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9184f288-9b47-4bb3-9792-0ee345ca37da\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:16:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002989a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 15 22:16:50.409: INFO: Pod "test-deployment-5xqvf-54bc444df-mzljr" is available:
&Pod{ObjectMeta:{test-deployment-5xqvf-54bc444df-mzljr test-deployment-5xqvf-54bc444df- deployment-5851  469f8d01-287f-46d9-98a2-5510ee2eb8fc 15556 0 2023-03-15 22:16:48 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-5xqvf-54bc444df 84a76336-8244-4539-ae7c-4bdb099392ad 0xc002989de0 0xc002989de1}] [] [{kube-controller-manager Update v1 2023-03-15 22:16:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84a76336-8244-4539-ae7c-4bdb099392ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:16:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qm65l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qm65l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:16:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.173,StartTime:2023-03-15 22:16:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:16:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cad6c4768255a3e90e916df46416e30b6ef3d3a94f13598555e6fed1b0971dfc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 15 22:16:50.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5851" for this suite. 03/15/23 22:16:50.413
------------------------------
• [2.121 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:16:48.305
    Mar 15 22:16:48.305: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename deployment 03/15/23 22:16:48.306
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:48.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:48.322
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 03/15/23 22:16:48.329
    Mar 15 22:16:48.329: INFO: Creating simple deployment test-deployment-5xqvf
    Mar 15 22:16:48.347: INFO: deployment "test-deployment-5xqvf" doesn't have the required revision set
    STEP: Getting /status 03/15/23 22:16:50.358
    Mar 15 22:16:50.361: INFO: Deployment test-deployment-5xqvf has Conditions: [{Available True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xqvf-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 03/15/23 22:16:50.362
    Mar 15 22:16:50.376: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 16, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 16, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 16, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 16, 48, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-5xqvf-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 03/15/23 22:16:50.376
    Mar 15 22:16:50.379: INFO: Observed &Deployment event: ADDED
    Mar 15 22:16:50.379: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5xqvf-54bc444df"}
    Mar 15 22:16:50.379: INFO: Observed &Deployment event: MODIFIED
    Mar 15 22:16:50.379: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5xqvf-54bc444df"}
    Mar 15 22:16:50.379: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 15 22:16:50.379: INFO: Observed &Deployment event: MODIFIED
    Mar 15 22:16:50.380: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 15 22:16:50.380: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5xqvf-54bc444df" is progressing.}
    Mar 15 22:16:50.380: INFO: Observed &Deployment event: MODIFIED
    Mar 15 22:16:50.380: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 15 22:16:50.380: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xqvf-54bc444df" has successfully progressed.}
    Mar 15 22:16:50.380: INFO: Observed &Deployment event: MODIFIED
    Mar 15 22:16:50.380: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 15 22:16:50.380: INFO: Observed Deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xqvf-54bc444df" has successfully progressed.}
    Mar 15 22:16:50.380: INFO: Found Deployment test-deployment-5xqvf in namespace deployment-5851 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 15 22:16:50.380: INFO: Deployment test-deployment-5xqvf has an updated status
    STEP: patching the Statefulset Status 03/15/23 22:16:50.38
    Mar 15 22:16:50.381: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 15 22:16:50.389: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 03/15/23 22:16:50.389
    Mar 15 22:16:50.393: INFO: Observed &Deployment event: ADDED
    Mar 15 22:16:50.393: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5xqvf-54bc444df"}
    Mar 15 22:16:50.393: INFO: Observed &Deployment event: MODIFIED
    Mar 15 22:16:50.393: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5xqvf-54bc444df"}
    Mar 15 22:16:50.394: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 15 22:16:50.394: INFO: Observed &Deployment event: MODIFIED
    Mar 15 22:16:50.394: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 15 22:16:50.394: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:48 +0000 UTC 2023-03-15 22:16:48 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5xqvf-54bc444df" is progressing.}
    Mar 15 22:16:50.395: INFO: Observed &Deployment event: MODIFIED
    Mar 15 22:16:50.395: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 15 22:16:50.395: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xqvf-54bc444df" has successfully progressed.}
    Mar 15 22:16:50.395: INFO: Observed &Deployment event: MODIFIED
    Mar 15 22:16:50.395: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 15 22:16:50.395: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-15 22:16:49 +0000 UTC 2023-03-15 22:16:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xqvf-54bc444df" has successfully progressed.}
    Mar 15 22:16:50.395: INFO: Observed deployment test-deployment-5xqvf in namespace deployment-5851 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 15 22:16:50.395: INFO: Observed &Deployment event: MODIFIED
    Mar 15 22:16:50.395: INFO: Found deployment test-deployment-5xqvf in namespace deployment-5851 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Mar 15 22:16:50.396: INFO: Deployment test-deployment-5xqvf has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 15 22:16:50.401: INFO: Deployment "test-deployment-5xqvf":
    &Deployment{ObjectMeta:{test-deployment-5xqvf  deployment-5851  9184f288-9b47-4bb3-9792-0ee345ca37da 15561 1 2023-03-15 22:16:48 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-15 22:16:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-15 22:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-15 22:16:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059f2a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-5xqvf-54bc444df",LastUpdateTime:2023-03-15 22:16:50 +0000 UTC,LastTransitionTime:2023-03-15 22:16:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 15 22:16:50.406: INFO: New ReplicaSet "test-deployment-5xqvf-54bc444df" of Deployment "test-deployment-5xqvf":
    &ReplicaSet{ObjectMeta:{test-deployment-5xqvf-54bc444df  deployment-5851  84a76336-8244-4539-ae7c-4bdb099392ad 15557 1 2023-03-15 22:16:48 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-5xqvf 9184f288-9b47-4bb3-9792-0ee345ca37da 0xc002989990 0xc002989991}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:16:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9184f288-9b47-4bb3-9792-0ee345ca37da\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:16:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002989a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 15 22:16:50.409: INFO: Pod "test-deployment-5xqvf-54bc444df-mzljr" is available:
    &Pod{ObjectMeta:{test-deployment-5xqvf-54bc444df-mzljr test-deployment-5xqvf-54bc444df- deployment-5851  469f8d01-287f-46d9-98a2-5510ee2eb8fc 15556 0 2023-03-15 22:16:48 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-5xqvf-54bc444df 84a76336-8244-4539-ae7c-4bdb099392ad 0xc002989de0 0xc002989de1}] [] [{kube-controller-manager Update v1 2023-03-15 22:16:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84a76336-8244-4539-ae7c-4bdb099392ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:16:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qm65l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qm65l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:16:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.173,StartTime:2023-03-15 22:16:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:16:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cad6c4768255a3e90e916df46416e30b6ef3d3a94f13598555e6fed1b0971dfc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:16:50.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5851" for this suite. 03/15/23 22:16:50.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:16:50.432
Mar 15 22:16:50.432: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename daemonsets 03/15/23 22:16:50.433
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:50.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:50.45
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 03/15/23 22:16:50.479
STEP: Check that daemon pods launch on every node of the cluster. 03/15/23 22:16:50.487
Mar 15 22:16:50.491: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:16:50.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:16:50.493: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:16:51.497: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:16:51.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:16:51.500: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:16:52.498: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:16:52.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 15 22:16:52.509: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/15/23 22:16:52.512
Mar 15 22:16:52.541: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:16:52.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 15 22:16:52.554: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:16:53.559: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:16:53.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 15 22:16:53.565: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:16:54.569: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:16:54.572: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 15 22:16:54.572: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 03/15/23 22:16:54.572
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/15/23 22:16:54.579
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6251, will wait for the garbage collector to delete the pods 03/15/23 22:16:54.579
Mar 15 22:16:54.644: INFO: Deleting DaemonSet.extensions daemon-set took: 8.91865ms
Mar 15 22:16:54.744: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.756063ms
Mar 15 22:16:57.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:16:57.152: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 15 22:16:57.156: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15687"},"items":null}

Mar 15 22:16:57.160: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15687"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:16:57.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6251" for this suite. 03/15/23 22:16:57.174
------------------------------
• [SLOW TEST] [6.749 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:16:50.432
    Mar 15 22:16:50.432: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename daemonsets 03/15/23 22:16:50.433
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:50.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:50.45
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 03/15/23 22:16:50.479
    STEP: Check that daemon pods launch on every node of the cluster. 03/15/23 22:16:50.487
    Mar 15 22:16:50.491: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:16:50.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:16:50.493: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:16:51.497: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:16:51.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:16:51.500: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:16:52.498: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:16:52.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 15 22:16:52.509: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/15/23 22:16:52.512
    Mar 15 22:16:52.541: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:16:52.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 15 22:16:52.554: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:16:53.559: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:16:53.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 15 22:16:53.565: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:16:54.569: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:16:54.572: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 15 22:16:54.572: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 03/15/23 22:16:54.572
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/15/23 22:16:54.579
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6251, will wait for the garbage collector to delete the pods 03/15/23 22:16:54.579
    Mar 15 22:16:54.644: INFO: Deleting DaemonSet.extensions daemon-set took: 8.91865ms
    Mar 15 22:16:54.744: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.756063ms
    Mar 15 22:16:57.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:16:57.152: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 15 22:16:57.156: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15687"},"items":null}

    Mar 15 22:16:57.160: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15687"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:16:57.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6251" for this suite. 03/15/23 22:16:57.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:16:57.184
Mar 15 22:16:57.184: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 22:16:57.185
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:57.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:57.199
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 03/15/23 22:16:57.202
Mar 15 22:16:57.210: INFO: Waiting up to 5m0s for pod "downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce" in namespace "downward-api-5684" to be "Succeeded or Failed"
Mar 15 22:16:57.213: INFO: Pod "downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.03419ms
Mar 15 22:16:59.217: INFO: Pod "downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006873557s
Mar 15 22:17:01.218: INFO: Pod "downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008045548s
STEP: Saw pod success 03/15/23 22:17:01.219
Mar 15 22:17:01.219: INFO: Pod "downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce" satisfied condition "Succeeded or Failed"
Mar 15 22:17:01.222: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce container dapi-container: <nil>
STEP: delete the pod 03/15/23 22:17:01.234
Mar 15 22:17:01.257: INFO: Waiting for pod downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce to disappear
Mar 15 22:17:01.263: INFO: Pod downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 15 22:17:01.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5684" for this suite. 03/15/23 22:17:01.277
------------------------------
• [4.111 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:16:57.184
    Mar 15 22:16:57.184: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 22:16:57.185
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:16:57.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:16:57.199
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 03/15/23 22:16:57.202
    Mar 15 22:16:57.210: INFO: Waiting up to 5m0s for pod "downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce" in namespace "downward-api-5684" to be "Succeeded or Failed"
    Mar 15 22:16:57.213: INFO: Pod "downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.03419ms
    Mar 15 22:16:59.217: INFO: Pod "downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006873557s
    Mar 15 22:17:01.218: INFO: Pod "downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008045548s
    STEP: Saw pod success 03/15/23 22:17:01.219
    Mar 15 22:17:01.219: INFO: Pod "downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce" satisfied condition "Succeeded or Failed"
    Mar 15 22:17:01.222: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce container dapi-container: <nil>
    STEP: delete the pod 03/15/23 22:17:01.234
    Mar 15 22:17:01.257: INFO: Waiting for pod downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce to disappear
    Mar 15 22:17:01.263: INFO: Pod downward-api-984bd698-5a4c-4b2e-985c-c4d3ed0f9fce no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:17:01.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5684" for this suite. 03/15/23 22:17:01.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:17:01.301
Mar 15 22:17:01.301: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename resourcequota 03/15/23 22:17:01.302
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:01.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:01.343
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 03/15/23 22:17:01.348
STEP: Creating a ResourceQuota 03/15/23 22:17:06.356
STEP: Ensuring resource quota status is calculated 03/15/23 22:17:06.362
STEP: Creating a Pod that fits quota 03/15/23 22:17:08.366
STEP: Ensuring ResourceQuota status captures the pod usage 03/15/23 22:17:08.384
STEP: Not allowing a pod to be created that exceeds remaining quota 03/15/23 22:17:10.388
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/15/23 22:17:10.391
STEP: Ensuring a pod cannot update its resource requirements 03/15/23 22:17:10.394
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/15/23 22:17:10.399
STEP: Deleting the pod 03/15/23 22:17:12.402
STEP: Ensuring resource quota status released the pod usage 03/15/23 22:17:12.419
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 15 22:17:14.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5223" for this suite. 03/15/23 22:17:14.431
------------------------------
• [SLOW TEST] [13.148 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:17:01.301
    Mar 15 22:17:01.301: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename resourcequota 03/15/23 22:17:01.302
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:01.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:01.343
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 03/15/23 22:17:01.348
    STEP: Creating a ResourceQuota 03/15/23 22:17:06.356
    STEP: Ensuring resource quota status is calculated 03/15/23 22:17:06.362
    STEP: Creating a Pod that fits quota 03/15/23 22:17:08.366
    STEP: Ensuring ResourceQuota status captures the pod usage 03/15/23 22:17:08.384
    STEP: Not allowing a pod to be created that exceeds remaining quota 03/15/23 22:17:10.388
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/15/23 22:17:10.391
    STEP: Ensuring a pod cannot update its resource requirements 03/15/23 22:17:10.394
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/15/23 22:17:10.399
    STEP: Deleting the pod 03/15/23 22:17:12.402
    STEP: Ensuring resource quota status released the pod usage 03/15/23 22:17:12.419
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:17:14.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5223" for this suite. 03/15/23 22:17:14.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:17:14.453
Mar 15 22:17:14.453: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 22:17:14.454
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:14.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:14.477
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 03/15/23 22:17:14.481
Mar 15 22:17:14.482: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: rename a version 03/15/23 22:17:18.643
STEP: check the new version name is served 03/15/23 22:17:18.658
STEP: check the old version name is removed 03/15/23 22:17:19.996
STEP: check the other version is not changed 03/15/23 22:17:20.707
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:17:23.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2190" for this suite. 03/15/23 22:17:23.816
------------------------------
• [SLOW TEST] [9.368 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:17:14.453
    Mar 15 22:17:14.453: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 22:17:14.454
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:14.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:14.477
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 03/15/23 22:17:14.481
    Mar 15 22:17:14.482: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: rename a version 03/15/23 22:17:18.643
    STEP: check the new version name is served 03/15/23 22:17:18.658
    STEP: check the old version name is removed 03/15/23 22:17:19.996
    STEP: check the other version is not changed 03/15/23 22:17:20.707
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:17:23.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2190" for this suite. 03/15/23 22:17:23.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:17:23.824
Mar 15 22:17:23.824: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename custom-resource-definition 03/15/23 22:17:23.825
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:23.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:23.837
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 03/15/23 22:17:23.84
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/15/23 22:17:23.841
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/15/23 22:17:23.841
STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/15/23 22:17:23.842
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/15/23 22:17:23.843
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/15/23 22:17:23.843
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/15/23 22:17:23.844
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:17:23.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6947" for this suite. 03/15/23 22:17:23.848
------------------------------
• [0.029 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:17:23.824
    Mar 15 22:17:23.824: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename custom-resource-definition 03/15/23 22:17:23.825
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:23.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:23.837
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 03/15/23 22:17:23.84
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/15/23 22:17:23.841
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/15/23 22:17:23.841
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/15/23 22:17:23.842
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/15/23 22:17:23.843
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/15/23 22:17:23.843
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/15/23 22:17:23.844
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:17:23.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6947" for this suite. 03/15/23 22:17:23.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:17:23.861
Mar 15 22:17:23.861: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename dns 03/15/23 22:17:23.861
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:23.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:23.877
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4334.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4334.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 03/15/23 22:17:23.88
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4334.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4334.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 03/15/23 22:17:23.881
STEP: creating a pod to probe /etc/hosts 03/15/23 22:17:23.881
STEP: submitting the pod to kubernetes 03/15/23 22:17:23.881
Mar 15 22:17:23.889: INFO: Waiting up to 15m0s for pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc" in namespace "dns-4334" to be "running"
Mar 15 22:17:23.893: INFO: Pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049043ms
Mar 15 22:17:25.899: INFO: Pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009305832s
Mar 15 22:17:27.897: INFO: Pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007413692s
Mar 15 22:17:29.900: INFO: Pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010776424s
Mar 15 22:17:31.898: INFO: Pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc": Phase="Running", Reason="", readiness=true. Elapsed: 8.008390394s
Mar 15 22:17:31.899: INFO: Pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc" satisfied condition "running"
STEP: retrieving the pod 03/15/23 22:17:31.899
STEP: looking for the results for each expected name from probers 03/15/23 22:17:31.902
Mar 15 22:17:31.925: INFO: DNS probes using dns-4334/dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc succeeded

STEP: deleting the pod 03/15/23 22:17:31.925
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 15 22:17:31.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4334" for this suite. 03/15/23 22:17:31.949
------------------------------
• [SLOW TEST] [8.101 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:17:23.861
    Mar 15 22:17:23.861: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename dns 03/15/23 22:17:23.861
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:23.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:23.877
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4334.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4334.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     03/15/23 22:17:23.88
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4334.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4334.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     03/15/23 22:17:23.881
    STEP: creating a pod to probe /etc/hosts 03/15/23 22:17:23.881
    STEP: submitting the pod to kubernetes 03/15/23 22:17:23.881
    Mar 15 22:17:23.889: INFO: Waiting up to 15m0s for pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc" in namespace "dns-4334" to be "running"
    Mar 15 22:17:23.893: INFO: Pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049043ms
    Mar 15 22:17:25.899: INFO: Pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009305832s
    Mar 15 22:17:27.897: INFO: Pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007413692s
    Mar 15 22:17:29.900: INFO: Pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010776424s
    Mar 15 22:17:31.898: INFO: Pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc": Phase="Running", Reason="", readiness=true. Elapsed: 8.008390394s
    Mar 15 22:17:31.899: INFO: Pod "dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc" satisfied condition "running"
    STEP: retrieving the pod 03/15/23 22:17:31.899
    STEP: looking for the results for each expected name from probers 03/15/23 22:17:31.902
    Mar 15 22:17:31.925: INFO: DNS probes using dns-4334/dns-test-4d6bc437-4648-4504-8eda-6370f3b5ebdc succeeded

    STEP: deleting the pod 03/15/23 22:17:31.925
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:17:31.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4334" for this suite. 03/15/23 22:17:31.949
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:17:31.962
Mar 15 22:17:31.962: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:17:31.963
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:31.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:31.981
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-7d2b05ad-3300-4c7a-8da0-279e4b3ed2ef 03/15/23 22:17:31.985
STEP: Creating a pod to test consume configMaps 03/15/23 22:17:31.989
Mar 15 22:17:31.998: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7" in namespace "configmap-4456" to be "Succeeded or Failed"
Mar 15 22:17:32.001: INFO: Pod "pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.180418ms
Mar 15 22:17:34.005: INFO: Pod "pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00700736s
Mar 15 22:17:36.004: INFO: Pod "pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006747063s
Mar 15 22:17:38.005: INFO: Pod "pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007329182s
STEP: Saw pod success 03/15/23 22:17:38.005
Mar 15 22:17:38.005: INFO: Pod "pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7" satisfied condition "Succeeded or Failed"
Mar 15 22:17:38.009: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7 container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:17:38.014
Mar 15 22:17:38.036: INFO: Waiting for pod pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7 to disappear
Mar 15 22:17:38.040: INFO: Pod pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:17:38.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4456" for this suite. 03/15/23 22:17:38.046
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:17:31.962
    Mar 15 22:17:31.962: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:17:31.963
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:31.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:31.981
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-7d2b05ad-3300-4c7a-8da0-279e4b3ed2ef 03/15/23 22:17:31.985
    STEP: Creating a pod to test consume configMaps 03/15/23 22:17:31.989
    Mar 15 22:17:31.998: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7" in namespace "configmap-4456" to be "Succeeded or Failed"
    Mar 15 22:17:32.001: INFO: Pod "pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.180418ms
    Mar 15 22:17:34.005: INFO: Pod "pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00700736s
    Mar 15 22:17:36.004: INFO: Pod "pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006747063s
    Mar 15 22:17:38.005: INFO: Pod "pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007329182s
    STEP: Saw pod success 03/15/23 22:17:38.005
    Mar 15 22:17:38.005: INFO: Pod "pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7" satisfied condition "Succeeded or Failed"
    Mar 15 22:17:38.009: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7 container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:17:38.014
    Mar 15 22:17:38.036: INFO: Waiting for pod pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7 to disappear
    Mar 15 22:17:38.040: INFO: Pod pod-configmaps-2ef18f0a-4c37-4e19-b6d2-fe29178730a7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:17:38.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4456" for this suite. 03/15/23 22:17:38.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:17:38.057
Mar 15 22:17:38.057: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:17:38.058
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:38.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:38.078
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-d8d5be6d-2e08-4d0d-a0d5-aa91abfb27e8 03/15/23 22:17:38.082
STEP: Creating a pod to test consume configMaps 03/15/23 22:17:38.087
Mar 15 22:17:38.094: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd" in namespace "projected-4936" to be "Succeeded or Failed"
Mar 15 22:17:38.099: INFO: Pod "pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.555158ms
Mar 15 22:17:40.102: INFO: Pod "pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd": Phase="Running", Reason="", readiness=false. Elapsed: 2.007667148s
Mar 15 22:17:42.105: INFO: Pod "pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010994076s
STEP: Saw pod success 03/15/23 22:17:42.105
Mar 15 22:17:42.106: INFO: Pod "pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd" satisfied condition "Succeeded or Failed"
Mar 15 22:17:42.109: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:17:42.116
Mar 15 22:17:42.151: INFO: Waiting for pod pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd to disappear
Mar 15 22:17:42.158: INFO: Pod pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:17:42.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4936" for this suite. 03/15/23 22:17:42.163
------------------------------
• [4.121 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:17:38.057
    Mar 15 22:17:38.057: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:17:38.058
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:38.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:38.078
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-d8d5be6d-2e08-4d0d-a0d5-aa91abfb27e8 03/15/23 22:17:38.082
    STEP: Creating a pod to test consume configMaps 03/15/23 22:17:38.087
    Mar 15 22:17:38.094: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd" in namespace "projected-4936" to be "Succeeded or Failed"
    Mar 15 22:17:38.099: INFO: Pod "pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.555158ms
    Mar 15 22:17:40.102: INFO: Pod "pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd": Phase="Running", Reason="", readiness=false. Elapsed: 2.007667148s
    Mar 15 22:17:42.105: INFO: Pod "pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010994076s
    STEP: Saw pod success 03/15/23 22:17:42.105
    Mar 15 22:17:42.106: INFO: Pod "pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd" satisfied condition "Succeeded or Failed"
    Mar 15 22:17:42.109: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:17:42.116
    Mar 15 22:17:42.151: INFO: Waiting for pod pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd to disappear
    Mar 15 22:17:42.158: INFO: Pod pod-projected-configmaps-8cb661bd-7963-41f9-98a0-c2b6f52f1ccd no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:17:42.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4936" for this suite. 03/15/23 22:17:42.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:17:42.182
Mar 15 22:17:42.182: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-probe 03/15/23 22:17:42.183
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:42.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:42.201
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 15 22:18:42.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1971" for this suite. 03/15/23 22:18:42.224
------------------------------
• [SLOW TEST] [60.051 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:17:42.182
    Mar 15 22:17:42.182: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-probe 03/15/23 22:17:42.183
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:17:42.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:17:42.201
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:18:42.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1971" for this suite. 03/15/23 22:18:42.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:18:42.24
Mar 15 22:18:42.241: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename replicaset 03/15/23 22:18:42.243
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:18:42.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:18:42.26
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/15/23 22:18:42.265
Mar 15 22:18:42.274: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 15 22:18:47.285: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/15/23 22:18:47.285
STEP: getting scale subresource 03/15/23 22:18:47.285
STEP: updating a scale subresource 03/15/23 22:18:47.288
STEP: verifying the replicaset Spec.Replicas was modified 03/15/23 22:18:47.3
STEP: Patch a scale subresource 03/15/23 22:18:47.314
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:18:47.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1367" for this suite. 03/15/23 22:18:47.387
------------------------------
• [SLOW TEST] [5.191 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:18:42.24
    Mar 15 22:18:42.241: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename replicaset 03/15/23 22:18:42.243
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:18:42.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:18:42.26
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/15/23 22:18:42.265
    Mar 15 22:18:42.274: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 15 22:18:47.285: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/15/23 22:18:47.285
    STEP: getting scale subresource 03/15/23 22:18:47.285
    STEP: updating a scale subresource 03/15/23 22:18:47.288
    STEP: verifying the replicaset Spec.Replicas was modified 03/15/23 22:18:47.3
    STEP: Patch a scale subresource 03/15/23 22:18:47.314
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:18:47.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1367" for this suite. 03/15/23 22:18:47.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:18:47.442
Mar 15 22:18:47.442: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename server-version 03/15/23 22:18:47.444
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:18:47.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:18:47.483
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 03/15/23 22:18:47.487
STEP: Confirm major version 03/15/23 22:18:47.488
Mar 15 22:18:47.488: INFO: Major version: 1
STEP: Confirm minor version 03/15/23 22:18:47.488
Mar 15 22:18:47.488: INFO: cleanMinorVersion: 26
Mar 15 22:18:47.488: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Mar 15 22:18:47.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-9581" for this suite. 03/15/23 22:18:47.492
------------------------------
• [0.056 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:18:47.442
    Mar 15 22:18:47.442: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename server-version 03/15/23 22:18:47.444
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:18:47.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:18:47.483
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 03/15/23 22:18:47.487
    STEP: Confirm major version 03/15/23 22:18:47.488
    Mar 15 22:18:47.488: INFO: Major version: 1
    STEP: Confirm minor version 03/15/23 22:18:47.488
    Mar 15 22:18:47.488: INFO: cleanMinorVersion: 26
    Mar 15 22:18:47.488: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:18:47.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-9581" for this suite. 03/15/23 22:18:47.492
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:18:47.499
Mar 15 22:18:47.499: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-lifecycle-hook 03/15/23 22:18:47.5
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:18:47.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:18:47.517
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/15/23 22:18:47.526
Mar 15 22:18:47.536: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1758" to be "running and ready"
Mar 15 22:18:47.545: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.146419ms
Mar 15 22:18:47.545: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:18:49.549: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012914932s
Mar 15 22:18:49.549: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 15 22:18:49.549: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 03/15/23 22:18:49.552
Mar 15 22:18:49.556: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1758" to be "running and ready"
Mar 15 22:18:49.559: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.664596ms
Mar 15 22:18:49.559: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:18:51.562: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005559378s
Mar 15 22:18:51.562: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Mar 15 22:18:51.562: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/15/23 22:18:51.564
Mar 15 22:18:51.575: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 15 22:18:51.589: INFO: Pod pod-with-prestop-http-hook still exists
Mar 15 22:18:53.590: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 15 22:18:53.594: INFO: Pod pod-with-prestop-http-hook still exists
Mar 15 22:18:55.591: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 15 22:18:55.594: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 03/15/23 22:18:55.594
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 15 22:18:55.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1758" for this suite. 03/15/23 22:18:55.626
------------------------------
• [SLOW TEST] [8.134 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:18:47.499
    Mar 15 22:18:47.499: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/15/23 22:18:47.5
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:18:47.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:18:47.517
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/15/23 22:18:47.526
    Mar 15 22:18:47.536: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1758" to be "running and ready"
    Mar 15 22:18:47.545: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.146419ms
    Mar 15 22:18:47.545: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:18:49.549: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012914932s
    Mar 15 22:18:49.549: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 15 22:18:49.549: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 03/15/23 22:18:49.552
    Mar 15 22:18:49.556: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1758" to be "running and ready"
    Mar 15 22:18:49.559: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.664596ms
    Mar 15 22:18:49.559: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:18:51.562: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005559378s
    Mar 15 22:18:51.562: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Mar 15 22:18:51.562: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/15/23 22:18:51.564
    Mar 15 22:18:51.575: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 15 22:18:51.589: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 15 22:18:53.590: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 15 22:18:53.594: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 15 22:18:55.591: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 15 22:18:55.594: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 03/15/23 22:18:55.594
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:18:55.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1758" for this suite. 03/15/23 22:18:55.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:18:55.634
Mar 15 22:18:55.634: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename resourcequota 03/15/23 22:18:55.635
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:18:55.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:18:55.675
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-mn4pb" 03/15/23 22:18:55.682
Mar 15 22:18:55.699: INFO: Resource quota "e2e-rq-status-mn4pb" reports spec: hard cpu limit of 500m
Mar 15 22:18:55.699: INFO: Resource quota "e2e-rq-status-mn4pb" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-mn4pb" /status 03/15/23 22:18:55.699
STEP: Confirm /status for "e2e-rq-status-mn4pb" resourceQuota via watch 03/15/23 22:18:55.713
Mar 15 22:18:55.719: INFO: observed resourceQuota "e2e-rq-status-mn4pb" in namespace "resourcequota-1577" with hard status: v1.ResourceList(nil)
Mar 15 22:18:55.719: INFO: Found resourceQuota "e2e-rq-status-mn4pb" in namespace "resourcequota-1577" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Mar 15 22:18:55.719: INFO: ResourceQuota "e2e-rq-status-mn4pb" /status was updated
STEP: Patching hard spec values for cpu & memory 03/15/23 22:18:55.722
Mar 15 22:18:55.730: INFO: Resource quota "e2e-rq-status-mn4pb" reports spec: hard cpu limit of 1
Mar 15 22:18:55.730: INFO: Resource quota "e2e-rq-status-mn4pb" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-mn4pb" /status 03/15/23 22:18:55.73
STEP: Confirm /status for "e2e-rq-status-mn4pb" resourceQuota via watch 03/15/23 22:18:55.739
Mar 15 22:18:55.743: INFO: observed resourceQuota "e2e-rq-status-mn4pb" in namespace "resourcequota-1577" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Mar 15 22:18:55.743: INFO: Found resourceQuota "e2e-rq-status-mn4pb" in namespace "resourcequota-1577" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Mar 15 22:18:55.743: INFO: ResourceQuota "e2e-rq-status-mn4pb" /status was patched
STEP: Get "e2e-rq-status-mn4pb" /status 03/15/23 22:18:55.743
Mar 15 22:18:55.746: INFO: Resourcequota "e2e-rq-status-mn4pb" reports status: hard cpu of 1
Mar 15 22:18:55.746: INFO: Resourcequota "e2e-rq-status-mn4pb" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-mn4pb" /status before checking Spec is unchanged 03/15/23 22:18:55.749
Mar 15 22:18:55.754: INFO: Resourcequota "e2e-rq-status-mn4pb" reports status: hard cpu of 2
Mar 15 22:18:55.754: INFO: Resourcequota "e2e-rq-status-mn4pb" reports status: hard memory of 2Gi
Mar 15 22:18:55.756: INFO: Found resourceQuota "e2e-rq-status-mn4pb" in namespace "resourcequota-1577" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Mar 15 22:19:55.764: INFO: ResourceQuota "e2e-rq-status-mn4pb" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 15 22:19:55.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1577" for this suite. 03/15/23 22:19:55.767
------------------------------
• [SLOW TEST] [60.138 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:18:55.634
    Mar 15 22:18:55.634: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename resourcequota 03/15/23 22:18:55.635
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:18:55.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:18:55.675
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-mn4pb" 03/15/23 22:18:55.682
    Mar 15 22:18:55.699: INFO: Resource quota "e2e-rq-status-mn4pb" reports spec: hard cpu limit of 500m
    Mar 15 22:18:55.699: INFO: Resource quota "e2e-rq-status-mn4pb" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-mn4pb" /status 03/15/23 22:18:55.699
    STEP: Confirm /status for "e2e-rq-status-mn4pb" resourceQuota via watch 03/15/23 22:18:55.713
    Mar 15 22:18:55.719: INFO: observed resourceQuota "e2e-rq-status-mn4pb" in namespace "resourcequota-1577" with hard status: v1.ResourceList(nil)
    Mar 15 22:18:55.719: INFO: Found resourceQuota "e2e-rq-status-mn4pb" in namespace "resourcequota-1577" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Mar 15 22:18:55.719: INFO: ResourceQuota "e2e-rq-status-mn4pb" /status was updated
    STEP: Patching hard spec values for cpu & memory 03/15/23 22:18:55.722
    Mar 15 22:18:55.730: INFO: Resource quota "e2e-rq-status-mn4pb" reports spec: hard cpu limit of 1
    Mar 15 22:18:55.730: INFO: Resource quota "e2e-rq-status-mn4pb" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-mn4pb" /status 03/15/23 22:18:55.73
    STEP: Confirm /status for "e2e-rq-status-mn4pb" resourceQuota via watch 03/15/23 22:18:55.739
    Mar 15 22:18:55.743: INFO: observed resourceQuota "e2e-rq-status-mn4pb" in namespace "resourcequota-1577" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Mar 15 22:18:55.743: INFO: Found resourceQuota "e2e-rq-status-mn4pb" in namespace "resourcequota-1577" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Mar 15 22:18:55.743: INFO: ResourceQuota "e2e-rq-status-mn4pb" /status was patched
    STEP: Get "e2e-rq-status-mn4pb" /status 03/15/23 22:18:55.743
    Mar 15 22:18:55.746: INFO: Resourcequota "e2e-rq-status-mn4pb" reports status: hard cpu of 1
    Mar 15 22:18:55.746: INFO: Resourcequota "e2e-rq-status-mn4pb" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-mn4pb" /status before checking Spec is unchanged 03/15/23 22:18:55.749
    Mar 15 22:18:55.754: INFO: Resourcequota "e2e-rq-status-mn4pb" reports status: hard cpu of 2
    Mar 15 22:18:55.754: INFO: Resourcequota "e2e-rq-status-mn4pb" reports status: hard memory of 2Gi
    Mar 15 22:18:55.756: INFO: Found resourceQuota "e2e-rq-status-mn4pb" in namespace "resourcequota-1577" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Mar 15 22:19:55.764: INFO: ResourceQuota "e2e-rq-status-mn4pb" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:19:55.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1577" for this suite. 03/15/23 22:19:55.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:19:55.774
Mar 15 22:19:55.774: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename daemonsets 03/15/23 22:19:55.775
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:19:55.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:19:55.789
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 03/15/23 22:19:55.81
STEP: Check that daemon pods launch on every node of the cluster. 03/15/23 22:19:55.82
Mar 15 22:19:55.825: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:19:55.828: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:19:55.828: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:19:56.839: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:19:56.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:19:56.843: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:19:57.833: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:19:57.836: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 15 22:19:57.836: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 03/15/23 22:19:57.838
Mar 15 22:19:57.857: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:19:57.862: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 15 22:19:57.862: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
Mar 15 22:19:58.867: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:19:58.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 15 22:19:58.871: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
Mar 15 22:19:59.866: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:19:59.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 15 22:19:59.877: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
Mar 15 22:20:00.867: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:20:00.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 15 22:20:00.871: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
Mar 15 22:20:01.866: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:20:01.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 15 22:20:01.884: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
Mar 15 22:20:02.867: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:20:02.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 15 22:20:02.870: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/15/23 22:20:02.873
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-801, will wait for the garbage collector to delete the pods 03/15/23 22:20:02.873
Mar 15 22:20:02.931: INFO: Deleting DaemonSet.extensions daemon-set took: 5.342975ms
Mar 15 22:20:03.032: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.972791ms
Mar 15 22:20:05.739: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:20:05.739: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 15 22:20:05.742: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16650"},"items":null}

Mar 15 22:20:05.744: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16650"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:20:05.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-801" for this suite. 03/15/23 22:20:05.759
------------------------------
• [SLOW TEST] [9.992 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:19:55.774
    Mar 15 22:19:55.774: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename daemonsets 03/15/23 22:19:55.775
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:19:55.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:19:55.789
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 03/15/23 22:19:55.81
    STEP: Check that daemon pods launch on every node of the cluster. 03/15/23 22:19:55.82
    Mar 15 22:19:55.825: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:19:55.828: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:19:55.828: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:19:56.839: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:19:56.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:19:56.843: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:19:57.833: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:19:57.836: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 15 22:19:57.836: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 03/15/23 22:19:57.838
    Mar 15 22:19:57.857: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:19:57.862: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 15 22:19:57.862: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
    Mar 15 22:19:58.867: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:19:58.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 15 22:19:58.871: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
    Mar 15 22:19:59.866: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:19:59.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 15 22:19:59.877: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
    Mar 15 22:20:00.867: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:20:00.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 15 22:20:00.871: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
    Mar 15 22:20:01.866: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:20:01.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 15 22:20:01.884: INFO: Node i-0baafb3f4e7bf826e is running 0 daemon pod, expected 1
    Mar 15 22:20:02.867: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:20:02.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 15 22:20:02.870: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/15/23 22:20:02.873
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-801, will wait for the garbage collector to delete the pods 03/15/23 22:20:02.873
    Mar 15 22:20:02.931: INFO: Deleting DaemonSet.extensions daemon-set took: 5.342975ms
    Mar 15 22:20:03.032: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.972791ms
    Mar 15 22:20:05.739: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:20:05.739: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 15 22:20:05.742: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16650"},"items":null}

    Mar 15 22:20:05.744: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16650"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:20:05.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-801" for this suite. 03/15/23 22:20:05.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:20:05.769
Mar 15 22:20:05.769: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename init-container 03/15/23 22:20:05.77
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:20:05.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:20:05.788
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 03/15/23 22:20:05.791
Mar 15 22:20:05.792: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:20:10.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4237" for this suite. 03/15/23 22:20:10.423
------------------------------
• [4.659 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:20:05.769
    Mar 15 22:20:05.769: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename init-container 03/15/23 22:20:05.77
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:20:05.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:20:05.788
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 03/15/23 22:20:05.791
    Mar 15 22:20:05.792: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:20:10.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4237" for this suite. 03/15/23 22:20:10.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:20:10.429
Mar 15 22:20:10.429: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename containers 03/15/23 22:20:10.43
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:20:10.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:20:10.447
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 03/15/23 22:20:10.45
Mar 15 22:20:10.458: INFO: Waiting up to 5m0s for pod "client-containers-2dd38d41-4327-468c-a208-9445f5ba5697" in namespace "containers-2228" to be "Succeeded or Failed"
Mar 15 22:20:10.461: INFO: Pod "client-containers-2dd38d41-4327-468c-a208-9445f5ba5697": Phase="Pending", Reason="", readiness=false. Elapsed: 2.978985ms
Mar 15 22:20:12.464: INFO: Pod "client-containers-2dd38d41-4327-468c-a208-9445f5ba5697": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006713581s
Mar 15 22:20:14.465: INFO: Pod "client-containers-2dd38d41-4327-468c-a208-9445f5ba5697": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007576221s
STEP: Saw pod success 03/15/23 22:20:14.465
Mar 15 22:20:14.465: INFO: Pod "client-containers-2dd38d41-4327-468c-a208-9445f5ba5697" satisfied condition "Succeeded or Failed"
Mar 15 22:20:14.468: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod client-containers-2dd38d41-4327-468c-a208-9445f5ba5697 container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:20:14.476
Mar 15 22:20:14.488: INFO: Waiting for pod client-containers-2dd38d41-4327-468c-a208-9445f5ba5697 to disappear
Mar 15 22:20:14.491: INFO: Pod client-containers-2dd38d41-4327-468c-a208-9445f5ba5697 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 15 22:20:14.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2228" for this suite. 03/15/23 22:20:14.494
------------------------------
• [4.073 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:20:10.429
    Mar 15 22:20:10.429: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename containers 03/15/23 22:20:10.43
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:20:10.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:20:10.447
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 03/15/23 22:20:10.45
    Mar 15 22:20:10.458: INFO: Waiting up to 5m0s for pod "client-containers-2dd38d41-4327-468c-a208-9445f5ba5697" in namespace "containers-2228" to be "Succeeded or Failed"
    Mar 15 22:20:10.461: INFO: Pod "client-containers-2dd38d41-4327-468c-a208-9445f5ba5697": Phase="Pending", Reason="", readiness=false. Elapsed: 2.978985ms
    Mar 15 22:20:12.464: INFO: Pod "client-containers-2dd38d41-4327-468c-a208-9445f5ba5697": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006713581s
    Mar 15 22:20:14.465: INFO: Pod "client-containers-2dd38d41-4327-468c-a208-9445f5ba5697": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007576221s
    STEP: Saw pod success 03/15/23 22:20:14.465
    Mar 15 22:20:14.465: INFO: Pod "client-containers-2dd38d41-4327-468c-a208-9445f5ba5697" satisfied condition "Succeeded or Failed"
    Mar 15 22:20:14.468: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod client-containers-2dd38d41-4327-468c-a208-9445f5ba5697 container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:20:14.476
    Mar 15 22:20:14.488: INFO: Waiting for pod client-containers-2dd38d41-4327-468c-a208-9445f5ba5697 to disappear
    Mar 15 22:20:14.491: INFO: Pod client-containers-2dd38d41-4327-468c-a208-9445f5ba5697 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:20:14.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2228" for this suite. 03/15/23 22:20:14.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:20:14.503
Mar 15 22:20:14.503: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename hostport 03/15/23 22:20:14.505
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:20:14.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:20:14.525
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/15/23 22:20:14.533
Mar 15 22:20:14.542: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1787" to be "running and ready"
Mar 15 22:20:14.551: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.033184ms
Mar 15 22:20:14.551: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:20:16.555: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013397724s
Mar 15 22:20:16.556: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 15 22:20:16.556: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.20.60.208 on the node which pod1 resides and expect scheduled 03/15/23 22:20:16.556
Mar 15 22:20:16.563: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1787" to be "running and ready"
Mar 15 22:20:16.565: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536787ms
Mar 15 22:20:16.565: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:20:18.569: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006493245s
Mar 15 22:20:18.569: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 15 22:20:18.569: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.20.60.208 but use UDP protocol on the node which pod2 resides 03/15/23 22:20:18.569
Mar 15 22:20:18.582: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1787" to be "running and ready"
Mar 15 22:20:18.586: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.59449ms
Mar 15 22:20:18.586: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:20:20.590: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007666637s
Mar 15 22:20:20.590: INFO: The phase of Pod pod3 is Running (Ready = true)
Mar 15 22:20:20.590: INFO: Pod "pod3" satisfied condition "running and ready"
Mar 15 22:20:20.596: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1787" to be "running and ready"
Mar 15 22:20:20.602: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.178494ms
Mar 15 22:20:20.602: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:20:22.606: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009690288s
Mar 15 22:20:22.606: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Mar 15 22:20:22.606: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/15/23 22:20:22.609
Mar 15 22:20:22.609: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.20.60.208 http://127.0.0.1:54323/hostname] Namespace:hostport-1787 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:20:22.609: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:20:22.610: INFO: ExecWithOptions: Clientset creation
Mar 15 22:20:22.610: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/hostport-1787/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.20.60.208+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.20.60.208, port: 54323 03/15/23 22:20:22.713
Mar 15 22:20:22.714: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.20.60.208:54323/hostname] Namespace:hostport-1787 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:20:22.714: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:20:22.714: INFO: ExecWithOptions: Clientset creation
Mar 15 22:20:22.715: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/hostport-1787/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.20.60.208%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.20.60.208, port: 54323 UDP 03/15/23 22:20:22.806
Mar 15 22:20:22.806: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.20.60.208 54323] Namespace:hostport-1787 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:20:22.806: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:20:22.807: INFO: ExecWithOptions: Clientset creation
Mar 15 22:20:22.807: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/hostport-1787/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.20.60.208+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Mar 15 22:20:27.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-1787" for this suite. 03/15/23 22:20:27.923
------------------------------
• [SLOW TEST] [13.427 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:20:14.503
    Mar 15 22:20:14.503: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename hostport 03/15/23 22:20:14.505
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:20:14.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:20:14.525
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/15/23 22:20:14.533
    Mar 15 22:20:14.542: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1787" to be "running and ready"
    Mar 15 22:20:14.551: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.033184ms
    Mar 15 22:20:14.551: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:20:16.555: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013397724s
    Mar 15 22:20:16.556: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 15 22:20:16.556: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.20.60.208 on the node which pod1 resides and expect scheduled 03/15/23 22:20:16.556
    Mar 15 22:20:16.563: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1787" to be "running and ready"
    Mar 15 22:20:16.565: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536787ms
    Mar 15 22:20:16.565: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:20:18.569: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006493245s
    Mar 15 22:20:18.569: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 15 22:20:18.569: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.20.60.208 but use UDP protocol on the node which pod2 resides 03/15/23 22:20:18.569
    Mar 15 22:20:18.582: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1787" to be "running and ready"
    Mar 15 22:20:18.586: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.59449ms
    Mar 15 22:20:18.586: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:20:20.590: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007666637s
    Mar 15 22:20:20.590: INFO: The phase of Pod pod3 is Running (Ready = true)
    Mar 15 22:20:20.590: INFO: Pod "pod3" satisfied condition "running and ready"
    Mar 15 22:20:20.596: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1787" to be "running and ready"
    Mar 15 22:20:20.602: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.178494ms
    Mar 15 22:20:20.602: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:20:22.606: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009690288s
    Mar 15 22:20:22.606: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Mar 15 22:20:22.606: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/15/23 22:20:22.609
    Mar 15 22:20:22.609: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.20.60.208 http://127.0.0.1:54323/hostname] Namespace:hostport-1787 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:20:22.609: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:20:22.610: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:20:22.610: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/hostport-1787/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.20.60.208+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.20.60.208, port: 54323 03/15/23 22:20:22.713
    Mar 15 22:20:22.714: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.20.60.208:54323/hostname] Namespace:hostport-1787 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:20:22.714: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:20:22.714: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:20:22.715: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/hostport-1787/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.20.60.208%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.20.60.208, port: 54323 UDP 03/15/23 22:20:22.806
    Mar 15 22:20:22.806: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.20.60.208 54323] Namespace:hostport-1787 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:20:22.806: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:20:22.807: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:20:22.807: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/hostport-1787/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.20.60.208+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:20:27.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-1787" for this suite. 03/15/23 22:20:27.923
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:20:27.932
Mar 15 22:20:27.934: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename ingressclass 03/15/23 22:20:27.935
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:20:27.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:20:27.96
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 03/15/23 22:20:27.969
STEP: getting /apis/networking.k8s.io 03/15/23 22:20:27.973
STEP: getting /apis/networking.k8s.iov1 03/15/23 22:20:27.974
STEP: creating 03/15/23 22:20:27.975
STEP: getting 03/15/23 22:20:27.994
STEP: listing 03/15/23 22:20:28
STEP: watching 03/15/23 22:20:28.004
Mar 15 22:20:28.004: INFO: starting watch
STEP: patching 03/15/23 22:20:28.006
STEP: updating 03/15/23 22:20:28.01
Mar 15 22:20:28.014: INFO: waiting for watch events with expected annotations
Mar 15 22:20:28.014: INFO: saw patched and updated annotations
STEP: deleting 03/15/23 22:20:28.014
STEP: deleting a collection 03/15/23 22:20:28.026
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Mar 15 22:20:28.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-5229" for this suite. 03/15/23 22:20:28.051
------------------------------
• [0.141 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:20:27.932
    Mar 15 22:20:27.934: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename ingressclass 03/15/23 22:20:27.935
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:20:27.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:20:27.96
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 03/15/23 22:20:27.969
    STEP: getting /apis/networking.k8s.io 03/15/23 22:20:27.973
    STEP: getting /apis/networking.k8s.iov1 03/15/23 22:20:27.974
    STEP: creating 03/15/23 22:20:27.975
    STEP: getting 03/15/23 22:20:27.994
    STEP: listing 03/15/23 22:20:28
    STEP: watching 03/15/23 22:20:28.004
    Mar 15 22:20:28.004: INFO: starting watch
    STEP: patching 03/15/23 22:20:28.006
    STEP: updating 03/15/23 22:20:28.01
    Mar 15 22:20:28.014: INFO: waiting for watch events with expected annotations
    Mar 15 22:20:28.014: INFO: saw patched and updated annotations
    STEP: deleting 03/15/23 22:20:28.014
    STEP: deleting a collection 03/15/23 22:20:28.026
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:20:28.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-5229" for this suite. 03/15/23 22:20:28.051
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:20:28.074
Mar 15 22:20:28.074: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename taint-multiple-pods 03/15/23 22:20:28.075
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:20:28.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:20:28.091
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Mar 15 22:20:28.094: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 15 22:21:28.129: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Mar 15 22:21:28.133: INFO: Starting informer...
STEP: Starting pods... 03/15/23 22:21:28.133
Mar 15 22:21:28.349: INFO: Pod1 is running on i-0faaf83f00b43c88c. Tainting Node
Mar 15 22:21:28.364: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-488" to be "running"
Mar 15 22:21:28.368: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052471ms
Mar 15 22:21:30.373: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00835491s
Mar 15 22:21:30.373: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Mar 15 22:21:30.373: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-488" to be "running"
Mar 15 22:21:30.378: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.127328ms
Mar 15 22:21:30.378: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Mar 15 22:21:30.378: INFO: Pod2 is running on i-0faaf83f00b43c88c. Tainting Node
STEP: Trying to apply a taint on the Node 03/15/23 22:21:30.378
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/15/23 22:21:30.397
STEP: Waiting for Pod1 and Pod2 to be deleted 03/15/23 22:21:30.411
Mar 15 22:21:36.698: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 15 22:21:56.686: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/15/23 22:21:56.702
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:21:56.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-488" for this suite. 03/15/23 22:21:56.711
------------------------------
• [SLOW TEST] [88.644 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:20:28.074
    Mar 15 22:20:28.074: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename taint-multiple-pods 03/15/23 22:20:28.075
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:20:28.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:20:28.091
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Mar 15 22:20:28.094: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 15 22:21:28.129: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Mar 15 22:21:28.133: INFO: Starting informer...
    STEP: Starting pods... 03/15/23 22:21:28.133
    Mar 15 22:21:28.349: INFO: Pod1 is running on i-0faaf83f00b43c88c. Tainting Node
    Mar 15 22:21:28.364: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-488" to be "running"
    Mar 15 22:21:28.368: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052471ms
    Mar 15 22:21:30.373: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00835491s
    Mar 15 22:21:30.373: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Mar 15 22:21:30.373: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-488" to be "running"
    Mar 15 22:21:30.378: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.127328ms
    Mar 15 22:21:30.378: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Mar 15 22:21:30.378: INFO: Pod2 is running on i-0faaf83f00b43c88c. Tainting Node
    STEP: Trying to apply a taint on the Node 03/15/23 22:21:30.378
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/15/23 22:21:30.397
    STEP: Waiting for Pod1 and Pod2 to be deleted 03/15/23 22:21:30.411
    Mar 15 22:21:36.698: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Mar 15 22:21:56.686: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/15/23 22:21:56.702
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:21:56.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-488" for this suite. 03/15/23 22:21:56.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:21:56.718
Mar 15 22:21:56.718: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 22:21:56.719
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:21:56.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:21:56.744
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Mar 15 22:21:56.748: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/15/23 22:21:58.569
Mar 15 22:21:58.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-8930 --namespace=crd-publish-openapi-8930 create -f -'
Mar 15 22:21:59.233: INFO: stderr: ""
Mar 15 22:21:59.233: INFO: stdout: "e2e-test-crd-publish-openapi-9368-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 15 22:21:59.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-8930 --namespace=crd-publish-openapi-8930 delete e2e-test-crd-publish-openapi-9368-crds test-cr'
Mar 15 22:21:59.303: INFO: stderr: ""
Mar 15 22:21:59.303: INFO: stdout: "e2e-test-crd-publish-openapi-9368-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 15 22:21:59.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-8930 --namespace=crd-publish-openapi-8930 apply -f -'
Mar 15 22:21:59.519: INFO: stderr: ""
Mar 15 22:21:59.519: INFO: stdout: "e2e-test-crd-publish-openapi-9368-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 15 22:21:59.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-8930 --namespace=crd-publish-openapi-8930 delete e2e-test-crd-publish-openapi-9368-crds test-cr'
Mar 15 22:21:59.597: INFO: stderr: ""
Mar 15 22:21:59.597: INFO: stdout: "e2e-test-crd-publish-openapi-9368-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/15/23 22:21:59.597
Mar 15 22:21:59.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-8930 explain e2e-test-crd-publish-openapi-9368-crds'
Mar 15 22:21:59.841: INFO: stderr: ""
Mar 15 22:21:59.841: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9368-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:22:01.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8930" for this suite. 03/15/23 22:22:01.574
------------------------------
• [4.862 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:21:56.718
    Mar 15 22:21:56.718: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 22:21:56.719
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:21:56.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:21:56.744
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Mar 15 22:21:56.748: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/15/23 22:21:58.569
    Mar 15 22:21:58.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-8930 --namespace=crd-publish-openapi-8930 create -f -'
    Mar 15 22:21:59.233: INFO: stderr: ""
    Mar 15 22:21:59.233: INFO: stdout: "e2e-test-crd-publish-openapi-9368-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 15 22:21:59.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-8930 --namespace=crd-publish-openapi-8930 delete e2e-test-crd-publish-openapi-9368-crds test-cr'
    Mar 15 22:21:59.303: INFO: stderr: ""
    Mar 15 22:21:59.303: INFO: stdout: "e2e-test-crd-publish-openapi-9368-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Mar 15 22:21:59.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-8930 --namespace=crd-publish-openapi-8930 apply -f -'
    Mar 15 22:21:59.519: INFO: stderr: ""
    Mar 15 22:21:59.519: INFO: stdout: "e2e-test-crd-publish-openapi-9368-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 15 22:21:59.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-8930 --namespace=crd-publish-openapi-8930 delete e2e-test-crd-publish-openapi-9368-crds test-cr'
    Mar 15 22:21:59.597: INFO: stderr: ""
    Mar 15 22:21:59.597: INFO: stdout: "e2e-test-crd-publish-openapi-9368-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/15/23 22:21:59.597
    Mar 15 22:21:59.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-8930 explain e2e-test-crd-publish-openapi-9368-crds'
    Mar 15 22:21:59.841: INFO: stderr: ""
    Mar 15 22:21:59.841: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9368-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:22:01.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8930" for this suite. 03/15/23 22:22:01.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:22:01.586
Mar 15 22:22:01.586: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:22:01.587
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:22:01.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:22:01.604
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9a81f921-7065-4eb8-a5b2-fa03a80381f0 03/15/23 22:22:01.611
STEP: Creating the pod 03/15/23 22:22:01.616
Mar 15 22:22:01.624: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a" in namespace "projected-2442" to be "running and ready"
Mar 15 22:22:01.628: INFO: Pod "pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.75322ms
Mar 15 22:22:01.629: INFO: The phase of Pod pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:22:03.633: INFO: Pod "pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008506976s
Mar 15 22:22:03.633: INFO: The phase of Pod pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:22:05.632: INFO: Pod "pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a": Phase="Running", Reason="", readiness=true. Elapsed: 4.007731363s
Mar 15 22:22:05.632: INFO: The phase of Pod pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a is Running (Ready = true)
Mar 15 22:22:05.632: INFO: Pod "pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-9a81f921-7065-4eb8-a5b2-fa03a80381f0 03/15/23 22:22:05.647
STEP: waiting to observe update in volume 03/15/23 22:22:05.652
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:23:36.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2442" for this suite. 03/15/23 22:23:36.079
------------------------------
• [SLOW TEST] [94.508 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:22:01.586
    Mar 15 22:22:01.586: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:22:01.587
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:22:01.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:22:01.604
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-9a81f921-7065-4eb8-a5b2-fa03a80381f0 03/15/23 22:22:01.611
    STEP: Creating the pod 03/15/23 22:22:01.616
    Mar 15 22:22:01.624: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a" in namespace "projected-2442" to be "running and ready"
    Mar 15 22:22:01.628: INFO: Pod "pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.75322ms
    Mar 15 22:22:01.629: INFO: The phase of Pod pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:22:03.633: INFO: Pod "pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008506976s
    Mar 15 22:22:03.633: INFO: The phase of Pod pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:22:05.632: INFO: Pod "pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a": Phase="Running", Reason="", readiness=true. Elapsed: 4.007731363s
    Mar 15 22:22:05.632: INFO: The phase of Pod pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a is Running (Ready = true)
    Mar 15 22:22:05.632: INFO: Pod "pod-projected-configmaps-5b0487eb-6889-46df-9429-0fd82b18541a" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-9a81f921-7065-4eb8-a5b2-fa03a80381f0 03/15/23 22:22:05.647
    STEP: waiting to observe update in volume 03/15/23 22:22:05.652
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:23:36.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2442" for this suite. 03/15/23 22:23:36.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:23:36.099
Mar 15 22:23:36.099: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 22:23:36.1
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:23:36.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:23:36.116
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 03/15/23 22:23:36.12
Mar 15 22:23:36.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c" in namespace "downward-api-3712" to be "Succeeded or Failed"
Mar 15 22:23:36.138: INFO: Pod "downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.585644ms
Mar 15 22:23:38.141: INFO: Pod "downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009429544s
Mar 15 22:23:40.151: INFO: Pod "downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019162185s
STEP: Saw pod success 03/15/23 22:23:40.151
Mar 15 22:23:40.151: INFO: Pod "downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c" satisfied condition "Succeeded or Failed"
Mar 15 22:23:40.158: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c container client-container: <nil>
STEP: delete the pod 03/15/23 22:23:40.169
Mar 15 22:23:40.188: INFO: Waiting for pod downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c to disappear
Mar 15 22:23:40.197: INFO: Pod downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 15 22:23:40.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3712" for this suite. 03/15/23 22:23:40.203
------------------------------
• [4.113 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:23:36.099
    Mar 15 22:23:36.099: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 22:23:36.1
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:23:36.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:23:36.116
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 03/15/23 22:23:36.12
    Mar 15 22:23:36.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c" in namespace "downward-api-3712" to be "Succeeded or Failed"
    Mar 15 22:23:36.138: INFO: Pod "downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.585644ms
    Mar 15 22:23:38.141: INFO: Pod "downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009429544s
    Mar 15 22:23:40.151: INFO: Pod "downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019162185s
    STEP: Saw pod success 03/15/23 22:23:40.151
    Mar 15 22:23:40.151: INFO: Pod "downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c" satisfied condition "Succeeded or Failed"
    Mar 15 22:23:40.158: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c container client-container: <nil>
    STEP: delete the pod 03/15/23 22:23:40.169
    Mar 15 22:23:40.188: INFO: Waiting for pod downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c to disappear
    Mar 15 22:23:40.197: INFO: Pod downwardapi-volume-632cda7d-bddd-4dc6-8b7d-42f5de63584c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:23:40.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3712" for this suite. 03/15/23 22:23:40.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:23:40.215
Mar 15 22:23:40.215: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename security-context 03/15/23 22:23:40.216
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:23:40.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:23:40.237
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/15/23 22:23:40.24
Mar 15 22:23:40.248: INFO: Waiting up to 5m0s for pod "security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6" in namespace "security-context-80" to be "Succeeded or Failed"
Mar 15 22:23:40.251: INFO: Pod "security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.142707ms
Mar 15 22:23:42.255: INFO: Pod "security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006951599s
Mar 15 22:23:44.255: INFO: Pod "security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007248166s
STEP: Saw pod success 03/15/23 22:23:44.255
Mar 15 22:23:44.255: INFO: Pod "security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6" satisfied condition "Succeeded or Failed"
Mar 15 22:23:44.258: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6 container test-container: <nil>
STEP: delete the pod 03/15/23 22:23:44.263
Mar 15 22:23:44.272: INFO: Waiting for pod security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6 to disappear
Mar 15 22:23:44.276: INFO: Pod security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 15 22:23:44.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-80" for this suite. 03/15/23 22:23:44.28
------------------------------
• [4.071 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:23:40.215
    Mar 15 22:23:40.215: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename security-context 03/15/23 22:23:40.216
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:23:40.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:23:40.237
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/15/23 22:23:40.24
    Mar 15 22:23:40.248: INFO: Waiting up to 5m0s for pod "security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6" in namespace "security-context-80" to be "Succeeded or Failed"
    Mar 15 22:23:40.251: INFO: Pod "security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.142707ms
    Mar 15 22:23:42.255: INFO: Pod "security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006951599s
    Mar 15 22:23:44.255: INFO: Pod "security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007248166s
    STEP: Saw pod success 03/15/23 22:23:44.255
    Mar 15 22:23:44.255: INFO: Pod "security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6" satisfied condition "Succeeded or Failed"
    Mar 15 22:23:44.258: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6 container test-container: <nil>
    STEP: delete the pod 03/15/23 22:23:44.263
    Mar 15 22:23:44.272: INFO: Waiting for pod security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6 to disappear
    Mar 15 22:23:44.276: INFO: Pod security-context-270b67f5-cab2-4590-90a1-d0a0e2392fa6 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:23:44.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-80" for this suite. 03/15/23 22:23:44.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:23:44.288
Mar 15 22:23:44.288: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename resourcequota 03/15/23 22:23:44.289
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:23:44.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:23:44.333
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 03/15/23 22:23:44.339
STEP: Counting existing ResourceQuota 03/15/23 22:23:49.343
STEP: Creating a ResourceQuota 03/15/23 22:23:54.349
STEP: Ensuring resource quota status is calculated 03/15/23 22:23:54.355
STEP: Creating a Secret 03/15/23 22:23:56.364
STEP: Ensuring resource quota status captures secret creation 03/15/23 22:23:56.381
STEP: Deleting a secret 03/15/23 22:23:58.386
STEP: Ensuring resource quota status released usage 03/15/23 22:23:58.391
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 15 22:24:00.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3171" for this suite. 03/15/23 22:24:00.404
------------------------------
• [SLOW TEST] [16.128 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:23:44.288
    Mar 15 22:23:44.288: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename resourcequota 03/15/23 22:23:44.289
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:23:44.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:23:44.333
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 03/15/23 22:23:44.339
    STEP: Counting existing ResourceQuota 03/15/23 22:23:49.343
    STEP: Creating a ResourceQuota 03/15/23 22:23:54.349
    STEP: Ensuring resource quota status is calculated 03/15/23 22:23:54.355
    STEP: Creating a Secret 03/15/23 22:23:56.364
    STEP: Ensuring resource quota status captures secret creation 03/15/23 22:23:56.381
    STEP: Deleting a secret 03/15/23 22:23:58.386
    STEP: Ensuring resource quota status released usage 03/15/23 22:23:58.391
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:24:00.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3171" for this suite. 03/15/23 22:24:00.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:24:00.418
Mar 15 22:24:00.418: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename job 03/15/23 22:24:00.42
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:24:00.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:24:00.443
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 03/15/23 22:24:00.449
STEP: Ensuring active pods == parallelism 03/15/23 22:24:00.46
STEP: delete a job 03/15/23 22:24:02.465
STEP: deleting Job.batch foo in namespace job-7164, will wait for the garbage collector to delete the pods 03/15/23 22:24:02.465
Mar 15 22:24:02.526: INFO: Deleting Job.batch foo took: 5.084392ms
Mar 15 22:24:02.626: INFO: Terminating Job.batch foo pods took: 100.833745ms
STEP: Ensuring job was deleted 03/15/23 22:24:35.227
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 15 22:24:35.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7164" for this suite. 03/15/23 22:24:35.234
------------------------------
• [SLOW TEST] [34.821 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:24:00.418
    Mar 15 22:24:00.418: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename job 03/15/23 22:24:00.42
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:24:00.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:24:00.443
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 03/15/23 22:24:00.449
    STEP: Ensuring active pods == parallelism 03/15/23 22:24:00.46
    STEP: delete a job 03/15/23 22:24:02.465
    STEP: deleting Job.batch foo in namespace job-7164, will wait for the garbage collector to delete the pods 03/15/23 22:24:02.465
    Mar 15 22:24:02.526: INFO: Deleting Job.batch foo took: 5.084392ms
    Mar 15 22:24:02.626: INFO: Terminating Job.batch foo pods took: 100.833745ms
    STEP: Ensuring job was deleted 03/15/23 22:24:35.227
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:24:35.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7164" for this suite. 03/15/23 22:24:35.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:24:35.24
Mar 15 22:24:35.240: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pods 03/15/23 22:24:35.241
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:24:35.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:24:35.256
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 03/15/23 22:24:35.259
STEP: submitting the pod to kubernetes 03/15/23 22:24:35.26
Mar 15 22:24:35.270: INFO: Waiting up to 5m0s for pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df" in namespace "pods-7594" to be "running and ready"
Mar 15 22:24:35.276: INFO: Pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.221208ms
Mar 15 22:24:35.276: INFO: The phase of Pod pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:24:37.279: INFO: Pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df": Phase="Running", Reason="", readiness=true. Elapsed: 2.009442598s
Mar 15 22:24:37.280: INFO: The phase of Pod pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df is Running (Ready = true)
Mar 15 22:24:37.280: INFO: Pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/15/23 22:24:37.29
STEP: updating the pod 03/15/23 22:24:37.294
Mar 15 22:24:37.807: INFO: Successfully updated pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df"
Mar 15 22:24:37.807: INFO: Waiting up to 5m0s for pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df" in namespace "pods-7594" to be "running"
Mar 15 22:24:37.811: INFO: Pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df": Phase="Running", Reason="", readiness=true. Elapsed: 3.863624ms
Mar 15 22:24:37.811: INFO: Pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 03/15/23 22:24:37.811
Mar 15 22:24:37.822: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 15 22:24:37.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7594" for this suite. 03/15/23 22:24:37.827
------------------------------
• [2.594 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:24:35.24
    Mar 15 22:24:35.240: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pods 03/15/23 22:24:35.241
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:24:35.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:24:35.256
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 03/15/23 22:24:35.259
    STEP: submitting the pod to kubernetes 03/15/23 22:24:35.26
    Mar 15 22:24:35.270: INFO: Waiting up to 5m0s for pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df" in namespace "pods-7594" to be "running and ready"
    Mar 15 22:24:35.276: INFO: Pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.221208ms
    Mar 15 22:24:35.276: INFO: The phase of Pod pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:24:37.279: INFO: Pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df": Phase="Running", Reason="", readiness=true. Elapsed: 2.009442598s
    Mar 15 22:24:37.280: INFO: The phase of Pod pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df is Running (Ready = true)
    Mar 15 22:24:37.280: INFO: Pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/15/23 22:24:37.29
    STEP: updating the pod 03/15/23 22:24:37.294
    Mar 15 22:24:37.807: INFO: Successfully updated pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df"
    Mar 15 22:24:37.807: INFO: Waiting up to 5m0s for pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df" in namespace "pods-7594" to be "running"
    Mar 15 22:24:37.811: INFO: Pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df": Phase="Running", Reason="", readiness=true. Elapsed: 3.863624ms
    Mar 15 22:24:37.811: INFO: Pod "pod-update-d4ac9b7d-3cc0-4ff6-b785-1f69ea0ca9df" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 03/15/23 22:24:37.811
    Mar 15 22:24:37.822: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:24:37.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7594" for this suite. 03/15/23 22:24:37.827
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:24:37.835
Mar 15 22:24:37.836: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename resourcequota 03/15/23 22:24:37.837
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:24:37.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:24:37.864
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 03/15/23 22:24:37.87
STEP: Creating a ResourceQuota 03/15/23 22:24:42.873
STEP: Ensuring resource quota status is calculated 03/15/23 22:24:42.88
STEP: Creating a ReplicationController 03/15/23 22:24:44.884
STEP: Ensuring resource quota status captures replication controller creation 03/15/23 22:24:44.897
STEP: Deleting a ReplicationController 03/15/23 22:24:46.902
STEP: Ensuring resource quota status released usage 03/15/23 22:24:46.923
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 15 22:24:48.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4979" for this suite. 03/15/23 22:24:48.931
------------------------------
• [SLOW TEST] [11.101 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:24:37.835
    Mar 15 22:24:37.836: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename resourcequota 03/15/23 22:24:37.837
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:24:37.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:24:37.864
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 03/15/23 22:24:37.87
    STEP: Creating a ResourceQuota 03/15/23 22:24:42.873
    STEP: Ensuring resource quota status is calculated 03/15/23 22:24:42.88
    STEP: Creating a ReplicationController 03/15/23 22:24:44.884
    STEP: Ensuring resource quota status captures replication controller creation 03/15/23 22:24:44.897
    STEP: Deleting a ReplicationController 03/15/23 22:24:46.902
    STEP: Ensuring resource quota status released usage 03/15/23 22:24:46.923
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:24:48.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4979" for this suite. 03/15/23 22:24:48.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:24:48.939
Mar 15 22:24:48.940: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename resourcequota 03/15/23 22:24:48.941
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:24:48.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:24:48.978
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 03/15/23 22:24:48.982
STEP: Getting a ResourceQuota 03/15/23 22:24:48.986
STEP: Updating a ResourceQuota 03/15/23 22:24:48.989
STEP: Verifying a ResourceQuota was modified 03/15/23 22:24:48.994
STEP: Deleting a ResourceQuota 03/15/23 22:24:48.997
STEP: Verifying the deleted ResourceQuota 03/15/23 22:24:49.004
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 15 22:24:49.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6639" for this suite. 03/15/23 22:24:49.01
------------------------------
• [0.087 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:24:48.939
    Mar 15 22:24:48.940: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename resourcequota 03/15/23 22:24:48.941
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:24:48.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:24:48.978
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 03/15/23 22:24:48.982
    STEP: Getting a ResourceQuota 03/15/23 22:24:48.986
    STEP: Updating a ResourceQuota 03/15/23 22:24:48.989
    STEP: Verifying a ResourceQuota was modified 03/15/23 22:24:48.994
    STEP: Deleting a ResourceQuota 03/15/23 22:24:48.997
    STEP: Verifying the deleted ResourceQuota 03/15/23 22:24:49.004
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:24:49.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6639" for this suite. 03/15/23 22:24:49.01
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:24:49.028
Mar 15 22:24:49.028: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename dns 03/15/23 22:24:49.029
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:24:49.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:24:49.097
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 03/15/23 22:24:49.101
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local;sleep 1; done
 03/15/23 22:24:49.115
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local;sleep 1; done
 03/15/23 22:24:49.115
STEP: creating a pod to probe DNS 03/15/23 22:24:49.115
STEP: submitting the pod to kubernetes 03/15/23 22:24:49.116
Mar 15 22:24:49.146: INFO: Waiting up to 15m0s for pod "dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6" in namespace "dns-5843" to be "running"
Mar 15 22:24:49.154: INFO: Pod "dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.506763ms
Mar 15 22:24:51.159: INFO: Pod "dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01324054s
Mar 15 22:24:53.159: INFO: Pod "dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6": Phase="Running", Reason="", readiness=true. Elapsed: 4.012966018s
Mar 15 22:24:53.159: INFO: Pod "dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6" satisfied condition "running"
STEP: retrieving the pod 03/15/23 22:24:53.159
STEP: looking for the results for each expected name from probers 03/15/23 22:24:53.164
Mar 15 22:24:53.183: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:24:53.186: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:24:53.189: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:24:53.197: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:24:53.200: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:24:53.200: INFO: Lookups using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 failed for: [wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local]

Mar 15 22:24:58.219: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:24:58.223: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:24:58.235: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:24:58.239: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:24:58.239: INFO: Lookups using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 failed for: [wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local]

Mar 15 22:25:03.213: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:03.217: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:03.228: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:03.231: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:03.231: INFO: Lookups using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 failed for: [wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local]

Mar 15 22:25:08.215: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:08.218: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:08.231: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:08.234: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:08.234: INFO: Lookups using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 failed for: [wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local]

Mar 15 22:25:13.216: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:13.220: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:13.233: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:13.241: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:13.241: INFO: Lookups using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 failed for: [wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local]

Mar 15 22:25:18.212: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:18.217: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:18.228: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:18.232: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
Mar 15 22:25:18.232: INFO: Lookups using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 failed for: [wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local]

Mar 15 22:25:23.253: INFO: DNS probes using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 succeeded

STEP: deleting the pod 03/15/23 22:25:23.253
STEP: deleting the test headless service 03/15/23 22:25:23.281
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 15 22:25:23.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5843" for this suite. 03/15/23 22:25:23.314
------------------------------
• [SLOW TEST] [34.300 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:24:49.028
    Mar 15 22:24:49.028: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename dns 03/15/23 22:24:49.029
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:24:49.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:24:49.097
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 03/15/23 22:24:49.101
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local;sleep 1; done
     03/15/23 22:24:49.115
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local;sleep 1; done
     03/15/23 22:24:49.115
    STEP: creating a pod to probe DNS 03/15/23 22:24:49.115
    STEP: submitting the pod to kubernetes 03/15/23 22:24:49.116
    Mar 15 22:24:49.146: INFO: Waiting up to 15m0s for pod "dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6" in namespace "dns-5843" to be "running"
    Mar 15 22:24:49.154: INFO: Pod "dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.506763ms
    Mar 15 22:24:51.159: INFO: Pod "dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01324054s
    Mar 15 22:24:53.159: INFO: Pod "dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6": Phase="Running", Reason="", readiness=true. Elapsed: 4.012966018s
    Mar 15 22:24:53.159: INFO: Pod "dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6" satisfied condition "running"
    STEP: retrieving the pod 03/15/23 22:24:53.159
    STEP: looking for the results for each expected name from probers 03/15/23 22:24:53.164
    Mar 15 22:24:53.183: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:24:53.186: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:24:53.189: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:24:53.197: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:24:53.200: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:24:53.200: INFO: Lookups using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 failed for: [wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local]

    Mar 15 22:24:58.219: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:24:58.223: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:24:58.235: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:24:58.239: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:24:58.239: INFO: Lookups using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 failed for: [wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local]

    Mar 15 22:25:03.213: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:03.217: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:03.228: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:03.231: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:03.231: INFO: Lookups using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 failed for: [wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local]

    Mar 15 22:25:08.215: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:08.218: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:08.231: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:08.234: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:08.234: INFO: Lookups using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 failed for: [wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local]

    Mar 15 22:25:13.216: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:13.220: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:13.233: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:13.241: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:13.241: INFO: Lookups using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 failed for: [wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local]

    Mar 15 22:25:18.212: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:18.217: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:18.228: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:18.232: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local from pod dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6: the server could not find the requested resource (get pods dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6)
    Mar 15 22:25:18.232: INFO: Lookups using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 failed for: [wheezy_udp@dns-test-service-2.dns-5843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5843.svc.cluster.local jessie_udp@dns-test-service-2.dns-5843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5843.svc.cluster.local]

    Mar 15 22:25:23.253: INFO: DNS probes using dns-5843/dns-test-efa1be66-ce36-4fa2-a32d-956b738667b6 succeeded

    STEP: deleting the pod 03/15/23 22:25:23.253
    STEP: deleting the test headless service 03/15/23 22:25:23.281
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:25:23.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5843" for this suite. 03/15/23 22:25:23.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:25:23.333
Mar 15 22:25:23.333: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 22:25:23.334
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:23.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:23.379
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/15/23 22:25:23.387
Mar 15 22:25:23.388: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:25:24.933: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:25:32.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5992" for this suite. 03/15/23 22:25:32.129
------------------------------
• [SLOW TEST] [8.801 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:25:23.333
    Mar 15 22:25:23.333: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 22:25:23.334
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:23.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:23.379
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/15/23 22:25:23.387
    Mar 15 22:25:23.388: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:25:24.933: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:25:32.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5992" for this suite. 03/15/23 22:25:32.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:25:32.136
Mar 15 22:25:32.136: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename var-expansion 03/15/23 22:25:32.137
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:32.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:32.149
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 03/15/23 22:25:32.152
Mar 15 22:25:32.164: INFO: Waiting up to 5m0s for pod "var-expansion-05171c33-f152-417b-89c3-5f7043b71159" in namespace "var-expansion-9866" to be "Succeeded or Failed"
Mar 15 22:25:32.167: INFO: Pod "var-expansion-05171c33-f152-417b-89c3-5f7043b71159": Phase="Pending", Reason="", readiness=false. Elapsed: 3.360576ms
Mar 15 22:25:34.171: INFO: Pod "var-expansion-05171c33-f152-417b-89c3-5f7043b71159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007471513s
Mar 15 22:25:36.171: INFO: Pod "var-expansion-05171c33-f152-417b-89c3-5f7043b71159": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007600403s
STEP: Saw pod success 03/15/23 22:25:36.172
Mar 15 22:25:36.172: INFO: Pod "var-expansion-05171c33-f152-417b-89c3-5f7043b71159" satisfied condition "Succeeded or Failed"
Mar 15 22:25:36.174: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod var-expansion-05171c33-f152-417b-89c3-5f7043b71159 container dapi-container: <nil>
STEP: delete the pod 03/15/23 22:25:36.19
Mar 15 22:25:36.202: INFO: Waiting for pod var-expansion-05171c33-f152-417b-89c3-5f7043b71159 to disappear
Mar 15 22:25:36.205: INFO: Pod var-expansion-05171c33-f152-417b-89c3-5f7043b71159 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 15 22:25:36.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9866" for this suite. 03/15/23 22:25:36.21
------------------------------
• [4.079 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:25:32.136
    Mar 15 22:25:32.136: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename var-expansion 03/15/23 22:25:32.137
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:32.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:32.149
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 03/15/23 22:25:32.152
    Mar 15 22:25:32.164: INFO: Waiting up to 5m0s for pod "var-expansion-05171c33-f152-417b-89c3-5f7043b71159" in namespace "var-expansion-9866" to be "Succeeded or Failed"
    Mar 15 22:25:32.167: INFO: Pod "var-expansion-05171c33-f152-417b-89c3-5f7043b71159": Phase="Pending", Reason="", readiness=false. Elapsed: 3.360576ms
    Mar 15 22:25:34.171: INFO: Pod "var-expansion-05171c33-f152-417b-89c3-5f7043b71159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007471513s
    Mar 15 22:25:36.171: INFO: Pod "var-expansion-05171c33-f152-417b-89c3-5f7043b71159": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007600403s
    STEP: Saw pod success 03/15/23 22:25:36.172
    Mar 15 22:25:36.172: INFO: Pod "var-expansion-05171c33-f152-417b-89c3-5f7043b71159" satisfied condition "Succeeded or Failed"
    Mar 15 22:25:36.174: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod var-expansion-05171c33-f152-417b-89c3-5f7043b71159 container dapi-container: <nil>
    STEP: delete the pod 03/15/23 22:25:36.19
    Mar 15 22:25:36.202: INFO: Waiting for pod var-expansion-05171c33-f152-417b-89c3-5f7043b71159 to disappear
    Mar 15 22:25:36.205: INFO: Pod var-expansion-05171c33-f152-417b-89c3-5f7043b71159 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:25:36.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9866" for this suite. 03/15/23 22:25:36.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:25:36.217
Mar 15 22:25:36.217: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 22:25:36.218
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:36.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:36.238
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-0136cde0-0c28-4330-b321-1ac4d4701388 03/15/23 22:25:36.242
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 22:25:36.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-80" for this suite. 03/15/23 22:25:36.252
------------------------------
• [0.041 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:25:36.217
    Mar 15 22:25:36.217: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 22:25:36.218
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:36.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:36.238
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-0136cde0-0c28-4330-b321-1ac4d4701388 03/15/23 22:25:36.242
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:25:36.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-80" for this suite. 03/15/23 22:25:36.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:25:36.266
Mar 15 22:25:36.266: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 22:25:36.267
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:36.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:36.289
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 03/15/23 22:25:36.292
Mar 15 22:25:36.304: INFO: Waiting up to 5m0s for pod "pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06" in namespace "emptydir-785" to be "Succeeded or Failed"
Mar 15 22:25:36.313: INFO: Pod "pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06": Phase="Pending", Reason="", readiness=false. Elapsed: 9.504908ms
Mar 15 22:25:38.317: INFO: Pod "pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013066758s
Mar 15 22:25:40.317: INFO: Pod "pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013416312s
STEP: Saw pod success 03/15/23 22:25:40.317
Mar 15 22:25:40.317: INFO: Pod "pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06" satisfied condition "Succeeded or Failed"
Mar 15 22:25:40.321: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06 container test-container: <nil>
STEP: delete the pod 03/15/23 22:25:40.328
Mar 15 22:25:40.339: INFO: Waiting for pod pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06 to disappear
Mar 15 22:25:40.342: INFO: Pod pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:25:40.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-785" for this suite. 03/15/23 22:25:40.346
------------------------------
• [4.085 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:25:36.266
    Mar 15 22:25:36.266: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 22:25:36.267
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:36.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:36.289
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/15/23 22:25:36.292
    Mar 15 22:25:36.304: INFO: Waiting up to 5m0s for pod "pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06" in namespace "emptydir-785" to be "Succeeded or Failed"
    Mar 15 22:25:36.313: INFO: Pod "pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06": Phase="Pending", Reason="", readiness=false. Elapsed: 9.504908ms
    Mar 15 22:25:38.317: INFO: Pod "pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013066758s
    Mar 15 22:25:40.317: INFO: Pod "pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013416312s
    STEP: Saw pod success 03/15/23 22:25:40.317
    Mar 15 22:25:40.317: INFO: Pod "pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06" satisfied condition "Succeeded or Failed"
    Mar 15 22:25:40.321: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06 container test-container: <nil>
    STEP: delete the pod 03/15/23 22:25:40.328
    Mar 15 22:25:40.339: INFO: Waiting for pod pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06 to disappear
    Mar 15 22:25:40.342: INFO: Pod pod-6fcced8d-7dba-4a6d-a157-1cc8b4d23f06 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:25:40.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-785" for this suite. 03/15/23 22:25:40.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:25:40.354
Mar 15 22:25:40.354: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 22:25:40.356
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:40.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:40.381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 22:25:40.437
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:25:40.941
STEP: Deploying the webhook pod 03/15/23 22:25:40.948
STEP: Wait for the deployment to be ready 03/15/23 22:25:40.961
Mar 15 22:25:40.971: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 22:25:42.984
STEP: Verifying the service has paired with the endpoint 03/15/23 22:25:43.02
Mar 15 22:25:44.021: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/15/23 22:25:44.025
STEP: create a pod that should be updated by the webhook 03/15/23 22:25:44.039
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:25:44.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5703" for this suite. 03/15/23 22:25:44.125
STEP: Destroying namespace "webhook-5703-markers" for this suite. 03/15/23 22:25:44.138
------------------------------
• [3.796 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:25:40.354
    Mar 15 22:25:40.354: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 22:25:40.356
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:40.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:40.381
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 22:25:40.437
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:25:40.941
    STEP: Deploying the webhook pod 03/15/23 22:25:40.948
    STEP: Wait for the deployment to be ready 03/15/23 22:25:40.961
    Mar 15 22:25:40.971: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 22:25:42.984
    STEP: Verifying the service has paired with the endpoint 03/15/23 22:25:43.02
    Mar 15 22:25:44.021: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/15/23 22:25:44.025
    STEP: create a pod that should be updated by the webhook 03/15/23 22:25:44.039
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:25:44.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5703" for this suite. 03/15/23 22:25:44.125
    STEP: Destroying namespace "webhook-5703-markers" for this suite. 03/15/23 22:25:44.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:25:44.154
Mar 15 22:25:44.154: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 22:25:44.155
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:44.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:44.173
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/15/23 22:25:44.178
Mar 15 22:25:44.185: INFO: Waiting up to 5m0s for pod "pod-f44b7722-a73d-4f49-be81-cfd2adc61722" in namespace "emptydir-9130" to be "Succeeded or Failed"
Mar 15 22:25:44.190: INFO: Pod "pod-f44b7722-a73d-4f49-be81-cfd2adc61722": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912933ms
Mar 15 22:25:46.194: INFO: Pod "pod-f44b7722-a73d-4f49-be81-cfd2adc61722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008899352s
Mar 15 22:25:48.194: INFO: Pod "pod-f44b7722-a73d-4f49-be81-cfd2adc61722": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008633177s
STEP: Saw pod success 03/15/23 22:25:48.194
Mar 15 22:25:48.194: INFO: Pod "pod-f44b7722-a73d-4f49-be81-cfd2adc61722" satisfied condition "Succeeded or Failed"
Mar 15 22:25:48.197: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-f44b7722-a73d-4f49-be81-cfd2adc61722 container test-container: <nil>
STEP: delete the pod 03/15/23 22:25:48.207
Mar 15 22:25:48.216: INFO: Waiting for pod pod-f44b7722-a73d-4f49-be81-cfd2adc61722 to disappear
Mar 15 22:25:48.218: INFO: Pod pod-f44b7722-a73d-4f49-be81-cfd2adc61722 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:25:48.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9130" for this suite. 03/15/23 22:25:48.222
------------------------------
• [4.072 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:25:44.154
    Mar 15 22:25:44.154: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 22:25:44.155
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:44.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:44.173
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/15/23 22:25:44.178
    Mar 15 22:25:44.185: INFO: Waiting up to 5m0s for pod "pod-f44b7722-a73d-4f49-be81-cfd2adc61722" in namespace "emptydir-9130" to be "Succeeded or Failed"
    Mar 15 22:25:44.190: INFO: Pod "pod-f44b7722-a73d-4f49-be81-cfd2adc61722": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912933ms
    Mar 15 22:25:46.194: INFO: Pod "pod-f44b7722-a73d-4f49-be81-cfd2adc61722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008899352s
    Mar 15 22:25:48.194: INFO: Pod "pod-f44b7722-a73d-4f49-be81-cfd2adc61722": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008633177s
    STEP: Saw pod success 03/15/23 22:25:48.194
    Mar 15 22:25:48.194: INFO: Pod "pod-f44b7722-a73d-4f49-be81-cfd2adc61722" satisfied condition "Succeeded or Failed"
    Mar 15 22:25:48.197: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-f44b7722-a73d-4f49-be81-cfd2adc61722 container test-container: <nil>
    STEP: delete the pod 03/15/23 22:25:48.207
    Mar 15 22:25:48.216: INFO: Waiting for pod pod-f44b7722-a73d-4f49-be81-cfd2adc61722 to disappear
    Mar 15 22:25:48.218: INFO: Pod pod-f44b7722-a73d-4f49-be81-cfd2adc61722 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:25:48.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9130" for this suite. 03/15/23 22:25:48.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:25:48.227
Mar 15 22:25:48.228: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pod-network-test 03/15/23 22:25:48.229
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:48.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:48.246
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-3036 03/15/23 22:25:48.249
STEP: creating a selector 03/15/23 22:25:48.25
STEP: Creating the service pods in kubernetes 03/15/23 22:25:48.25
Mar 15 22:25:48.250: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 15 22:25:48.335: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3036" to be "running and ready"
Mar 15 22:25:48.342: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.680784ms
Mar 15 22:25:48.342: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:25:50.346: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011282273s
Mar 15 22:25:50.346: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:25:52.348: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013367887s
Mar 15 22:25:52.348: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:25:54.346: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011796457s
Mar 15 22:25:54.346: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:25:56.345: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010802053s
Mar 15 22:25:56.345: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:25:58.347: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012027236s
Mar 15 22:25:58.347: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:26:00.345: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010844547s
Mar 15 22:26:00.345: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:26:02.347: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012170074s
Mar 15 22:26:02.347: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:26:04.348: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01318323s
Mar 15 22:26:04.348: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:26:06.346: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011477085s
Mar 15 22:26:06.346: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:26:08.347: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.012087677s
Mar 15 22:26:08.347: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 15 22:26:10.349: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01400527s
Mar 15 22:26:10.349: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 15 22:26:10.349: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 15 22:26:10.354: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3036" to be "running and ready"
Mar 15 22:26:10.360: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.511068ms
Mar 15 22:26:10.360: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 15 22:26:10.360: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 15 22:26:10.364: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3036" to be "running and ready"
Mar 15 22:26:10.367: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.338297ms
Mar 15 22:26:10.367: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 15 22:26:10.367: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/15/23 22:26:10.375
Mar 15 22:26:10.407: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3036" to be "running"
Mar 15 22:26:10.413: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.561123ms
Mar 15 22:26:12.418: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010592615s
Mar 15 22:26:14.418: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010898737s
Mar 15 22:26:14.418: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 15 22:26:14.424: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3036" to be "running"
Mar 15 22:26:14.429: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.474615ms
Mar 15 22:26:14.429: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 15 22:26:14.437: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 15 22:26:14.437: INFO: Going to poll 100.96.2.71 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 15 22:26:14.467: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.2.71:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3036 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:26:14.467: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:26:14.470: INFO: ExecWithOptions: Clientset creation
Mar 15 22:26:14.471: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-3036/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.96.2.71%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 15 22:26:14.573: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 15 22:26:14.573: INFO: Going to poll 100.96.1.147 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 15 22:26:14.577: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.147:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3036 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:26:14.577: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:26:14.578: INFO: ExecWithOptions: Clientset creation
Mar 15 22:26:14.578: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-3036/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.96.1.147%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 15 22:26:14.683: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 15 22:26:14.683: INFO: Going to poll 100.96.3.175 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 15 22:26:14.686: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.3.175:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3036 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:26:14.686: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:26:14.687: INFO: ExecWithOptions: Clientset creation
Mar 15 22:26:14.687: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-3036/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.96.3.175%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 15 22:26:14.800: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 15 22:26:14.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3036" for this suite. 03/15/23 22:26:14.807
------------------------------
• [SLOW TEST] [26.592 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:25:48.227
    Mar 15 22:25:48.228: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pod-network-test 03/15/23 22:25:48.229
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:25:48.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:25:48.246
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-3036 03/15/23 22:25:48.249
    STEP: creating a selector 03/15/23 22:25:48.25
    STEP: Creating the service pods in kubernetes 03/15/23 22:25:48.25
    Mar 15 22:25:48.250: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 15 22:25:48.335: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3036" to be "running and ready"
    Mar 15 22:25:48.342: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.680784ms
    Mar 15 22:25:48.342: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:25:50.346: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011282273s
    Mar 15 22:25:50.346: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:25:52.348: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013367887s
    Mar 15 22:25:52.348: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:25:54.346: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011796457s
    Mar 15 22:25:54.346: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:25:56.345: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010802053s
    Mar 15 22:25:56.345: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:25:58.347: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012027236s
    Mar 15 22:25:58.347: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:26:00.345: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010844547s
    Mar 15 22:26:00.345: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:26:02.347: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012170074s
    Mar 15 22:26:02.347: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:26:04.348: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01318323s
    Mar 15 22:26:04.348: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:26:06.346: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011477085s
    Mar 15 22:26:06.346: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:26:08.347: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.012087677s
    Mar 15 22:26:08.347: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 15 22:26:10.349: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01400527s
    Mar 15 22:26:10.349: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 15 22:26:10.349: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 15 22:26:10.354: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3036" to be "running and ready"
    Mar 15 22:26:10.360: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.511068ms
    Mar 15 22:26:10.360: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 15 22:26:10.360: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 15 22:26:10.364: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3036" to be "running and ready"
    Mar 15 22:26:10.367: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.338297ms
    Mar 15 22:26:10.367: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 15 22:26:10.367: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/15/23 22:26:10.375
    Mar 15 22:26:10.407: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3036" to be "running"
    Mar 15 22:26:10.413: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.561123ms
    Mar 15 22:26:12.418: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010592615s
    Mar 15 22:26:14.418: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010898737s
    Mar 15 22:26:14.418: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 15 22:26:14.424: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3036" to be "running"
    Mar 15 22:26:14.429: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.474615ms
    Mar 15 22:26:14.429: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 15 22:26:14.437: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 15 22:26:14.437: INFO: Going to poll 100.96.2.71 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 15 22:26:14.467: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.2.71:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3036 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:26:14.467: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:26:14.470: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:26:14.471: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-3036/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.96.2.71%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 15 22:26:14.573: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 15 22:26:14.573: INFO: Going to poll 100.96.1.147 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 15 22:26:14.577: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.147:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3036 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:26:14.577: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:26:14.578: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:26:14.578: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-3036/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.96.1.147%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 15 22:26:14.683: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 15 22:26:14.683: INFO: Going to poll 100.96.3.175 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 15 22:26:14.686: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.3.175:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3036 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:26:14.686: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:26:14.687: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:26:14.687: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-3036/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.96.3.175%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 15 22:26:14.800: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:26:14.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3036" for this suite. 03/15/23 22:26:14.807
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:26:14.821
Mar 15 22:26:14.821: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:26:14.823
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:26:14.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:26:14.841
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 03/15/23 22:26:14.846
Mar 15 22:26:14.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3596 create -f -'
Mar 15 22:26:16.104: INFO: stderr: ""
Mar 15 22:26:16.104: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 03/15/23 22:26:16.104
Mar 15 22:26:16.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3596 diff -f -'
Mar 15 22:26:16.414: INFO: rc: 1
Mar 15 22:26:16.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3596 delete -f -'
Mar 15 22:26:16.530: INFO: stderr: ""
Mar 15 22:26:16.530: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:26:16.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3596" for this suite. 03/15/23 22:26:16.542
------------------------------
• [1.729 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:26:14.821
    Mar 15 22:26:14.821: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:26:14.823
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:26:14.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:26:14.841
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 03/15/23 22:26:14.846
    Mar 15 22:26:14.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3596 create -f -'
    Mar 15 22:26:16.104: INFO: stderr: ""
    Mar 15 22:26:16.104: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 03/15/23 22:26:16.104
    Mar 15 22:26:16.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3596 diff -f -'
    Mar 15 22:26:16.414: INFO: rc: 1
    Mar 15 22:26:16.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3596 delete -f -'
    Mar 15 22:26:16.530: INFO: stderr: ""
    Mar 15 22:26:16.530: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:26:16.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3596" for this suite. 03/15/23 22:26:16.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:26:16.551
Mar 15 22:26:16.551: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pods 03/15/23 22:26:16.552
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:26:16.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:26:16.568
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Mar 15 22:26:16.578: INFO: Waiting up to 5m0s for pod "server-envvars-3abe9f57-514e-4b20-b4db-4df2e2e49fec" in namespace "pods-2978" to be "running and ready"
Mar 15 22:26:16.582: INFO: Pod "server-envvars-3abe9f57-514e-4b20-b4db-4df2e2e49fec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.354934ms
Mar 15 22:26:16.582: INFO: The phase of Pod server-envvars-3abe9f57-514e-4b20-b4db-4df2e2e49fec is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:26:18.587: INFO: Pod "server-envvars-3abe9f57-514e-4b20-b4db-4df2e2e49fec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008166263s
Mar 15 22:26:18.587: INFO: The phase of Pod server-envvars-3abe9f57-514e-4b20-b4db-4df2e2e49fec is Running (Ready = true)
Mar 15 22:26:18.587: INFO: Pod "server-envvars-3abe9f57-514e-4b20-b4db-4df2e2e49fec" satisfied condition "running and ready"
Mar 15 22:26:18.664: INFO: Waiting up to 5m0s for pod "client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc" in namespace "pods-2978" to be "Succeeded or Failed"
Mar 15 22:26:18.675: INFO: Pod "client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.431777ms
Mar 15 22:26:20.702: INFO: Pod "client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038099012s
Mar 15 22:26:22.679: INFO: Pod "client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015568219s
STEP: Saw pod success 03/15/23 22:26:22.679
Mar 15 22:26:22.680: INFO: Pod "client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc" satisfied condition "Succeeded or Failed"
Mar 15 22:26:22.683: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc container env3cont: <nil>
STEP: delete the pod 03/15/23 22:26:22.688
Mar 15 22:26:22.710: INFO: Waiting for pod client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc to disappear
Mar 15 22:26:22.728: INFO: Pod client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 15 22:26:22.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2978" for this suite. 03/15/23 22:26:22.739
------------------------------
• [SLOW TEST] [6.197 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:26:16.551
    Mar 15 22:26:16.551: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pods 03/15/23 22:26:16.552
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:26:16.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:26:16.568
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Mar 15 22:26:16.578: INFO: Waiting up to 5m0s for pod "server-envvars-3abe9f57-514e-4b20-b4db-4df2e2e49fec" in namespace "pods-2978" to be "running and ready"
    Mar 15 22:26:16.582: INFO: Pod "server-envvars-3abe9f57-514e-4b20-b4db-4df2e2e49fec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.354934ms
    Mar 15 22:26:16.582: INFO: The phase of Pod server-envvars-3abe9f57-514e-4b20-b4db-4df2e2e49fec is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:26:18.587: INFO: Pod "server-envvars-3abe9f57-514e-4b20-b4db-4df2e2e49fec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008166263s
    Mar 15 22:26:18.587: INFO: The phase of Pod server-envvars-3abe9f57-514e-4b20-b4db-4df2e2e49fec is Running (Ready = true)
    Mar 15 22:26:18.587: INFO: Pod "server-envvars-3abe9f57-514e-4b20-b4db-4df2e2e49fec" satisfied condition "running and ready"
    Mar 15 22:26:18.664: INFO: Waiting up to 5m0s for pod "client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc" in namespace "pods-2978" to be "Succeeded or Failed"
    Mar 15 22:26:18.675: INFO: Pod "client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.431777ms
    Mar 15 22:26:20.702: INFO: Pod "client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038099012s
    Mar 15 22:26:22.679: INFO: Pod "client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015568219s
    STEP: Saw pod success 03/15/23 22:26:22.679
    Mar 15 22:26:22.680: INFO: Pod "client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc" satisfied condition "Succeeded or Failed"
    Mar 15 22:26:22.683: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc container env3cont: <nil>
    STEP: delete the pod 03/15/23 22:26:22.688
    Mar 15 22:26:22.710: INFO: Waiting for pod client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc to disappear
    Mar 15 22:26:22.728: INFO: Pod client-envvars-284c1258-ab94-4225-b63d-ee9e7febf6cc no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:26:22.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2978" for this suite. 03/15/23 22:26:22.739
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:26:22.75
Mar 15 22:26:22.750: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-probe 03/15/23 22:26:22.751
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:26:22.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:26:22.773
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 in namespace container-probe-7475 03/15/23 22:26:22.776
Mar 15 22:26:22.783: INFO: Waiting up to 5m0s for pod "liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203" in namespace "container-probe-7475" to be "not pending"
Mar 15 22:26:22.790: INFO: Pod "liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.884617ms
Mar 15 22:26:24.794: INFO: Pod "liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203": Phase="Running", Reason="", readiness=true. Elapsed: 2.011128078s
Mar 15 22:26:24.794: INFO: Pod "liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203" satisfied condition "not pending"
Mar 15 22:26:24.794: INFO: Started pod liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 in namespace container-probe-7475
STEP: checking the pod's current state and verifying that restartCount is present 03/15/23 22:26:24.794
Mar 15 22:26:24.798: INFO: Initial restart count of pod liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 is 0
Mar 15 22:26:44.846: INFO: Restart count of pod container-probe-7475/liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 is now 1 (20.047385274s elapsed)
Mar 15 22:27:04.885: INFO: Restart count of pod container-probe-7475/liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 is now 2 (40.087271594s elapsed)
Mar 15 22:27:24.935: INFO: Restart count of pod container-probe-7475/liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 is now 3 (1m0.136794209s elapsed)
Mar 15 22:27:44.982: INFO: Restart count of pod container-probe-7475/liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 is now 4 (1m20.183518623s elapsed)
Mar 15 22:28:45.118: INFO: Restart count of pod container-probe-7475/liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 is now 5 (2m20.319494738s elapsed)
STEP: deleting the pod 03/15/23 22:28:45.118
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 15 22:28:45.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7475" for this suite. 03/15/23 22:28:45.149
------------------------------
• [SLOW TEST] [142.421 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:26:22.75
    Mar 15 22:26:22.750: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-probe 03/15/23 22:26:22.751
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:26:22.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:26:22.773
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 in namespace container-probe-7475 03/15/23 22:26:22.776
    Mar 15 22:26:22.783: INFO: Waiting up to 5m0s for pod "liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203" in namespace "container-probe-7475" to be "not pending"
    Mar 15 22:26:22.790: INFO: Pod "liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.884617ms
    Mar 15 22:26:24.794: INFO: Pod "liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203": Phase="Running", Reason="", readiness=true. Elapsed: 2.011128078s
    Mar 15 22:26:24.794: INFO: Pod "liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203" satisfied condition "not pending"
    Mar 15 22:26:24.794: INFO: Started pod liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 in namespace container-probe-7475
    STEP: checking the pod's current state and verifying that restartCount is present 03/15/23 22:26:24.794
    Mar 15 22:26:24.798: INFO: Initial restart count of pod liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 is 0
    Mar 15 22:26:44.846: INFO: Restart count of pod container-probe-7475/liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 is now 1 (20.047385274s elapsed)
    Mar 15 22:27:04.885: INFO: Restart count of pod container-probe-7475/liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 is now 2 (40.087271594s elapsed)
    Mar 15 22:27:24.935: INFO: Restart count of pod container-probe-7475/liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 is now 3 (1m0.136794209s elapsed)
    Mar 15 22:27:44.982: INFO: Restart count of pod container-probe-7475/liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 is now 4 (1m20.183518623s elapsed)
    Mar 15 22:28:45.118: INFO: Restart count of pod container-probe-7475/liveness-d56b6939-86bd-44c3-ac3a-4e4561bda203 is now 5 (2m20.319494738s elapsed)
    STEP: deleting the pod 03/15/23 22:28:45.118
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:28:45.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7475" for this suite. 03/15/23 22:28:45.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:28:45.18
Mar 15 22:28:45.180: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:28:45.181
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:28:45.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:28:45.252
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 03/15/23 22:28:45.256
Mar 15 22:28:45.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-6038 api-versions'
Mar 15 22:28:45.428: INFO: stderr: ""
Mar 15 22:28:45.428: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\niamauthenticator.k8s.aws/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:28:45.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6038" for this suite. 03/15/23 22:28:45.432
------------------------------
• [0.259 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:28:45.18
    Mar 15 22:28:45.180: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:28:45.181
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:28:45.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:28:45.252
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 03/15/23 22:28:45.256
    Mar 15 22:28:45.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-6038 api-versions'
    Mar 15 22:28:45.428: INFO: stderr: ""
    Mar 15 22:28:45.428: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\niamauthenticator.k8s.aws/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:28:45.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6038" for this suite. 03/15/23 22:28:45.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:28:45.439
Mar 15 22:28:45.439: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:28:45.441
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:28:45.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:28:45.457
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-41156b5d-1a75-427f-a9be-7fe290603a3c 03/15/23 22:28:45.463
STEP: Creating the pod 03/15/23 22:28:45.468
Mar 15 22:28:45.479: INFO: Waiting up to 5m0s for pod "pod-configmaps-83992d93-5e4d-4e48-b261-cca85bf3324e" in namespace "configmap-7885" to be "running and ready"
Mar 15 22:28:45.482: INFO: Pod "pod-configmaps-83992d93-5e4d-4e48-b261-cca85bf3324e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.426381ms
Mar 15 22:28:45.482: INFO: The phase of Pod pod-configmaps-83992d93-5e4d-4e48-b261-cca85bf3324e is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:28:47.485: INFO: Pod "pod-configmaps-83992d93-5e4d-4e48-b261-cca85bf3324e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006680711s
Mar 15 22:28:47.485: INFO: The phase of Pod pod-configmaps-83992d93-5e4d-4e48-b261-cca85bf3324e is Running (Ready = true)
Mar 15 22:28:47.485: INFO: Pod "pod-configmaps-83992d93-5e4d-4e48-b261-cca85bf3324e" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-41156b5d-1a75-427f-a9be-7fe290603a3c 03/15/23 22:28:47.501
STEP: waiting to observe update in volume 03/15/23 22:28:47.507
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:28:49.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7885" for this suite. 03/15/23 22:28:49.525
------------------------------
• [4.092 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:28:45.439
    Mar 15 22:28:45.439: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:28:45.441
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:28:45.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:28:45.457
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-41156b5d-1a75-427f-a9be-7fe290603a3c 03/15/23 22:28:45.463
    STEP: Creating the pod 03/15/23 22:28:45.468
    Mar 15 22:28:45.479: INFO: Waiting up to 5m0s for pod "pod-configmaps-83992d93-5e4d-4e48-b261-cca85bf3324e" in namespace "configmap-7885" to be "running and ready"
    Mar 15 22:28:45.482: INFO: Pod "pod-configmaps-83992d93-5e4d-4e48-b261-cca85bf3324e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.426381ms
    Mar 15 22:28:45.482: INFO: The phase of Pod pod-configmaps-83992d93-5e4d-4e48-b261-cca85bf3324e is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:28:47.485: INFO: Pod "pod-configmaps-83992d93-5e4d-4e48-b261-cca85bf3324e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006680711s
    Mar 15 22:28:47.485: INFO: The phase of Pod pod-configmaps-83992d93-5e4d-4e48-b261-cca85bf3324e is Running (Ready = true)
    Mar 15 22:28:47.485: INFO: Pod "pod-configmaps-83992d93-5e4d-4e48-b261-cca85bf3324e" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-41156b5d-1a75-427f-a9be-7fe290603a3c 03/15/23 22:28:47.501
    STEP: waiting to observe update in volume 03/15/23 22:28:47.507
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:28:49.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7885" for this suite. 03/15/23 22:28:49.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:28:49.536
Mar 15 22:28:49.536: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 22:28:49.537
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:28:49.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:28:49.553
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 03/15/23 22:28:49.556
Mar 15 22:28:49.565: INFO: Waiting up to 5m0s for pod "pod-45933ee3-5105-4f43-aa1b-c5b11bddba57" in namespace "emptydir-8789" to be "Succeeded or Failed"
Mar 15 22:28:49.571: INFO: Pod "pod-45933ee3-5105-4f43-aa1b-c5b11bddba57": Phase="Pending", Reason="", readiness=false. Elapsed: 5.683564ms
Mar 15 22:28:51.575: INFO: Pod "pod-45933ee3-5105-4f43-aa1b-c5b11bddba57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00975249s
Mar 15 22:28:53.575: INFO: Pod "pod-45933ee3-5105-4f43-aa1b-c5b11bddba57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010300004s
STEP: Saw pod success 03/15/23 22:28:53.575
Mar 15 22:28:53.575: INFO: Pod "pod-45933ee3-5105-4f43-aa1b-c5b11bddba57" satisfied condition "Succeeded or Failed"
Mar 15 22:28:53.580: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-45933ee3-5105-4f43-aa1b-c5b11bddba57 container test-container: <nil>
STEP: delete the pod 03/15/23 22:28:53.586
Mar 15 22:28:53.598: INFO: Waiting for pod pod-45933ee3-5105-4f43-aa1b-c5b11bddba57 to disappear
Mar 15 22:28:53.602: INFO: Pod pod-45933ee3-5105-4f43-aa1b-c5b11bddba57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:28:53.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8789" for this suite. 03/15/23 22:28:53.606
------------------------------
• [4.075 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:28:49.536
    Mar 15 22:28:49.536: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 22:28:49.537
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:28:49.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:28:49.553
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 03/15/23 22:28:49.556
    Mar 15 22:28:49.565: INFO: Waiting up to 5m0s for pod "pod-45933ee3-5105-4f43-aa1b-c5b11bddba57" in namespace "emptydir-8789" to be "Succeeded or Failed"
    Mar 15 22:28:49.571: INFO: Pod "pod-45933ee3-5105-4f43-aa1b-c5b11bddba57": Phase="Pending", Reason="", readiness=false. Elapsed: 5.683564ms
    Mar 15 22:28:51.575: INFO: Pod "pod-45933ee3-5105-4f43-aa1b-c5b11bddba57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00975249s
    Mar 15 22:28:53.575: INFO: Pod "pod-45933ee3-5105-4f43-aa1b-c5b11bddba57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010300004s
    STEP: Saw pod success 03/15/23 22:28:53.575
    Mar 15 22:28:53.575: INFO: Pod "pod-45933ee3-5105-4f43-aa1b-c5b11bddba57" satisfied condition "Succeeded or Failed"
    Mar 15 22:28:53.580: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-45933ee3-5105-4f43-aa1b-c5b11bddba57 container test-container: <nil>
    STEP: delete the pod 03/15/23 22:28:53.586
    Mar 15 22:28:53.598: INFO: Waiting for pod pod-45933ee3-5105-4f43-aa1b-c5b11bddba57 to disappear
    Mar 15 22:28:53.602: INFO: Pod pod-45933ee3-5105-4f43-aa1b-c5b11bddba57 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:28:53.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8789" for this suite. 03/15/23 22:28:53.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:28:53.612
Mar 15 22:28:53.612: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename sched-preemption 03/15/23 22:28:53.614
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:28:53.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:28:53.63
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 15 22:28:53.644: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 15 22:29:53.679: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:29:53.687
Mar 15 22:29:53.688: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename sched-preemption-path 03/15/23 22:29:53.689
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:53.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:53.726
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Mar 15 22:29:53.749: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Mar 15 22:29:53.752: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Mar 15 22:29:53.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:29:53.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7575" for this suite. 03/15/23 22:29:53.892
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4870" for this suite. 03/15/23 22:29:53.901
------------------------------
• [SLOW TEST] [60.296 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:28:53.612
    Mar 15 22:28:53.612: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename sched-preemption 03/15/23 22:28:53.614
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:28:53.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:28:53.63
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 15 22:28:53.644: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 15 22:29:53.679: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:29:53.687
    Mar 15 22:29:53.688: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename sched-preemption-path 03/15/23 22:29:53.689
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:53.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:53.726
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Mar 15 22:29:53.749: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Mar 15 22:29:53.752: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:29:53.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:29:53.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7575" for this suite. 03/15/23 22:29:53.892
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4870" for this suite. 03/15/23 22:29:53.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:29:53.912
Mar 15 22:29:53.912: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:29:53.913
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:53.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:53.929
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:29:53.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5828" for this suite. 03/15/23 22:29:53.968
------------------------------
• [0.062 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:29:53.912
    Mar 15 22:29:53.912: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:29:53.913
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:53.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:53.929
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:29:53.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5828" for this suite. 03/15/23 22:29:53.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:29:53.977
Mar 15 22:29:53.977: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:29:53.978
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:54.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:54.013
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 03/15/23 22:29:54.019
Mar 15 22:29:54.020: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-5037 proxy --unix-socket=/tmp/kubectl-proxy-unix376073596/test'
STEP: retrieving proxy /api/ output 03/15/23 22:29:54.087
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:29:54.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5037" for this suite. 03/15/23 22:29:54.093
------------------------------
• [0.121 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:29:53.977
    Mar 15 22:29:53.977: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:29:53.978
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:54.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:54.013
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 03/15/23 22:29:54.019
    Mar 15 22:29:54.020: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-5037 proxy --unix-socket=/tmp/kubectl-proxy-unix376073596/test'
    STEP: retrieving proxy /api/ output 03/15/23 22:29:54.087
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:29:54.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5037" for this suite. 03/15/23 22:29:54.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:29:54.1
Mar 15 22:29:54.100: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:29:54.101
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:54.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:54.128
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 03/15/23 22:29:54.131
STEP: fetching the ConfigMap 03/15/23 22:29:54.135
STEP: patching the ConfigMap 03/15/23 22:29:54.137
STEP: listing all ConfigMaps in all namespaces with a label selector 03/15/23 22:29:54.141
STEP: deleting the ConfigMap by collection with a label selector 03/15/23 22:29:54.144
STEP: listing all ConfigMaps in test namespace 03/15/23 22:29:54.149
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:29:54.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2594" for this suite. 03/15/23 22:29:54.155
------------------------------
• [0.067 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:29:54.1
    Mar 15 22:29:54.100: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:29:54.101
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:54.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:54.128
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 03/15/23 22:29:54.131
    STEP: fetching the ConfigMap 03/15/23 22:29:54.135
    STEP: patching the ConfigMap 03/15/23 22:29:54.137
    STEP: listing all ConfigMaps in all namespaces with a label selector 03/15/23 22:29:54.141
    STEP: deleting the ConfigMap by collection with a label selector 03/15/23 22:29:54.144
    STEP: listing all ConfigMaps in test namespace 03/15/23 22:29:54.149
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:29:54.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2594" for this suite. 03/15/23 22:29:54.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:29:54.172
Mar 15 22:29:54.172: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename watch 03/15/23 22:29:54.173
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:54.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:54.199
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 03/15/23 22:29:54.203
STEP: modifying the configmap once 03/15/23 22:29:54.207
STEP: modifying the configmap a second time 03/15/23 22:29:54.213
STEP: deleting the configmap 03/15/23 22:29:54.219
STEP: creating a watch on configmaps from the resource version returned by the first update 03/15/23 22:29:54.225
STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/15/23 22:29:54.228
Mar 15 22:29:54.228: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6804  6b00b0f5-64f2-4a9f-80ff-59e27d116108 19572 0 2023-03-15 22:29:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-15 22:29:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 22:29:54.229: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6804  6b00b0f5-64f2-4a9f-80ff-59e27d116108 19573 0 2023-03-15 22:29:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-15 22:29:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 15 22:29:54.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6804" for this suite. 03/15/23 22:29:54.233
------------------------------
• [0.071 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:29:54.172
    Mar 15 22:29:54.172: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename watch 03/15/23 22:29:54.173
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:54.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:54.199
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 03/15/23 22:29:54.203
    STEP: modifying the configmap once 03/15/23 22:29:54.207
    STEP: modifying the configmap a second time 03/15/23 22:29:54.213
    STEP: deleting the configmap 03/15/23 22:29:54.219
    STEP: creating a watch on configmaps from the resource version returned by the first update 03/15/23 22:29:54.225
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/15/23 22:29:54.228
    Mar 15 22:29:54.228: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6804  6b00b0f5-64f2-4a9f-80ff-59e27d116108 19572 0 2023-03-15 22:29:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-15 22:29:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 22:29:54.229: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6804  6b00b0f5-64f2-4a9f-80ff-59e27d116108 19573 0 2023-03-15 22:29:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-15 22:29:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:29:54.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6804" for this suite. 03/15/23 22:29:54.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:29:54.249
Mar 15 22:29:54.249: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename var-expansion 03/15/23 22:29:54.25
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:54.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:54.272
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 03/15/23 22:29:54.275
Mar 15 22:29:54.285: INFO: Waiting up to 5m0s for pod "var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66" in namespace "var-expansion-6095" to be "Succeeded or Failed"
Mar 15 22:29:54.288: INFO: Pod "var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66": Phase="Pending", Reason="", readiness=false. Elapsed: 3.34109ms
Mar 15 22:29:56.292: INFO: Pod "var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007750667s
Mar 15 22:29:58.294: INFO: Pod "var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009014577s
STEP: Saw pod success 03/15/23 22:29:58.294
Mar 15 22:29:58.294: INFO: Pod "var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66" satisfied condition "Succeeded or Failed"
Mar 15 22:29:58.297: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66 container dapi-container: <nil>
STEP: delete the pod 03/15/23 22:29:58.305
Mar 15 22:29:58.315: INFO: Waiting for pod var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66 to disappear
Mar 15 22:29:58.319: INFO: Pod var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 15 22:29:58.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6095" for this suite. 03/15/23 22:29:58.322
------------------------------
• [4.079 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:29:54.249
    Mar 15 22:29:54.249: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename var-expansion 03/15/23 22:29:54.25
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:54.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:54.272
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 03/15/23 22:29:54.275
    Mar 15 22:29:54.285: INFO: Waiting up to 5m0s for pod "var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66" in namespace "var-expansion-6095" to be "Succeeded or Failed"
    Mar 15 22:29:54.288: INFO: Pod "var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66": Phase="Pending", Reason="", readiness=false. Elapsed: 3.34109ms
    Mar 15 22:29:56.292: INFO: Pod "var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007750667s
    Mar 15 22:29:58.294: INFO: Pod "var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009014577s
    STEP: Saw pod success 03/15/23 22:29:58.294
    Mar 15 22:29:58.294: INFO: Pod "var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66" satisfied condition "Succeeded or Failed"
    Mar 15 22:29:58.297: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66 container dapi-container: <nil>
    STEP: delete the pod 03/15/23 22:29:58.305
    Mar 15 22:29:58.315: INFO: Waiting for pod var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66 to disappear
    Mar 15 22:29:58.319: INFO: Pod var-expansion-b01186ef-f1c1-4bc4-b16d-336b9efe1a66 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:29:58.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6095" for this suite. 03/15/23 22:29:58.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:29:58.33
Mar 15 22:29:58.330: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename dns 03/15/23 22:29:58.331
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:58.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:58.348
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 03/15/23 22:29:58.351
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4638 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4638;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4638 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4638;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4638.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4638.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4638.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4638.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4638.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4638.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4638.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4638.svc;check="$$(dig +notcp +noall +answer +search 16.109.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.109.16_udp@PTR;check="$$(dig +tcp +noall +answer +search 16.109.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.109.16_tcp@PTR;sleep 1; done
 03/15/23 22:29:58.382
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4638 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4638;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4638 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4638;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4638.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4638.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4638.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4638.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4638.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4638.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4638.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4638.svc;check="$$(dig +notcp +noall +answer +search 16.109.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.109.16_udp@PTR;check="$$(dig +tcp +noall +answer +search 16.109.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.109.16_tcp@PTR;sleep 1; done
 03/15/23 22:29:58.382
STEP: creating a pod to probe DNS 03/15/23 22:29:58.382
STEP: submitting the pod to kubernetes 03/15/23 22:29:58.382
Mar 15 22:29:58.401: INFO: Waiting up to 15m0s for pod "dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4" in namespace "dns-4638" to be "running"
Mar 15 22:29:58.406: INFO: Pod "dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.433895ms
Mar 15 22:30:00.410: INFO: Pod "dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009485997s
Mar 15 22:30:00.410: INFO: Pod "dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4" satisfied condition "running"
STEP: retrieving the pod 03/15/23 22:30:00.41
STEP: looking for the results for each expected name from probers 03/15/23 22:30:00.413
Mar 15 22:30:00.421: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:00.424: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:00.427: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:00.432: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:00.438: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:00.442: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:00.473: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:00.476: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:00.479: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:00.483: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:00.486: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:00.489: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:00.509: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4638 wheezy_tcp@dns-test-service.dns-4638 wheezy_udp@dns-test-service.dns-4638.svc wheezy_tcp@dns-test-service.dns-4638.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

Mar 15 22:30:05.515: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:05.520: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:05.527: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:05.531: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:05.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:05.539: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:05.582: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:05.587: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:05.590: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:05.595: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:05.604: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:05.608: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:05.635: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4638 wheezy_tcp@dns-test-service.dns-4638 wheezy_udp@dns-test-service.dns-4638.svc wheezy_tcp@dns-test-service.dns-4638.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

Mar 15 22:30:10.514: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:10.519: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:10.523: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:10.529: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:10.534: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:10.538: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:10.581: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:10.587: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:10.594: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:10.600: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:10.604: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:10.612: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:10.663: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4638 wheezy_tcp@dns-test-service.dns-4638 wheezy_udp@dns-test-service.dns-4638.svc wheezy_tcp@dns-test-service.dns-4638.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

Mar 15 22:30:15.516: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:15.525: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:15.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:15.539: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:15.550: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:15.556: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:15.597: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:15.601: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:15.605: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:15.609: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:15.612: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:15.615: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:15.649: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4638 wheezy_tcp@dns-test-service.dns-4638 wheezy_udp@dns-test-service.dns-4638.svc wheezy_tcp@dns-test-service.dns-4638.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

Mar 15 22:30:20.520: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:20.524: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:20.528: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:20.531: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:20.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:20.538: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:20.567: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:20.570: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:20.575: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:20.578: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:20.581: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:20.584: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:20.613: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4638 wheezy_tcp@dns-test-service.dns-4638 wheezy_udp@dns-test-service.dns-4638.svc wheezy_tcp@dns-test-service.dns-4638.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

Mar 15 22:30:25.516: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:25.519: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:25.522: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:25.525: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:25.528: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:25.531: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:25.555: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:25.557: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:25.560: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:25.563: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:25.567: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:25.569: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:25.615: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4638 wheezy_tcp@dns-test-service.dns-4638 wheezy_udp@dns-test-service.dns-4638.svc wheezy_tcp@dns-test-service.dns-4638.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

Mar 15 22:30:30.571: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:30.576: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:30.580: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:30.583: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:30.588: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:30.591: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
Mar 15 22:30:30.619: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

Mar 15 22:30:35.629: INFO: DNS probes using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 succeeded

STEP: deleting the pod 03/15/23 22:30:35.629
STEP: deleting the test service 03/15/23 22:30:35.649
STEP: deleting the test headless service 03/15/23 22:30:35.704
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 15 22:30:35.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4638" for this suite. 03/15/23 22:30:35.811
------------------------------
• [SLOW TEST] [37.494 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:29:58.33
    Mar 15 22:29:58.330: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename dns 03/15/23 22:29:58.331
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:29:58.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:29:58.348
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 03/15/23 22:29:58.351
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4638 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4638;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4638 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4638;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4638.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4638.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4638.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4638.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4638.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4638.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4638.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4638.svc;check="$$(dig +notcp +noall +answer +search 16.109.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.109.16_udp@PTR;check="$$(dig +tcp +noall +answer +search 16.109.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.109.16_tcp@PTR;sleep 1; done
     03/15/23 22:29:58.382
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4638 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4638;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4638 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4638;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4638.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4638.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4638.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4638.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4638.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4638.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4638.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4638.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4638.svc;check="$$(dig +notcp +noall +answer +search 16.109.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.109.16_udp@PTR;check="$$(dig +tcp +noall +answer +search 16.109.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.109.16_tcp@PTR;sleep 1; done
     03/15/23 22:29:58.382
    STEP: creating a pod to probe DNS 03/15/23 22:29:58.382
    STEP: submitting the pod to kubernetes 03/15/23 22:29:58.382
    Mar 15 22:29:58.401: INFO: Waiting up to 15m0s for pod "dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4" in namespace "dns-4638" to be "running"
    Mar 15 22:29:58.406: INFO: Pod "dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.433895ms
    Mar 15 22:30:00.410: INFO: Pod "dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009485997s
    Mar 15 22:30:00.410: INFO: Pod "dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4" satisfied condition "running"
    STEP: retrieving the pod 03/15/23 22:30:00.41
    STEP: looking for the results for each expected name from probers 03/15/23 22:30:00.413
    Mar 15 22:30:00.421: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:00.424: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:00.427: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:00.432: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:00.438: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:00.442: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:00.473: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:00.476: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:00.479: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:00.483: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:00.486: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:00.489: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:00.509: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4638 wheezy_tcp@dns-test-service.dns-4638 wheezy_udp@dns-test-service.dns-4638.svc wheezy_tcp@dns-test-service.dns-4638.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

    Mar 15 22:30:05.515: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:05.520: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:05.527: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:05.531: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:05.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:05.539: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:05.582: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:05.587: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:05.590: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:05.595: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:05.604: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:05.608: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:05.635: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4638 wheezy_tcp@dns-test-service.dns-4638 wheezy_udp@dns-test-service.dns-4638.svc wheezy_tcp@dns-test-service.dns-4638.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

    Mar 15 22:30:10.514: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:10.519: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:10.523: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:10.529: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:10.534: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:10.538: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:10.581: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:10.587: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:10.594: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:10.600: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:10.604: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:10.612: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:10.663: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4638 wheezy_tcp@dns-test-service.dns-4638 wheezy_udp@dns-test-service.dns-4638.svc wheezy_tcp@dns-test-service.dns-4638.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

    Mar 15 22:30:15.516: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:15.525: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:15.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:15.539: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:15.550: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:15.556: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:15.597: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:15.601: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:15.605: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:15.609: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:15.612: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:15.615: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:15.649: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4638 wheezy_tcp@dns-test-service.dns-4638 wheezy_udp@dns-test-service.dns-4638.svc wheezy_tcp@dns-test-service.dns-4638.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

    Mar 15 22:30:20.520: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:20.524: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:20.528: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:20.531: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:20.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:20.538: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:20.567: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:20.570: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:20.575: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:20.578: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:20.581: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:20.584: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:20.613: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4638 wheezy_tcp@dns-test-service.dns-4638 wheezy_udp@dns-test-service.dns-4638.svc wheezy_tcp@dns-test-service.dns-4638.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

    Mar 15 22:30:25.516: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:25.519: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:25.522: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:25.525: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:25.528: INFO: Unable to read wheezy_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:25.531: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:25.555: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:25.557: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:25.560: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:25.563: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:25.567: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:25.569: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:25.615: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4638 wheezy_tcp@dns-test-service.dns-4638 wheezy_udp@dns-test-service.dns-4638.svc wheezy_tcp@dns-test-service.dns-4638.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

    Mar 15 22:30:30.571: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:30.576: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:30.580: INFO: Unable to read jessie_udp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:30.583: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638 from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:30.588: INFO: Unable to read jessie_udp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:30.591: INFO: Unable to read jessie_tcp@dns-test-service.dns-4638.svc from pod dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4: the server could not find the requested resource (get pods dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4)
    Mar 15 22:30:30.619: INFO: Lookups using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4638 jessie_tcp@dns-test-service.dns-4638 jessie_udp@dns-test-service.dns-4638.svc jessie_tcp@dns-test-service.dns-4638.svc]

    Mar 15 22:30:35.629: INFO: DNS probes using dns-4638/dns-test-923ce26a-9d7c-48d5-9a50-060abc72d4a4 succeeded

    STEP: deleting the pod 03/15/23 22:30:35.629
    STEP: deleting the test service 03/15/23 22:30:35.649
    STEP: deleting the test headless service 03/15/23 22:30:35.704
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:30:35.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4638" for this suite. 03/15/23 22:30:35.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:30:35.828
Mar 15 22:30:35.828: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename runtimeclass 03/15/23 22:30:35.829
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:30:35.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:30:35.87
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-9369-delete-me 03/15/23 22:30:35.88
STEP: Waiting for the RuntimeClass to disappear 03/15/23 22:30:35.886
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 15 22:30:35.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9369" for this suite. 03/15/23 22:30:35.906
------------------------------
• [0.086 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:30:35.828
    Mar 15 22:30:35.828: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename runtimeclass 03/15/23 22:30:35.829
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:30:35.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:30:35.87
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-9369-delete-me 03/15/23 22:30:35.88
    STEP: Waiting for the RuntimeClass to disappear 03/15/23 22:30:35.886
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:30:35.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9369" for this suite. 03/15/23 22:30:35.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:30:35.921
Mar 15 22:30:35.922: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename replication-controller 03/15/23 22:30:35.923
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:30:35.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:30:35.941
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 03/15/23 22:30:35.946
STEP: When the matched label of one of its pods change 03/15/23 22:30:35.951
Mar 15 22:30:35.955: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 15 22:30:40.967: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 03/15/23 22:30:40.986
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 15 22:30:41.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8050" for this suite. 03/15/23 22:30:41.017
------------------------------
• [SLOW TEST] [5.122 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:30:35.921
    Mar 15 22:30:35.922: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename replication-controller 03/15/23 22:30:35.923
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:30:35.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:30:35.941
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 03/15/23 22:30:35.946
    STEP: When the matched label of one of its pods change 03/15/23 22:30:35.951
    Mar 15 22:30:35.955: INFO: Pod name pod-release: Found 0 pods out of 1
    Mar 15 22:30:40.967: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/15/23 22:30:40.986
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:30:41.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8050" for this suite. 03/15/23 22:30:41.017
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:30:41.044
Mar 15 22:30:41.044: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename sched-pred 03/15/23 22:30:41.046
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:30:41.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:30:41.085
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 15 22:30:41.090: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 15 22:30:41.098: INFO: Waiting for terminating namespaces to be deleted...
Mar 15 22:30:41.101: INFO: 
Logging pods the apiserver thinks is on node i-077ee0eb7ec5a02aa before test
Mar 15 22:30:41.110: INFO: etcd1 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.110: INFO: 	Container etcd1 ready: true, restart count 0
Mar 15 22:30:41.110: INFO: cilium-hvkmm from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.110: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 15 22:30:41.110: INFO: coredns-85dfcfb87f-p4x25 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.110: INFO: 	Container coredns ready: true, restart count 0
Mar 15 22:30:41.110: INFO: coredns-autoscaler-7cb5c5b969-tn76l from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.110: INFO: 	Container autoscaler ready: true, restart count 0
Mar 15 22:30:41.110: INFO: ebs-csi-node-9zwns from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
Mar 15 22:30:41.110: INFO: 	Container ebs-plugin ready: true, restart count 0
Mar 15 22:30:41.110: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 22:30:41.110: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 22:30:41.110: INFO: kube-proxy-i-077ee0eb7ec5a02aa from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.110: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 15 22:30:41.110: INFO: metrics-server-79b7f8b6d9-g4qd4 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.110: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 22:30:41.110: INFO: sonobuoy-e2e-job-cc297e458b6c49af from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 22:30:41.110: INFO: 	Container e2e ready: true, restart count 0
Mar 15 22:30:41.110: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 22:30:41.110: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-hkt9j from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 22:30:41.110: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 22:30:41.110: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 22:30:41.110: INFO: 
Logging pods the apiserver thinks is on node i-0baafb3f4e7bf826e before test
Mar 15 22:30:41.116: INFO: csi-mockplugin-85d5674684-nwzd7 from default started at 2023-03-15 21:42:36 +0000 UTC (7 container statuses recorded)
Mar 15 22:30:41.117: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 15 22:30:41.117: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 15 22:30:41.117: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 15 22:30:41.117: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 15 22:30:41.117: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 22:30:41.117: INFO: 	Container mock-driver ready: true, restart count 0
Mar 15 22:30:41.117: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 22:30:41.117: INFO: etcd2 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.117: INFO: 	Container etcd2 ready: true, restart count 0
Mar 15 22:30:41.117: INFO: web-server from default started at 2023-03-15 21:42:51 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.117: INFO: 	Container web-server ready: true, restart count 0
Mar 15 22:30:41.117: INFO: cilium-8q84b from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.117: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 15 22:30:41.117: INFO: coredns-85dfcfb87f-9x682 from kube-system started at 2023-03-15 22:21:30 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.117: INFO: 	Container coredns ready: true, restart count 0
Mar 15 22:30:41.117: INFO: ebs-csi-node-8vkhc from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
Mar 15 22:30:41.117: INFO: 	Container ebs-plugin ready: true, restart count 0
Mar 15 22:30:41.118: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 22:30:41.118: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 22:30:41.118: INFO: kube-proxy-i-0baafb3f4e7bf826e from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.118: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 15 22:30:41.118: INFO: metrics-server-79b7f8b6d9-dgk5h from kube-system started at 2023-03-15 21:41:14 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.118: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 22:30:41.118: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-49vvc from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 22:30:41.118: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 22:30:41.118: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 22:30:41.118: INFO: 
Logging pods the apiserver thinks is on node i-0faaf83f00b43c88c before test
Mar 15 22:30:41.126: INFO: cilium-nht5t from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.126: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 15 22:30:41.126: INFO: ebs-csi-node-kkqjj from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
Mar 15 22:30:41.126: INFO: 	Container ebs-plugin ready: true, restart count 0
Mar 15 22:30:41.126: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 22:30:41.126: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 22:30:41.126: INFO: kube-proxy-i-0faaf83f00b43c88c from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.127: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 15 22:30:41.127: INFO: pod-release-h4mwj from replication-controller-8050 started at 2023-03-15 22:30:41 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.127: INFO: 	Container pod-release ready: false, restart count 0
Mar 15 22:30:41.127: INFO: pod-release-wmkmr from replication-controller-8050 started at 2023-03-15 22:30:35 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.127: INFO: 	Container pod-release ready: true, restart count 0
Mar 15 22:30:41.127: INFO: sonobuoy from sonobuoy started at 2023-03-15 21:43:05 +0000 UTC (1 container statuses recorded)
Mar 15 22:30:41.127: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 15 22:30:41.127: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-dc6rq from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 22:30:41.127: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 22:30:41.127: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 03/15/23 22:30:41.127
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.174cb85620687806], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] 03/15/23 22:30:41.162
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:30:42.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3691" for this suite. 03/15/23 22:30:42.163
------------------------------
• [1.124 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:30:41.044
    Mar 15 22:30:41.044: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename sched-pred 03/15/23 22:30:41.046
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:30:41.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:30:41.085
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 15 22:30:41.090: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 15 22:30:41.098: INFO: Waiting for terminating namespaces to be deleted...
    Mar 15 22:30:41.101: INFO: 
    Logging pods the apiserver thinks is on node i-077ee0eb7ec5a02aa before test
    Mar 15 22:30:41.110: INFO: etcd1 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.110: INFO: 	Container etcd1 ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: cilium-hvkmm from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.110: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: coredns-85dfcfb87f-p4x25 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.110: INFO: 	Container coredns ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: coredns-autoscaler-7cb5c5b969-tn76l from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.110: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: ebs-csi-node-9zwns from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
    Mar 15 22:30:41.110: INFO: 	Container ebs-plugin ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: kube-proxy-i-077ee0eb7ec5a02aa from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.110: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: metrics-server-79b7f8b6d9-g4qd4 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.110: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: sonobuoy-e2e-job-cc297e458b6c49af from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 22:30:41.110: INFO: 	Container e2e ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-hkt9j from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 22:30:41.110: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 15 22:30:41.110: INFO: 
    Logging pods the apiserver thinks is on node i-0baafb3f4e7bf826e before test
    Mar 15 22:30:41.116: INFO: csi-mockplugin-85d5674684-nwzd7 from default started at 2023-03-15 21:42:36 +0000 UTC (7 container statuses recorded)
    Mar 15 22:30:41.117: INFO: 	Container csi-attacher ready: true, restart count 0
    Mar 15 22:30:41.117: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar 15 22:30:41.117: INFO: 	Container csi-resizer ready: true, restart count 0
    Mar 15 22:30:41.117: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Mar 15 22:30:41.117: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 22:30:41.117: INFO: 	Container mock-driver ready: true, restart count 0
    Mar 15 22:30:41.117: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 22:30:41.117: INFO: etcd2 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.117: INFO: 	Container etcd2 ready: true, restart count 0
    Mar 15 22:30:41.117: INFO: web-server from default started at 2023-03-15 21:42:51 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.117: INFO: 	Container web-server ready: true, restart count 0
    Mar 15 22:30:41.117: INFO: cilium-8q84b from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.117: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 15 22:30:41.117: INFO: coredns-85dfcfb87f-9x682 from kube-system started at 2023-03-15 22:21:30 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.117: INFO: 	Container coredns ready: true, restart count 0
    Mar 15 22:30:41.117: INFO: ebs-csi-node-8vkhc from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
    Mar 15 22:30:41.117: INFO: 	Container ebs-plugin ready: true, restart count 0
    Mar 15 22:30:41.118: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 22:30:41.118: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 22:30:41.118: INFO: kube-proxy-i-0baafb3f4e7bf826e from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.118: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 15 22:30:41.118: INFO: metrics-server-79b7f8b6d9-dgk5h from kube-system started at 2023-03-15 21:41:14 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.118: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 15 22:30:41.118: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-49vvc from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 22:30:41.118: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 22:30:41.118: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 15 22:30:41.118: INFO: 
    Logging pods the apiserver thinks is on node i-0faaf83f00b43c88c before test
    Mar 15 22:30:41.126: INFO: cilium-nht5t from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.126: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 15 22:30:41.126: INFO: ebs-csi-node-kkqjj from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
    Mar 15 22:30:41.126: INFO: 	Container ebs-plugin ready: true, restart count 0
    Mar 15 22:30:41.126: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 22:30:41.126: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 22:30:41.126: INFO: kube-proxy-i-0faaf83f00b43c88c from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.127: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 15 22:30:41.127: INFO: pod-release-h4mwj from replication-controller-8050 started at 2023-03-15 22:30:41 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.127: INFO: 	Container pod-release ready: false, restart count 0
    Mar 15 22:30:41.127: INFO: pod-release-wmkmr from replication-controller-8050 started at 2023-03-15 22:30:35 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.127: INFO: 	Container pod-release ready: true, restart count 0
    Mar 15 22:30:41.127: INFO: sonobuoy from sonobuoy started at 2023-03-15 21:43:05 +0000 UTC (1 container statuses recorded)
    Mar 15 22:30:41.127: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 15 22:30:41.127: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-dc6rq from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 22:30:41.127: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 22:30:41.127: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 03/15/23 22:30:41.127
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.174cb85620687806], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] 03/15/23 22:30:41.162
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:30:42.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3691" for this suite. 03/15/23 22:30:42.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:30:42.171
Mar 15 22:30:42.171: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename crd-watch 03/15/23 22:30:42.172
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:30:42.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:30:42.189
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Mar 15 22:30:42.193: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Creating first CR  03/15/23 22:30:44.749
Mar 15 22:30:44.754: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-15T22:30:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-15T22:30:44Z]] name:name1 resourceVersion:19883 uid:b33357e4-6374-440b-86c1-68b89cf0949d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 03/15/23 22:30:54.756
Mar 15 22:30:54.761: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-15T22:30:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-15T22:30:54Z]] name:name2 resourceVersion:19937 uid:c70d97ed-e774-4ee4-9c66-ccb5d22d0f59] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 03/15/23 22:31:04.761
Mar 15 22:31:04.769: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-15T22:30:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-15T22:31:04Z]] name:name1 resourceVersion:19973 uid:b33357e4-6374-440b-86c1-68b89cf0949d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 03/15/23 22:31:14.77
Mar 15 22:31:14.776: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-15T22:30:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-15T22:31:14Z]] name:name2 resourceVersion:20009 uid:c70d97ed-e774-4ee4-9c66-ccb5d22d0f59] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 03/15/23 22:31:24.776
Mar 15 22:31:24.791: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-15T22:30:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-15T22:31:04Z]] name:name1 resourceVersion:20045 uid:b33357e4-6374-440b-86c1-68b89cf0949d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 03/15/23 22:31:34.793
Mar 15 22:31:34.801: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-15T22:30:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-15T22:31:14Z]] name:name2 resourceVersion:20081 uid:c70d97ed-e774-4ee4-9c66-ccb5d22d0f59] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:31:45.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-6181" for this suite. 03/15/23 22:31:45.323
------------------------------
• [SLOW TEST] [63.157 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:30:42.171
    Mar 15 22:30:42.171: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename crd-watch 03/15/23 22:30:42.172
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:30:42.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:30:42.189
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Mar 15 22:30:42.193: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Creating first CR  03/15/23 22:30:44.749
    Mar 15 22:30:44.754: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-15T22:30:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-15T22:30:44Z]] name:name1 resourceVersion:19883 uid:b33357e4-6374-440b-86c1-68b89cf0949d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 03/15/23 22:30:54.756
    Mar 15 22:30:54.761: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-15T22:30:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-15T22:30:54Z]] name:name2 resourceVersion:19937 uid:c70d97ed-e774-4ee4-9c66-ccb5d22d0f59] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 03/15/23 22:31:04.761
    Mar 15 22:31:04.769: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-15T22:30:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-15T22:31:04Z]] name:name1 resourceVersion:19973 uid:b33357e4-6374-440b-86c1-68b89cf0949d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 03/15/23 22:31:14.77
    Mar 15 22:31:14.776: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-15T22:30:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-15T22:31:14Z]] name:name2 resourceVersion:20009 uid:c70d97ed-e774-4ee4-9c66-ccb5d22d0f59] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 03/15/23 22:31:24.776
    Mar 15 22:31:24.791: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-15T22:30:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-15T22:31:04Z]] name:name1 resourceVersion:20045 uid:b33357e4-6374-440b-86c1-68b89cf0949d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 03/15/23 22:31:34.793
    Mar 15 22:31:34.801: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-15T22:30:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-15T22:31:14Z]] name:name2 resourceVersion:20081 uid:c70d97ed-e774-4ee4-9c66-ccb5d22d0f59] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:31:45.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-6181" for this suite. 03/15/23 22:31:45.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:31:45.332
Mar 15 22:31:45.332: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:31:45.333
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:31:45.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:31:45.353
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 03/15/23 22:31:45.356
Mar 15 22:31:45.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-8902 create -f -'
Mar 15 22:31:46.466: INFO: stderr: ""
Mar 15 22:31:46.466: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/15/23 22:31:46.466
Mar 15 22:31:47.470: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 22:31:47.470: INFO: Found 0 / 1
Mar 15 22:31:48.469: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 22:31:48.469: INFO: Found 1 / 1
Mar 15 22:31:48.469: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 03/15/23 22:31:48.469
Mar 15 22:31:48.476: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 22:31:48.476: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 15 22:31:48.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-8902 patch pod agnhost-primary-t9r57 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 15 22:31:48.602: INFO: stderr: ""
Mar 15 22:31:48.602: INFO: stdout: "pod/agnhost-primary-t9r57 patched\n"
STEP: checking annotations 03/15/23 22:31:48.602
Mar 15 22:31:48.605: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 22:31:48.605: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:31:48.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8902" for this suite. 03/15/23 22:31:48.612
------------------------------
• [3.288 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:31:45.332
    Mar 15 22:31:45.332: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:31:45.333
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:31:45.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:31:45.353
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 03/15/23 22:31:45.356
    Mar 15 22:31:45.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-8902 create -f -'
    Mar 15 22:31:46.466: INFO: stderr: ""
    Mar 15 22:31:46.466: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/15/23 22:31:46.466
    Mar 15 22:31:47.470: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 15 22:31:47.470: INFO: Found 0 / 1
    Mar 15 22:31:48.469: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 15 22:31:48.469: INFO: Found 1 / 1
    Mar 15 22:31:48.469: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 03/15/23 22:31:48.469
    Mar 15 22:31:48.476: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 15 22:31:48.476: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 15 22:31:48.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-8902 patch pod agnhost-primary-t9r57 -p {"metadata":{"annotations":{"x":"y"}}}'
    Mar 15 22:31:48.602: INFO: stderr: ""
    Mar 15 22:31:48.602: INFO: stdout: "pod/agnhost-primary-t9r57 patched\n"
    STEP: checking annotations 03/15/23 22:31:48.602
    Mar 15 22:31:48.605: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 15 22:31:48.605: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:31:48.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8902" for this suite. 03/15/23 22:31:48.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:31:48.62
Mar 15 22:31:48.621: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:31:48.621
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:31:48.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:31:48.64
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 03/15/23 22:31:48.645
Mar 15 22:31:48.654: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5" in namespace "projected-1161" to be "Succeeded or Failed"
Mar 15 22:31:48.659: INFO: Pod "downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.857715ms
Mar 15 22:31:50.663: INFO: Pod "downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009591896s
Mar 15 22:31:52.665: INFO: Pod "downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011489047s
STEP: Saw pod success 03/15/23 22:31:52.665
Mar 15 22:31:52.665: INFO: Pod "downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5" satisfied condition "Succeeded or Failed"
Mar 15 22:31:52.668: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5 container client-container: <nil>
STEP: delete the pod 03/15/23 22:31:52.681
Mar 15 22:31:52.704: INFO: Waiting for pod downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5 to disappear
Mar 15 22:31:52.709: INFO: Pod downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 15 22:31:52.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1161" for this suite. 03/15/23 22:31:52.715
------------------------------
• [4.117 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:31:48.62
    Mar 15 22:31:48.621: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:31:48.621
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:31:48.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:31:48.64
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 03/15/23 22:31:48.645
    Mar 15 22:31:48.654: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5" in namespace "projected-1161" to be "Succeeded or Failed"
    Mar 15 22:31:48.659: INFO: Pod "downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.857715ms
    Mar 15 22:31:50.663: INFO: Pod "downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009591896s
    Mar 15 22:31:52.665: INFO: Pod "downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011489047s
    STEP: Saw pod success 03/15/23 22:31:52.665
    Mar 15 22:31:52.665: INFO: Pod "downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5" satisfied condition "Succeeded or Failed"
    Mar 15 22:31:52.668: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5 container client-container: <nil>
    STEP: delete the pod 03/15/23 22:31:52.681
    Mar 15 22:31:52.704: INFO: Waiting for pod downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5 to disappear
    Mar 15 22:31:52.709: INFO: Pod downwardapi-volume-83122543-4e7b-46d8-8fd2-51ee45fc77a5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:31:52.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1161" for this suite. 03/15/23 22:31:52.715
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:31:52.739
Mar 15 22:31:52.739: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 22:31:52.74
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:31:52.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:31:52.764
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 03/15/23 22:31:52.767
Mar 15 22:31:52.776: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7" in namespace "downward-api-2777" to be "Succeeded or Failed"
Mar 15 22:31:52.783: INFO: Pod "downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.772615ms
Mar 15 22:31:54.787: INFO: Pod "downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01135476s
Mar 15 22:31:56.788: INFO: Pod "downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012294528s
STEP: Saw pod success 03/15/23 22:31:56.788
Mar 15 22:31:56.788: INFO: Pod "downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7" satisfied condition "Succeeded or Failed"
Mar 15 22:31:56.791: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7 container client-container: <nil>
STEP: delete the pod 03/15/23 22:31:56.797
Mar 15 22:31:56.809: INFO: Waiting for pod downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7 to disappear
Mar 15 22:31:56.812: INFO: Pod downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 15 22:31:56.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2777" for this suite. 03/15/23 22:31:56.819
------------------------------
• [4.086 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:31:52.739
    Mar 15 22:31:52.739: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 22:31:52.74
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:31:52.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:31:52.764
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 03/15/23 22:31:52.767
    Mar 15 22:31:52.776: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7" in namespace "downward-api-2777" to be "Succeeded or Failed"
    Mar 15 22:31:52.783: INFO: Pod "downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.772615ms
    Mar 15 22:31:54.787: INFO: Pod "downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01135476s
    Mar 15 22:31:56.788: INFO: Pod "downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012294528s
    STEP: Saw pod success 03/15/23 22:31:56.788
    Mar 15 22:31:56.788: INFO: Pod "downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7" satisfied condition "Succeeded or Failed"
    Mar 15 22:31:56.791: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7 container client-container: <nil>
    STEP: delete the pod 03/15/23 22:31:56.797
    Mar 15 22:31:56.809: INFO: Waiting for pod downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7 to disappear
    Mar 15 22:31:56.812: INFO: Pod downwardapi-volume-3ef26084-db20-4485-ae00-99c0a60f70c7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:31:56.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2777" for this suite. 03/15/23 22:31:56.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:31:56.831
Mar 15 22:31:56.831: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir-wrapper 03/15/23 22:31:56.833
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:31:56.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:31:56.851
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 03/15/23 22:31:56.855
STEP: Creating RC which spawns configmap-volume pods 03/15/23 22:31:57.095
Mar 15 22:31:57.214: INFO: Pod name wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317: Found 3 pods out of 5
Mar 15 22:32:02.252: INFO: Pod name wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/15/23 22:32:02.252
Mar 15 22:32:02.253: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:02.264: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.459241ms
Mar 15 22:32:04.270: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01775205s
Mar 15 22:32:06.279: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026552528s
Mar 15 22:32:08.269: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016305049s
Mar 15 22:32:10.279: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026421826s
Mar 15 22:32:12.272: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6": Phase="Running", Reason="", readiness=true. Elapsed: 10.019639027s
Mar 15 22:32:12.272: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6" satisfied condition "running"
Mar 15 22:32:12.272: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-67gkc" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:12.276: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-67gkc": Phase="Running", Reason="", readiness=true. Elapsed: 4.187234ms
Mar 15 22:32:12.276: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-67gkc" satisfied condition "running"
Mar 15 22:32:12.276: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-fvhqd" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:12.280: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-fvhqd": Phase="Running", Reason="", readiness=true. Elapsed: 3.315329ms
Mar 15 22:32:12.280: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-fvhqd" satisfied condition "running"
Mar 15 22:32:12.280: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-qj96n" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:12.289: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-qj96n": Phase="Running", Reason="", readiness=true. Elapsed: 9.174617ms
Mar 15 22:32:12.289: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-qj96n" satisfied condition "running"
Mar 15 22:32:12.289: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-tz4s4" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:12.294: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-tz4s4": Phase="Running", Reason="", readiness=true. Elapsed: 5.266693ms
Mar 15 22:32:12.294: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-tz4s4" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317 in namespace emptydir-wrapper-9363, will wait for the garbage collector to delete the pods 03/15/23 22:32:12.294
Mar 15 22:32:12.355: INFO: Deleting ReplicationController wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317 took: 5.747853ms
Mar 15 22:32:12.556: INFO: Terminating ReplicationController wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317 pods took: 200.199857ms
STEP: Creating RC which spawns configmap-volume pods 03/15/23 22:32:16.361
Mar 15 22:32:16.377: INFO: Pod name wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4: Found 0 pods out of 5
Mar 15 22:32:21.386: INFO: Pod name wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/15/23 22:32:21.386
Mar 15 22:32:21.386: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:21.389: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.987675ms
Mar 15 22:32:23.393: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007003975s
Mar 15 22:32:25.395: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00932841s
Mar 15 22:32:27.393: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00702633s
Mar 15 22:32:29.393: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0075506s
Mar 15 22:32:31.394: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008159601s
Mar 15 22:32:33.393: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Running", Reason="", readiness=true. Elapsed: 12.00666551s
Mar 15 22:32:33.393: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76" satisfied condition "running"
Mar 15 22:32:33.393: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-fgjwg" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:33.396: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-fgjwg": Phase="Running", Reason="", readiness=true. Elapsed: 3.200382ms
Mar 15 22:32:33.396: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-fgjwg" satisfied condition "running"
Mar 15 22:32:33.396: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-fstj6" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:33.399: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-fstj6": Phase="Running", Reason="", readiness=true. Elapsed: 2.783614ms
Mar 15 22:32:33.399: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-fstj6" satisfied condition "running"
Mar 15 22:32:33.399: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-wr9cx" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:33.402: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-wr9cx": Phase="Running", Reason="", readiness=true. Elapsed: 2.70918ms
Mar 15 22:32:33.402: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-wr9cx" satisfied condition "running"
Mar 15 22:32:33.402: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-zf82r" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:33.406: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-zf82r": Phase="Running", Reason="", readiness=true. Elapsed: 3.748901ms
Mar 15 22:32:33.406: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-zf82r" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4 in namespace emptydir-wrapper-9363, will wait for the garbage collector to delete the pods 03/15/23 22:32:33.406
Mar 15 22:32:33.467: INFO: Deleting ReplicationController wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4 took: 6.57451ms
Mar 15 22:32:33.568: INFO: Terminating ReplicationController wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4 pods took: 101.182668ms
STEP: Creating RC which spawns configmap-volume pods 03/15/23 22:32:36.373
Mar 15 22:32:36.391: INFO: Pod name wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df: Found 0 pods out of 5
Mar 15 22:32:41.402: INFO: Pod name wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/15/23 22:32:41.402
Mar 15 22:32:41.403: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:41.406: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.311538ms
Mar 15 22:32:43.410: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007335583s
Mar 15 22:32:45.411: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008349264s
Mar 15 22:32:47.410: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007697036s
Mar 15 22:32:49.414: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011753845s
Mar 15 22:32:51.421: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018182303s
Mar 15 22:32:53.412: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Running", Reason="", readiness=true. Elapsed: 12.009353008s
Mar 15 22:32:53.412: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp" satisfied condition "running"
Mar 15 22:32:53.412: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-4rrlz" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:53.416: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-4rrlz": Phase="Running", Reason="", readiness=true. Elapsed: 4.265341ms
Mar 15 22:32:53.416: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-4rrlz" satisfied condition "running"
Mar 15 22:32:53.416: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-9dcjf" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:53.420: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-9dcjf": Phase="Running", Reason="", readiness=true. Elapsed: 3.311594ms
Mar 15 22:32:53.420: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-9dcjf" satisfied condition "running"
Mar 15 22:32:53.420: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-k7qch" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:53.423: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-k7qch": Phase="Running", Reason="", readiness=true. Elapsed: 3.731216ms
Mar 15 22:32:53.424: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-k7qch" satisfied condition "running"
Mar 15 22:32:53.424: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-q5btb" in namespace "emptydir-wrapper-9363" to be "running"
Mar 15 22:32:53.428: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-q5btb": Phase="Running", Reason="", readiness=true. Elapsed: 4.55474ms
Mar 15 22:32:53.428: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-q5btb" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df in namespace emptydir-wrapper-9363, will wait for the garbage collector to delete the pods 03/15/23 22:32:53.428
Mar 15 22:32:53.490: INFO: Deleting ReplicationController wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df took: 6.989754ms
Mar 15 22:32:53.591: INFO: Terminating ReplicationController wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df pods took: 101.186872ms
STEP: Cleaning up the configMaps 03/15/23 22:32:55.891
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:32:56.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-9363" for this suite. 03/15/23 22:32:56.21
------------------------------
• [SLOW TEST] [59.387 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:31:56.831
    Mar 15 22:31:56.831: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir-wrapper 03/15/23 22:31:56.833
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:31:56.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:31:56.851
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 03/15/23 22:31:56.855
    STEP: Creating RC which spawns configmap-volume pods 03/15/23 22:31:57.095
    Mar 15 22:31:57.214: INFO: Pod name wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317: Found 3 pods out of 5
    Mar 15 22:32:02.252: INFO: Pod name wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/15/23 22:32:02.252
    Mar 15 22:32:02.253: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:02.264: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.459241ms
    Mar 15 22:32:04.270: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01775205s
    Mar 15 22:32:06.279: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026552528s
    Mar 15 22:32:08.269: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016305049s
    Mar 15 22:32:10.279: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026421826s
    Mar 15 22:32:12.272: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6": Phase="Running", Reason="", readiness=true. Elapsed: 10.019639027s
    Mar 15 22:32:12.272: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-455m6" satisfied condition "running"
    Mar 15 22:32:12.272: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-67gkc" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:12.276: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-67gkc": Phase="Running", Reason="", readiness=true. Elapsed: 4.187234ms
    Mar 15 22:32:12.276: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-67gkc" satisfied condition "running"
    Mar 15 22:32:12.276: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-fvhqd" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:12.280: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-fvhqd": Phase="Running", Reason="", readiness=true. Elapsed: 3.315329ms
    Mar 15 22:32:12.280: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-fvhqd" satisfied condition "running"
    Mar 15 22:32:12.280: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-qj96n" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:12.289: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-qj96n": Phase="Running", Reason="", readiness=true. Elapsed: 9.174617ms
    Mar 15 22:32:12.289: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-qj96n" satisfied condition "running"
    Mar 15 22:32:12.289: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-tz4s4" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:12.294: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-tz4s4": Phase="Running", Reason="", readiness=true. Elapsed: 5.266693ms
    Mar 15 22:32:12.294: INFO: Pod "wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317-tz4s4" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317 in namespace emptydir-wrapper-9363, will wait for the garbage collector to delete the pods 03/15/23 22:32:12.294
    Mar 15 22:32:12.355: INFO: Deleting ReplicationController wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317 took: 5.747853ms
    Mar 15 22:32:12.556: INFO: Terminating ReplicationController wrapped-volume-race-be983bfe-84bb-4b2d-a09c-80201b485317 pods took: 200.199857ms
    STEP: Creating RC which spawns configmap-volume pods 03/15/23 22:32:16.361
    Mar 15 22:32:16.377: INFO: Pod name wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4: Found 0 pods out of 5
    Mar 15 22:32:21.386: INFO: Pod name wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/15/23 22:32:21.386
    Mar 15 22:32:21.386: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:21.389: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.987675ms
    Mar 15 22:32:23.393: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007003975s
    Mar 15 22:32:25.395: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00932841s
    Mar 15 22:32:27.393: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00702633s
    Mar 15 22:32:29.393: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0075506s
    Mar 15 22:32:31.394: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008159601s
    Mar 15 22:32:33.393: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76": Phase="Running", Reason="", readiness=true. Elapsed: 12.00666551s
    Mar 15 22:32:33.393: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-4qw76" satisfied condition "running"
    Mar 15 22:32:33.393: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-fgjwg" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:33.396: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-fgjwg": Phase="Running", Reason="", readiness=true. Elapsed: 3.200382ms
    Mar 15 22:32:33.396: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-fgjwg" satisfied condition "running"
    Mar 15 22:32:33.396: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-fstj6" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:33.399: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-fstj6": Phase="Running", Reason="", readiness=true. Elapsed: 2.783614ms
    Mar 15 22:32:33.399: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-fstj6" satisfied condition "running"
    Mar 15 22:32:33.399: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-wr9cx" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:33.402: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-wr9cx": Phase="Running", Reason="", readiness=true. Elapsed: 2.70918ms
    Mar 15 22:32:33.402: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-wr9cx" satisfied condition "running"
    Mar 15 22:32:33.402: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-zf82r" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:33.406: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-zf82r": Phase="Running", Reason="", readiness=true. Elapsed: 3.748901ms
    Mar 15 22:32:33.406: INFO: Pod "wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4-zf82r" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4 in namespace emptydir-wrapper-9363, will wait for the garbage collector to delete the pods 03/15/23 22:32:33.406
    Mar 15 22:32:33.467: INFO: Deleting ReplicationController wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4 took: 6.57451ms
    Mar 15 22:32:33.568: INFO: Terminating ReplicationController wrapped-volume-race-2e927ebe-de70-4373-a5a2-afd5c14c4bc4 pods took: 101.182668ms
    STEP: Creating RC which spawns configmap-volume pods 03/15/23 22:32:36.373
    Mar 15 22:32:36.391: INFO: Pod name wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df: Found 0 pods out of 5
    Mar 15 22:32:41.402: INFO: Pod name wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/15/23 22:32:41.402
    Mar 15 22:32:41.403: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:41.406: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.311538ms
    Mar 15 22:32:43.410: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007335583s
    Mar 15 22:32:45.411: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008349264s
    Mar 15 22:32:47.410: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007697036s
    Mar 15 22:32:49.414: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011753845s
    Mar 15 22:32:51.421: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018182303s
    Mar 15 22:32:53.412: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp": Phase="Running", Reason="", readiness=true. Elapsed: 12.009353008s
    Mar 15 22:32:53.412: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-29zgp" satisfied condition "running"
    Mar 15 22:32:53.412: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-4rrlz" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:53.416: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-4rrlz": Phase="Running", Reason="", readiness=true. Elapsed: 4.265341ms
    Mar 15 22:32:53.416: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-4rrlz" satisfied condition "running"
    Mar 15 22:32:53.416: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-9dcjf" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:53.420: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-9dcjf": Phase="Running", Reason="", readiness=true. Elapsed: 3.311594ms
    Mar 15 22:32:53.420: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-9dcjf" satisfied condition "running"
    Mar 15 22:32:53.420: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-k7qch" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:53.423: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-k7qch": Phase="Running", Reason="", readiness=true. Elapsed: 3.731216ms
    Mar 15 22:32:53.424: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-k7qch" satisfied condition "running"
    Mar 15 22:32:53.424: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-q5btb" in namespace "emptydir-wrapper-9363" to be "running"
    Mar 15 22:32:53.428: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-q5btb": Phase="Running", Reason="", readiness=true. Elapsed: 4.55474ms
    Mar 15 22:32:53.428: INFO: Pod "wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df-q5btb" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df in namespace emptydir-wrapper-9363, will wait for the garbage collector to delete the pods 03/15/23 22:32:53.428
    Mar 15 22:32:53.490: INFO: Deleting ReplicationController wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df took: 6.989754ms
    Mar 15 22:32:53.591: INFO: Terminating ReplicationController wrapped-volume-race-6319189c-c4f5-4221-af1d-489d752fa6df pods took: 101.186872ms
    STEP: Cleaning up the configMaps 03/15/23 22:32:55.891
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:32:56.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-9363" for this suite. 03/15/23 22:32:56.21
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:32:56.221
Mar 15 22:32:56.221: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename runtimeclass 03/15/23 22:32:56.222
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:32:56.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:32:56.253
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Mar 15 22:32:56.277: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-329 to be scheduled
Mar 15 22:32:56.286: INFO: 1 pods are not scheduled: [runtimeclass-329/test-runtimeclass-runtimeclass-329-preconfigured-handler-qjd59(8f40684e-88df-4234-819f-48481ceeb736)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 15 22:32:58.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-329" for this suite. 03/15/23 22:32:58.302
------------------------------
• [2.086 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:32:56.221
    Mar 15 22:32:56.221: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename runtimeclass 03/15/23 22:32:56.222
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:32:56.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:32:56.253
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Mar 15 22:32:56.277: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-329 to be scheduled
    Mar 15 22:32:56.286: INFO: 1 pods are not scheduled: [runtimeclass-329/test-runtimeclass-runtimeclass-329-preconfigured-handler-qjd59(8f40684e-88df-4234-819f-48481ceeb736)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:32:58.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-329" for this suite. 03/15/23 22:32:58.302
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:32:58.308
Mar 15 22:32:58.308: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:32:58.309
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:32:58.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:32:58.377
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 03/15/23 22:32:58.381
Mar 15 22:32:58.381: INFO: namespace kubectl-6061
Mar 15 22:32:58.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-6061 create -f -'
Mar 15 22:32:58.824: INFO: stderr: ""
Mar 15 22:32:58.825: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/15/23 22:32:58.825
Mar 15 22:32:59.829: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 22:32:59.829: INFO: Found 0 / 1
Mar 15 22:33:00.829: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 22:33:00.829: INFO: Found 1 / 1
Mar 15 22:33:00.829: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 15 22:33:00.833: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 22:33:00.833: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 15 22:33:00.833: INFO: wait on agnhost-primary startup in kubectl-6061 
Mar 15 22:33:00.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-6061 logs agnhost-primary-vcm4c agnhost-primary'
Mar 15 22:33:00.929: INFO: stderr: ""
Mar 15 22:33:00.929: INFO: stdout: "Paused\n"
STEP: exposing RC 03/15/23 22:33:00.929
Mar 15 22:33:00.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-6061 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 15 22:33:01.031: INFO: stderr: ""
Mar 15 22:33:01.031: INFO: stdout: "service/rm2 exposed\n"
Mar 15 22:33:01.042: INFO: Service rm2 in namespace kubectl-6061 found.
STEP: exposing service 03/15/23 22:33:03.053
Mar 15 22:33:03.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-6061 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 15 22:33:03.155: INFO: stderr: ""
Mar 15 22:33:03.155: INFO: stdout: "service/rm3 exposed\n"
Mar 15 22:33:03.163: INFO: Service rm3 in namespace kubectl-6061 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:33:05.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6061" for this suite. 03/15/23 22:33:05.183
------------------------------
• [SLOW TEST] [6.881 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:32:58.308
    Mar 15 22:32:58.308: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:32:58.309
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:32:58.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:32:58.377
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 03/15/23 22:32:58.381
    Mar 15 22:32:58.381: INFO: namespace kubectl-6061
    Mar 15 22:32:58.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-6061 create -f -'
    Mar 15 22:32:58.824: INFO: stderr: ""
    Mar 15 22:32:58.825: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/15/23 22:32:58.825
    Mar 15 22:32:59.829: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 15 22:32:59.829: INFO: Found 0 / 1
    Mar 15 22:33:00.829: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 15 22:33:00.829: INFO: Found 1 / 1
    Mar 15 22:33:00.829: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 15 22:33:00.833: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 15 22:33:00.833: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 15 22:33:00.833: INFO: wait on agnhost-primary startup in kubectl-6061 
    Mar 15 22:33:00.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-6061 logs agnhost-primary-vcm4c agnhost-primary'
    Mar 15 22:33:00.929: INFO: stderr: ""
    Mar 15 22:33:00.929: INFO: stdout: "Paused\n"
    STEP: exposing RC 03/15/23 22:33:00.929
    Mar 15 22:33:00.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-6061 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Mar 15 22:33:01.031: INFO: stderr: ""
    Mar 15 22:33:01.031: INFO: stdout: "service/rm2 exposed\n"
    Mar 15 22:33:01.042: INFO: Service rm2 in namespace kubectl-6061 found.
    STEP: exposing service 03/15/23 22:33:03.053
    Mar 15 22:33:03.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-6061 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Mar 15 22:33:03.155: INFO: stderr: ""
    Mar 15 22:33:03.155: INFO: stdout: "service/rm3 exposed\n"
    Mar 15 22:33:03.163: INFO: Service rm3 in namespace kubectl-6061 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:33:05.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6061" for this suite. 03/15/23 22:33:05.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:33:05.19
Mar 15 22:33:05.190: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 22:33:05.191
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:33:05.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:33:05.207
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 03/15/23 22:33:05.21
Mar 15 22:33:05.226: INFO: Waiting up to 5m0s for pod "downward-api-8edc40be-87af-4193-b630-4d06205e51ec" in namespace "downward-api-4703" to be "Succeeded or Failed"
Mar 15 22:33:05.248: INFO: Pod "downward-api-8edc40be-87af-4193-b630-4d06205e51ec": Phase="Pending", Reason="", readiness=false. Elapsed: 22.235694ms
Mar 15 22:33:07.252: INFO: Pod "downward-api-8edc40be-87af-4193-b630-4d06205e51ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025920621s
Mar 15 22:33:09.252: INFO: Pod "downward-api-8edc40be-87af-4193-b630-4d06205e51ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02633099s
STEP: Saw pod success 03/15/23 22:33:09.253
Mar 15 22:33:09.253: INFO: Pod "downward-api-8edc40be-87af-4193-b630-4d06205e51ec" satisfied condition "Succeeded or Failed"
Mar 15 22:33:09.256: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downward-api-8edc40be-87af-4193-b630-4d06205e51ec container dapi-container: <nil>
STEP: delete the pod 03/15/23 22:33:09.261
Mar 15 22:33:09.281: INFO: Waiting for pod downward-api-8edc40be-87af-4193-b630-4d06205e51ec to disappear
Mar 15 22:33:09.289: INFO: Pod downward-api-8edc40be-87af-4193-b630-4d06205e51ec no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 15 22:33:09.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4703" for this suite. 03/15/23 22:33:09.302
------------------------------
• [4.121 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:33:05.19
    Mar 15 22:33:05.190: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 22:33:05.191
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:33:05.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:33:05.207
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 03/15/23 22:33:05.21
    Mar 15 22:33:05.226: INFO: Waiting up to 5m0s for pod "downward-api-8edc40be-87af-4193-b630-4d06205e51ec" in namespace "downward-api-4703" to be "Succeeded or Failed"
    Mar 15 22:33:05.248: INFO: Pod "downward-api-8edc40be-87af-4193-b630-4d06205e51ec": Phase="Pending", Reason="", readiness=false. Elapsed: 22.235694ms
    Mar 15 22:33:07.252: INFO: Pod "downward-api-8edc40be-87af-4193-b630-4d06205e51ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025920621s
    Mar 15 22:33:09.252: INFO: Pod "downward-api-8edc40be-87af-4193-b630-4d06205e51ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02633099s
    STEP: Saw pod success 03/15/23 22:33:09.253
    Mar 15 22:33:09.253: INFO: Pod "downward-api-8edc40be-87af-4193-b630-4d06205e51ec" satisfied condition "Succeeded or Failed"
    Mar 15 22:33:09.256: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downward-api-8edc40be-87af-4193-b630-4d06205e51ec container dapi-container: <nil>
    STEP: delete the pod 03/15/23 22:33:09.261
    Mar 15 22:33:09.281: INFO: Waiting for pod downward-api-8edc40be-87af-4193-b630-4d06205e51ec to disappear
    Mar 15 22:33:09.289: INFO: Pod downward-api-8edc40be-87af-4193-b630-4d06205e51ec no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:33:09.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4703" for this suite. 03/15/23 22:33:09.302
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:33:09.316
Mar 15 22:33:09.316: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename gc 03/15/23 22:33:09.317
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:33:09.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:33:09.351
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 03/15/23 22:33:09.366
STEP: delete the rc 03/15/23 22:33:14.401
STEP: wait for the rc to be deleted 03/15/23 22:33:14.413
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/15/23 22:33:19.425
STEP: Gathering metrics 03/15/23 22:33:49.441
Mar 15 22:33:49.486: INFO: Waiting up to 5m0s for pod "kube-controller-manager-i-00864a1c8c75a434c" in namespace "kube-system" to be "running and ready"
Mar 15 22:33:49.490: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c": Phase="Running", Reason="", readiness=true. Elapsed: 3.875333ms
Mar 15 22:33:49.490: INFO: The phase of Pod kube-controller-manager-i-00864a1c8c75a434c is Running (Ready = true)
Mar 15 22:33:49.490: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c" satisfied condition "running and ready"
Mar 15 22:33:49.554: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 15 22:33:49.554: INFO: Deleting pod "simpletest.rc-2bnwj" in namespace "gc-3432"
Mar 15 22:33:49.567: INFO: Deleting pod "simpletest.rc-2j22b" in namespace "gc-3432"
Mar 15 22:33:49.594: INFO: Deleting pod "simpletest.rc-2x8dn" in namespace "gc-3432"
Mar 15 22:33:49.618: INFO: Deleting pod "simpletest.rc-445pl" in namespace "gc-3432"
Mar 15 22:33:49.638: INFO: Deleting pod "simpletest.rc-49v9j" in namespace "gc-3432"
Mar 15 22:33:49.656: INFO: Deleting pod "simpletest.rc-4dsb9" in namespace "gc-3432"
Mar 15 22:33:49.681: INFO: Deleting pod "simpletest.rc-5llg5" in namespace "gc-3432"
Mar 15 22:33:49.697: INFO: Deleting pod "simpletest.rc-6dggg" in namespace "gc-3432"
Mar 15 22:33:49.723: INFO: Deleting pod "simpletest.rc-6rfdg" in namespace "gc-3432"
Mar 15 22:33:49.741: INFO: Deleting pod "simpletest.rc-6z6fv" in namespace "gc-3432"
Mar 15 22:33:49.774: INFO: Deleting pod "simpletest.rc-7569h" in namespace "gc-3432"
Mar 15 22:33:49.834: INFO: Deleting pod "simpletest.rc-7d8g8" in namespace "gc-3432"
Mar 15 22:33:49.893: INFO: Deleting pod "simpletest.rc-7jl9g" in namespace "gc-3432"
Mar 15 22:33:49.972: INFO: Deleting pod "simpletest.rc-7mt7z" in namespace "gc-3432"
Mar 15 22:33:50.067: INFO: Deleting pod "simpletest.rc-7sg8h" in namespace "gc-3432"
Mar 15 22:33:50.087: INFO: Deleting pod "simpletest.rc-7tmd9" in namespace "gc-3432"
Mar 15 22:33:50.100: INFO: Deleting pod "simpletest.rc-82cvq" in namespace "gc-3432"
Mar 15 22:33:50.115: INFO: Deleting pod "simpletest.rc-8hmm6" in namespace "gc-3432"
Mar 15 22:33:50.134: INFO: Deleting pod "simpletest.rc-8tftn" in namespace "gc-3432"
Mar 15 22:33:50.145: INFO: Deleting pod "simpletest.rc-8tm6k" in namespace "gc-3432"
Mar 15 22:33:50.170: INFO: Deleting pod "simpletest.rc-92ccl" in namespace "gc-3432"
Mar 15 22:33:50.185: INFO: Deleting pod "simpletest.rc-92d9d" in namespace "gc-3432"
Mar 15 22:33:50.199: INFO: Deleting pod "simpletest.rc-94qhm" in namespace "gc-3432"
Mar 15 22:33:50.215: INFO: Deleting pod "simpletest.rc-9b8k5" in namespace "gc-3432"
Mar 15 22:33:50.227: INFO: Deleting pod "simpletest.rc-9jshq" in namespace "gc-3432"
Mar 15 22:33:50.244: INFO: Deleting pod "simpletest.rc-9njxl" in namespace "gc-3432"
Mar 15 22:33:50.268: INFO: Deleting pod "simpletest.rc-b5j7g" in namespace "gc-3432"
Mar 15 22:33:50.287: INFO: Deleting pod "simpletest.rc-bhqg5" in namespace "gc-3432"
Mar 15 22:33:50.299: INFO: Deleting pod "simpletest.rc-bll84" in namespace "gc-3432"
Mar 15 22:33:50.317: INFO: Deleting pod "simpletest.rc-bmzhx" in namespace "gc-3432"
Mar 15 22:33:50.334: INFO: Deleting pod "simpletest.rc-bwfnn" in namespace "gc-3432"
Mar 15 22:33:50.354: INFO: Deleting pod "simpletest.rc-bx5mn" in namespace "gc-3432"
Mar 15 22:33:50.370: INFO: Deleting pod "simpletest.rc-c2qcp" in namespace "gc-3432"
Mar 15 22:33:50.390: INFO: Deleting pod "simpletest.rc-cbtfj" in namespace "gc-3432"
Mar 15 22:33:50.405: INFO: Deleting pod "simpletest.rc-cd6hb" in namespace "gc-3432"
Mar 15 22:33:50.415: INFO: Deleting pod "simpletest.rc-cgzjl" in namespace "gc-3432"
Mar 15 22:33:50.433: INFO: Deleting pod "simpletest.rc-cqfk2" in namespace "gc-3432"
Mar 15 22:33:50.443: INFO: Deleting pod "simpletest.rc-cr2d4" in namespace "gc-3432"
Mar 15 22:33:50.459: INFO: Deleting pod "simpletest.rc-cvzls" in namespace "gc-3432"
Mar 15 22:33:50.475: INFO: Deleting pod "simpletest.rc-d4qcd" in namespace "gc-3432"
Mar 15 22:33:50.500: INFO: Deleting pod "simpletest.rc-dc9qn" in namespace "gc-3432"
Mar 15 22:33:50.510: INFO: Deleting pod "simpletest.rc-dgchr" in namespace "gc-3432"
Mar 15 22:33:50.523: INFO: Deleting pod "simpletest.rc-djmzp" in namespace "gc-3432"
Mar 15 22:33:50.538: INFO: Deleting pod "simpletest.rc-dnk7f" in namespace "gc-3432"
Mar 15 22:33:50.551: INFO: Deleting pod "simpletest.rc-dnl7x" in namespace "gc-3432"
Mar 15 22:33:50.562: INFO: Deleting pod "simpletest.rc-g6xvt" in namespace "gc-3432"
Mar 15 22:33:50.585: INFO: Deleting pod "simpletest.rc-g9lgp" in namespace "gc-3432"
Mar 15 22:33:50.603: INFO: Deleting pod "simpletest.rc-gcfpp" in namespace "gc-3432"
Mar 15 22:33:50.619: INFO: Deleting pod "simpletest.rc-gd9c6" in namespace "gc-3432"
Mar 15 22:33:50.631: INFO: Deleting pod "simpletest.rc-gjqtp" in namespace "gc-3432"
Mar 15 22:33:50.644: INFO: Deleting pod "simpletest.rc-gtcf6" in namespace "gc-3432"
Mar 15 22:33:50.659: INFO: Deleting pod "simpletest.rc-gxwmv" in namespace "gc-3432"
Mar 15 22:33:50.670: INFO: Deleting pod "simpletest.rc-jh2zl" in namespace "gc-3432"
Mar 15 22:33:50.692: INFO: Deleting pod "simpletest.rc-jjv9t" in namespace "gc-3432"
Mar 15 22:33:50.711: INFO: Deleting pod "simpletest.rc-jpxtt" in namespace "gc-3432"
Mar 15 22:33:50.732: INFO: Deleting pod "simpletest.rc-jtq9s" in namespace "gc-3432"
Mar 15 22:33:50.744: INFO: Deleting pod "simpletest.rc-jvfc6" in namespace "gc-3432"
Mar 15 22:33:50.756: INFO: Deleting pod "simpletest.rc-jwf5q" in namespace "gc-3432"
Mar 15 22:33:50.767: INFO: Deleting pod "simpletest.rc-jxjmw" in namespace "gc-3432"
Mar 15 22:33:50.785: INFO: Deleting pod "simpletest.rc-jxzjh" in namespace "gc-3432"
Mar 15 22:33:50.803: INFO: Deleting pod "simpletest.rc-kcl94" in namespace "gc-3432"
Mar 15 22:33:50.823: INFO: Deleting pod "simpletest.rc-kh2mr" in namespace "gc-3432"
Mar 15 22:33:50.836: INFO: Deleting pod "simpletest.rc-khgqh" in namespace "gc-3432"
Mar 15 22:33:50.848: INFO: Deleting pod "simpletest.rc-knhzn" in namespace "gc-3432"
Mar 15 22:33:50.859: INFO: Deleting pod "simpletest.rc-kzjzl" in namespace "gc-3432"
Mar 15 22:33:50.870: INFO: Deleting pod "simpletest.rc-l2pnc" in namespace "gc-3432"
Mar 15 22:33:50.885: INFO: Deleting pod "simpletest.rc-lcgmd" in namespace "gc-3432"
Mar 15 22:33:50.898: INFO: Deleting pod "simpletest.rc-ljl7q" in namespace "gc-3432"
Mar 15 22:33:50.981: INFO: Deleting pod "simpletest.rc-lwt9p" in namespace "gc-3432"
Mar 15 22:33:51.050: INFO: Deleting pod "simpletest.rc-lz5mc" in namespace "gc-3432"
Mar 15 22:33:51.098: INFO: Deleting pod "simpletest.rc-mzjhw" in namespace "gc-3432"
Mar 15 22:33:51.122: INFO: Deleting pod "simpletest.rc-nfkf2" in namespace "gc-3432"
Mar 15 22:33:51.132: INFO: Deleting pod "simpletest.rc-nlj67" in namespace "gc-3432"
Mar 15 22:33:51.143: INFO: Deleting pod "simpletest.rc-nrlf5" in namespace "gc-3432"
Mar 15 22:33:51.163: INFO: Deleting pod "simpletest.rc-nvg4b" in namespace "gc-3432"
Mar 15 22:33:51.176: INFO: Deleting pod "simpletest.rc-nxp5r" in namespace "gc-3432"
Mar 15 22:33:51.188: INFO: Deleting pod "simpletest.rc-p26q5" in namespace "gc-3432"
Mar 15 22:33:51.204: INFO: Deleting pod "simpletest.rc-p6pfh" in namespace "gc-3432"
Mar 15 22:33:51.218: INFO: Deleting pod "simpletest.rc-ppcg9" in namespace "gc-3432"
Mar 15 22:33:51.234: INFO: Deleting pod "simpletest.rc-pswzw" in namespace "gc-3432"
Mar 15 22:33:51.246: INFO: Deleting pod "simpletest.rc-px48h" in namespace "gc-3432"
Mar 15 22:33:51.260: INFO: Deleting pod "simpletest.rc-q56d9" in namespace "gc-3432"
Mar 15 22:33:51.271: INFO: Deleting pod "simpletest.rc-rtqnj" in namespace "gc-3432"
Mar 15 22:33:51.291: INFO: Deleting pod "simpletest.rc-sd7js" in namespace "gc-3432"
Mar 15 22:33:51.340: INFO: Deleting pod "simpletest.rc-ss9t8" in namespace "gc-3432"
Mar 15 22:33:51.391: INFO: Deleting pod "simpletest.rc-t792w" in namespace "gc-3432"
Mar 15 22:33:51.442: INFO: Deleting pod "simpletest.rc-t9299" in namespace "gc-3432"
Mar 15 22:33:51.493: INFO: Deleting pod "simpletest.rc-tlk5z" in namespace "gc-3432"
Mar 15 22:33:51.541: INFO: Deleting pod "simpletest.rc-tmt52" in namespace "gc-3432"
Mar 15 22:33:51.592: INFO: Deleting pod "simpletest.rc-tzrgf" in namespace "gc-3432"
Mar 15 22:33:51.641: INFO: Deleting pod "simpletest.rc-v7tft" in namespace "gc-3432"
Mar 15 22:33:51.701: INFO: Deleting pod "simpletest.rc-v86pr" in namespace "gc-3432"
Mar 15 22:33:51.743: INFO: Deleting pod "simpletest.rc-vqsh6" in namespace "gc-3432"
Mar 15 22:33:51.821: INFO: Deleting pod "simpletest.rc-vxj5f" in namespace "gc-3432"
Mar 15 22:33:51.841: INFO: Deleting pod "simpletest.rc-vzkpd" in namespace "gc-3432"
Mar 15 22:33:51.908: INFO: Deleting pod "simpletest.rc-x2f9j" in namespace "gc-3432"
Mar 15 22:33:51.942: INFO: Deleting pod "simpletest.rc-xq6wm" in namespace "gc-3432"
Mar 15 22:33:51.992: INFO: Deleting pod "simpletest.rc-zdplj" in namespace "gc-3432"
Mar 15 22:33:52.042: INFO: Deleting pod "simpletest.rc-zqqd2" in namespace "gc-3432"
Mar 15 22:33:52.093: INFO: Deleting pod "simpletest.rc-zrpcd" in namespace "gc-3432"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 15 22:33:52.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3432" for this suite. 03/15/23 22:33:52.184
------------------------------
• [SLOW TEST] [42.919 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:33:09.316
    Mar 15 22:33:09.316: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename gc 03/15/23 22:33:09.317
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:33:09.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:33:09.351
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 03/15/23 22:33:09.366
    STEP: delete the rc 03/15/23 22:33:14.401
    STEP: wait for the rc to be deleted 03/15/23 22:33:14.413
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/15/23 22:33:19.425
    STEP: Gathering metrics 03/15/23 22:33:49.441
    Mar 15 22:33:49.486: INFO: Waiting up to 5m0s for pod "kube-controller-manager-i-00864a1c8c75a434c" in namespace "kube-system" to be "running and ready"
    Mar 15 22:33:49.490: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c": Phase="Running", Reason="", readiness=true. Elapsed: 3.875333ms
    Mar 15 22:33:49.490: INFO: The phase of Pod kube-controller-manager-i-00864a1c8c75a434c is Running (Ready = true)
    Mar 15 22:33:49.490: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c" satisfied condition "running and ready"
    Mar 15 22:33:49.554: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 15 22:33:49.554: INFO: Deleting pod "simpletest.rc-2bnwj" in namespace "gc-3432"
    Mar 15 22:33:49.567: INFO: Deleting pod "simpletest.rc-2j22b" in namespace "gc-3432"
    Mar 15 22:33:49.594: INFO: Deleting pod "simpletest.rc-2x8dn" in namespace "gc-3432"
    Mar 15 22:33:49.618: INFO: Deleting pod "simpletest.rc-445pl" in namespace "gc-3432"
    Mar 15 22:33:49.638: INFO: Deleting pod "simpletest.rc-49v9j" in namespace "gc-3432"
    Mar 15 22:33:49.656: INFO: Deleting pod "simpletest.rc-4dsb9" in namespace "gc-3432"
    Mar 15 22:33:49.681: INFO: Deleting pod "simpletest.rc-5llg5" in namespace "gc-3432"
    Mar 15 22:33:49.697: INFO: Deleting pod "simpletest.rc-6dggg" in namespace "gc-3432"
    Mar 15 22:33:49.723: INFO: Deleting pod "simpletest.rc-6rfdg" in namespace "gc-3432"
    Mar 15 22:33:49.741: INFO: Deleting pod "simpletest.rc-6z6fv" in namespace "gc-3432"
    Mar 15 22:33:49.774: INFO: Deleting pod "simpletest.rc-7569h" in namespace "gc-3432"
    Mar 15 22:33:49.834: INFO: Deleting pod "simpletest.rc-7d8g8" in namespace "gc-3432"
    Mar 15 22:33:49.893: INFO: Deleting pod "simpletest.rc-7jl9g" in namespace "gc-3432"
    Mar 15 22:33:49.972: INFO: Deleting pod "simpletest.rc-7mt7z" in namespace "gc-3432"
    Mar 15 22:33:50.067: INFO: Deleting pod "simpletest.rc-7sg8h" in namespace "gc-3432"
    Mar 15 22:33:50.087: INFO: Deleting pod "simpletest.rc-7tmd9" in namespace "gc-3432"
    Mar 15 22:33:50.100: INFO: Deleting pod "simpletest.rc-82cvq" in namespace "gc-3432"
    Mar 15 22:33:50.115: INFO: Deleting pod "simpletest.rc-8hmm6" in namespace "gc-3432"
    Mar 15 22:33:50.134: INFO: Deleting pod "simpletest.rc-8tftn" in namespace "gc-3432"
    Mar 15 22:33:50.145: INFO: Deleting pod "simpletest.rc-8tm6k" in namespace "gc-3432"
    Mar 15 22:33:50.170: INFO: Deleting pod "simpletest.rc-92ccl" in namespace "gc-3432"
    Mar 15 22:33:50.185: INFO: Deleting pod "simpletest.rc-92d9d" in namespace "gc-3432"
    Mar 15 22:33:50.199: INFO: Deleting pod "simpletest.rc-94qhm" in namespace "gc-3432"
    Mar 15 22:33:50.215: INFO: Deleting pod "simpletest.rc-9b8k5" in namespace "gc-3432"
    Mar 15 22:33:50.227: INFO: Deleting pod "simpletest.rc-9jshq" in namespace "gc-3432"
    Mar 15 22:33:50.244: INFO: Deleting pod "simpletest.rc-9njxl" in namespace "gc-3432"
    Mar 15 22:33:50.268: INFO: Deleting pod "simpletest.rc-b5j7g" in namespace "gc-3432"
    Mar 15 22:33:50.287: INFO: Deleting pod "simpletest.rc-bhqg5" in namespace "gc-3432"
    Mar 15 22:33:50.299: INFO: Deleting pod "simpletest.rc-bll84" in namespace "gc-3432"
    Mar 15 22:33:50.317: INFO: Deleting pod "simpletest.rc-bmzhx" in namespace "gc-3432"
    Mar 15 22:33:50.334: INFO: Deleting pod "simpletest.rc-bwfnn" in namespace "gc-3432"
    Mar 15 22:33:50.354: INFO: Deleting pod "simpletest.rc-bx5mn" in namespace "gc-3432"
    Mar 15 22:33:50.370: INFO: Deleting pod "simpletest.rc-c2qcp" in namespace "gc-3432"
    Mar 15 22:33:50.390: INFO: Deleting pod "simpletest.rc-cbtfj" in namespace "gc-3432"
    Mar 15 22:33:50.405: INFO: Deleting pod "simpletest.rc-cd6hb" in namespace "gc-3432"
    Mar 15 22:33:50.415: INFO: Deleting pod "simpletest.rc-cgzjl" in namespace "gc-3432"
    Mar 15 22:33:50.433: INFO: Deleting pod "simpletest.rc-cqfk2" in namespace "gc-3432"
    Mar 15 22:33:50.443: INFO: Deleting pod "simpletest.rc-cr2d4" in namespace "gc-3432"
    Mar 15 22:33:50.459: INFO: Deleting pod "simpletest.rc-cvzls" in namespace "gc-3432"
    Mar 15 22:33:50.475: INFO: Deleting pod "simpletest.rc-d4qcd" in namespace "gc-3432"
    Mar 15 22:33:50.500: INFO: Deleting pod "simpletest.rc-dc9qn" in namespace "gc-3432"
    Mar 15 22:33:50.510: INFO: Deleting pod "simpletest.rc-dgchr" in namespace "gc-3432"
    Mar 15 22:33:50.523: INFO: Deleting pod "simpletest.rc-djmzp" in namespace "gc-3432"
    Mar 15 22:33:50.538: INFO: Deleting pod "simpletest.rc-dnk7f" in namespace "gc-3432"
    Mar 15 22:33:50.551: INFO: Deleting pod "simpletest.rc-dnl7x" in namespace "gc-3432"
    Mar 15 22:33:50.562: INFO: Deleting pod "simpletest.rc-g6xvt" in namespace "gc-3432"
    Mar 15 22:33:50.585: INFO: Deleting pod "simpletest.rc-g9lgp" in namespace "gc-3432"
    Mar 15 22:33:50.603: INFO: Deleting pod "simpletest.rc-gcfpp" in namespace "gc-3432"
    Mar 15 22:33:50.619: INFO: Deleting pod "simpletest.rc-gd9c6" in namespace "gc-3432"
    Mar 15 22:33:50.631: INFO: Deleting pod "simpletest.rc-gjqtp" in namespace "gc-3432"
    Mar 15 22:33:50.644: INFO: Deleting pod "simpletest.rc-gtcf6" in namespace "gc-3432"
    Mar 15 22:33:50.659: INFO: Deleting pod "simpletest.rc-gxwmv" in namespace "gc-3432"
    Mar 15 22:33:50.670: INFO: Deleting pod "simpletest.rc-jh2zl" in namespace "gc-3432"
    Mar 15 22:33:50.692: INFO: Deleting pod "simpletest.rc-jjv9t" in namespace "gc-3432"
    Mar 15 22:33:50.711: INFO: Deleting pod "simpletest.rc-jpxtt" in namespace "gc-3432"
    Mar 15 22:33:50.732: INFO: Deleting pod "simpletest.rc-jtq9s" in namespace "gc-3432"
    Mar 15 22:33:50.744: INFO: Deleting pod "simpletest.rc-jvfc6" in namespace "gc-3432"
    Mar 15 22:33:50.756: INFO: Deleting pod "simpletest.rc-jwf5q" in namespace "gc-3432"
    Mar 15 22:33:50.767: INFO: Deleting pod "simpletest.rc-jxjmw" in namespace "gc-3432"
    Mar 15 22:33:50.785: INFO: Deleting pod "simpletest.rc-jxzjh" in namespace "gc-3432"
    Mar 15 22:33:50.803: INFO: Deleting pod "simpletest.rc-kcl94" in namespace "gc-3432"
    Mar 15 22:33:50.823: INFO: Deleting pod "simpletest.rc-kh2mr" in namespace "gc-3432"
    Mar 15 22:33:50.836: INFO: Deleting pod "simpletest.rc-khgqh" in namespace "gc-3432"
    Mar 15 22:33:50.848: INFO: Deleting pod "simpletest.rc-knhzn" in namespace "gc-3432"
    Mar 15 22:33:50.859: INFO: Deleting pod "simpletest.rc-kzjzl" in namespace "gc-3432"
    Mar 15 22:33:50.870: INFO: Deleting pod "simpletest.rc-l2pnc" in namespace "gc-3432"
    Mar 15 22:33:50.885: INFO: Deleting pod "simpletest.rc-lcgmd" in namespace "gc-3432"
    Mar 15 22:33:50.898: INFO: Deleting pod "simpletest.rc-ljl7q" in namespace "gc-3432"
    Mar 15 22:33:50.981: INFO: Deleting pod "simpletest.rc-lwt9p" in namespace "gc-3432"
    Mar 15 22:33:51.050: INFO: Deleting pod "simpletest.rc-lz5mc" in namespace "gc-3432"
    Mar 15 22:33:51.098: INFO: Deleting pod "simpletest.rc-mzjhw" in namespace "gc-3432"
    Mar 15 22:33:51.122: INFO: Deleting pod "simpletest.rc-nfkf2" in namespace "gc-3432"
    Mar 15 22:33:51.132: INFO: Deleting pod "simpletest.rc-nlj67" in namespace "gc-3432"
    Mar 15 22:33:51.143: INFO: Deleting pod "simpletest.rc-nrlf5" in namespace "gc-3432"
    Mar 15 22:33:51.163: INFO: Deleting pod "simpletest.rc-nvg4b" in namespace "gc-3432"
    Mar 15 22:33:51.176: INFO: Deleting pod "simpletest.rc-nxp5r" in namespace "gc-3432"
    Mar 15 22:33:51.188: INFO: Deleting pod "simpletest.rc-p26q5" in namespace "gc-3432"
    Mar 15 22:33:51.204: INFO: Deleting pod "simpletest.rc-p6pfh" in namespace "gc-3432"
    Mar 15 22:33:51.218: INFO: Deleting pod "simpletest.rc-ppcg9" in namespace "gc-3432"
    Mar 15 22:33:51.234: INFO: Deleting pod "simpletest.rc-pswzw" in namespace "gc-3432"
    Mar 15 22:33:51.246: INFO: Deleting pod "simpletest.rc-px48h" in namespace "gc-3432"
    Mar 15 22:33:51.260: INFO: Deleting pod "simpletest.rc-q56d9" in namespace "gc-3432"
    Mar 15 22:33:51.271: INFO: Deleting pod "simpletest.rc-rtqnj" in namespace "gc-3432"
    Mar 15 22:33:51.291: INFO: Deleting pod "simpletest.rc-sd7js" in namespace "gc-3432"
    Mar 15 22:33:51.340: INFO: Deleting pod "simpletest.rc-ss9t8" in namespace "gc-3432"
    Mar 15 22:33:51.391: INFO: Deleting pod "simpletest.rc-t792w" in namespace "gc-3432"
    Mar 15 22:33:51.442: INFO: Deleting pod "simpletest.rc-t9299" in namespace "gc-3432"
    Mar 15 22:33:51.493: INFO: Deleting pod "simpletest.rc-tlk5z" in namespace "gc-3432"
    Mar 15 22:33:51.541: INFO: Deleting pod "simpletest.rc-tmt52" in namespace "gc-3432"
    Mar 15 22:33:51.592: INFO: Deleting pod "simpletest.rc-tzrgf" in namespace "gc-3432"
    Mar 15 22:33:51.641: INFO: Deleting pod "simpletest.rc-v7tft" in namespace "gc-3432"
    Mar 15 22:33:51.701: INFO: Deleting pod "simpletest.rc-v86pr" in namespace "gc-3432"
    Mar 15 22:33:51.743: INFO: Deleting pod "simpletest.rc-vqsh6" in namespace "gc-3432"
    Mar 15 22:33:51.821: INFO: Deleting pod "simpletest.rc-vxj5f" in namespace "gc-3432"
    Mar 15 22:33:51.841: INFO: Deleting pod "simpletest.rc-vzkpd" in namespace "gc-3432"
    Mar 15 22:33:51.908: INFO: Deleting pod "simpletest.rc-x2f9j" in namespace "gc-3432"
    Mar 15 22:33:51.942: INFO: Deleting pod "simpletest.rc-xq6wm" in namespace "gc-3432"
    Mar 15 22:33:51.992: INFO: Deleting pod "simpletest.rc-zdplj" in namespace "gc-3432"
    Mar 15 22:33:52.042: INFO: Deleting pod "simpletest.rc-zqqd2" in namespace "gc-3432"
    Mar 15 22:33:52.093: INFO: Deleting pod "simpletest.rc-zrpcd" in namespace "gc-3432"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:33:52.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3432" for this suite. 03/15/23 22:33:52.184
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:33:52.236
Mar 15 22:33:52.236: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename statefulset 03/15/23 22:33:52.237
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:33:52.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:33:52.258
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9255 03/15/23 22:33:52.262
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-9255 03/15/23 22:33:52.267
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9255 03/15/23 22:33:52.278
Mar 15 22:33:52.285: INFO: Found 0 stateful pods, waiting for 1
Mar 15 22:34:02.288: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/15/23 22:34:02.288
Mar 15 22:34:02.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 22:34:02.464: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 22:34:02.464: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 22:34:02.464: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 22:34:02.478: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 15 22:34:12.483: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 22:34:12.483: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 22:34:12.502: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 15 22:34:12.502: INFO: ss-0  i-0faaf83f00b43c88c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:33:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:33:52 +0000 UTC  }]
Mar 15 22:34:12.502: INFO: 
Mar 15 22:34:12.502: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 15 22:34:13.516: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994245066s
Mar 15 22:34:14.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980232046s
Mar 15 22:34:15.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977018948s
Mar 15 22:34:16.529: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971580421s
Mar 15 22:34:17.533: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967502526s
Mar 15 22:34:18.537: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963026815s
Mar 15 22:34:19.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958834265s
Mar 15 22:34:20.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955297869s
Mar 15 22:34:21.548: INFO: Verifying statefulset ss doesn't scale past 3 for another 951.55902ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9255 03/15/23 22:34:22.549
Mar 15 22:34:22.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 22:34:22.719: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 22:34:22.719: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 22:34:22.719: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 22:34:22.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 22:34:22.901: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 15 22:34:22.901: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 22:34:22.901: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 22:34:22.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 22:34:23.103: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 15 22:34:23.103: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 22:34:23.103: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 22:34:23.107: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar 15 22:34:33.117: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 22:34:33.117: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 22:34:33.117: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 03/15/23 22:34:33.117
Mar 15 22:34:33.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 22:34:33.296: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 22:34:33.296: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 22:34:33.296: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 22:34:33.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 22:34:33.497: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 22:34:33.497: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 22:34:33.497: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 22:34:33.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 22:34:33.721: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 22:34:33.721: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 22:34:33.721: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 22:34:33.721: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 22:34:33.727: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 15 22:34:43.735: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 22:34:43.735: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 22:34:43.735: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 22:34:43.754: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 15 22:34:43.754: INFO: ss-0  i-0faaf83f00b43c88c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:33:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:33:52 +0000 UTC  }]
Mar 15 22:34:43.754: INFO: ss-1  i-077ee0eb7ec5a02aa  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:12 +0000 UTC  }]
Mar 15 22:34:43.754: INFO: ss-2  i-0baafb3f4e7bf826e  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:12 +0000 UTC  }]
Mar 15 22:34:43.754: INFO: 
Mar 15 22:34:43.754: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 15 22:34:44.758: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 15 22:34:44.758: INFO: ss-0  i-0faaf83f00b43c88c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:33:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:33:52 +0000 UTC  }]
Mar 15 22:34:44.758: INFO: 
Mar 15 22:34:44.758: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 15 22:34:45.761: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.987279795s
Mar 15 22:34:46.765: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.983465414s
Mar 15 22:34:47.769: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.979512716s
Mar 15 22:34:48.772: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.976672397s
Mar 15 22:34:49.775: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.972727315s
Mar 15 22:34:50.779: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.969937919s
Mar 15 22:34:51.782: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.966130277s
Mar 15 22:34:52.786: INFO: Verifying statefulset ss doesn't scale past 0 for another 962.395032ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9255 03/15/23 22:34:53.786
Mar 15 22:34:53.793: INFO: Scaling statefulset ss to 0
Mar 15 22:34:53.821: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 15 22:34:53.827: INFO: Deleting all statefulset in ns statefulset-9255
Mar 15 22:34:53.830: INFO: Scaling statefulset ss to 0
Mar 15 22:34:53.841: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 22:34:53.845: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:34:53.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9255" for this suite. 03/15/23 22:34:53.875
------------------------------
• [SLOW TEST] [61.649 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:33:52.236
    Mar 15 22:33:52.236: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename statefulset 03/15/23 22:33:52.237
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:33:52.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:33:52.258
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9255 03/15/23 22:33:52.262
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-9255 03/15/23 22:33:52.267
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9255 03/15/23 22:33:52.278
    Mar 15 22:33:52.285: INFO: Found 0 stateful pods, waiting for 1
    Mar 15 22:34:02.288: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/15/23 22:34:02.288
    Mar 15 22:34:02.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 15 22:34:02.464: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 15 22:34:02.464: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 15 22:34:02.464: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 15 22:34:02.478: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 15 22:34:12.483: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 15 22:34:12.483: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 15 22:34:12.502: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
    Mar 15 22:34:12.502: INFO: ss-0  i-0faaf83f00b43c88c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:33:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:33:52 +0000 UTC  }]
    Mar 15 22:34:12.502: INFO: 
    Mar 15 22:34:12.502: INFO: StatefulSet ss has not reached scale 3, at 1
    Mar 15 22:34:13.516: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994245066s
    Mar 15 22:34:14.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980232046s
    Mar 15 22:34:15.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977018948s
    Mar 15 22:34:16.529: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971580421s
    Mar 15 22:34:17.533: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967502526s
    Mar 15 22:34:18.537: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963026815s
    Mar 15 22:34:19.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958834265s
    Mar 15 22:34:20.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955297869s
    Mar 15 22:34:21.548: INFO: Verifying statefulset ss doesn't scale past 3 for another 951.55902ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9255 03/15/23 22:34:22.549
    Mar 15 22:34:22.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 15 22:34:22.719: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 15 22:34:22.719: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 15 22:34:22.719: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 15 22:34:22.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 15 22:34:22.901: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 15 22:34:22.901: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 15 22:34:22.901: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 15 22:34:22.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 15 22:34:23.103: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 15 22:34:23.103: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 15 22:34:23.103: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 15 22:34:23.107: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Mar 15 22:34:33.117: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 15 22:34:33.117: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 15 22:34:33.117: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 03/15/23 22:34:33.117
    Mar 15 22:34:33.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 15 22:34:33.296: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 15 22:34:33.296: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 15 22:34:33.296: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 15 22:34:33.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 15 22:34:33.497: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 15 22:34:33.497: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 15 22:34:33.497: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 15 22:34:33.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-9255 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 15 22:34:33.721: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 15 22:34:33.721: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 15 22:34:33.721: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 15 22:34:33.721: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 15 22:34:33.727: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Mar 15 22:34:43.735: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 15 22:34:43.735: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 15 22:34:43.735: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 15 22:34:43.754: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
    Mar 15 22:34:43.754: INFO: ss-0  i-0faaf83f00b43c88c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:33:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:33:52 +0000 UTC  }]
    Mar 15 22:34:43.754: INFO: ss-1  i-077ee0eb7ec5a02aa  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:12 +0000 UTC  }]
    Mar 15 22:34:43.754: INFO: ss-2  i-0baafb3f4e7bf826e  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:12 +0000 UTC  }]
    Mar 15 22:34:43.754: INFO: 
    Mar 15 22:34:43.754: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar 15 22:34:44.758: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
    Mar 15 22:34:44.758: INFO: ss-0  i-0faaf83f00b43c88c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:33:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:34:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-15 22:33:52 +0000 UTC  }]
    Mar 15 22:34:44.758: INFO: 
    Mar 15 22:34:44.758: INFO: StatefulSet ss has not reached scale 0, at 1
    Mar 15 22:34:45.761: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.987279795s
    Mar 15 22:34:46.765: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.983465414s
    Mar 15 22:34:47.769: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.979512716s
    Mar 15 22:34:48.772: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.976672397s
    Mar 15 22:34:49.775: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.972727315s
    Mar 15 22:34:50.779: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.969937919s
    Mar 15 22:34:51.782: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.966130277s
    Mar 15 22:34:52.786: INFO: Verifying statefulset ss doesn't scale past 0 for another 962.395032ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9255 03/15/23 22:34:53.786
    Mar 15 22:34:53.793: INFO: Scaling statefulset ss to 0
    Mar 15 22:34:53.821: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 15 22:34:53.827: INFO: Deleting all statefulset in ns statefulset-9255
    Mar 15 22:34:53.830: INFO: Scaling statefulset ss to 0
    Mar 15 22:34:53.841: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 15 22:34:53.845: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:34:53.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9255" for this suite. 03/15/23 22:34:53.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:34:53.889
Mar 15 22:34:53.889: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 22:34:53.89
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:34:53.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:34:53.954
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/15/23 22:34:53.958
Mar 15 22:34:53.965: INFO: Waiting up to 5m0s for pod "pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8" in namespace "emptydir-1044" to be "Succeeded or Failed"
Mar 15 22:34:53.968: INFO: Pod "pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673022ms
Mar 15 22:34:55.972: INFO: Pod "pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006983087s
Mar 15 22:34:57.973: INFO: Pod "pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008190612s
STEP: Saw pod success 03/15/23 22:34:57.973
Mar 15 22:34:57.974: INFO: Pod "pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8" satisfied condition "Succeeded or Failed"
Mar 15 22:34:57.976: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8 container test-container: <nil>
STEP: delete the pod 03/15/23 22:34:57.988
Mar 15 22:34:58.000: INFO: Waiting for pod pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8 to disappear
Mar 15 22:34:58.004: INFO: Pod pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:34:58.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1044" for this suite. 03/15/23 22:34:58.009
------------------------------
• [4.126 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:34:53.889
    Mar 15 22:34:53.889: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 22:34:53.89
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:34:53.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:34:53.954
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/15/23 22:34:53.958
    Mar 15 22:34:53.965: INFO: Waiting up to 5m0s for pod "pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8" in namespace "emptydir-1044" to be "Succeeded or Failed"
    Mar 15 22:34:53.968: INFO: Pod "pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673022ms
    Mar 15 22:34:55.972: INFO: Pod "pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006983087s
    Mar 15 22:34:57.973: INFO: Pod "pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008190612s
    STEP: Saw pod success 03/15/23 22:34:57.973
    Mar 15 22:34:57.974: INFO: Pod "pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8" satisfied condition "Succeeded or Failed"
    Mar 15 22:34:57.976: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8 container test-container: <nil>
    STEP: delete the pod 03/15/23 22:34:57.988
    Mar 15 22:34:58.000: INFO: Waiting for pod pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8 to disappear
    Mar 15 22:34:58.004: INFO: Pod pod-5f05a00a-832c-4ab3-ac31-29b3f6f313f8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:34:58.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1044" for this suite. 03/15/23 22:34:58.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:34:58.023
Mar 15 22:34:58.023: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename statefulset 03/15/23 22:34:58.024
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:34:58.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:34:58.04
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5414 03/15/23 22:34:58.044
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-5414 03/15/23 22:34:58.049
Mar 15 22:34:58.064: INFO: Found 0 stateful pods, waiting for 1
Mar 15 22:35:08.069: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 03/15/23 22:35:08.074
STEP: updating a scale subresource 03/15/23 22:35:08.076
STEP: verifying the statefulset Spec.Replicas was modified 03/15/23 22:35:08.081
STEP: Patch a scale subresource 03/15/23 22:35:08.084
STEP: verifying the statefulset Spec.Replicas was modified 03/15/23 22:35:08.094
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 15 22:35:08.104: INFO: Deleting all statefulset in ns statefulset-5414
Mar 15 22:35:08.117: INFO: Scaling statefulset ss to 0
Mar 15 22:35:18.139: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 22:35:18.141: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:35:18.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5414" for this suite. 03/15/23 22:35:18.17
------------------------------
• [SLOW TEST] [20.154 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:34:58.023
    Mar 15 22:34:58.023: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename statefulset 03/15/23 22:34:58.024
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:34:58.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:34:58.04
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5414 03/15/23 22:34:58.044
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-5414 03/15/23 22:34:58.049
    Mar 15 22:34:58.064: INFO: Found 0 stateful pods, waiting for 1
    Mar 15 22:35:08.069: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 03/15/23 22:35:08.074
    STEP: updating a scale subresource 03/15/23 22:35:08.076
    STEP: verifying the statefulset Spec.Replicas was modified 03/15/23 22:35:08.081
    STEP: Patch a scale subresource 03/15/23 22:35:08.084
    STEP: verifying the statefulset Spec.Replicas was modified 03/15/23 22:35:08.094
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 15 22:35:08.104: INFO: Deleting all statefulset in ns statefulset-5414
    Mar 15 22:35:08.117: INFO: Scaling statefulset ss to 0
    Mar 15 22:35:18.139: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 15 22:35:18.141: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:35:18.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5414" for this suite. 03/15/23 22:35:18.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:35:18.184
Mar 15 22:35:18.184: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:35:18.185
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:18.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:18.204
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 03/15/23 22:35:18.208
Mar 15 22:35:18.220: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155" in namespace "projected-5771" to be "Succeeded or Failed"
Mar 15 22:35:18.225: INFO: Pod "downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155": Phase="Pending", Reason="", readiness=false. Elapsed: 4.732912ms
Mar 15 22:35:20.228: INFO: Pod "downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008269742s
Mar 15 22:35:22.229: INFO: Pod "downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008834204s
STEP: Saw pod success 03/15/23 22:35:22.229
Mar 15 22:35:22.229: INFO: Pod "downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155" satisfied condition "Succeeded or Failed"
Mar 15 22:35:22.231: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155 container client-container: <nil>
STEP: delete the pod 03/15/23 22:35:22.236
Mar 15 22:35:22.253: INFO: Waiting for pod downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155 to disappear
Mar 15 22:35:22.260: INFO: Pod downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 15 22:35:22.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5771" for this suite. 03/15/23 22:35:22.267
------------------------------
• [4.089 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:35:18.184
    Mar 15 22:35:18.184: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:35:18.185
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:18.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:18.204
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 03/15/23 22:35:18.208
    Mar 15 22:35:18.220: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155" in namespace "projected-5771" to be "Succeeded or Failed"
    Mar 15 22:35:18.225: INFO: Pod "downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155": Phase="Pending", Reason="", readiness=false. Elapsed: 4.732912ms
    Mar 15 22:35:20.228: INFO: Pod "downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008269742s
    Mar 15 22:35:22.229: INFO: Pod "downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008834204s
    STEP: Saw pod success 03/15/23 22:35:22.229
    Mar 15 22:35:22.229: INFO: Pod "downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155" satisfied condition "Succeeded or Failed"
    Mar 15 22:35:22.231: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155 container client-container: <nil>
    STEP: delete the pod 03/15/23 22:35:22.236
    Mar 15 22:35:22.253: INFO: Waiting for pod downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155 to disappear
    Mar 15 22:35:22.260: INFO: Pod downwardapi-volume-c5c0c709-9334-4f1c-a955-c97ca1428155 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:35:22.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5771" for this suite. 03/15/23 22:35:22.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:35:22.281
Mar 15 22:35:22.281: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:35:22.282
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:22.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:22.298
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-db6c5b64-4ead-4622-9f08-6e0acabbfddc 03/15/23 22:35:22.301
STEP: Creating a pod to test consume secrets 03/15/23 22:35:22.308
Mar 15 22:35:22.316: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8" in namespace "projected-2925" to be "Succeeded or Failed"
Mar 15 22:35:22.320: INFO: Pod "pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.922625ms
Mar 15 22:35:24.323: INFO: Pod "pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006857845s
Mar 15 22:35:26.324: INFO: Pod "pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00779368s
STEP: Saw pod success 03/15/23 22:35:26.324
Mar 15 22:35:26.324: INFO: Pod "pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8" satisfied condition "Succeeded or Failed"
Mar 15 22:35:26.327: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/15/23 22:35:26.333
Mar 15 22:35:26.346: INFO: Waiting for pod pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8 to disappear
Mar 15 22:35:26.349: INFO: Pod pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 15 22:35:26.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2925" for this suite. 03/15/23 22:35:26.352
------------------------------
• [4.077 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:35:22.281
    Mar 15 22:35:22.281: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:35:22.282
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:22.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:22.298
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-db6c5b64-4ead-4622-9f08-6e0acabbfddc 03/15/23 22:35:22.301
    STEP: Creating a pod to test consume secrets 03/15/23 22:35:22.308
    Mar 15 22:35:22.316: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8" in namespace "projected-2925" to be "Succeeded or Failed"
    Mar 15 22:35:22.320: INFO: Pod "pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.922625ms
    Mar 15 22:35:24.323: INFO: Pod "pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006857845s
    Mar 15 22:35:26.324: INFO: Pod "pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00779368s
    STEP: Saw pod success 03/15/23 22:35:26.324
    Mar 15 22:35:26.324: INFO: Pod "pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8" satisfied condition "Succeeded or Failed"
    Mar 15 22:35:26.327: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 22:35:26.333
    Mar 15 22:35:26.346: INFO: Waiting for pod pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8 to disappear
    Mar 15 22:35:26.349: INFO: Pod pod-projected-secrets-1b259119-7636-4a78-9469-30447c702ca8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:35:26.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2925" for this suite. 03/15/23 22:35:26.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:35:26.36
Mar 15 22:35:26.360: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 22:35:26.363
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:26.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:26.383
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 03/15/23 22:35:26.388
Mar 15 22:35:26.402: INFO: Waiting up to 5m0s for pod "pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37" in namespace "emptydir-3791" to be "Succeeded or Failed"
Mar 15 22:35:26.411: INFO: Pod "pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37": Phase="Pending", Reason="", readiness=false. Elapsed: 9.100139ms
Mar 15 22:35:28.414: INFO: Pod "pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012477451s
Mar 15 22:35:30.417: INFO: Pod "pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01542083s
STEP: Saw pod success 03/15/23 22:35:30.417
Mar 15 22:35:30.418: INFO: Pod "pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37" satisfied condition "Succeeded or Failed"
Mar 15 22:35:30.421: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37 container test-container: <nil>
STEP: delete the pod 03/15/23 22:35:30.427
Mar 15 22:35:30.437: INFO: Waiting for pod pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37 to disappear
Mar 15 22:35:30.441: INFO: Pod pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:35:30.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3791" for this suite. 03/15/23 22:35:30.446
------------------------------
• [4.091 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:35:26.36
    Mar 15 22:35:26.360: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 22:35:26.363
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:26.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:26.383
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/15/23 22:35:26.388
    Mar 15 22:35:26.402: INFO: Waiting up to 5m0s for pod "pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37" in namespace "emptydir-3791" to be "Succeeded or Failed"
    Mar 15 22:35:26.411: INFO: Pod "pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37": Phase="Pending", Reason="", readiness=false. Elapsed: 9.100139ms
    Mar 15 22:35:28.414: INFO: Pod "pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012477451s
    Mar 15 22:35:30.417: INFO: Pod "pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01542083s
    STEP: Saw pod success 03/15/23 22:35:30.417
    Mar 15 22:35:30.418: INFO: Pod "pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37" satisfied condition "Succeeded or Failed"
    Mar 15 22:35:30.421: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37 container test-container: <nil>
    STEP: delete the pod 03/15/23 22:35:30.427
    Mar 15 22:35:30.437: INFO: Waiting for pod pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37 to disappear
    Mar 15 22:35:30.441: INFO: Pod pod-e37e84f1-b1be-4a1a-a5cd-8ad31b255b37 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:35:30.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3791" for this suite. 03/15/23 22:35:30.446
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:35:30.452
Mar 15 22:35:30.453: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:35:30.455
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:30.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:30.484
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 03/15/23 22:35:30.49
Mar 15 22:35:30.490: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 15 22:35:30.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 create -f -'
Mar 15 22:35:30.788: INFO: stderr: ""
Mar 15 22:35:30.788: INFO: stdout: "service/agnhost-replica created\n"
Mar 15 22:35:30.788: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 15 22:35:30.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 create -f -'
Mar 15 22:35:31.170: INFO: stderr: ""
Mar 15 22:35:31.170: INFO: stdout: "service/agnhost-primary created\n"
Mar 15 22:35:31.170: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 15 22:35:31.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 create -f -'
Mar 15 22:35:31.387: INFO: stderr: ""
Mar 15 22:35:31.387: INFO: stdout: "service/frontend created\n"
Mar 15 22:35:31.387: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 15 22:35:31.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 create -f -'
Mar 15 22:35:31.607: INFO: stderr: ""
Mar 15 22:35:31.607: INFO: stdout: "deployment.apps/frontend created\n"
Mar 15 22:35:31.608: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 15 22:35:31.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 create -f -'
Mar 15 22:35:31.876: INFO: stderr: ""
Mar 15 22:35:31.876: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 15 22:35:31.876: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 15 22:35:31.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 create -f -'
Mar 15 22:35:32.308: INFO: stderr: ""
Mar 15 22:35:32.308: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 03/15/23 22:35:32.308
Mar 15 22:35:32.309: INFO: Waiting for all frontend pods to be Running.
Mar 15 22:35:37.360: INFO: Waiting for frontend to serve content.
Mar 15 22:35:37.369: INFO: Trying to add a new entry to the guestbook.
Mar 15 22:35:37.383: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 03/15/23 22:35:37.392
Mar 15 22:35:37.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 delete --grace-period=0 --force -f -'
Mar 15 22:35:37.491: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 22:35:37.491: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 03/15/23 22:35:37.491
Mar 15 22:35:37.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 delete --grace-period=0 --force -f -'
Mar 15 22:35:37.653: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 22:35:37.653: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/15/23 22:35:37.653
Mar 15 22:35:37.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 delete --grace-period=0 --force -f -'
Mar 15 22:35:37.761: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 22:35:37.761: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/15/23 22:35:37.761
Mar 15 22:35:37.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 delete --grace-period=0 --force -f -'
Mar 15 22:35:37.842: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 22:35:37.842: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/15/23 22:35:37.842
Mar 15 22:35:37.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 delete --grace-period=0 --force -f -'
Mar 15 22:35:38.007: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 22:35:38.007: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/15/23 22:35:38.007
Mar 15 22:35:38.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 delete --grace-period=0 --force -f -'
Mar 15 22:35:38.097: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 22:35:38.097: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:35:38.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4556" for this suite. 03/15/23 22:35:38.102
------------------------------
• [SLOW TEST] [7.655 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:35:30.452
    Mar 15 22:35:30.453: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:35:30.455
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:30.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:30.484
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 03/15/23 22:35:30.49
    Mar 15 22:35:30.490: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Mar 15 22:35:30.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 create -f -'
    Mar 15 22:35:30.788: INFO: stderr: ""
    Mar 15 22:35:30.788: INFO: stdout: "service/agnhost-replica created\n"
    Mar 15 22:35:30.788: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Mar 15 22:35:30.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 create -f -'
    Mar 15 22:35:31.170: INFO: stderr: ""
    Mar 15 22:35:31.170: INFO: stdout: "service/agnhost-primary created\n"
    Mar 15 22:35:31.170: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Mar 15 22:35:31.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 create -f -'
    Mar 15 22:35:31.387: INFO: stderr: ""
    Mar 15 22:35:31.387: INFO: stdout: "service/frontend created\n"
    Mar 15 22:35:31.387: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Mar 15 22:35:31.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 create -f -'
    Mar 15 22:35:31.607: INFO: stderr: ""
    Mar 15 22:35:31.607: INFO: stdout: "deployment.apps/frontend created\n"
    Mar 15 22:35:31.608: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 15 22:35:31.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 create -f -'
    Mar 15 22:35:31.876: INFO: stderr: ""
    Mar 15 22:35:31.876: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Mar 15 22:35:31.876: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 15 22:35:31.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 create -f -'
    Mar 15 22:35:32.308: INFO: stderr: ""
    Mar 15 22:35:32.308: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 03/15/23 22:35:32.308
    Mar 15 22:35:32.309: INFO: Waiting for all frontend pods to be Running.
    Mar 15 22:35:37.360: INFO: Waiting for frontend to serve content.
    Mar 15 22:35:37.369: INFO: Trying to add a new entry to the guestbook.
    Mar 15 22:35:37.383: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 03/15/23 22:35:37.392
    Mar 15 22:35:37.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 delete --grace-period=0 --force -f -'
    Mar 15 22:35:37.491: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 15 22:35:37.491: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 03/15/23 22:35:37.491
    Mar 15 22:35:37.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 delete --grace-period=0 --force -f -'
    Mar 15 22:35:37.653: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 15 22:35:37.653: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/15/23 22:35:37.653
    Mar 15 22:35:37.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 delete --grace-period=0 --force -f -'
    Mar 15 22:35:37.761: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 15 22:35:37.761: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/15/23 22:35:37.761
    Mar 15 22:35:37.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 delete --grace-period=0 --force -f -'
    Mar 15 22:35:37.842: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 15 22:35:37.842: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/15/23 22:35:37.842
    Mar 15 22:35:37.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 delete --grace-period=0 --force -f -'
    Mar 15 22:35:38.007: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 15 22:35:38.007: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/15/23 22:35:38.007
    Mar 15 22:35:38.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4556 delete --grace-period=0 --force -f -'
    Mar 15 22:35:38.097: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 15 22:35:38.097: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:35:38.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4556" for this suite. 03/15/23 22:35:38.102
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:35:38.109
Mar 15 22:35:38.109: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename discovery 03/15/23 22:35:38.11
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:38.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:38.134
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 03/15/23 22:35:38.138
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Mar 15 22:35:38.824: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 15 22:35:38.826: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 15 22:35:38.826: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar 15 22:35:38.826: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 15 22:35:38.826: INFO: Checking APIGroup: apps
Mar 15 22:35:38.827: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 15 22:35:38.827: INFO: Versions found [{apps/v1 v1}]
Mar 15 22:35:38.827: INFO: apps/v1 matches apps/v1
Mar 15 22:35:38.827: INFO: Checking APIGroup: events.k8s.io
Mar 15 22:35:38.828: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 15 22:35:38.828: INFO: Versions found [{events.k8s.io/v1 v1}]
Mar 15 22:35:38.828: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 15 22:35:38.828: INFO: Checking APIGroup: authentication.k8s.io
Mar 15 22:35:38.829: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 15 22:35:38.829: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar 15 22:35:38.829: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 15 22:35:38.829: INFO: Checking APIGroup: authorization.k8s.io
Mar 15 22:35:38.830: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 15 22:35:38.830: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar 15 22:35:38.830: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 15 22:35:38.830: INFO: Checking APIGroup: autoscaling
Mar 15 22:35:38.831: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar 15 22:35:38.831: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Mar 15 22:35:38.831: INFO: autoscaling/v2 matches autoscaling/v2
Mar 15 22:35:38.831: INFO: Checking APIGroup: batch
Mar 15 22:35:38.832: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 15 22:35:38.832: INFO: Versions found [{batch/v1 v1}]
Mar 15 22:35:38.832: INFO: batch/v1 matches batch/v1
Mar 15 22:35:38.832: INFO: Checking APIGroup: certificates.k8s.io
Mar 15 22:35:38.834: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 15 22:35:38.834: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar 15 22:35:38.834: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 15 22:35:38.834: INFO: Checking APIGroup: networking.k8s.io
Mar 15 22:35:38.835: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 15 22:35:38.835: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar 15 22:35:38.835: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 15 22:35:38.835: INFO: Checking APIGroup: policy
Mar 15 22:35:38.836: INFO: PreferredVersion.GroupVersion: policy/v1
Mar 15 22:35:38.836: INFO: Versions found [{policy/v1 v1}]
Mar 15 22:35:38.836: INFO: policy/v1 matches policy/v1
Mar 15 22:35:38.836: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 15 22:35:38.837: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 15 22:35:38.837: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar 15 22:35:38.837: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 15 22:35:38.837: INFO: Checking APIGroup: storage.k8s.io
Mar 15 22:35:38.838: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 15 22:35:38.838: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 15 22:35:38.838: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 15 22:35:38.838: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 15 22:35:38.839: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 15 22:35:38.839: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar 15 22:35:38.839: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 15 22:35:38.839: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 15 22:35:38.840: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 15 22:35:38.840: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar 15 22:35:38.840: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 15 22:35:38.840: INFO: Checking APIGroup: scheduling.k8s.io
Mar 15 22:35:38.841: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 15 22:35:38.841: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar 15 22:35:38.841: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 15 22:35:38.841: INFO: Checking APIGroup: coordination.k8s.io
Mar 15 22:35:38.842: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 15 22:35:38.842: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar 15 22:35:38.842: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 15 22:35:38.842: INFO: Checking APIGroup: node.k8s.io
Mar 15 22:35:38.843: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 15 22:35:38.843: INFO: Versions found [{node.k8s.io/v1 v1}]
Mar 15 22:35:38.843: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 15 22:35:38.843: INFO: Checking APIGroup: discovery.k8s.io
Mar 15 22:35:38.844: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar 15 22:35:38.844: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Mar 15 22:35:38.844: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar 15 22:35:38.844: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 15 22:35:38.846: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Mar 15 22:35:38.846: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Mar 15 22:35:38.846: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Mar 15 22:35:38.846: INFO: Checking APIGroup: iamauthenticator.k8s.aws
Mar 15 22:35:38.847: INFO: PreferredVersion.GroupVersion: iamauthenticator.k8s.aws/v1alpha1
Mar 15 22:35:38.847: INFO: Versions found [{iamauthenticator.k8s.aws/v1alpha1 v1alpha1}]
Mar 15 22:35:38.847: INFO: iamauthenticator.k8s.aws/v1alpha1 matches iamauthenticator.k8s.aws/v1alpha1
Mar 15 22:35:38.847: INFO: Checking APIGroup: cilium.io
Mar 15 22:35:38.848: INFO: PreferredVersion.GroupVersion: cilium.io/v2
Mar 15 22:35:38.848: INFO: Versions found [{cilium.io/v2 v2}]
Mar 15 22:35:38.848: INFO: cilium.io/v2 matches cilium.io/v2
Mar 15 22:35:38.848: INFO: Checking APIGroup: metrics.k8s.io
Mar 15 22:35:38.849: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar 15 22:35:38.849: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar 15 22:35:38.849: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Mar 15 22:35:38.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-2044" for this suite. 03/15/23 22:35:38.853
------------------------------
• [0.749 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:35:38.109
    Mar 15 22:35:38.109: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename discovery 03/15/23 22:35:38.11
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:38.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:38.134
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 03/15/23 22:35:38.138
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Mar 15 22:35:38.824: INFO: Checking APIGroup: apiregistration.k8s.io
    Mar 15 22:35:38.826: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Mar 15 22:35:38.826: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Mar 15 22:35:38.826: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Mar 15 22:35:38.826: INFO: Checking APIGroup: apps
    Mar 15 22:35:38.827: INFO: PreferredVersion.GroupVersion: apps/v1
    Mar 15 22:35:38.827: INFO: Versions found [{apps/v1 v1}]
    Mar 15 22:35:38.827: INFO: apps/v1 matches apps/v1
    Mar 15 22:35:38.827: INFO: Checking APIGroup: events.k8s.io
    Mar 15 22:35:38.828: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Mar 15 22:35:38.828: INFO: Versions found [{events.k8s.io/v1 v1}]
    Mar 15 22:35:38.828: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Mar 15 22:35:38.828: INFO: Checking APIGroup: authentication.k8s.io
    Mar 15 22:35:38.829: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Mar 15 22:35:38.829: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Mar 15 22:35:38.829: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Mar 15 22:35:38.829: INFO: Checking APIGroup: authorization.k8s.io
    Mar 15 22:35:38.830: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Mar 15 22:35:38.830: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Mar 15 22:35:38.830: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Mar 15 22:35:38.830: INFO: Checking APIGroup: autoscaling
    Mar 15 22:35:38.831: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Mar 15 22:35:38.831: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Mar 15 22:35:38.831: INFO: autoscaling/v2 matches autoscaling/v2
    Mar 15 22:35:38.831: INFO: Checking APIGroup: batch
    Mar 15 22:35:38.832: INFO: PreferredVersion.GroupVersion: batch/v1
    Mar 15 22:35:38.832: INFO: Versions found [{batch/v1 v1}]
    Mar 15 22:35:38.832: INFO: batch/v1 matches batch/v1
    Mar 15 22:35:38.832: INFO: Checking APIGroup: certificates.k8s.io
    Mar 15 22:35:38.834: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Mar 15 22:35:38.834: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Mar 15 22:35:38.834: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Mar 15 22:35:38.834: INFO: Checking APIGroup: networking.k8s.io
    Mar 15 22:35:38.835: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Mar 15 22:35:38.835: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Mar 15 22:35:38.835: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Mar 15 22:35:38.835: INFO: Checking APIGroup: policy
    Mar 15 22:35:38.836: INFO: PreferredVersion.GroupVersion: policy/v1
    Mar 15 22:35:38.836: INFO: Versions found [{policy/v1 v1}]
    Mar 15 22:35:38.836: INFO: policy/v1 matches policy/v1
    Mar 15 22:35:38.836: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Mar 15 22:35:38.837: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Mar 15 22:35:38.837: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Mar 15 22:35:38.837: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Mar 15 22:35:38.837: INFO: Checking APIGroup: storage.k8s.io
    Mar 15 22:35:38.838: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Mar 15 22:35:38.838: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Mar 15 22:35:38.838: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Mar 15 22:35:38.838: INFO: Checking APIGroup: admissionregistration.k8s.io
    Mar 15 22:35:38.839: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Mar 15 22:35:38.839: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Mar 15 22:35:38.839: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Mar 15 22:35:38.839: INFO: Checking APIGroup: apiextensions.k8s.io
    Mar 15 22:35:38.840: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Mar 15 22:35:38.840: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Mar 15 22:35:38.840: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Mar 15 22:35:38.840: INFO: Checking APIGroup: scheduling.k8s.io
    Mar 15 22:35:38.841: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Mar 15 22:35:38.841: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Mar 15 22:35:38.841: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Mar 15 22:35:38.841: INFO: Checking APIGroup: coordination.k8s.io
    Mar 15 22:35:38.842: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Mar 15 22:35:38.842: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Mar 15 22:35:38.842: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Mar 15 22:35:38.842: INFO: Checking APIGroup: node.k8s.io
    Mar 15 22:35:38.843: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Mar 15 22:35:38.843: INFO: Versions found [{node.k8s.io/v1 v1}]
    Mar 15 22:35:38.843: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Mar 15 22:35:38.843: INFO: Checking APIGroup: discovery.k8s.io
    Mar 15 22:35:38.844: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Mar 15 22:35:38.844: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Mar 15 22:35:38.844: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Mar 15 22:35:38.844: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Mar 15 22:35:38.846: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Mar 15 22:35:38.846: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Mar 15 22:35:38.846: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Mar 15 22:35:38.846: INFO: Checking APIGroup: iamauthenticator.k8s.aws
    Mar 15 22:35:38.847: INFO: PreferredVersion.GroupVersion: iamauthenticator.k8s.aws/v1alpha1
    Mar 15 22:35:38.847: INFO: Versions found [{iamauthenticator.k8s.aws/v1alpha1 v1alpha1}]
    Mar 15 22:35:38.847: INFO: iamauthenticator.k8s.aws/v1alpha1 matches iamauthenticator.k8s.aws/v1alpha1
    Mar 15 22:35:38.847: INFO: Checking APIGroup: cilium.io
    Mar 15 22:35:38.848: INFO: PreferredVersion.GroupVersion: cilium.io/v2
    Mar 15 22:35:38.848: INFO: Versions found [{cilium.io/v2 v2}]
    Mar 15 22:35:38.848: INFO: cilium.io/v2 matches cilium.io/v2
    Mar 15 22:35:38.848: INFO: Checking APIGroup: metrics.k8s.io
    Mar 15 22:35:38.849: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Mar 15 22:35:38.849: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Mar 15 22:35:38.849: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:35:38.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-2044" for this suite. 03/15/23 22:35:38.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:35:38.86
Mar 15 22:35:38.861: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename var-expansion 03/15/23 22:35:38.861
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:38.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:38.879
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 03/15/23 22:35:38.882
Mar 15 22:35:38.891: INFO: Waiting up to 5m0s for pod "var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4" in namespace "var-expansion-2395" to be "Succeeded or Failed"
Mar 15 22:35:38.895: INFO: Pod "var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10453ms
Mar 15 22:35:40.899: INFO: Pod "var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008396968s
Mar 15 22:35:42.899: INFO: Pod "var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007572364s
Mar 15 22:35:44.899: INFO: Pod "var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007959444s
STEP: Saw pod success 03/15/23 22:35:44.899
Mar 15 22:35:44.899: INFO: Pod "var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4" satisfied condition "Succeeded or Failed"
Mar 15 22:35:44.902: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4 container dapi-container: <nil>
STEP: delete the pod 03/15/23 22:35:44.914
Mar 15 22:35:44.924: INFO: Waiting for pod var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4 to disappear
Mar 15 22:35:44.928: INFO: Pod var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 15 22:35:44.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2395" for this suite. 03/15/23 22:35:44.933
------------------------------
• [SLOW TEST] [6.077 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:35:38.86
    Mar 15 22:35:38.861: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename var-expansion 03/15/23 22:35:38.861
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:38.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:38.879
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 03/15/23 22:35:38.882
    Mar 15 22:35:38.891: INFO: Waiting up to 5m0s for pod "var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4" in namespace "var-expansion-2395" to be "Succeeded or Failed"
    Mar 15 22:35:38.895: INFO: Pod "var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10453ms
    Mar 15 22:35:40.899: INFO: Pod "var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008396968s
    Mar 15 22:35:42.899: INFO: Pod "var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007572364s
    Mar 15 22:35:44.899: INFO: Pod "var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007959444s
    STEP: Saw pod success 03/15/23 22:35:44.899
    Mar 15 22:35:44.899: INFO: Pod "var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4" satisfied condition "Succeeded or Failed"
    Mar 15 22:35:44.902: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4 container dapi-container: <nil>
    STEP: delete the pod 03/15/23 22:35:44.914
    Mar 15 22:35:44.924: INFO: Waiting for pod var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4 to disappear
    Mar 15 22:35:44.928: INFO: Pod var-expansion-dd0b2389-1c4f-4b46-b841-aeb11eca36e4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:35:44.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2395" for this suite. 03/15/23 22:35:44.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:35:44.953
Mar 15 22:35:44.953: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename containers 03/15/23 22:35:44.955
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:44.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:44.975
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 03/15/23 22:35:44.978
Mar 15 22:35:44.985: INFO: Waiting up to 5m0s for pod "client-containers-61ea2220-14bb-4308-8b32-0e1642171a87" in namespace "containers-5058" to be "Succeeded or Failed"
Mar 15 22:35:44.991: INFO: Pod "client-containers-61ea2220-14bb-4308-8b32-0e1642171a87": Phase="Pending", Reason="", readiness=false. Elapsed: 6.914444ms
Mar 15 22:35:46.996: INFO: Pod "client-containers-61ea2220-14bb-4308-8b32-0e1642171a87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011617019s
Mar 15 22:35:48.996: INFO: Pod "client-containers-61ea2220-14bb-4308-8b32-0e1642171a87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011100634s
STEP: Saw pod success 03/15/23 22:35:48.996
Mar 15 22:35:48.996: INFO: Pod "client-containers-61ea2220-14bb-4308-8b32-0e1642171a87" satisfied condition "Succeeded or Failed"
Mar 15 22:35:49.001: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod client-containers-61ea2220-14bb-4308-8b32-0e1642171a87 container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:35:49.006
Mar 15 22:35:49.017: INFO: Waiting for pod client-containers-61ea2220-14bb-4308-8b32-0e1642171a87 to disappear
Mar 15 22:35:49.019: INFO: Pod client-containers-61ea2220-14bb-4308-8b32-0e1642171a87 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 15 22:35:49.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5058" for this suite. 03/15/23 22:35:49.023
------------------------------
• [4.079 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:35:44.953
    Mar 15 22:35:44.953: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename containers 03/15/23 22:35:44.955
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:44.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:44.975
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 03/15/23 22:35:44.978
    Mar 15 22:35:44.985: INFO: Waiting up to 5m0s for pod "client-containers-61ea2220-14bb-4308-8b32-0e1642171a87" in namespace "containers-5058" to be "Succeeded or Failed"
    Mar 15 22:35:44.991: INFO: Pod "client-containers-61ea2220-14bb-4308-8b32-0e1642171a87": Phase="Pending", Reason="", readiness=false. Elapsed: 6.914444ms
    Mar 15 22:35:46.996: INFO: Pod "client-containers-61ea2220-14bb-4308-8b32-0e1642171a87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011617019s
    Mar 15 22:35:48.996: INFO: Pod "client-containers-61ea2220-14bb-4308-8b32-0e1642171a87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011100634s
    STEP: Saw pod success 03/15/23 22:35:48.996
    Mar 15 22:35:48.996: INFO: Pod "client-containers-61ea2220-14bb-4308-8b32-0e1642171a87" satisfied condition "Succeeded or Failed"
    Mar 15 22:35:49.001: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod client-containers-61ea2220-14bb-4308-8b32-0e1642171a87 container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:35:49.006
    Mar 15 22:35:49.017: INFO: Waiting for pod client-containers-61ea2220-14bb-4308-8b32-0e1642171a87 to disappear
    Mar 15 22:35:49.019: INFO: Pod client-containers-61ea2220-14bb-4308-8b32-0e1642171a87 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:35:49.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5058" for this suite. 03/15/23 22:35:49.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:35:49.033
Mar 15 22:35:49.034: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename job 03/15/23 22:35:49.034
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:49.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:49.05
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 03/15/23 22:35:49.057
STEP: Patching the Job 03/15/23 22:35:49.065
STEP: Watching for Job to be patched 03/15/23 22:35:49.089
Mar 15 22:35:49.092: INFO: Event ADDED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 15 22:35:49.092: INFO: Event MODIFIED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 15 22:35:49.092: INFO: Event MODIFIED found for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 03/15/23 22:35:49.092
STEP: Watching for Job to be updated 03/15/23 22:35:49.103
Mar 15 22:35:49.109: INFO: Event MODIFIED found for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 15 22:35:49.110: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 03/15/23 22:35:49.11
Mar 15 22:35:49.117: INFO: Job: e2e-hmpd6 as labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6]
STEP: Waiting for job to complete 03/15/23 22:35:49.117
STEP: Delete a job collection with a labelselector 03/15/23 22:35:59.121
STEP: Watching for Job to be deleted 03/15/23 22:35:59.127
Mar 15 22:35:59.129: INFO: Event MODIFIED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 15 22:35:59.129: INFO: Event MODIFIED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 15 22:35:59.129: INFO: Event MODIFIED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 15 22:35:59.129: INFO: Event MODIFIED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 15 22:35:59.129: INFO: Event MODIFIED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 15 22:35:59.129: INFO: Event DELETED found for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 03/15/23 22:35:59.129
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 15 22:35:59.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7027" for this suite. 03/15/23 22:35:59.145
------------------------------
• [SLOW TEST] [10.130 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:35:49.033
    Mar 15 22:35:49.034: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename job 03/15/23 22:35:49.034
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:49.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:49.05
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 03/15/23 22:35:49.057
    STEP: Patching the Job 03/15/23 22:35:49.065
    STEP: Watching for Job to be patched 03/15/23 22:35:49.089
    Mar 15 22:35:49.092: INFO: Event ADDED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 15 22:35:49.092: INFO: Event MODIFIED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 15 22:35:49.092: INFO: Event MODIFIED found for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 03/15/23 22:35:49.092
    STEP: Watching for Job to be updated 03/15/23 22:35:49.103
    Mar 15 22:35:49.109: INFO: Event MODIFIED found for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 15 22:35:49.110: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 03/15/23 22:35:49.11
    Mar 15 22:35:49.117: INFO: Job: e2e-hmpd6 as labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6]
    STEP: Waiting for job to complete 03/15/23 22:35:49.117
    STEP: Delete a job collection with a labelselector 03/15/23 22:35:59.121
    STEP: Watching for Job to be deleted 03/15/23 22:35:59.127
    Mar 15 22:35:59.129: INFO: Event MODIFIED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 15 22:35:59.129: INFO: Event MODIFIED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 15 22:35:59.129: INFO: Event MODIFIED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 15 22:35:59.129: INFO: Event MODIFIED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 15 22:35:59.129: INFO: Event MODIFIED observed for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 15 22:35:59.129: INFO: Event DELETED found for Job e2e-hmpd6 in namespace job-7027 with labels: map[e2e-hmpd6:patched e2e-job-label:e2e-hmpd6] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 03/15/23 22:35:59.129
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:35:59.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7027" for this suite. 03/15/23 22:35:59.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:35:59.17
Mar 15 22:35:59.170: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:35:59.171
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:59.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:59.195
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-15eaed8d-988b-45f0-8d4d-27bdf7b521e2 03/15/23 22:35:59.199
STEP: Creating a pod to test consume configMaps 03/15/23 22:35:59.203
Mar 15 22:35:59.209: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b" in namespace "projected-5156" to be "Succeeded or Failed"
Mar 15 22:35:59.213: INFO: Pod "pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.274817ms
Mar 15 22:36:01.216: INFO: Pod "pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b": Phase="Running", Reason="", readiness=false. Elapsed: 2.007360175s
Mar 15 22:36:03.217: INFO: Pod "pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008375878s
STEP: Saw pod success 03/15/23 22:36:03.217
Mar 15 22:36:03.217: INFO: Pod "pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b" satisfied condition "Succeeded or Failed"
Mar 15 22:36:03.220: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:36:03.231
Mar 15 22:36:03.245: INFO: Waiting for pod pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b to disappear
Mar 15 22:36:03.250: INFO: Pod pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:36:03.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5156" for this suite. 03/15/23 22:36:03.255
------------------------------
• [4.091 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:35:59.17
    Mar 15 22:35:59.170: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:35:59.171
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:35:59.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:35:59.195
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-15eaed8d-988b-45f0-8d4d-27bdf7b521e2 03/15/23 22:35:59.199
    STEP: Creating a pod to test consume configMaps 03/15/23 22:35:59.203
    Mar 15 22:35:59.209: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b" in namespace "projected-5156" to be "Succeeded or Failed"
    Mar 15 22:35:59.213: INFO: Pod "pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.274817ms
    Mar 15 22:36:01.216: INFO: Pod "pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b": Phase="Running", Reason="", readiness=false. Elapsed: 2.007360175s
    Mar 15 22:36:03.217: INFO: Pod "pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008375878s
    STEP: Saw pod success 03/15/23 22:36:03.217
    Mar 15 22:36:03.217: INFO: Pod "pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b" satisfied condition "Succeeded or Failed"
    Mar 15 22:36:03.220: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:36:03.231
    Mar 15 22:36:03.245: INFO: Waiting for pod pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b to disappear
    Mar 15 22:36:03.250: INFO: Pod pod-projected-configmaps-35186158-e66c-4712-8775-46846d7d828b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:36:03.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5156" for this suite. 03/15/23 22:36:03.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:36:03.265
Mar 15 22:36:03.265: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename dns 03/15/23 22:36:03.266
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:03.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:36:03.288
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/15/23 22:36:03.291
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/15/23 22:36:03.291
STEP: creating a pod to probe DNS 03/15/23 22:36:03.291
STEP: submitting the pod to kubernetes 03/15/23 22:36:03.292
Mar 15 22:36:03.301: INFO: Waiting up to 15m0s for pod "dns-test-2f19af70-f63b-4af6-b307-83d561d4ca66" in namespace "dns-3266" to be "running"
Mar 15 22:36:03.306: INFO: Pod "dns-test-2f19af70-f63b-4af6-b307-83d561d4ca66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.644576ms
Mar 15 22:36:05.310: INFO: Pod "dns-test-2f19af70-f63b-4af6-b307-83d561d4ca66": Phase="Running", Reason="", readiness=true. Elapsed: 2.008721901s
Mar 15 22:36:05.310: INFO: Pod "dns-test-2f19af70-f63b-4af6-b307-83d561d4ca66" satisfied condition "running"
STEP: retrieving the pod 03/15/23 22:36:05.31
STEP: looking for the results for each expected name from probers 03/15/23 22:36:05.313
Mar 15 22:36:05.327: INFO: DNS probes using dns-3266/dns-test-2f19af70-f63b-4af6-b307-83d561d4ca66 succeeded

STEP: deleting the pod 03/15/23 22:36:05.327
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 15 22:36:05.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3266" for this suite. 03/15/23 22:36:05.353
------------------------------
• [2.098 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:36:03.265
    Mar 15 22:36:03.265: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename dns 03/15/23 22:36:03.266
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:03.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:36:03.288
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/15/23 22:36:03.291
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/15/23 22:36:03.291
    STEP: creating a pod to probe DNS 03/15/23 22:36:03.291
    STEP: submitting the pod to kubernetes 03/15/23 22:36:03.292
    Mar 15 22:36:03.301: INFO: Waiting up to 15m0s for pod "dns-test-2f19af70-f63b-4af6-b307-83d561d4ca66" in namespace "dns-3266" to be "running"
    Mar 15 22:36:03.306: INFO: Pod "dns-test-2f19af70-f63b-4af6-b307-83d561d4ca66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.644576ms
    Mar 15 22:36:05.310: INFO: Pod "dns-test-2f19af70-f63b-4af6-b307-83d561d4ca66": Phase="Running", Reason="", readiness=true. Elapsed: 2.008721901s
    Mar 15 22:36:05.310: INFO: Pod "dns-test-2f19af70-f63b-4af6-b307-83d561d4ca66" satisfied condition "running"
    STEP: retrieving the pod 03/15/23 22:36:05.31
    STEP: looking for the results for each expected name from probers 03/15/23 22:36:05.313
    Mar 15 22:36:05.327: INFO: DNS probes using dns-3266/dns-test-2f19af70-f63b-4af6-b307-83d561d4ca66 succeeded

    STEP: deleting the pod 03/15/23 22:36:05.327
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:36:05.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3266" for this suite. 03/15/23 22:36:05.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:36:05.366
Mar 15 22:36:05.366: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:36:05.367
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:05.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:36:05.382
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/15/23 22:36:05.385
Mar 15 22:36:05.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-5368 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 15 22:36:05.463: INFO: stderr: ""
Mar 15 22:36:05.463: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 03/15/23 22:36:05.463
STEP: verifying the pod e2e-test-httpd-pod was created 03/15/23 22:36:10.515
Mar 15 22:36:10.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-5368 get pod e2e-test-httpd-pod -o json'
Mar 15 22:36:10.619: INFO: stderr: ""
Mar 15 22:36:10.619: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-03-15T22:36:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5368\",\n        \"resourceVersion\": \"22954\",\n        \"uid\": \"f7ffd1a1-e142-4eff-ac36-c6dd98c92c5c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-crsrb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"i-0faaf83f00b43c88c\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-crsrb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-15T22:36:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-15T22:36:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-15T22:36:07Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-15T22:36:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://1e38de4554225dbfe0f65803aaba2eb806bf3cd86f40deaf7b09604362947115\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-15T22:36:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.20.126.23\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.3.17\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.96.3.17\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-15T22:36:05Z\"\n    }\n}\n"
STEP: replace the image in the pod 03/15/23 22:36:10.619
Mar 15 22:36:10.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-5368 replace -f -'
Mar 15 22:36:10.979: INFO: stderr: ""
Mar 15 22:36:10.979: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 03/15/23 22:36:10.979
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Mar 15 22:36:10.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-5368 delete pods e2e-test-httpd-pod'
Mar 15 22:36:13.165: INFO: stderr: ""
Mar 15 22:36:13.165: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:36:13.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5368" for this suite. 03/15/23 22:36:13.171
------------------------------
• [SLOW TEST] [7.811 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:36:05.366
    Mar 15 22:36:05.366: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:36:05.367
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:05.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:36:05.382
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/15/23 22:36:05.385
    Mar 15 22:36:05.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-5368 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 15 22:36:05.463: INFO: stderr: ""
    Mar 15 22:36:05.463: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 03/15/23 22:36:05.463
    STEP: verifying the pod e2e-test-httpd-pod was created 03/15/23 22:36:10.515
    Mar 15 22:36:10.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-5368 get pod e2e-test-httpd-pod -o json'
    Mar 15 22:36:10.619: INFO: stderr: ""
    Mar 15 22:36:10.619: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-03-15T22:36:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5368\",\n        \"resourceVersion\": \"22954\",\n        \"uid\": \"f7ffd1a1-e142-4eff-ac36-c6dd98c92c5c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-crsrb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"i-0faaf83f00b43c88c\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-crsrb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-15T22:36:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-15T22:36:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-15T22:36:07Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-15T22:36:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://1e38de4554225dbfe0f65803aaba2eb806bf3cd86f40deaf7b09604362947115\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-15T22:36:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.20.126.23\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.3.17\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.96.3.17\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-15T22:36:05Z\"\n    }\n}\n"
    STEP: replace the image in the pod 03/15/23 22:36:10.619
    Mar 15 22:36:10.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-5368 replace -f -'
    Mar 15 22:36:10.979: INFO: stderr: ""
    Mar 15 22:36:10.979: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 03/15/23 22:36:10.979
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Mar 15 22:36:10.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-5368 delete pods e2e-test-httpd-pod'
    Mar 15 22:36:13.165: INFO: stderr: ""
    Mar 15 22:36:13.165: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:36:13.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5368" for this suite. 03/15/23 22:36:13.171
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:36:13.177
Mar 15 22:36:13.178: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename namespaces 03/15/23 22:36:13.179
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:13.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:36:13.199
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 03/15/23 22:36:13.204
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:13.225
STEP: Creating a pod in the namespace 03/15/23 22:36:13.229
STEP: Waiting for the pod to have running status 03/15/23 22:36:13.236
Mar 15 22:36:13.236: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9967" to be "running"
Mar 15 22:36:13.239: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.447907ms
Mar 15 22:36:15.243: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007114413s
Mar 15 22:36:15.243: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 03/15/23 22:36:15.243
STEP: Waiting for the namespace to be removed. 03/15/23 22:36:15.249
STEP: Recreating the namespace 03/15/23 22:36:26.252
STEP: Verifying there are no pods in the namespace 03/15/23 22:36:26.293
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:36:26.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8316" for this suite. 03/15/23 22:36:26.3
STEP: Destroying namespace "nsdeletetest-9967" for this suite. 03/15/23 22:36:26.32
Mar 15 22:36:26.332: INFO: Namespace nsdeletetest-9967 was already deleted
STEP: Destroying namespace "nsdeletetest-5239" for this suite. 03/15/23 22:36:26.332
------------------------------
• [SLOW TEST] [13.173 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:36:13.177
    Mar 15 22:36:13.178: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename namespaces 03/15/23 22:36:13.179
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:13.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:36:13.199
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 03/15/23 22:36:13.204
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:13.225
    STEP: Creating a pod in the namespace 03/15/23 22:36:13.229
    STEP: Waiting for the pod to have running status 03/15/23 22:36:13.236
    Mar 15 22:36:13.236: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9967" to be "running"
    Mar 15 22:36:13.239: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.447907ms
    Mar 15 22:36:15.243: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007114413s
    Mar 15 22:36:15.243: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 03/15/23 22:36:15.243
    STEP: Waiting for the namespace to be removed. 03/15/23 22:36:15.249
    STEP: Recreating the namespace 03/15/23 22:36:26.252
    STEP: Verifying there are no pods in the namespace 03/15/23 22:36:26.293
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:36:26.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8316" for this suite. 03/15/23 22:36:26.3
    STEP: Destroying namespace "nsdeletetest-9967" for this suite. 03/15/23 22:36:26.32
    Mar 15 22:36:26.332: INFO: Namespace nsdeletetest-9967 was already deleted
    STEP: Destroying namespace "nsdeletetest-5239" for this suite. 03/15/23 22:36:26.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:36:26.352
Mar 15 22:36:26.352: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:36:26.353
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:26.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:36:26.382
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Mar 15 22:36:26.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 create -f -'
Mar 15 22:36:26.612: INFO: stderr: ""
Mar 15 22:36:26.612: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 15 22:36:26.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 create -f -'
Mar 15 22:36:26.850: INFO: stderr: ""
Mar 15 22:36:26.850: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/15/23 22:36:26.85
Mar 15 22:36:27.853: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 22:36:27.853: INFO: Found 0 / 1
Mar 15 22:36:28.853: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 22:36:28.853: INFO: Found 1 / 1
Mar 15 22:36:28.853: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 15 22:36:28.856: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 22:36:28.856: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 15 22:36:28.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 describe pod agnhost-primary-gl8tr'
Mar 15 22:36:28.959: INFO: stderr: ""
Mar 15 22:36:28.959: INFO: stdout: "Name:             agnhost-primary-gl8tr\nNamespace:        kubectl-3010\nPriority:         0\nService Account:  default\nNode:             i-0faaf83f00b43c88c/172.20.126.23\nStart Time:       Wed, 15 Mar 2023 22:36:26 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               100.96.3.158\nIPs:\n  IP:           100.96.3.158\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6427d228f4ad3933aafb830064218cc1c277f5db558413445fd20a20f2818b46\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 15 Mar 2023 22:36:27 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s7q2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-4s7q2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-3010/agnhost-primary-gl8tr to i-0faaf83f00b43c88c\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Mar 15 22:36:28.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 describe rc agnhost-primary'
Mar 15 22:36:29.094: INFO: stderr: ""
Mar 15 22:36:29.095: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3010\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-gl8tr\n"
Mar 15 22:36:29.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 describe service agnhost-primary'
Mar 15 22:36:29.190: INFO: stderr: ""
Mar 15 22:36:29.190: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3010\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.65.194.180\nIPs:               100.65.194.180\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.96.3.158:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 15 22:36:29.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 describe node i-00864a1c8c75a434c'
Mar 15 22:36:29.340: INFO: stderr: ""
Mar 15 22:36:29.340: INFO: stdout: "Name:               i-00864a1c8c75a434c\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2a\n                    kops.k8s.io/instancegroup=control-plane-us-west-2a\n                    kops.k8s.io/kops-controller-pki=\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=i-00864a1c8c75a434c\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=t3.medium\n                    topology.ebs.csi.aws.com/zone=us-west-2a\n                    topology.kubernetes.io/region=us-west-2\n                    topology.kubernetes.io/zone=us-west-2a\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-00864a1c8c75a434c\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 15 Mar 2023 21:39:23 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  i-00864a1c8c75a434c\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 15 Mar 2023 22:36:26 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 15 Mar 2023 22:34:51 +0000   Wed, 15 Mar 2023 21:39:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 15 Mar 2023 22:34:51 +0000   Wed, 15 Mar 2023 21:39:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 15 Mar 2023 22:34:51 +0000   Wed, 15 Mar 2023 21:39:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 15 Mar 2023 22:34:51 +0000   Wed, 15 Mar 2023 21:40:10 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.20.56.193\n  ExternalIP:   52.32.9.234\n  InternalDNS:  i-00864a1c8c75a434c.us-west-2.compute.internal\n  Hostname:     i-00864a1c8c75a434c.us-west-2.compute.internal\n  ExternalDNS:  ec2-52-32-9-234.us-west-2.compute.amazonaws.com\nCapacity:\n  cpu:                2\n  ephemeral-storage:  64847800Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3956720Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  59763732382\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3854320Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ec2d5e0d65005e20a86e680de6ef0720\n  System UUID:                ec2d5e0d-6500-5e20-a86e-680de6ef0720\n  Boot ID:                    dd9cc034-1635-41f8-a28d-60d88e83419f\n  Kernel Version:             5.15.0-1022-aws\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.26.2-eks-b106822\n  Kube-Proxy Version:         v1.26.2-eks-b106822\nPodCIDR:                      100.96.0.0/24\nPodCIDRs:                     100.96.0.0/24\nProviderID:                   aws:///us-west-2a/i-00864a1c8c75a434c\nNon-terminated Pods:          (15 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 aws-cloud-controller-manager-gd4zb                         200m (10%)    0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 aws-iam-authenticator-n8mbj                                10m (0%)      100m (5%)   20Mi (0%)        20Mi (0%)      56m\n  kube-system                 cilium-operator-5998bc87b9-vp4xk                           25m (1%)      0 (0%)      128Mi (3%)       0 (0%)         55m\n  kube-system                 cilium-pjwlr                                               100m (5%)     0 (0%)      128Mi (3%)       100Mi (2%)     55m\n  kube-system                 dns-controller-85bd475d78-rr8x9                            50m (2%)      0 (0%)      50Mi (1%)        0 (0%)         56m\n  kube-system                 ebs-csi-controller-7964c6697-rv5jx                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 ebs-csi-node-rzjph                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 etcd-manager-events-i-00864a1c8c75a434c                    100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         56m\n  kube-system                 etcd-manager-main-i-00864a1c8c75a434c                      200m (10%)    0 (0%)      100Mi (2%)       0 (0%)         56m\n  kube-system                 kops-controller-dt9jl                                      50m (2%)      0 (0%)      50Mi (1%)        0 (0%)         56m\n  kube-system                 kube-apiserver-i-00864a1c8c75a434c                         150m (7%)     0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 kube-controller-manager-i-00864a1c8c75a434c                100m (5%)     0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 kube-proxy-i-00864a1c8c75a434c                             100m (5%)     0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 kube-scheduler-i-00864a1c8c75a434c                         100m (5%)     0 (0%)      0 (0%)           0 (0%)         56m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-m28f5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1185m (59%)  100m (5%)\n  memory             576Mi (15%)  120Mi (3%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age                From                   Message\n  ----    ------                   ----               ----                   -------\n  Normal  Starting                 57m                kube-proxy             \n  Normal  NodeAllocatableEnforced  57m                kubelet                Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  57m (x8 over 57m)  kubelet                Node i-00864a1c8c75a434c status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    57m (x7 over 57m)  kubelet                Node i-00864a1c8c75a434c status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     57m (x7 over 57m)  kubelet                Node i-00864a1c8c75a434c status is now: NodeHasSufficientPID\n  Normal  RegisteredNode           56m                node-controller        Node i-00864a1c8c75a434c event: Registered Node i-00864a1c8c75a434c in Controller\n  Normal  Synced                   56m                cloud-node-controller  Node synced successfully\n"
Mar 15 22:36:29.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 describe namespace kubectl-3010'
Mar 15 22:36:29.438: INFO: stderr: ""
Mar 15 22:36:29.439: INFO: stdout: "Name:         kubectl-3010\nLabels:       e2e-framework=kubectl\n              e2e-run=5f02fcbf-8b49-43fa-a80d-391846325739\n              kubernetes.io/metadata.name=kubectl-3010\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:36:29.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3010" for this suite. 03/15/23 22:36:29.443
------------------------------
• [3.096 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:36:26.352
    Mar 15 22:36:26.352: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:36:26.353
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:26.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:36:26.382
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Mar 15 22:36:26.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 create -f -'
    Mar 15 22:36:26.612: INFO: stderr: ""
    Mar 15 22:36:26.612: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Mar 15 22:36:26.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 create -f -'
    Mar 15 22:36:26.850: INFO: stderr: ""
    Mar 15 22:36:26.850: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/15/23 22:36:26.85
    Mar 15 22:36:27.853: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 15 22:36:27.853: INFO: Found 0 / 1
    Mar 15 22:36:28.853: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 15 22:36:28.853: INFO: Found 1 / 1
    Mar 15 22:36:28.853: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 15 22:36:28.856: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 15 22:36:28.856: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 15 22:36:28.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 describe pod agnhost-primary-gl8tr'
    Mar 15 22:36:28.959: INFO: stderr: ""
    Mar 15 22:36:28.959: INFO: stdout: "Name:             agnhost-primary-gl8tr\nNamespace:        kubectl-3010\nPriority:         0\nService Account:  default\nNode:             i-0faaf83f00b43c88c/172.20.126.23\nStart Time:       Wed, 15 Mar 2023 22:36:26 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               100.96.3.158\nIPs:\n  IP:           100.96.3.158\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6427d228f4ad3933aafb830064218cc1c277f5db558413445fd20a20f2818b46\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 15 Mar 2023 22:36:27 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s7q2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-4s7q2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-3010/agnhost-primary-gl8tr to i-0faaf83f00b43c88c\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Mar 15 22:36:28.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 describe rc agnhost-primary'
    Mar 15 22:36:29.094: INFO: stderr: ""
    Mar 15 22:36:29.095: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3010\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-gl8tr\n"
    Mar 15 22:36:29.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 describe service agnhost-primary'
    Mar 15 22:36:29.190: INFO: stderr: ""
    Mar 15 22:36:29.190: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3010\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.65.194.180\nIPs:               100.65.194.180\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.96.3.158:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Mar 15 22:36:29.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 describe node i-00864a1c8c75a434c'
    Mar 15 22:36:29.340: INFO: stderr: ""
    Mar 15 22:36:29.340: INFO: stdout: "Name:               i-00864a1c8c75a434c\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2a\n                    kops.k8s.io/instancegroup=control-plane-us-west-2a\n                    kops.k8s.io/kops-controller-pki=\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=i-00864a1c8c75a434c\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=t3.medium\n                    topology.ebs.csi.aws.com/zone=us-west-2a\n                    topology.kubernetes.io/region=us-west-2\n                    topology.kubernetes.io/zone=us-west-2a\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-00864a1c8c75a434c\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 15 Mar 2023 21:39:23 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  i-00864a1c8c75a434c\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 15 Mar 2023 22:36:26 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 15 Mar 2023 22:34:51 +0000   Wed, 15 Mar 2023 21:39:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 15 Mar 2023 22:34:51 +0000   Wed, 15 Mar 2023 21:39:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 15 Mar 2023 22:34:51 +0000   Wed, 15 Mar 2023 21:39:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 15 Mar 2023 22:34:51 +0000   Wed, 15 Mar 2023 21:40:10 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.20.56.193\n  ExternalIP:   52.32.9.234\n  InternalDNS:  i-00864a1c8c75a434c.us-west-2.compute.internal\n  Hostname:     i-00864a1c8c75a434c.us-west-2.compute.internal\n  ExternalDNS:  ec2-52-32-9-234.us-west-2.compute.amazonaws.com\nCapacity:\n  cpu:                2\n  ephemeral-storage:  64847800Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3956720Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  59763732382\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3854320Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ec2d5e0d65005e20a86e680de6ef0720\n  System UUID:                ec2d5e0d-6500-5e20-a86e-680de6ef0720\n  Boot ID:                    dd9cc034-1635-41f8-a28d-60d88e83419f\n  Kernel Version:             5.15.0-1022-aws\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.26.2-eks-b106822\n  Kube-Proxy Version:         v1.26.2-eks-b106822\nPodCIDR:                      100.96.0.0/24\nPodCIDRs:                     100.96.0.0/24\nProviderID:                   aws:///us-west-2a/i-00864a1c8c75a434c\nNon-terminated Pods:          (15 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 aws-cloud-controller-manager-gd4zb                         200m (10%)    0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 aws-iam-authenticator-n8mbj                                10m (0%)      100m (5%)   20Mi (0%)        20Mi (0%)      56m\n  kube-system                 cilium-operator-5998bc87b9-vp4xk                           25m (1%)      0 (0%)      128Mi (3%)       0 (0%)         55m\n  kube-system                 cilium-pjwlr                                               100m (5%)     0 (0%)      128Mi (3%)       100Mi (2%)     55m\n  kube-system                 dns-controller-85bd475d78-rr8x9                            50m (2%)      0 (0%)      50Mi (1%)        0 (0%)         56m\n  kube-system                 ebs-csi-controller-7964c6697-rv5jx                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 ebs-csi-node-rzjph                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 etcd-manager-events-i-00864a1c8c75a434c                    100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         56m\n  kube-system                 etcd-manager-main-i-00864a1c8c75a434c                      200m (10%)    0 (0%)      100Mi (2%)       0 (0%)         56m\n  kube-system                 kops-controller-dt9jl                                      50m (2%)      0 (0%)      50Mi (1%)        0 (0%)         56m\n  kube-system                 kube-apiserver-i-00864a1c8c75a434c                         150m (7%)     0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 kube-controller-manager-i-00864a1c8c75a434c                100m (5%)     0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 kube-proxy-i-00864a1c8c75a434c                             100m (5%)     0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 kube-scheduler-i-00864a1c8c75a434c                         100m (5%)     0 (0%)      0 (0%)           0 (0%)         56m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-m28f5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1185m (59%)  100m (5%)\n  memory             576Mi (15%)  120Mi (3%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age                From                   Message\n  ----    ------                   ----               ----                   -------\n  Normal  Starting                 57m                kube-proxy             \n  Normal  NodeAllocatableEnforced  57m                kubelet                Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  57m (x8 over 57m)  kubelet                Node i-00864a1c8c75a434c status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    57m (x7 over 57m)  kubelet                Node i-00864a1c8c75a434c status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     57m (x7 over 57m)  kubelet                Node i-00864a1c8c75a434c status is now: NodeHasSufficientPID\n  Normal  RegisteredNode           56m                node-controller        Node i-00864a1c8c75a434c event: Registered Node i-00864a1c8c75a434c in Controller\n  Normal  Synced                   56m                cloud-node-controller  Node synced successfully\n"
    Mar 15 22:36:29.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-3010 describe namespace kubectl-3010'
    Mar 15 22:36:29.438: INFO: stderr: ""
    Mar 15 22:36:29.439: INFO: stdout: "Name:         kubectl-3010\nLabels:       e2e-framework=kubectl\n              e2e-run=5f02fcbf-8b49-43fa-a80d-391846325739\n              kubernetes.io/metadata.name=kubectl-3010\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:36:29.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3010" for this suite. 03/15/23 22:36:29.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:36:29.449
Mar 15 22:36:29.449: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-probe 03/15/23 22:36:29.45
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:29.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:36:29.465
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b in namespace container-probe-6495 03/15/23 22:36:29.47
Mar 15 22:36:29.478: INFO: Waiting up to 5m0s for pod "liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b" in namespace "container-probe-6495" to be "not pending"
Mar 15 22:36:29.482: INFO: Pod "liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.688719ms
Mar 15 22:36:31.488: INFO: Pod "liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009668049s
Mar 15 22:36:31.488: INFO: Pod "liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b" satisfied condition "not pending"
Mar 15 22:36:31.488: INFO: Started pod liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b in namespace container-probe-6495
STEP: checking the pod's current state and verifying that restartCount is present 03/15/23 22:36:31.488
Mar 15 22:36:31.491: INFO: Initial restart count of pod liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b is 0
Mar 15 22:36:51.534: INFO: Restart count of pod container-probe-6495/liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b is now 1 (20.042680581s elapsed)
STEP: deleting the pod 03/15/23 22:36:51.534
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 15 22:36:51.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6495" for this suite. 03/15/23 22:36:51.561
------------------------------
• [SLOW TEST] [22.121 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:36:29.449
    Mar 15 22:36:29.449: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-probe 03/15/23 22:36:29.45
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:29.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:36:29.465
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b in namespace container-probe-6495 03/15/23 22:36:29.47
    Mar 15 22:36:29.478: INFO: Waiting up to 5m0s for pod "liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b" in namespace "container-probe-6495" to be "not pending"
    Mar 15 22:36:29.482: INFO: Pod "liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.688719ms
    Mar 15 22:36:31.488: INFO: Pod "liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009668049s
    Mar 15 22:36:31.488: INFO: Pod "liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b" satisfied condition "not pending"
    Mar 15 22:36:31.488: INFO: Started pod liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b in namespace container-probe-6495
    STEP: checking the pod's current state and verifying that restartCount is present 03/15/23 22:36:31.488
    Mar 15 22:36:31.491: INFO: Initial restart count of pod liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b is 0
    Mar 15 22:36:51.534: INFO: Restart count of pod container-probe-6495/liveness-d40319ef-2aa4-4a6a-b132-2bda55429a4b is now 1 (20.042680581s elapsed)
    STEP: deleting the pod 03/15/23 22:36:51.534
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:36:51.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6495" for this suite. 03/15/23 22:36:51.561
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:36:51.571
Mar 15 22:36:51.571: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename dns 03/15/23 22:36:51.572
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:51.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:36:51.614
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 03/15/23 22:36:51.618
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 0.80.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.80.0_udp@PTR;check="$$(dig +tcp +noall +answer +search 0.80.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.80.0_tcp@PTR;sleep 1; done
 03/15/23 22:36:51.648
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 0.80.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.80.0_udp@PTR;check="$$(dig +tcp +noall +answer +search 0.80.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.80.0_tcp@PTR;sleep 1; done
 03/15/23 22:36:51.649
STEP: creating a pod to probe DNS 03/15/23 22:36:51.649
STEP: submitting the pod to kubernetes 03/15/23 22:36:51.65
Mar 15 22:36:51.667: INFO: Waiting up to 15m0s for pod "dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5" in namespace "dns-9941" to be "running"
Mar 15 22:36:51.675: INFO: Pod "dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.983039ms
Mar 15 22:36:53.679: INFO: Pod "dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010291336s
Mar 15 22:36:53.679: INFO: Pod "dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5" satisfied condition "running"
STEP: retrieving the pod 03/15/23 22:36:53.679
STEP: looking for the results for each expected name from probers 03/15/23 22:36:53.683
Mar 15 22:36:53.691: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:53.696: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:53.699: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:53.702: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:53.731: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:53.736: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:53.740: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:53.745: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:53.758: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

Mar 15 22:36:58.767: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:58.771: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:58.775: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:58.779: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:58.796: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:58.800: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:58.803: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:58.809: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:36:58.827: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

Mar 15 22:37:03.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:03.768: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:03.778: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:03.785: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:03.811: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:03.815: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:03.819: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:03.823: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:03.851: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

Mar 15 22:37:08.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:08.766: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:08.769: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:08.772: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:08.786: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:08.789: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:08.792: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:08.795: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:08.806: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

Mar 15 22:37:13.762: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:13.767: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:13.770: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:13.773: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:13.791: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:13.795: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:13.798: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:13.801: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:13.815: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

Mar 15 22:37:18.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:18.767: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:18.770: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:18.773: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:18.788: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:18.792: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:18.795: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:18.798: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:18.811: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

Mar 15 22:37:23.762: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:23.766: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:23.773: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:23.777: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:23.801: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:23.805: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:23.808: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:23.813: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
Mar 15 22:37:23.832: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

Mar 15 22:37:28.815: INFO: DNS probes using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 succeeded

STEP: deleting the pod 03/15/23 22:37:28.815
STEP: deleting the test service 03/15/23 22:37:28.841
STEP: deleting the test headless service 03/15/23 22:37:28.895
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 15 22:37:28.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9941" for this suite. 03/15/23 22:37:28.946
------------------------------
• [SLOW TEST] [37.388 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:36:51.571
    Mar 15 22:36:51.571: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename dns 03/15/23 22:36:51.572
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:36:51.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:36:51.614
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 03/15/23 22:36:51.618
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 0.80.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.80.0_udp@PTR;check="$$(dig +tcp +noall +answer +search 0.80.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.80.0_tcp@PTR;sleep 1; done
     03/15/23 22:36:51.648
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 0.80.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.80.0_udp@PTR;check="$$(dig +tcp +noall +answer +search 0.80.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.80.0_tcp@PTR;sleep 1; done
     03/15/23 22:36:51.649
    STEP: creating a pod to probe DNS 03/15/23 22:36:51.649
    STEP: submitting the pod to kubernetes 03/15/23 22:36:51.65
    Mar 15 22:36:51.667: INFO: Waiting up to 15m0s for pod "dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5" in namespace "dns-9941" to be "running"
    Mar 15 22:36:51.675: INFO: Pod "dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.983039ms
    Mar 15 22:36:53.679: INFO: Pod "dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010291336s
    Mar 15 22:36:53.679: INFO: Pod "dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5" satisfied condition "running"
    STEP: retrieving the pod 03/15/23 22:36:53.679
    STEP: looking for the results for each expected name from probers 03/15/23 22:36:53.683
    Mar 15 22:36:53.691: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:53.696: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:53.699: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:53.702: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:53.731: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:53.736: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:53.740: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:53.745: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:53.758: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

    Mar 15 22:36:58.767: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:58.771: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:58.775: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:58.779: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:58.796: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:58.800: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:58.803: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:58.809: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:36:58.827: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

    Mar 15 22:37:03.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:03.768: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:03.778: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:03.785: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:03.811: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:03.815: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:03.819: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:03.823: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:03.851: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

    Mar 15 22:37:08.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:08.766: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:08.769: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:08.772: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:08.786: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:08.789: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:08.792: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:08.795: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:08.806: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

    Mar 15 22:37:13.762: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:13.767: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:13.770: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:13.773: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:13.791: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:13.795: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:13.798: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:13.801: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:13.815: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

    Mar 15 22:37:18.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:18.767: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:18.770: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:18.773: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:18.788: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:18.792: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:18.795: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:18.798: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:18.811: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

    Mar 15 22:37:23.762: INFO: Unable to read wheezy_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:23.766: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:23.773: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:23.777: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:23.801: INFO: Unable to read jessie_udp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:23.805: INFO: Unable to read jessie_tcp@dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:23.808: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:23.813: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local from pod dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5: the server could not find the requested resource (get pods dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5)
    Mar 15 22:37:23.832: INFO: Lookups using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 failed for: [wheezy_udp@dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@dns-test-service.dns-9941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_udp@dns-test-service.dns-9941.svc.cluster.local jessie_tcp@dns-test-service.dns-9941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9941.svc.cluster.local]

    Mar 15 22:37:28.815: INFO: DNS probes using dns-9941/dns-test-15d10693-cc86-43f2-9fbb-6a37757c95d5 succeeded

    STEP: deleting the pod 03/15/23 22:37:28.815
    STEP: deleting the test service 03/15/23 22:37:28.841
    STEP: deleting the test headless service 03/15/23 22:37:28.895
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:37:28.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9941" for this suite. 03/15/23 22:37:28.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:37:28.97
Mar 15 22:37:28.970: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename cronjob 03/15/23 22:37:28.971
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:37:28.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:37:28.997
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 03/15/23 22:37:29.011
STEP: Ensuring a job is scheduled 03/15/23 22:37:29.022
STEP: Ensuring exactly one is scheduled 03/15/23 22:38:01.03
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/15/23 22:38:01.035
STEP: Ensuring the job is replaced with a new one 03/15/23 22:38:01.039
STEP: Removing cronjob 03/15/23 22:39:01.044
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:01.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7666" for this suite. 03/15/23 22:39:01.062
------------------------------
• [SLOW TEST] [92.116 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:37:28.97
    Mar 15 22:37:28.970: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename cronjob 03/15/23 22:37:28.971
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:37:28.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:37:28.997
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 03/15/23 22:37:29.011
    STEP: Ensuring a job is scheduled 03/15/23 22:37:29.022
    STEP: Ensuring exactly one is scheduled 03/15/23 22:38:01.03
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/15/23 22:38:01.035
    STEP: Ensuring the job is replaced with a new one 03/15/23 22:38:01.039
    STEP: Removing cronjob 03/15/23 22:39:01.044
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:01.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7666" for this suite. 03/15/23 22:39:01.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:01.092
Mar 15 22:39:01.093: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename deployment 03/15/23 22:39:01.094
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:01.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:01.115
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Mar 15 22:39:01.130: INFO: Creating deployment "test-recreate-deployment"
Mar 15 22:39:01.146: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 15 22:39:01.175: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 15 22:39:03.182: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 15 22:39:03.186: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 15 22:39:03.194: INFO: Updating deployment test-recreate-deployment
Mar 15 22:39:03.194: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 15 22:39:03.327: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7227  67cac392-04b6-4a5d-83c5-1752caeb372b 23791 2 2023-03-15 22:39:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002989778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-15 22:39:03 +0000 UTC,LastTransitionTime:2023-03-15 22:39:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-03-15 22:39:03 +0000 UTC,LastTransitionTime:2023-03-15 22:39:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 15 22:39:03.330: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-7227  0313d231-0b1a-4d40-8de9-73709504621a 23789 1 2023-03-15 22:39:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 67cac392-04b6-4a5d-83c5-1752caeb372b 0xc002989c40 0xc002989c41}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cac392-04b6-4a5d-83c5-1752caeb372b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002989cd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 15 22:39:03.330: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 15 22:39:03.331: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-7227  91122535-3943-4813-adc9-ad087ca6e37c 23782 2 2023-03-15 22:39:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 67cac392-04b6-4a5d-83c5-1752caeb372b 0xc002989b27 0xc002989b28}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cac392-04b6-4a5d-83c5-1752caeb372b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002989bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 15 22:39:03.337: INFO: Pod "test-recreate-deployment-cff6dc657-zwvqj" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-zwvqj test-recreate-deployment-cff6dc657- deployment-7227  380ed3d1-fb82-4d97-b984-26ab6720cd43 23790 0 2023-03-15 22:39:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 0313d231-0b1a-4d40-8de9-73709504621a 0xc002f5b7c0 0xc002f5b7c1}] [] [{kube-controller-manager Update v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0313d231-0b1a-4d40-8de9-73709504621a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zq45r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zq45r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:39:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:03.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7227" for this suite. 03/15/23 22:39:03.341
------------------------------
• [2.254 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:01.092
    Mar 15 22:39:01.093: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename deployment 03/15/23 22:39:01.094
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:01.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:01.115
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Mar 15 22:39:01.130: INFO: Creating deployment "test-recreate-deployment"
    Mar 15 22:39:01.146: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Mar 15 22:39:01.175: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Mar 15 22:39:03.182: INFO: Waiting deployment "test-recreate-deployment" to complete
    Mar 15 22:39:03.186: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Mar 15 22:39:03.194: INFO: Updating deployment test-recreate-deployment
    Mar 15 22:39:03.194: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 15 22:39:03.327: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-7227  67cac392-04b6-4a5d-83c5-1752caeb372b 23791 2 2023-03-15 22:39:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002989778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-15 22:39:03 +0000 UTC,LastTransitionTime:2023-03-15 22:39:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-03-15 22:39:03 +0000 UTC,LastTransitionTime:2023-03-15 22:39:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Mar 15 22:39:03.330: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-7227  0313d231-0b1a-4d40-8de9-73709504621a 23789 1 2023-03-15 22:39:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 67cac392-04b6-4a5d-83c5-1752caeb372b 0xc002989c40 0xc002989c41}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cac392-04b6-4a5d-83c5-1752caeb372b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002989cd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 15 22:39:03.330: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Mar 15 22:39:03.331: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-7227  91122535-3943-4813-adc9-ad087ca6e37c 23782 2 2023-03-15 22:39:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 67cac392-04b6-4a5d-83c5-1752caeb372b 0xc002989b27 0xc002989b28}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cac392-04b6-4a5d-83c5-1752caeb372b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002989bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 15 22:39:03.337: INFO: Pod "test-recreate-deployment-cff6dc657-zwvqj" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-zwvqj test-recreate-deployment-cff6dc657- deployment-7227  380ed3d1-fb82-4d97-b984-26ab6720cd43 23790 0 2023-03-15 22:39:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 0313d231-0b1a-4d40-8de9-73709504621a 0xc002f5b7c0 0xc002f5b7c1}] [] [{kube-controller-manager Update v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0313d231-0b1a-4d40-8de9-73709504621a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:39:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zq45r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zq45r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:39:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:03.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7227" for this suite. 03/15/23 22:39:03.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:03.354
Mar 15 22:39:03.354: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:39:03.355
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:03.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:03.374
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-3651ba53-efcb-46c6-ae02-8cd0a2db085e 03/15/23 22:39:03.378
STEP: Creating a pod to test consume configMaps 03/15/23 22:39:03.382
Mar 15 22:39:03.394: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795" in namespace "configmap-3149" to be "Succeeded or Failed"
Mar 15 22:39:03.400: INFO: Pod "pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795": Phase="Pending", Reason="", readiness=false. Elapsed: 5.881614ms
Mar 15 22:39:05.418: INFO: Pod "pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023504911s
Mar 15 22:39:07.404: INFO: Pod "pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009038458s
STEP: Saw pod success 03/15/23 22:39:07.404
Mar 15 22:39:07.404: INFO: Pod "pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795" satisfied condition "Succeeded or Failed"
Mar 15 22:39:07.407: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795 container configmap-volume-test: <nil>
STEP: delete the pod 03/15/23 22:39:07.424
Mar 15 22:39:07.440: INFO: Waiting for pod pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795 to disappear
Mar 15 22:39:07.443: INFO: Pod pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:07.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3149" for this suite. 03/15/23 22:39:07.448
------------------------------
• [4.100 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:03.354
    Mar 15 22:39:03.354: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:39:03.355
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:03.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:03.374
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-3651ba53-efcb-46c6-ae02-8cd0a2db085e 03/15/23 22:39:03.378
    STEP: Creating a pod to test consume configMaps 03/15/23 22:39:03.382
    Mar 15 22:39:03.394: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795" in namespace "configmap-3149" to be "Succeeded or Failed"
    Mar 15 22:39:03.400: INFO: Pod "pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795": Phase="Pending", Reason="", readiness=false. Elapsed: 5.881614ms
    Mar 15 22:39:05.418: INFO: Pod "pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023504911s
    Mar 15 22:39:07.404: INFO: Pod "pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009038458s
    STEP: Saw pod success 03/15/23 22:39:07.404
    Mar 15 22:39:07.404: INFO: Pod "pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795" satisfied condition "Succeeded or Failed"
    Mar 15 22:39:07.407: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795 container configmap-volume-test: <nil>
    STEP: delete the pod 03/15/23 22:39:07.424
    Mar 15 22:39:07.440: INFO: Waiting for pod pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795 to disappear
    Mar 15 22:39:07.443: INFO: Pod pod-configmaps-a3b54b45-8db3-456f-ac19-94b7213d0795 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:07.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3149" for this suite. 03/15/23 22:39:07.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:07.462
Mar 15 22:39:07.462: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename runtimeclass 03/15/23 22:39:07.463
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:07.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:07.488
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:07.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9675" for this suite. 03/15/23 22:39:07.503
------------------------------
• [0.048 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:07.462
    Mar 15 22:39:07.462: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename runtimeclass 03/15/23 22:39:07.463
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:07.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:07.488
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:07.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9675" for this suite. 03/15/23 22:39:07.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:07.512
Mar 15 22:39:07.512: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename security-context-test 03/15/23 22:39:07.513
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:07.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:07.53
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Mar 15 22:39:07.540: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-331e974a-ec7d-4de1-95be-9f09567cd9c9" in namespace "security-context-test-165" to be "Succeeded or Failed"
Mar 15 22:39:07.547: INFO: Pod "alpine-nnp-false-331e974a-ec7d-4de1-95be-9f09567cd9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.72506ms
Mar 15 22:39:09.551: INFO: Pod "alpine-nnp-false-331e974a-ec7d-4de1-95be-9f09567cd9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010507602s
Mar 15 22:39:11.552: INFO: Pod "alpine-nnp-false-331e974a-ec7d-4de1-95be-9f09567cd9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011851878s
Mar 15 22:39:13.554: INFO: Pod "alpine-nnp-false-331e974a-ec7d-4de1-95be-9f09567cd9c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0137791s
Mar 15 22:39:13.554: INFO: Pod "alpine-nnp-false-331e974a-ec7d-4de1-95be-9f09567cd9c9" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:13.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-165" for this suite. 03/15/23 22:39:13.564
------------------------------
• [SLOW TEST] [6.057 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:07.512
    Mar 15 22:39:07.512: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename security-context-test 03/15/23 22:39:07.513
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:07.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:07.53
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Mar 15 22:39:07.540: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-331e974a-ec7d-4de1-95be-9f09567cd9c9" in namespace "security-context-test-165" to be "Succeeded or Failed"
    Mar 15 22:39:07.547: INFO: Pod "alpine-nnp-false-331e974a-ec7d-4de1-95be-9f09567cd9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.72506ms
    Mar 15 22:39:09.551: INFO: Pod "alpine-nnp-false-331e974a-ec7d-4de1-95be-9f09567cd9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010507602s
    Mar 15 22:39:11.552: INFO: Pod "alpine-nnp-false-331e974a-ec7d-4de1-95be-9f09567cd9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011851878s
    Mar 15 22:39:13.554: INFO: Pod "alpine-nnp-false-331e974a-ec7d-4de1-95be-9f09567cd9c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0137791s
    Mar 15 22:39:13.554: INFO: Pod "alpine-nnp-false-331e974a-ec7d-4de1-95be-9f09567cd9c9" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:13.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-165" for this suite. 03/15/23 22:39:13.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:13.571
Mar 15 22:39:13.571: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:39:13.572
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:13.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:13.589
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-3206/configmap-test-e8f954f8-10e2-466d-b24e-b61f652dd3c6 03/15/23 22:39:13.592
STEP: Creating a pod to test consume configMaps 03/15/23 22:39:13.596
Mar 15 22:39:13.603: INFO: Waiting up to 5m0s for pod "pod-configmaps-2599f426-d747-458b-980a-6844cce43757" in namespace "configmap-3206" to be "Succeeded or Failed"
Mar 15 22:39:13.608: INFO: Pod "pod-configmaps-2599f426-d747-458b-980a-6844cce43757": Phase="Pending", Reason="", readiness=false. Elapsed: 4.799682ms
Mar 15 22:39:15.613: INFO: Pod "pod-configmaps-2599f426-d747-458b-980a-6844cce43757": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01034754s
Mar 15 22:39:17.611: INFO: Pod "pod-configmaps-2599f426-d747-458b-980a-6844cce43757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007929548s
STEP: Saw pod success 03/15/23 22:39:17.611
Mar 15 22:39:17.611: INFO: Pod "pod-configmaps-2599f426-d747-458b-980a-6844cce43757" satisfied condition "Succeeded or Failed"
Mar 15 22:39:17.614: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-2599f426-d747-458b-980a-6844cce43757 container env-test: <nil>
STEP: delete the pod 03/15/23 22:39:17.621
Mar 15 22:39:17.642: INFO: Waiting for pod pod-configmaps-2599f426-d747-458b-980a-6844cce43757 to disappear
Mar 15 22:39:17.644: INFO: Pod pod-configmaps-2599f426-d747-458b-980a-6844cce43757 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:17.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3206" for this suite. 03/15/23 22:39:17.649
------------------------------
• [4.086 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:13.571
    Mar 15 22:39:13.571: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:39:13.572
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:13.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:13.589
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-3206/configmap-test-e8f954f8-10e2-466d-b24e-b61f652dd3c6 03/15/23 22:39:13.592
    STEP: Creating a pod to test consume configMaps 03/15/23 22:39:13.596
    Mar 15 22:39:13.603: INFO: Waiting up to 5m0s for pod "pod-configmaps-2599f426-d747-458b-980a-6844cce43757" in namespace "configmap-3206" to be "Succeeded or Failed"
    Mar 15 22:39:13.608: INFO: Pod "pod-configmaps-2599f426-d747-458b-980a-6844cce43757": Phase="Pending", Reason="", readiness=false. Elapsed: 4.799682ms
    Mar 15 22:39:15.613: INFO: Pod "pod-configmaps-2599f426-d747-458b-980a-6844cce43757": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01034754s
    Mar 15 22:39:17.611: INFO: Pod "pod-configmaps-2599f426-d747-458b-980a-6844cce43757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007929548s
    STEP: Saw pod success 03/15/23 22:39:17.611
    Mar 15 22:39:17.611: INFO: Pod "pod-configmaps-2599f426-d747-458b-980a-6844cce43757" satisfied condition "Succeeded or Failed"
    Mar 15 22:39:17.614: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-2599f426-d747-458b-980a-6844cce43757 container env-test: <nil>
    STEP: delete the pod 03/15/23 22:39:17.621
    Mar 15 22:39:17.642: INFO: Waiting for pod pod-configmaps-2599f426-d747-458b-980a-6844cce43757 to disappear
    Mar 15 22:39:17.644: INFO: Pod pod-configmaps-2599f426-d747-458b-980a-6844cce43757 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:17.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3206" for this suite. 03/15/23 22:39:17.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:17.658
Mar 15 22:39:17.658: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename runtimeclass 03/15/23 22:39:17.66
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:17.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:17.675
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 03/15/23 22:39:17.678
STEP: getting /apis/node.k8s.io 03/15/23 22:39:17.685
STEP: getting /apis/node.k8s.io/v1 03/15/23 22:39:17.692
STEP: creating 03/15/23 22:39:17.694
STEP: watching 03/15/23 22:39:17.71
Mar 15 22:39:17.710: INFO: starting watch
STEP: getting 03/15/23 22:39:17.715
STEP: listing 03/15/23 22:39:17.717
STEP: patching 03/15/23 22:39:17.72
STEP: updating 03/15/23 22:39:17.725
Mar 15 22:39:17.729: INFO: waiting for watch events with expected annotations
STEP: deleting 03/15/23 22:39:17.73
STEP: deleting a collection 03/15/23 22:39:17.739
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:17.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3035" for this suite. 03/15/23 22:39:17.756
------------------------------
• [0.106 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:17.658
    Mar 15 22:39:17.658: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename runtimeclass 03/15/23 22:39:17.66
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:17.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:17.675
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 03/15/23 22:39:17.678
    STEP: getting /apis/node.k8s.io 03/15/23 22:39:17.685
    STEP: getting /apis/node.k8s.io/v1 03/15/23 22:39:17.692
    STEP: creating 03/15/23 22:39:17.694
    STEP: watching 03/15/23 22:39:17.71
    Mar 15 22:39:17.710: INFO: starting watch
    STEP: getting 03/15/23 22:39:17.715
    STEP: listing 03/15/23 22:39:17.717
    STEP: patching 03/15/23 22:39:17.72
    STEP: updating 03/15/23 22:39:17.725
    Mar 15 22:39:17.729: INFO: waiting for watch events with expected annotations
    STEP: deleting 03/15/23 22:39:17.73
    STEP: deleting a collection 03/15/23 22:39:17.739
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:17.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3035" for this suite. 03/15/23 22:39:17.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:17.765
Mar 15 22:39:17.765: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename security-context-test 03/15/23 22:39:17.766
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:17.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:17.782
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Mar 15 22:39:17.799: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b6b08a6e-821a-467d-8c6e-8d520eedf1dd" in namespace "security-context-test-4965" to be "Succeeded or Failed"
Mar 15 22:39:17.806: INFO: Pod "busybox-user-65534-b6b08a6e-821a-467d-8c6e-8d520eedf1dd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.802171ms
Mar 15 22:39:19.809: INFO: Pod "busybox-user-65534-b6b08a6e-821a-467d-8c6e-8d520eedf1dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01009992s
Mar 15 22:39:21.810: INFO: Pod "busybox-user-65534-b6b08a6e-821a-467d-8c6e-8d520eedf1dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011291334s
Mar 15 22:39:21.810: INFO: Pod "busybox-user-65534-b6b08a6e-821a-467d-8c6e-8d520eedf1dd" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:21.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4965" for this suite. 03/15/23 22:39:21.814
------------------------------
• [4.067 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:17.765
    Mar 15 22:39:17.765: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename security-context-test 03/15/23 22:39:17.766
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:17.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:17.782
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Mar 15 22:39:17.799: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b6b08a6e-821a-467d-8c6e-8d520eedf1dd" in namespace "security-context-test-4965" to be "Succeeded or Failed"
    Mar 15 22:39:17.806: INFO: Pod "busybox-user-65534-b6b08a6e-821a-467d-8c6e-8d520eedf1dd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.802171ms
    Mar 15 22:39:19.809: INFO: Pod "busybox-user-65534-b6b08a6e-821a-467d-8c6e-8d520eedf1dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01009992s
    Mar 15 22:39:21.810: INFO: Pod "busybox-user-65534-b6b08a6e-821a-467d-8c6e-8d520eedf1dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011291334s
    Mar 15 22:39:21.810: INFO: Pod "busybox-user-65534-b6b08a6e-821a-467d-8c6e-8d520eedf1dd" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:21.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4965" for this suite. 03/15/23 22:39:21.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:21.835
Mar 15 22:39:21.835: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubelet-test 03/15/23 22:39:21.836
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:21.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:21.897
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:25.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-57" for this suite. 03/15/23 22:39:25.971
------------------------------
• [4.143 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:21.835
    Mar 15 22:39:21.835: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubelet-test 03/15/23 22:39:21.836
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:21.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:21.897
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:25.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-57" for this suite. 03/15/23 22:39:25.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:25.981
Mar 15 22:39:25.981: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename podtemplate 03/15/23 22:39:25.981
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:25.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:26.002
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:26.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7819" for this suite. 03/15/23 22:39:26.072
------------------------------
• [0.098 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:25.981
    Mar 15 22:39:25.981: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename podtemplate 03/15/23 22:39:25.981
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:25.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:26.002
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:26.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7819" for this suite. 03/15/23 22:39:26.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:26.086
Mar 15 22:39:26.086: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename deployment 03/15/23 22:39:26.089
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:26.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:26.103
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Mar 15 22:39:26.121: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 15 22:39:31.130: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/15/23 22:39:31.13
Mar 15 22:39:31.130: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 15 22:39:33.134: INFO: Creating deployment "test-rollover-deployment"
Mar 15 22:39:33.142: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 15 22:39:35.148: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 15 22:39:35.153: INFO: Ensure that both replica sets have 1 created replica
Mar 15 22:39:35.160: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 15 22:39:35.172: INFO: Updating deployment test-rollover-deployment
Mar 15 22:39:35.172: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 15 22:39:37.182: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 15 22:39:37.189: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 15 22:39:37.195: INFO: all replica sets need to contain the pod-template-hash label
Mar 15 22:39:37.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:39:39.202: INFO: all replica sets need to contain the pod-template-hash label
Mar 15 22:39:39.202: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:39:41.204: INFO: all replica sets need to contain the pod-template-hash label
Mar 15 22:39:41.204: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:39:43.201: INFO: all replica sets need to contain the pod-template-hash label
Mar 15 22:39:43.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:39:45.201: INFO: all replica sets need to contain the pod-template-hash label
Mar 15 22:39:45.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 22:39:47.201: INFO: 
Mar 15 22:39:47.201: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 15 22:39:47.209: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2195  2be9623a-dc58-489c-ae6f-cb6edfc38d27 24159 2 2023-03-15 22:39:33 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-15 22:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039b1a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-15 22:39:33 +0000 UTC,LastTransitionTime:2023-03-15 22:39:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-03-15 22:39:46 +0000 UTC,LastTransitionTime:2023-03-15 22:39:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 15 22:39:47.212: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2195  c6c71878-a205-4611-bb04-ab10ff7a5048 24151 2 2023-03-15 22:39:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 2be9623a-dc58-489c-ae6f-cb6edfc38d27 0xc002d946d7 0xc002d946d8}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2be9623a-dc58-489c-ae6f-cb6edfc38d27\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d94798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 15 22:39:47.212: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 15 22:39:47.212: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2195  8b4ce1a1-dce4-4533-a139-4c893ef62638 24157 2 2023-03-15 22:39:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 2be9623a-dc58-489c-ae6f-cb6edfc38d27 0xc002d94587 0xc002d94588}] [] [{e2e.test Update apps/v1 2023-03-15 22:39:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2be9623a-dc58-489c-ae6f-cb6edfc38d27\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002d94668 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 15 22:39:47.213: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2195  4232b32a-1169-4f85-b909-86ba3f61e589 24105 2 2023-03-15 22:39:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 2be9623a-dc58-489c-ae6f-cb6edfc38d27 0xc002d94817 0xc002d94818}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2be9623a-dc58-489c-ae6f-cb6edfc38d27\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d948c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 15 22:39:47.215: INFO: Pod "test-rollover-deployment-6c6df9974f-97sfm" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-97sfm test-rollover-deployment-6c6df9974f- deployment-2195  47cd4dae-620a-4a94-8dac-2995e79afca5 24111 0 2023-03-15 22:39:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f c6c71878-a205-4611-bb04-ab10ff7a5048 0xc002d94e17 0xc002d94e18}] [] [{kube-controller-manager Update v1 2023-03-15 22:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6c71878-a205-4611-bb04-ab10ff7a5048\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:39:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xq7tc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xq7tc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.203,StartTime:2023-03-15 22:39:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:39:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://1419e487140092bb940cb70938eab0a006040fcb6809e50b2ca501767348e756,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:47.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2195" for this suite. 03/15/23 22:39:47.219
------------------------------
• [SLOW TEST] [21.141 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:26.086
    Mar 15 22:39:26.086: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename deployment 03/15/23 22:39:26.089
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:26.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:26.103
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Mar 15 22:39:26.121: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Mar 15 22:39:31.130: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/15/23 22:39:31.13
    Mar 15 22:39:31.130: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Mar 15 22:39:33.134: INFO: Creating deployment "test-rollover-deployment"
    Mar 15 22:39:33.142: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Mar 15 22:39:35.148: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Mar 15 22:39:35.153: INFO: Ensure that both replica sets have 1 created replica
    Mar 15 22:39:35.160: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Mar 15 22:39:35.172: INFO: Updating deployment test-rollover-deployment
    Mar 15 22:39:35.172: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Mar 15 22:39:37.182: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Mar 15 22:39:37.189: INFO: Make sure deployment "test-rollover-deployment" is complete
    Mar 15 22:39:37.195: INFO: all replica sets need to contain the pod-template-hash label
    Mar 15 22:39:37.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:39:39.202: INFO: all replica sets need to contain the pod-template-hash label
    Mar 15 22:39:39.202: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:39:41.204: INFO: all replica sets need to contain the pod-template-hash label
    Mar 15 22:39:41.204: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:39:43.201: INFO: all replica sets need to contain the pod-template-hash label
    Mar 15 22:39:43.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:39:45.201: INFO: all replica sets need to contain the pod-template-hash label
    Mar 15 22:39:45.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 15, 22, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 15, 22, 39, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 15 22:39:47.201: INFO: 
    Mar 15 22:39:47.201: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 15 22:39:47.209: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2195  2be9623a-dc58-489c-ae6f-cb6edfc38d27 24159 2 2023-03-15 22:39:33 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-15 22:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039b1a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-15 22:39:33 +0000 UTC,LastTransitionTime:2023-03-15 22:39:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-03-15 22:39:46 +0000 UTC,LastTransitionTime:2023-03-15 22:39:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 15 22:39:47.212: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2195  c6c71878-a205-4611-bb04-ab10ff7a5048 24151 2 2023-03-15 22:39:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 2be9623a-dc58-489c-ae6f-cb6edfc38d27 0xc002d946d7 0xc002d946d8}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2be9623a-dc58-489c-ae6f-cb6edfc38d27\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d94798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 15 22:39:47.212: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Mar 15 22:39:47.212: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2195  8b4ce1a1-dce4-4533-a139-4c893ef62638 24157 2 2023-03-15 22:39:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 2be9623a-dc58-489c-ae6f-cb6edfc38d27 0xc002d94587 0xc002d94588}] [] [{e2e.test Update apps/v1 2023-03-15 22:39:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2be9623a-dc58-489c-ae6f-cb6edfc38d27\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002d94668 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 15 22:39:47.213: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2195  4232b32a-1169-4f85-b909-86ba3f61e589 24105 2 2023-03-15 22:39:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 2be9623a-dc58-489c-ae6f-cb6edfc38d27 0xc002d94817 0xc002d94818}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2be9623a-dc58-489c-ae6f-cb6edfc38d27\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:39:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d948c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 15 22:39:47.215: INFO: Pod "test-rollover-deployment-6c6df9974f-97sfm" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-97sfm test-rollover-deployment-6c6df9974f- deployment-2195  47cd4dae-620a-4a94-8dac-2995e79afca5 24111 0 2023-03-15 22:39:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f c6c71878-a205-4611-bb04-ab10ff7a5048 0xc002d94e17 0xc002d94e18}] [] [{kube-controller-manager Update v1 2023-03-15 22:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6c71878-a205-4611-bb04-ab10ff7a5048\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:39:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xq7tc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xq7tc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:39:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.203,StartTime:2023-03-15 22:39:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:39:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://1419e487140092bb940cb70938eab0a006040fcb6809e50b2ca501767348e756,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:47.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2195" for this suite. 03/15/23 22:39:47.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:47.233
Mar 15 22:39:47.233: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename security-context-test 03/15/23 22:39:47.234
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:47.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:47.25
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Mar 15 22:39:47.262: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-8ddab76c-ddc8-4e07-a46d-d260395dd69a" in namespace "security-context-test-1398" to be "Succeeded or Failed"
Mar 15 22:39:47.267: INFO: Pod "busybox-readonly-false-8ddab76c-ddc8-4e07-a46d-d260395dd69a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.319102ms
Mar 15 22:39:49.271: INFO: Pod "busybox-readonly-false-8ddab76c-ddc8-4e07-a46d-d260395dd69a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008242433s
Mar 15 22:39:51.272: INFO: Pod "busybox-readonly-false-8ddab76c-ddc8-4e07-a46d-d260395dd69a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009251546s
Mar 15 22:39:51.272: INFO: Pod "busybox-readonly-false-8ddab76c-ddc8-4e07-a46d-d260395dd69a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:51.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1398" for this suite. 03/15/23 22:39:51.281
------------------------------
• [4.059 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:47.233
    Mar 15 22:39:47.233: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename security-context-test 03/15/23 22:39:47.234
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:47.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:47.25
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Mar 15 22:39:47.262: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-8ddab76c-ddc8-4e07-a46d-d260395dd69a" in namespace "security-context-test-1398" to be "Succeeded or Failed"
    Mar 15 22:39:47.267: INFO: Pod "busybox-readonly-false-8ddab76c-ddc8-4e07-a46d-d260395dd69a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.319102ms
    Mar 15 22:39:49.271: INFO: Pod "busybox-readonly-false-8ddab76c-ddc8-4e07-a46d-d260395dd69a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008242433s
    Mar 15 22:39:51.272: INFO: Pod "busybox-readonly-false-8ddab76c-ddc8-4e07-a46d-d260395dd69a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009251546s
    Mar 15 22:39:51.272: INFO: Pod "busybox-readonly-false-8ddab76c-ddc8-4e07-a46d-d260395dd69a" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:51.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1398" for this suite. 03/15/23 22:39:51.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:51.298
Mar 15 22:39:51.299: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename replication-controller 03/15/23 22:39:51.3
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:51.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:51.319
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Mar 15 22:39:51.323: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/15/23 22:39:51.342
STEP: Checking rc "condition-test" has the desired failure condition set 03/15/23 22:39:51.347
STEP: Scaling down rc "condition-test" to satisfy pod quota 03/15/23 22:39:52.362
Mar 15 22:39:52.384: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 03/15/23 22:39:52.384
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:52.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8222" for this suite. 03/15/23 22:39:52.399
------------------------------
• [1.115 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:51.298
    Mar 15 22:39:51.299: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename replication-controller 03/15/23 22:39:51.3
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:51.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:51.319
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Mar 15 22:39:51.323: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/15/23 22:39:51.342
    STEP: Checking rc "condition-test" has the desired failure condition set 03/15/23 22:39:51.347
    STEP: Scaling down rc "condition-test" to satisfy pod quota 03/15/23 22:39:52.362
    Mar 15 22:39:52.384: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 03/15/23 22:39:52.384
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:52.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8222" for this suite. 03/15/23 22:39:52.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:52.414
Mar 15 22:39:52.414: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename cronjob 03/15/23 22:39:52.415
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:52.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:52.451
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 03/15/23 22:39:52.455
STEP: creating 03/15/23 22:39:52.455
STEP: getting 03/15/23 22:39:52.462
STEP: listing 03/15/23 22:39:52.465
STEP: watching 03/15/23 22:39:52.468
Mar 15 22:39:52.468: INFO: starting watch
STEP: cluster-wide listing 03/15/23 22:39:52.469
STEP: cluster-wide watching 03/15/23 22:39:52.472
Mar 15 22:39:52.472: INFO: starting watch
STEP: patching 03/15/23 22:39:52.473
STEP: updating 03/15/23 22:39:52.482
Mar 15 22:39:52.491: INFO: waiting for watch events with expected annotations
Mar 15 22:39:52.491: INFO: saw patched and updated annotations
STEP: patching /status 03/15/23 22:39:52.491
STEP: updating /status 03/15/23 22:39:52.513
STEP: get /status 03/15/23 22:39:52.524
STEP: deleting 03/15/23 22:39:52.53
STEP: deleting a collection 03/15/23 22:39:52.544
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:52.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6399" for this suite. 03/15/23 22:39:52.557
------------------------------
• [0.148 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:52.414
    Mar 15 22:39:52.414: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename cronjob 03/15/23 22:39:52.415
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:52.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:52.451
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 03/15/23 22:39:52.455
    STEP: creating 03/15/23 22:39:52.455
    STEP: getting 03/15/23 22:39:52.462
    STEP: listing 03/15/23 22:39:52.465
    STEP: watching 03/15/23 22:39:52.468
    Mar 15 22:39:52.468: INFO: starting watch
    STEP: cluster-wide listing 03/15/23 22:39:52.469
    STEP: cluster-wide watching 03/15/23 22:39:52.472
    Mar 15 22:39:52.472: INFO: starting watch
    STEP: patching 03/15/23 22:39:52.473
    STEP: updating 03/15/23 22:39:52.482
    Mar 15 22:39:52.491: INFO: waiting for watch events with expected annotations
    Mar 15 22:39:52.491: INFO: saw patched and updated annotations
    STEP: patching /status 03/15/23 22:39:52.491
    STEP: updating /status 03/15/23 22:39:52.513
    STEP: get /status 03/15/23 22:39:52.524
    STEP: deleting 03/15/23 22:39:52.53
    STEP: deleting a collection 03/15/23 22:39:52.544
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:52.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6399" for this suite. 03/15/23 22:39:52.557
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:52.564
Mar 15 22:39:52.565: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename var-expansion 03/15/23 22:39:52.565
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:52.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:52.586
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Mar 15 22:39:52.603: INFO: Waiting up to 2m0s for pod "var-expansion-71694780-b837-41b1-b0af-086212c9c872" in namespace "var-expansion-2025" to be "container 0 failed with reason CreateContainerConfigError"
Mar 15 22:39:52.609: INFO: Pod "var-expansion-71694780-b837-41b1-b0af-086212c9c872": Phase="Pending", Reason="", readiness=false. Elapsed: 5.750392ms
Mar 15 22:39:54.612: INFO: Pod "var-expansion-71694780-b837-41b1-b0af-086212c9c872": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009632335s
Mar 15 22:39:54.613: INFO: Pod "var-expansion-71694780-b837-41b1-b0af-086212c9c872" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 15 22:39:54.613: INFO: Deleting pod "var-expansion-71694780-b837-41b1-b0af-086212c9c872" in namespace "var-expansion-2025"
Mar 15 22:39:54.625: INFO: Wait up to 5m0s for pod "var-expansion-71694780-b837-41b1-b0af-086212c9c872" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 15 22:39:56.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2025" for this suite. 03/15/23 22:39:56.641
------------------------------
• [4.087 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:52.564
    Mar 15 22:39:52.565: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename var-expansion 03/15/23 22:39:52.565
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:52.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:52.586
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Mar 15 22:39:52.603: INFO: Waiting up to 2m0s for pod "var-expansion-71694780-b837-41b1-b0af-086212c9c872" in namespace "var-expansion-2025" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 15 22:39:52.609: INFO: Pod "var-expansion-71694780-b837-41b1-b0af-086212c9c872": Phase="Pending", Reason="", readiness=false. Elapsed: 5.750392ms
    Mar 15 22:39:54.612: INFO: Pod "var-expansion-71694780-b837-41b1-b0af-086212c9c872": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009632335s
    Mar 15 22:39:54.613: INFO: Pod "var-expansion-71694780-b837-41b1-b0af-086212c9c872" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 15 22:39:54.613: INFO: Deleting pod "var-expansion-71694780-b837-41b1-b0af-086212c9c872" in namespace "var-expansion-2025"
    Mar 15 22:39:54.625: INFO: Wait up to 5m0s for pod "var-expansion-71694780-b837-41b1-b0af-086212c9c872" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:39:56.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2025" for this suite. 03/15/23 22:39:56.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:39:56.657
Mar 15 22:39:56.657: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename replication-controller 03/15/23 22:39:56.658
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:56.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:56.674
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 03/15/23 22:39:56.681
STEP: waiting for RC to be added 03/15/23 22:39:56.686
STEP: waiting for available Replicas 03/15/23 22:39:56.687
STEP: patching ReplicationController 03/15/23 22:39:57.851
STEP: waiting for RC to be modified 03/15/23 22:39:57.872
STEP: patching ReplicationController status 03/15/23 22:39:57.877
STEP: waiting for RC to be modified 03/15/23 22:39:57.902
STEP: waiting for available Replicas 03/15/23 22:39:57.902
STEP: fetching ReplicationController status 03/15/23 22:39:57.914
STEP: patching ReplicationController scale 03/15/23 22:39:57.926
STEP: waiting for RC to be modified 03/15/23 22:39:57.947
STEP: waiting for ReplicationController's scale to be the max amount 03/15/23 22:39:57.948
STEP: fetching ReplicationController; ensuring that it's patched 03/15/23 22:40:01.138
STEP: updating ReplicationController status 03/15/23 22:40:01.146
STEP: waiting for RC to be modified 03/15/23 22:40:01.174
STEP: listing all ReplicationControllers 03/15/23 22:40:01.174
STEP: checking that ReplicationController has expected values 03/15/23 22:40:01.194
STEP: deleting ReplicationControllers by collection 03/15/23 22:40:01.194
STEP: waiting for ReplicationController to have a DELETED watchEvent 03/15/23 22:40:01.239
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 15 22:40:01.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1345" for this suite. 03/15/23 22:40:01.347
------------------------------
• [4.704 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:39:56.657
    Mar 15 22:39:56.657: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename replication-controller 03/15/23 22:39:56.658
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:39:56.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:39:56.674
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 03/15/23 22:39:56.681
    STEP: waiting for RC to be added 03/15/23 22:39:56.686
    STEP: waiting for available Replicas 03/15/23 22:39:56.687
    STEP: patching ReplicationController 03/15/23 22:39:57.851
    STEP: waiting for RC to be modified 03/15/23 22:39:57.872
    STEP: patching ReplicationController status 03/15/23 22:39:57.877
    STEP: waiting for RC to be modified 03/15/23 22:39:57.902
    STEP: waiting for available Replicas 03/15/23 22:39:57.902
    STEP: fetching ReplicationController status 03/15/23 22:39:57.914
    STEP: patching ReplicationController scale 03/15/23 22:39:57.926
    STEP: waiting for RC to be modified 03/15/23 22:39:57.947
    STEP: waiting for ReplicationController's scale to be the max amount 03/15/23 22:39:57.948
    STEP: fetching ReplicationController; ensuring that it's patched 03/15/23 22:40:01.138
    STEP: updating ReplicationController status 03/15/23 22:40:01.146
    STEP: waiting for RC to be modified 03/15/23 22:40:01.174
    STEP: listing all ReplicationControllers 03/15/23 22:40:01.174
    STEP: checking that ReplicationController has expected values 03/15/23 22:40:01.194
    STEP: deleting ReplicationControllers by collection 03/15/23 22:40:01.194
    STEP: waiting for ReplicationController to have a DELETED watchEvent 03/15/23 22:40:01.239
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:40:01.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1345" for this suite. 03/15/23 22:40:01.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:40:01.366
Mar 15 22:40:01.366: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename events 03/15/23 22:40:01.367
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:40:01.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:40:01.414
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 03/15/23 22:40:01.428
STEP: listing all events in all namespaces 03/15/23 22:40:01.439
STEP: patching the test event 03/15/23 22:40:01.513
STEP: fetching the test event 03/15/23 22:40:01.553
STEP: updating the test event 03/15/23 22:40:01.563
STEP: getting the test event 03/15/23 22:40:01.626
STEP: deleting the test event 03/15/23 22:40:01.647
STEP: listing all events in all namespaces 03/15/23 22:40:01.682
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Mar 15 22:40:01.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9262" for this suite. 03/15/23 22:40:01.877
------------------------------
• [0.554 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:40:01.366
    Mar 15 22:40:01.366: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename events 03/15/23 22:40:01.367
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:40:01.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:40:01.414
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 03/15/23 22:40:01.428
    STEP: listing all events in all namespaces 03/15/23 22:40:01.439
    STEP: patching the test event 03/15/23 22:40:01.513
    STEP: fetching the test event 03/15/23 22:40:01.553
    STEP: updating the test event 03/15/23 22:40:01.563
    STEP: getting the test event 03/15/23 22:40:01.626
    STEP: deleting the test event 03/15/23 22:40:01.647
    STEP: listing all events in all namespaces 03/15/23 22:40:01.682
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:40:01.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9262" for this suite. 03/15/23 22:40:01.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:40:01.922
Mar 15 22:40:01.923: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename watch 03/15/23 22:40:01.928
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:40:02.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:40:02.049
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 03/15/23 22:40:02.06
STEP: creating a watch on configmaps with label B 03/15/23 22:40:02.063
STEP: creating a watch on configmaps with label A or B 03/15/23 22:40:02.078
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/15/23 22:40:02.08
Mar 15 22:40:02.124: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24359 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 22:40:02.125: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24359 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/15/23 22:40:02.125
Mar 15 22:40:02.169: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24360 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 22:40:02.169: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24360 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/15/23 22:40:02.169
Mar 15 22:40:02.206: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24361 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 22:40:02.206: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24361 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/15/23 22:40:02.207
Mar 15 22:40:02.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24362 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 22:40:02.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24362 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/15/23 22:40:02.232
Mar 15 22:40:02.247: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4064  45299b5e-d1f6-4c71-80c5-4a4c9a77012f 24363 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 22:40:02.247: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4064  45299b5e-d1f6-4c71-80c5-4a4c9a77012f 24363 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/15/23 22:40:12.248
Mar 15 22:40:12.256: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4064  45299b5e-d1f6-4c71-80c5-4a4c9a77012f 24422 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 22:40:12.256: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4064  45299b5e-d1f6-4c71-80c5-4a4c9a77012f 24422 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 15 22:40:22.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4064" for this suite. 03/15/23 22:40:22.261
------------------------------
• [SLOW TEST] [20.346 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:40:01.922
    Mar 15 22:40:01.923: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename watch 03/15/23 22:40:01.928
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:40:02.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:40:02.049
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 03/15/23 22:40:02.06
    STEP: creating a watch on configmaps with label B 03/15/23 22:40:02.063
    STEP: creating a watch on configmaps with label A or B 03/15/23 22:40:02.078
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/15/23 22:40:02.08
    Mar 15 22:40:02.124: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24359 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 22:40:02.125: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24359 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/15/23 22:40:02.125
    Mar 15 22:40:02.169: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24360 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 22:40:02.169: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24360 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/15/23 22:40:02.169
    Mar 15 22:40:02.206: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24361 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 22:40:02.206: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24361 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/15/23 22:40:02.207
    Mar 15 22:40:02.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24362 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 22:40:02.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4064  09d25a9a-6a7c-472e-a588-a980ce9c4bba 24362 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/15/23 22:40:02.232
    Mar 15 22:40:02.247: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4064  45299b5e-d1f6-4c71-80c5-4a4c9a77012f 24363 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 22:40:02.247: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4064  45299b5e-d1f6-4c71-80c5-4a4c9a77012f 24363 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/15/23 22:40:12.248
    Mar 15 22:40:12.256: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4064  45299b5e-d1f6-4c71-80c5-4a4c9a77012f 24422 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 15 22:40:12.256: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4064  45299b5e-d1f6-4c71-80c5-4a4c9a77012f 24422 0 2023-03-15 22:40:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-15 22:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:40:22.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4064" for this suite. 03/15/23 22:40:22.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:40:22.271
Mar 15 22:40:22.271: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:40:22.272
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:40:22.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:40:22.3
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-25bf0c1f-a54d-4c16-bbe3-fbe2657fe6b1 03/15/23 22:40:22.304
STEP: Creating a pod to test consume secrets 03/15/23 22:40:22.31
Mar 15 22:40:22.329: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d" in namespace "projected-9181" to be "Succeeded or Failed"
Mar 15 22:40:22.335: INFO: Pod "pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.706574ms
Mar 15 22:40:24.339: INFO: Pod "pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010045938s
Mar 15 22:40:26.340: INFO: Pod "pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010961462s
STEP: Saw pod success 03/15/23 22:40:26.34
Mar 15 22:40:26.340: INFO: Pod "pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d" satisfied condition "Succeeded or Failed"
Mar 15 22:40:26.342: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d container projected-secret-volume-test: <nil>
STEP: delete the pod 03/15/23 22:40:26.347
Mar 15 22:40:26.358: INFO: Waiting for pod pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d to disappear
Mar 15 22:40:26.361: INFO: Pod pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 15 22:40:26.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9181" for this suite. 03/15/23 22:40:26.366
------------------------------
• [4.100 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:40:22.271
    Mar 15 22:40:22.271: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:40:22.272
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:40:22.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:40:22.3
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-25bf0c1f-a54d-4c16-bbe3-fbe2657fe6b1 03/15/23 22:40:22.304
    STEP: Creating a pod to test consume secrets 03/15/23 22:40:22.31
    Mar 15 22:40:22.329: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d" in namespace "projected-9181" to be "Succeeded or Failed"
    Mar 15 22:40:22.335: INFO: Pod "pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.706574ms
    Mar 15 22:40:24.339: INFO: Pod "pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010045938s
    Mar 15 22:40:26.340: INFO: Pod "pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010961462s
    STEP: Saw pod success 03/15/23 22:40:26.34
    Mar 15 22:40:26.340: INFO: Pod "pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d" satisfied condition "Succeeded or Failed"
    Mar 15 22:40:26.342: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 22:40:26.347
    Mar 15 22:40:26.358: INFO: Waiting for pod pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d to disappear
    Mar 15 22:40:26.361: INFO: Pod pod-projected-secrets-32d61947-ff2e-406d-99a0-ecfdf6832f6d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:40:26.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9181" for this suite. 03/15/23 22:40:26.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:40:26.377
Mar 15 22:40:26.377: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-probe 03/15/23 22:40:26.379
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:40:26.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:40:26.394
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c in namespace container-probe-5961 03/15/23 22:40:26.4
Mar 15 22:40:26.407: INFO: Waiting up to 5m0s for pod "liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c" in namespace "container-probe-5961" to be "not pending"
Mar 15 22:40:26.410: INFO: Pod "liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915429ms
Mar 15 22:40:28.414: INFO: Pod "liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007116327s
Mar 15 22:40:28.414: INFO: Pod "liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c" satisfied condition "not pending"
Mar 15 22:40:28.414: INFO: Started pod liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c in namespace container-probe-5961
STEP: checking the pod's current state and verifying that restartCount is present 03/15/23 22:40:28.414
Mar 15 22:40:28.417: INFO: Initial restart count of pod liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c is 0
STEP: deleting the pod 03/15/23 22:44:28.952
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 15 22:44:28.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5961" for this suite. 03/15/23 22:44:28.969
------------------------------
• [SLOW TEST] [242.613 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:40:26.377
    Mar 15 22:40:26.377: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-probe 03/15/23 22:40:26.379
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:40:26.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:40:26.394
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c in namespace container-probe-5961 03/15/23 22:40:26.4
    Mar 15 22:40:26.407: INFO: Waiting up to 5m0s for pod "liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c" in namespace "container-probe-5961" to be "not pending"
    Mar 15 22:40:26.410: INFO: Pod "liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915429ms
    Mar 15 22:40:28.414: INFO: Pod "liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007116327s
    Mar 15 22:40:28.414: INFO: Pod "liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c" satisfied condition "not pending"
    Mar 15 22:40:28.414: INFO: Started pod liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c in namespace container-probe-5961
    STEP: checking the pod's current state and verifying that restartCount is present 03/15/23 22:40:28.414
    Mar 15 22:40:28.417: INFO: Initial restart count of pod liveness-04d77257-177b-4eb7-86ad-9f6e323bbd5c is 0
    STEP: deleting the pod 03/15/23 22:44:28.952
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:44:28.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5961" for this suite. 03/15/23 22:44:28.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:44:28.995
Mar 15 22:44:28.995: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename secrets 03/15/23 22:44:28.997
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:44:29.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:44:29.016
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-fa421c53-4300-4d7d-bd6b-a5877102fb3d 03/15/23 22:44:29.02
STEP: Creating a pod to test consume secrets 03/15/23 22:44:29.026
Mar 15 22:44:29.033: INFO: Waiting up to 5m0s for pod "pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987" in namespace "secrets-1457" to be "Succeeded or Failed"
Mar 15 22:44:29.042: INFO: Pod "pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987": Phase="Pending", Reason="", readiness=false. Elapsed: 8.996222ms
Mar 15 22:44:31.046: INFO: Pod "pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012973805s
Mar 15 22:44:33.050: INFO: Pod "pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017322576s
STEP: Saw pod success 03/15/23 22:44:33.05
Mar 15 22:44:33.050: INFO: Pod "pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987" satisfied condition "Succeeded or Failed"
Mar 15 22:44:33.054: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987 container secret-volume-test: <nil>
STEP: delete the pod 03/15/23 22:44:33.069
Mar 15 22:44:33.086: INFO: Waiting for pod pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987 to disappear
Mar 15 22:44:33.089: INFO: Pod pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 15 22:44:33.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1457" for this suite. 03/15/23 22:44:33.093
------------------------------
• [4.114 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:44:28.995
    Mar 15 22:44:28.995: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename secrets 03/15/23 22:44:28.997
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:44:29.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:44:29.016
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-fa421c53-4300-4d7d-bd6b-a5877102fb3d 03/15/23 22:44:29.02
    STEP: Creating a pod to test consume secrets 03/15/23 22:44:29.026
    Mar 15 22:44:29.033: INFO: Waiting up to 5m0s for pod "pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987" in namespace "secrets-1457" to be "Succeeded or Failed"
    Mar 15 22:44:29.042: INFO: Pod "pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987": Phase="Pending", Reason="", readiness=false. Elapsed: 8.996222ms
    Mar 15 22:44:31.046: INFO: Pod "pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012973805s
    Mar 15 22:44:33.050: INFO: Pod "pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017322576s
    STEP: Saw pod success 03/15/23 22:44:33.05
    Mar 15 22:44:33.050: INFO: Pod "pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987" satisfied condition "Succeeded or Failed"
    Mar 15 22:44:33.054: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987 container secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 22:44:33.069
    Mar 15 22:44:33.086: INFO: Waiting for pod pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987 to disappear
    Mar 15 22:44:33.089: INFO: Pod pod-secrets-b90198c3-7eeb-44b8-aca5-c1c1dc63b987 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:44:33.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1457" for this suite. 03/15/23 22:44:33.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:44:33.112
Mar 15 22:44:33.113: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename crd-webhook 03/15/23 22:44:33.114
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:44:33.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:44:33.131
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/15/23 22:44:33.134
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/15/23 22:44:33.699
STEP: Deploying the custom resource conversion webhook pod 03/15/23 22:44:33.707
STEP: Wait for the deployment to be ready 03/15/23 22:44:33.72
Mar 15 22:44:33.731: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 22:44:35.741
STEP: Verifying the service has paired with the endpoint 03/15/23 22:44:35.751
Mar 15 22:44:36.753: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Mar 15 22:44:36.756: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Creating a v1 custom resource 03/15/23 22:44:39.344
STEP: v2 custom resource should be converted 03/15/23 22:44:39.349
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:44:39.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9981" for this suite. 03/15/23 22:44:40.049
------------------------------
• [SLOW TEST] [7.013 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:44:33.112
    Mar 15 22:44:33.113: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename crd-webhook 03/15/23 22:44:33.114
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:44:33.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:44:33.131
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/15/23 22:44:33.134
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/15/23 22:44:33.699
    STEP: Deploying the custom resource conversion webhook pod 03/15/23 22:44:33.707
    STEP: Wait for the deployment to be ready 03/15/23 22:44:33.72
    Mar 15 22:44:33.731: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 22:44:35.741
    STEP: Verifying the service has paired with the endpoint 03/15/23 22:44:35.751
    Mar 15 22:44:36.753: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Mar 15 22:44:36.756: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Creating a v1 custom resource 03/15/23 22:44:39.344
    STEP: v2 custom resource should be converted 03/15/23 22:44:39.349
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:44:39.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9981" for this suite. 03/15/23 22:44:40.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:44:40.127
Mar 15 22:44:40.127: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:44:40.129
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:44:40.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:44:40.196
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-2cad9b87-c355-4d55-a4eb-4a634d6f90f3 03/15/23 22:44:40.2
STEP: Creating a pod to test consume secrets 03/15/23 22:44:40.204
Mar 15 22:44:40.210: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb" in namespace "projected-9980" to be "Succeeded or Failed"
Mar 15 22:44:40.213: INFO: Pod "pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.267798ms
Mar 15 22:44:42.217: INFO: Pod "pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007156515s
Mar 15 22:44:44.217: INFO: Pod "pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00734182s
STEP: Saw pod success 03/15/23 22:44:44.217
Mar 15 22:44:44.217: INFO: Pod "pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb" satisfied condition "Succeeded or Failed"
Mar 15 22:44:44.220: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb container projected-secret-volume-test: <nil>
STEP: delete the pod 03/15/23 22:44:44.228
Mar 15 22:44:44.242: INFO: Waiting for pod pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb to disappear
Mar 15 22:44:44.245: INFO: Pod pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 15 22:44:44.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9980" for this suite. 03/15/23 22:44:44.249
------------------------------
• [4.127 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:44:40.127
    Mar 15 22:44:40.127: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:44:40.129
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:44:40.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:44:40.196
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-2cad9b87-c355-4d55-a4eb-4a634d6f90f3 03/15/23 22:44:40.2
    STEP: Creating a pod to test consume secrets 03/15/23 22:44:40.204
    Mar 15 22:44:40.210: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb" in namespace "projected-9980" to be "Succeeded or Failed"
    Mar 15 22:44:40.213: INFO: Pod "pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.267798ms
    Mar 15 22:44:42.217: INFO: Pod "pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007156515s
    Mar 15 22:44:44.217: INFO: Pod "pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00734182s
    STEP: Saw pod success 03/15/23 22:44:44.217
    Mar 15 22:44:44.217: INFO: Pod "pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb" satisfied condition "Succeeded or Failed"
    Mar 15 22:44:44.220: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 22:44:44.228
    Mar 15 22:44:44.242: INFO: Waiting for pod pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb to disappear
    Mar 15 22:44:44.245: INFO: Pod pod-projected-secrets-bfd9e266-9820-40df-92d9-731df04be5bb no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:44:44.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9980" for this suite. 03/15/23 22:44:44.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:44:44.256
Mar 15 22:44:44.257: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename deployment 03/15/23 22:44:44.258
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:44:44.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:44:44.274
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Mar 15 22:44:44.277: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 15 22:44:44.290: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 15 22:44:49.295: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/15/23 22:44:49.295
Mar 15 22:44:49.296: INFO: Creating deployment "test-rolling-update-deployment"
Mar 15 22:44:49.300: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 15 22:44:49.310: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 15 22:44:51.318: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 15 22:44:51.320: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 15 22:44:51.332: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6600  2c041326-00dc-477e-879f-918bf095ffbc 25686 1 2023-03-15 22:44:49 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-15 22:44:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:44:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027e2ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-15 22:44:49 +0000 UTC,LastTransitionTime:2023-03-15 22:44:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-03-15 22:44:50 +0000 UTC,LastTransitionTime:2023-03-15 22:44:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 15 22:44:51.335: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-6600  87e90278-844b-4d90-a7eb-b28f2c43792a 25678 1 2023-03-15 22:44:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 2c041326-00dc-477e-879f-918bf095ffbc 0xc0027e2fa7 0xc0027e2fa8}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:44:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c041326-00dc-477e-879f-918bf095ffbc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:44:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027e3058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 15 22:44:51.335: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 15 22:44:51.335: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6600  0bbd6b51-a505-4f98-a901-9c3ab7dc11d2 25684 2 2023-03-15 22:44:44 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 2c041326-00dc-477e-879f-918bf095ffbc 0xc0027e2e67 0xc0027e2e68}] [] [{e2e.test Update apps/v1 2023-03-15 22:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:44:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c041326-00dc-477e-879f-918bf095ffbc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:44:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0027e2f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 15 22:44:51.339: INFO: Pod "test-rolling-update-deployment-7549d9f46d-65xgc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-65xgc test-rolling-update-deployment-7549d9f46d- deployment-6600  e08e3a12-b5bb-4af3-8d14-cb064b2a663b 25677 0 2023-03-15 22:44:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 87e90278-844b-4d90-a7eb-b28f2c43792a 0xc0027e34b7 0xc0027e34b8}] [] [{kube-controller-manager Update v1 2023-03-15 22:44:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87e90278-844b-4d90-a7eb-b28f2c43792a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:44:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c4t9k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c4t9k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:44:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:44:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:44:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:44:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.32,StartTime:2023-03-15 22:44:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:44:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://b54dc7360c71bc41436a81fc694fbd40d57d666f1371d52a1f66e6dd5dba9435,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 15 22:44:51.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6600" for this suite. 03/15/23 22:44:51.343
------------------------------
• [SLOW TEST] [7.092 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:44:44.256
    Mar 15 22:44:44.257: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename deployment 03/15/23 22:44:44.258
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:44:44.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:44:44.274
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Mar 15 22:44:44.277: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Mar 15 22:44:44.290: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 15 22:44:49.295: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/15/23 22:44:49.295
    Mar 15 22:44:49.296: INFO: Creating deployment "test-rolling-update-deployment"
    Mar 15 22:44:49.300: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Mar 15 22:44:49.310: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Mar 15 22:44:51.318: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Mar 15 22:44:51.320: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 15 22:44:51.332: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6600  2c041326-00dc-477e-879f-918bf095ffbc 25686 1 2023-03-15 22:44:49 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-15 22:44:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:44:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027e2ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-15 22:44:49 +0000 UTC,LastTransitionTime:2023-03-15 22:44:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-03-15 22:44:50 +0000 UTC,LastTransitionTime:2023-03-15 22:44:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 15 22:44:51.335: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-6600  87e90278-844b-4d90-a7eb-b28f2c43792a 25678 1 2023-03-15 22:44:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 2c041326-00dc-477e-879f-918bf095ffbc 0xc0027e2fa7 0xc0027e2fa8}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:44:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c041326-00dc-477e-879f-918bf095ffbc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:44:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027e3058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 15 22:44:51.335: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Mar 15 22:44:51.335: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6600  0bbd6b51-a505-4f98-a901-9c3ab7dc11d2 25684 2 2023-03-15 22:44:44 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 2c041326-00dc-477e-879f-918bf095ffbc 0xc0027e2e67 0xc0027e2e68}] [] [{e2e.test Update apps/v1 2023-03-15 22:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:44:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c041326-00dc-477e-879f-918bf095ffbc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:44:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0027e2f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 15 22:44:51.339: INFO: Pod "test-rolling-update-deployment-7549d9f46d-65xgc" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-65xgc test-rolling-update-deployment-7549d9f46d- deployment-6600  e08e3a12-b5bb-4af3-8d14-cb064b2a663b 25677 0 2023-03-15 22:44:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 87e90278-844b-4d90-a7eb-b28f2c43792a 0xc0027e34b7 0xc0027e34b8}] [] [{kube-controller-manager Update v1 2023-03-15 22:44:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87e90278-844b-4d90-a7eb-b28f2c43792a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:44:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c4t9k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c4t9k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:44:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:44:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:44:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:44:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.32,StartTime:2023-03-15 22:44:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:44:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://b54dc7360c71bc41436a81fc694fbd40d57d666f1371d52a1f66e6dd5dba9435,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:44:51.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6600" for this suite. 03/15/23 22:44:51.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:44:51.362
Mar 15 22:44:51.362: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename endpointslice 03/15/23 22:44:51.363
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:44:51.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:44:51.378
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 03/15/23 22:44:51.381
STEP: getting /apis/discovery.k8s.io 03/15/23 22:44:51.384
STEP: getting /apis/discovery.k8s.iov1 03/15/23 22:44:51.385
STEP: creating 03/15/23 22:44:51.387
STEP: getting 03/15/23 22:44:51.4
STEP: listing 03/15/23 22:44:51.403
STEP: watching 03/15/23 22:44:51.408
Mar 15 22:44:51.408: INFO: starting watch
STEP: cluster-wide listing 03/15/23 22:44:51.41
STEP: cluster-wide watching 03/15/23 22:44:51.413
Mar 15 22:44:51.413: INFO: starting watch
STEP: patching 03/15/23 22:44:51.415
STEP: updating 03/15/23 22:44:51.423
Mar 15 22:44:51.430: INFO: waiting for watch events with expected annotations
Mar 15 22:44:51.430: INFO: saw patched and updated annotations
STEP: deleting 03/15/23 22:44:51.431
STEP: deleting a collection 03/15/23 22:44:51.446
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 15 22:44:51.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3479" for this suite. 03/15/23 22:44:51.486
------------------------------
• [0.129 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:44:51.362
    Mar 15 22:44:51.362: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename endpointslice 03/15/23 22:44:51.363
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:44:51.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:44:51.378
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 03/15/23 22:44:51.381
    STEP: getting /apis/discovery.k8s.io 03/15/23 22:44:51.384
    STEP: getting /apis/discovery.k8s.iov1 03/15/23 22:44:51.385
    STEP: creating 03/15/23 22:44:51.387
    STEP: getting 03/15/23 22:44:51.4
    STEP: listing 03/15/23 22:44:51.403
    STEP: watching 03/15/23 22:44:51.408
    Mar 15 22:44:51.408: INFO: starting watch
    STEP: cluster-wide listing 03/15/23 22:44:51.41
    STEP: cluster-wide watching 03/15/23 22:44:51.413
    Mar 15 22:44:51.413: INFO: starting watch
    STEP: patching 03/15/23 22:44:51.415
    STEP: updating 03/15/23 22:44:51.423
    Mar 15 22:44:51.430: INFO: waiting for watch events with expected annotations
    Mar 15 22:44:51.430: INFO: saw patched and updated annotations
    STEP: deleting 03/15/23 22:44:51.431
    STEP: deleting a collection 03/15/23 22:44:51.446
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:44:51.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3479" for this suite. 03/15/23 22:44:51.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:44:51.496
Mar 15 22:44:51.496: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename sched-preemption 03/15/23 22:44:51.497
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:44:51.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:44:51.541
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 15 22:44:51.559: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 15 22:45:51.611: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 03/15/23 22:45:51.614
Mar 15 22:45:51.649: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 15 22:45:51.665: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 15 22:45:51.714: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 15 22:45:51.733: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 15 22:45:51.788: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 15 22:45:51.802: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/15/23 22:45:51.802
Mar 15 22:45:51.803: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3331" to be "running"
Mar 15 22:45:51.812: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.69855ms
Mar 15 22:45:53.815: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012200423s
Mar 15 22:45:55.818: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.014801538s
Mar 15 22:45:55.818: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 15 22:45:55.818: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3331" to be "running"
Mar 15 22:45:55.825: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.902158ms
Mar 15 22:45:55.825: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 15 22:45:55.825: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3331" to be "running"
Mar 15 22:45:55.829: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.928224ms
Mar 15 22:45:55.829: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 15 22:45:55.829: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3331" to be "running"
Mar 15 22:45:55.832: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.65557ms
Mar 15 22:45:55.833: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 15 22:45:55.833: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3331" to be "running"
Mar 15 22:45:55.846: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.085352ms
Mar 15 22:45:55.846: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 15 22:45:55.846: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3331" to be "running"
Mar 15 22:45:55.864: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.508524ms
Mar 15 22:45:55.864: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/15/23 22:45:55.864
Mar 15 22:45:55.885: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3331" to be "running"
Mar 15 22:45:55.895: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.419439ms
Mar 15 22:45:57.899: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014145577s
Mar 15 22:45:59.899: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014112605s
Mar 15 22:45:59.899: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:45:59.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3331" for this suite. 03/15/23 22:45:59.96
------------------------------
• [SLOW TEST] [68.469 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:44:51.496
    Mar 15 22:44:51.496: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename sched-preemption 03/15/23 22:44:51.497
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:44:51.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:44:51.541
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 15 22:44:51.559: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 15 22:45:51.611: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 03/15/23 22:45:51.614
    Mar 15 22:45:51.649: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 15 22:45:51.665: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 15 22:45:51.714: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 15 22:45:51.733: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 15 22:45:51.788: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 15 22:45:51.802: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/15/23 22:45:51.802
    Mar 15 22:45:51.803: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3331" to be "running"
    Mar 15 22:45:51.812: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.69855ms
    Mar 15 22:45:53.815: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012200423s
    Mar 15 22:45:55.818: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.014801538s
    Mar 15 22:45:55.818: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 15 22:45:55.818: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3331" to be "running"
    Mar 15 22:45:55.825: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.902158ms
    Mar 15 22:45:55.825: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 15 22:45:55.825: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3331" to be "running"
    Mar 15 22:45:55.829: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.928224ms
    Mar 15 22:45:55.829: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 15 22:45:55.829: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3331" to be "running"
    Mar 15 22:45:55.832: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.65557ms
    Mar 15 22:45:55.833: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 15 22:45:55.833: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3331" to be "running"
    Mar 15 22:45:55.846: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.085352ms
    Mar 15 22:45:55.846: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 15 22:45:55.846: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3331" to be "running"
    Mar 15 22:45:55.864: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.508524ms
    Mar 15 22:45:55.864: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/15/23 22:45:55.864
    Mar 15 22:45:55.885: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3331" to be "running"
    Mar 15 22:45:55.895: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.419439ms
    Mar 15 22:45:57.899: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014145577s
    Mar 15 22:45:59.899: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014112605s
    Mar 15 22:45:59.899: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:45:59.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3331" for this suite. 03/15/23 22:45:59.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:45:59.969
Mar 15 22:45:59.969: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 22:45:59.969
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:45:59.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:45:59.983
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 03/15/23 22:45:59.987
Mar 15 22:45:59.987: INFO: Creating e2e-svc-a-m7fq8
Mar 15 22:45:59.997: INFO: Creating e2e-svc-b-b7mrw
Mar 15 22:46:00.059: INFO: Creating e2e-svc-c-wvmx6
STEP: deleting service collection 03/15/23 22:46:00.158
Mar 15 22:46:00.205: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 22:46:00.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5256" for this suite. 03/15/23 22:46:00.231
------------------------------
• [0.313 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:45:59.969
    Mar 15 22:45:59.969: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 22:45:59.969
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:45:59.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:45:59.983
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 03/15/23 22:45:59.987
    Mar 15 22:45:59.987: INFO: Creating e2e-svc-a-m7fq8
    Mar 15 22:45:59.997: INFO: Creating e2e-svc-b-b7mrw
    Mar 15 22:46:00.059: INFO: Creating e2e-svc-c-wvmx6
    STEP: deleting service collection 03/15/23 22:46:00.158
    Mar 15 22:46:00.205: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:46:00.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5256" for this suite. 03/15/23 22:46:00.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:46:00.332
Mar 15 22:46:00.332: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename job 03/15/23 22:46:00.333
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:00.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:00.349
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 03/15/23 22:46:00.354
STEP: Ensure pods equal to parallelism count is attached to the job 03/15/23 22:46:00.361
STEP: patching /status 03/15/23 22:46:04.366
STEP: updating /status 03/15/23 22:46:04.374
STEP: get /status 03/15/23 22:46:04.383
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 15 22:46:04.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8001" for this suite. 03/15/23 22:46:04.39
------------------------------
• [4.066 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:46:00.332
    Mar 15 22:46:00.332: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename job 03/15/23 22:46:00.333
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:00.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:00.349
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 03/15/23 22:46:00.354
    STEP: Ensure pods equal to parallelism count is attached to the job 03/15/23 22:46:00.361
    STEP: patching /status 03/15/23 22:46:04.366
    STEP: updating /status 03/15/23 22:46:04.374
    STEP: get /status 03/15/23 22:46:04.383
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:46:04.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8001" for this suite. 03/15/23 22:46:04.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:46:04.416
Mar 15 22:46:04.416: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename podtemplate 03/15/23 22:46:04.417
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:04.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:04.44
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 03/15/23 22:46:04.446
Mar 15 22:46:04.450: INFO: created test-podtemplate-1
Mar 15 22:46:04.456: INFO: created test-podtemplate-2
Mar 15 22:46:04.463: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 03/15/23 22:46:04.463
STEP: delete collection of pod templates 03/15/23 22:46:04.467
Mar 15 22:46:04.467: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 03/15/23 22:46:04.479
Mar 15 22:46:04.479: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 15 22:46:04.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-5112" for this suite. 03/15/23 22:46:04.489
------------------------------
• [0.080 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:46:04.416
    Mar 15 22:46:04.416: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename podtemplate 03/15/23 22:46:04.417
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:04.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:04.44
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 03/15/23 22:46:04.446
    Mar 15 22:46:04.450: INFO: created test-podtemplate-1
    Mar 15 22:46:04.456: INFO: created test-podtemplate-2
    Mar 15 22:46:04.463: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 03/15/23 22:46:04.463
    STEP: delete collection of pod templates 03/15/23 22:46:04.467
    Mar 15 22:46:04.467: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 03/15/23 22:46:04.479
    Mar 15 22:46:04.479: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:46:04.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-5112" for this suite. 03/15/23 22:46:04.489
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:46:04.5
Mar 15 22:46:04.500: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename resourcequota 03/15/23 22:46:04.502
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:04.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:04.531
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 03/15/23 22:46:21.559
STEP: Creating a ResourceQuota 03/15/23 22:46:26.563
STEP: Ensuring resource quota status is calculated 03/15/23 22:46:26.568
STEP: Creating a ConfigMap 03/15/23 22:46:28.573
STEP: Ensuring resource quota status captures configMap creation 03/15/23 22:46:28.589
STEP: Deleting a ConfigMap 03/15/23 22:46:30.593
STEP: Ensuring resource quota status released usage 03/15/23 22:46:30.599
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 15 22:46:32.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5128" for this suite. 03/15/23 22:46:32.607
------------------------------
• [SLOW TEST] [28.115 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:46:04.5
    Mar 15 22:46:04.500: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename resourcequota 03/15/23 22:46:04.502
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:04.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:04.531
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 03/15/23 22:46:21.559
    STEP: Creating a ResourceQuota 03/15/23 22:46:26.563
    STEP: Ensuring resource quota status is calculated 03/15/23 22:46:26.568
    STEP: Creating a ConfigMap 03/15/23 22:46:28.573
    STEP: Ensuring resource quota status captures configMap creation 03/15/23 22:46:28.589
    STEP: Deleting a ConfigMap 03/15/23 22:46:30.593
    STEP: Ensuring resource quota status released usage 03/15/23 22:46:30.599
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:46:32.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5128" for this suite. 03/15/23 22:46:32.607
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:46:32.614
Mar 15 22:46:32.614: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename proxy 03/15/23 22:46:32.615
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:32.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:32.631
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 03/15/23 22:46:32.649
STEP: creating replication controller proxy-service-h9fbh in namespace proxy-2733 03/15/23 22:46:32.649
I0315 22:46:32.664912      20 runners.go:193] Created replication controller with name: proxy-service-h9fbh, namespace: proxy-2733, replica count: 1
I0315 22:46:33.716820      20 runners.go:193] proxy-service-h9fbh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0315 22:46:34.717235      20 runners.go:193] proxy-service-h9fbh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0315 22:46:35.717623      20 runners.go:193] proxy-service-h9fbh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 22:46:35.720: INFO: setup took 3.085836334s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/15/23 22:46:35.72
Mar 15 22:46:35.728: INFO: (0) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 7.255352ms)
Mar 15 22:46:35.736: INFO: (0) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 14.940052ms)
Mar 15 22:46:35.736: INFO: (0) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 14.696319ms)
Mar 15 22:46:35.738: INFO: (0) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 16.860491ms)
Mar 15 22:46:35.740: INFO: (0) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 19.08765ms)
Mar 15 22:46:35.742: INFO: (0) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 21.098722ms)
Mar 15 22:46:35.742: INFO: (0) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 20.694917ms)
Mar 15 22:46:35.742: INFO: (0) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 20.987062ms)
Mar 15 22:46:35.743: INFO: (0) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 21.826022ms)
Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 26.115916ms)
Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 26.272093ms)
Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 26.630915ms)
Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 26.493914ms)
Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 26.876236ms)
Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 27.175853ms)
Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 27.03387ms)
Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 12.151161ms)
Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.278097ms)
Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 12.528658ms)
Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 12.731969ms)
Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 12.687091ms)
Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.735729ms)
Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 12.295776ms)
Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 12.377607ms)
Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 12.851993ms)
Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 12.955488ms)
Mar 15 22:46:35.764: INFO: (1) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 15.907601ms)
Mar 15 22:46:35.764: INFO: (1) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 15.74926ms)
Mar 15 22:46:35.764: INFO: (1) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 15.742373ms)
Mar 15 22:46:35.764: INFO: (1) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 15.724912ms)
Mar 15 22:46:35.765: INFO: (1) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 15.534042ms)
Mar 15 22:46:35.765: INFO: (1) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 15.655298ms)
Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 14.16191ms)
Mar 15 22:46:35.780: INFO: (2) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 14.459435ms)
Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 14.405017ms)
Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 14.53631ms)
Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 14.361079ms)
Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 14.284853ms)
Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 14.17295ms)
Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 14.341824ms)
Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 13.839316ms)
Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 14.281448ms)
Mar 15 22:46:35.780: INFO: (2) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 14.524408ms)
Mar 15 22:46:35.780: INFO: (2) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 14.75995ms)
Mar 15 22:46:35.784: INFO: (2) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 18.743018ms)
Mar 15 22:46:35.784: INFO: (2) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 18.977563ms)
Mar 15 22:46:35.784: INFO: (2) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 18.875664ms)
Mar 15 22:46:35.784: INFO: (2) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 18.824771ms)
Mar 15 22:46:35.798: INFO: (3) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 12.864477ms)
Mar 15 22:46:35.798: INFO: (3) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 13.209609ms)
Mar 15 22:46:35.798: INFO: (3) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 13.406291ms)
Mar 15 22:46:35.798: INFO: (3) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 13.901324ms)
Mar 15 22:46:35.799: INFO: (3) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 13.828278ms)
Mar 15 22:46:35.799: INFO: (3) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 14.226001ms)
Mar 15 22:46:35.799: INFO: (3) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 14.341886ms)
Mar 15 22:46:35.799: INFO: (3) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 14.017214ms)
Mar 15 22:46:35.799: INFO: (3) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 14.224102ms)
Mar 15 22:46:35.799: INFO: (3) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 14.349728ms)
Mar 15 22:46:35.800: INFO: (3) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 15.642983ms)
Mar 15 22:46:35.801: INFO: (3) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 15.950837ms)
Mar 15 22:46:35.801: INFO: (3) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 16.346042ms)
Mar 15 22:46:35.801: INFO: (3) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 16.376373ms)
Mar 15 22:46:35.801: INFO: (3) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 16.422006ms)
Mar 15 22:46:35.802: INFO: (3) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 16.834189ms)
Mar 15 22:46:35.827: INFO: (4) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 25.00411ms)
Mar 15 22:46:35.827: INFO: (4) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 25.040691ms)
Mar 15 22:46:35.831: INFO: (4) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 29.380775ms)
Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 29.66014ms)
Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 29.248705ms)
Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 29.375036ms)
Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 29.828474ms)
Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 30.398572ms)
Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 30.643631ms)
Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 30.076754ms)
Mar 15 22:46:35.833: INFO: (4) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 30.615435ms)
Mar 15 22:46:35.833: INFO: (4) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 30.754395ms)
Mar 15 22:46:35.835: INFO: (4) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 33.152886ms)
Mar 15 22:46:35.848: INFO: (4) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 45.519882ms)
Mar 15 22:46:35.848: INFO: (4) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 45.399579ms)
Mar 15 22:46:35.848: INFO: (4) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 45.284301ms)
Mar 15 22:46:35.866: INFO: (5) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 17.83286ms)
Mar 15 22:46:35.867: INFO: (5) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 19.261882ms)
Mar 15 22:46:35.876: INFO: (5) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 27.851602ms)
Mar 15 22:46:35.876: INFO: (5) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 27.945141ms)
Mar 15 22:46:35.876: INFO: (5) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 27.746969ms)
Mar 15 22:46:35.876: INFO: (5) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 28.04731ms)
Mar 15 22:46:35.876: INFO: (5) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 27.816077ms)
Mar 15 22:46:35.881: INFO: (5) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 32.587911ms)
Mar 15 22:46:35.894: INFO: (5) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 46.200806ms)
Mar 15 22:46:35.894: INFO: (5) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 46.183915ms)
Mar 15 22:46:35.894: INFO: (5) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 46.403885ms)
Mar 15 22:46:35.896: INFO: (5) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 47.927396ms)
Mar 15 22:46:35.896: INFO: (5) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 47.677855ms)
Mar 15 22:46:35.896: INFO: (5) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 47.727651ms)
Mar 15 22:46:35.896: INFO: (5) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 47.973552ms)
Mar 15 22:46:35.896: INFO: (5) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 48.334489ms)
Mar 15 22:46:35.909: INFO: (6) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 12.549027ms)
Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.661111ms)
Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 13.460101ms)
Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 13.382783ms)
Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 13.366937ms)
Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 13.599811ms)
Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 13.825335ms)
Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 13.407294ms)
Mar 15 22:46:35.911: INFO: (6) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 14.094588ms)
Mar 15 22:46:35.911: INFO: (6) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 13.566646ms)
Mar 15 22:46:35.914: INFO: (6) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 17.912478ms)
Mar 15 22:46:35.914: INFO: (6) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 17.759022ms)
Mar 15 22:46:35.915: INFO: (6) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 17.99755ms)
Mar 15 22:46:35.915: INFO: (6) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 17.855677ms)
Mar 15 22:46:35.915: INFO: (6) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 17.680815ms)
Mar 15 22:46:35.915: INFO: (6) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 17.931098ms)
Mar 15 22:46:35.922: INFO: (7) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 6.698016ms)
Mar 15 22:46:35.922: INFO: (7) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 7.440639ms)
Mar 15 22:46:35.928: INFO: (7) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 12.55469ms)
Mar 15 22:46:35.930: INFO: (7) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 14.165545ms)
Mar 15 22:46:35.930: INFO: (7) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 14.404837ms)
Mar 15 22:46:35.930: INFO: (7) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 14.860816ms)
Mar 15 22:46:35.930: INFO: (7) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 14.947761ms)
Mar 15 22:46:35.930: INFO: (7) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 14.375139ms)
Mar 15 22:46:35.930: INFO: (7) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 14.555084ms)
Mar 15 22:46:35.931: INFO: (7) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 15.018346ms)
Mar 15 22:46:35.931: INFO: (7) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 15.438317ms)
Mar 15 22:46:35.934: INFO: (7) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 17.903182ms)
Mar 15 22:46:35.934: INFO: (7) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 18.645468ms)
Mar 15 22:46:35.934: INFO: (7) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 18.541477ms)
Mar 15 22:46:35.934: INFO: (7) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 18.885584ms)
Mar 15 22:46:35.936: INFO: (7) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 20.779291ms)
Mar 15 22:46:35.943: INFO: (8) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 6.476045ms)
Mar 15 22:46:35.948: INFO: (8) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 10.59425ms)
Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 11.443722ms)
Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 11.764545ms)
Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 11.978488ms)
Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 11.782223ms)
Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 12.562527ms)
Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 11.870379ms)
Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 12.219285ms)
Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 12.450195ms)
Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 12.336223ms)
Mar 15 22:46:35.955: INFO: (8) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 18.033601ms)
Mar 15 22:46:35.956: INFO: (8) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 18.83612ms)
Mar 15 22:46:35.956: INFO: (8) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 18.440896ms)
Mar 15 22:46:35.956: INFO: (8) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 19.143315ms)
Mar 15 22:46:35.956: INFO: (8) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 19.689266ms)
Mar 15 22:46:35.968: INFO: (9) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 11.126739ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 12.018025ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 11.897687ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 12.403739ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 12.323715ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 11.754183ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 12.021941ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.109407ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 11.773307ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 11.753979ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.006771ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 12.322548ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 12.718382ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 12.879064ms)
Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 12.260842ms)
Mar 15 22:46:35.970: INFO: (9) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 12.210292ms)
Mar 15 22:46:35.980: INFO: (10) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 10.559471ms)
Mar 15 22:46:35.980: INFO: (10) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 10.708606ms)
Mar 15 22:46:35.982: INFO: (10) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 12.057627ms)
Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 15.826016ms)
Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 15.703726ms)
Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 15.616603ms)
Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 15.849124ms)
Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 15.806015ms)
Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 15.90697ms)
Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 16.040917ms)
Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 16.048284ms)
Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 16.051857ms)
Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 16.069169ms)
Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 16.268256ms)
Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 16.646271ms)
Mar 15 22:46:35.987: INFO: (10) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 16.899261ms)
Mar 15 22:46:35.998: INFO: (11) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 10.802532ms)
Mar 15 22:46:35.998: INFO: (11) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 10.679432ms)
Mar 15 22:46:35.998: INFO: (11) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 11.248188ms)
Mar 15 22:46:35.999: INFO: (11) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 12.073149ms)
Mar 15 22:46:35.999: INFO: (11) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 12.323277ms)
Mar 15 22:46:35.999: INFO: (11) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 12.689409ms)
Mar 15 22:46:35.999: INFO: (11) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.2784ms)
Mar 15 22:46:35.999: INFO: (11) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 12.35906ms)
Mar 15 22:46:35.999: INFO: (11) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 12.549052ms)
Mar 15 22:46:36.001: INFO: (11) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 14.335522ms)
Mar 15 22:46:36.001: INFO: (11) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 14.034162ms)
Mar 15 22:46:36.002: INFO: (11) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 14.616488ms)
Mar 15 22:46:36.002: INFO: (11) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 14.53136ms)
Mar 15 22:46:36.002: INFO: (11) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 15.237223ms)
Mar 15 22:46:36.003: INFO: (11) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 15.510378ms)
Mar 15 22:46:36.003: INFO: (11) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 15.663269ms)
Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.285813ms)
Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 12.058754ms)
Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 12.495437ms)
Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.282492ms)
Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 12.19203ms)
Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 12.913232ms)
Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 13.317269ms)
Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 12.871558ms)
Mar 15 22:46:36.017: INFO: (12) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 13.147372ms)
Mar 15 22:46:36.017: INFO: (12) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 13.23325ms)
Mar 15 22:46:36.018: INFO: (12) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 13.987493ms)
Mar 15 22:46:36.018: INFO: (12) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 14.442191ms)
Mar 15 22:46:36.018: INFO: (12) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 14.537042ms)
Mar 15 22:46:36.018: INFO: (12) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 14.405041ms)
Mar 15 22:46:36.018: INFO: (12) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 14.552766ms)
Mar 15 22:46:36.018: INFO: (12) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 15.391131ms)
Mar 15 22:46:36.030: INFO: (13) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 10.848101ms)
Mar 15 22:46:36.030: INFO: (13) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 11.50063ms)
Mar 15 22:46:36.030: INFO: (13) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 11.328368ms)
Mar 15 22:46:36.030: INFO: (13) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 11.871235ms)
Mar 15 22:46:36.031: INFO: (13) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 11.375488ms)
Mar 15 22:46:36.031: INFO: (13) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 11.785063ms)
Mar 15 22:46:36.031: INFO: (13) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.49683ms)
Mar 15 22:46:36.031: INFO: (13) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 12.688888ms)
Mar 15 22:46:36.032: INFO: (13) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 13.126844ms)
Mar 15 22:46:36.032: INFO: (13) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 13.50776ms)
Mar 15 22:46:36.036: INFO: (13) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 16.733938ms)
Mar 15 22:46:36.036: INFO: (13) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 17.331957ms)
Mar 15 22:46:36.036: INFO: (13) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 17.437315ms)
Mar 15 22:46:36.036: INFO: (13) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 17.436982ms)
Mar 15 22:46:36.036: INFO: (13) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 17.355636ms)
Mar 15 22:46:36.037: INFO: (13) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 17.512237ms)
Mar 15 22:46:36.044: INFO: (14) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 6.37884ms)
Mar 15 22:46:36.044: INFO: (14) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 6.553801ms)
Mar 15 22:46:36.044: INFO: (14) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 7.237177ms)
Mar 15 22:46:36.044: INFO: (14) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 7.221506ms)
Mar 15 22:46:36.045: INFO: (14) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 7.60707ms)
Mar 15 22:46:36.047: INFO: (14) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 9.449852ms)
Mar 15 22:46:36.048: INFO: (14) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 10.197348ms)
Mar 15 22:46:36.048: INFO: (14) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 10.425104ms)
Mar 15 22:46:36.048: INFO: (14) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 11.379214ms)
Mar 15 22:46:36.048: INFO: (14) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 10.801485ms)
Mar 15 22:46:36.049: INFO: (14) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 11.13864ms)
Mar 15 22:46:36.051: INFO: (14) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 12.799735ms)
Mar 15 22:46:36.051: INFO: (14) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 13.176868ms)
Mar 15 22:46:36.051: INFO: (14) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 13.447756ms)
Mar 15 22:46:36.051: INFO: (14) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 13.222387ms)
Mar 15 22:46:36.051: INFO: (14) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 13.816811ms)
Mar 15 22:46:36.059: INFO: (15) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 7.681487ms)
Mar 15 22:46:36.061: INFO: (15) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 10.013039ms)
Mar 15 22:46:36.061: INFO: (15) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 10.089613ms)
Mar 15 22:46:36.061: INFO: (15) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 9.441488ms)
Mar 15 22:46:36.061: INFO: (15) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 10.170049ms)
Mar 15 22:46:36.062: INFO: (15) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 10.470247ms)
Mar 15 22:46:36.062: INFO: (15) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 10.856271ms)
Mar 15 22:46:36.062: INFO: (15) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 10.963015ms)
Mar 15 22:46:36.062: INFO: (15) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 10.450844ms)
Mar 15 22:46:36.062: INFO: (15) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 10.182812ms)
Mar 15 22:46:36.065: INFO: (15) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 13.185009ms)
Mar 15 22:46:36.066: INFO: (15) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 14.532425ms)
Mar 15 22:46:36.066: INFO: (15) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 14.646535ms)
Mar 15 22:46:36.066: INFO: (15) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 14.73927ms)
Mar 15 22:46:36.067: INFO: (15) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 15.35423ms)
Mar 15 22:46:36.067: INFO: (15) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 15.655948ms)
Mar 15 22:46:36.075: INFO: (16) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 7.345501ms)
Mar 15 22:46:36.076: INFO: (16) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 7.81884ms)
Mar 15 22:46:36.076: INFO: (16) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 8.868462ms)
Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 8.495578ms)
Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 9.005762ms)
Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 8.6374ms)
Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 9.458379ms)
Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 8.727431ms)
Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 9.414538ms)
Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 9.986397ms)
Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 9.489067ms)
Mar 15 22:46:36.080: INFO: (16) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 12.334371ms)
Mar 15 22:46:36.080: INFO: (16) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 11.544774ms)
Mar 15 22:46:36.080: INFO: (16) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 12.800208ms)
Mar 15 22:46:36.080: INFO: (16) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 12.085462ms)
Mar 15 22:46:36.080: INFO: (16) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 12.316043ms)
Mar 15 22:46:36.109: INFO: (17) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 28.221061ms)
Mar 15 22:46:36.110: INFO: (17) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 28.539009ms)
Mar 15 22:46:36.111: INFO: (17) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 29.732648ms)
Mar 15 22:46:36.111: INFO: (17) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 30.790324ms)
Mar 15 22:46:36.112: INFO: (17) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 30.299139ms)
Mar 15 22:46:36.112: INFO: (17) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 31.038679ms)
Mar 15 22:46:36.112: INFO: (17) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 31.407098ms)
Mar 15 22:46:36.112: INFO: (17) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 31.234443ms)
Mar 15 22:46:36.112: INFO: (17) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 30.988715ms)
Mar 15 22:46:36.112: INFO: (17) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 31.61947ms)
Mar 15 22:46:36.113: INFO: (17) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 31.0275ms)
Mar 15 22:46:36.113: INFO: (17) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 31.110626ms)
Mar 15 22:46:36.113: INFO: (17) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 32.305159ms)
Mar 15 22:46:36.113: INFO: (17) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 31.375627ms)
Mar 15 22:46:36.113: INFO: (17) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 31.654964ms)
Mar 15 22:46:36.113: INFO: (17) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 31.77774ms)
Mar 15 22:46:36.119: INFO: (18) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 5.558286ms)
Mar 15 22:46:36.119: INFO: (18) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 5.730804ms)
Mar 15 22:46:36.119: INFO: (18) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 6.052458ms)
Mar 15 22:46:36.119: INFO: (18) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 5.808576ms)
Mar 15 22:46:36.120: INFO: (18) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 6.808813ms)
Mar 15 22:46:36.131: INFO: (18) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 17.341205ms)
Mar 15 22:46:36.131: INFO: (18) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 17.716698ms)
Mar 15 22:46:36.132: INFO: (18) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 16.692699ms)
Mar 15 22:46:36.132: INFO: (18) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 16.892516ms)
Mar 15 22:46:36.132: INFO: (18) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 18.573535ms)
Mar 15 22:46:36.132: INFO: (18) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 18.917005ms)
Mar 15 22:46:36.132: INFO: (18) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 18.787943ms)
Mar 15 22:46:36.132: INFO: (18) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 18.997446ms)
Mar 15 22:46:36.133: INFO: (18) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 18.029842ms)
Mar 15 22:46:36.133: INFO: (18) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 19.704588ms)
Mar 15 22:46:36.133: INFO: (18) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 18.541951ms)
Mar 15 22:46:36.163: INFO: (19) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 28.91346ms)
Mar 15 22:46:36.163: INFO: (19) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 28.093953ms)
Mar 15 22:46:36.163: INFO: (19) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 29.158485ms)
Mar 15 22:46:36.163: INFO: (19) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 28.45166ms)
Mar 15 22:46:36.163: INFO: (19) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 29.473096ms)
Mar 15 22:46:36.163: INFO: (19) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 29.455086ms)
Mar 15 22:46:36.164: INFO: (19) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 30.274998ms)
Mar 15 22:46:36.164: INFO: (19) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 29.778459ms)
Mar 15 22:46:36.172: INFO: (19) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 37.585488ms)
Mar 15 22:46:36.172: INFO: (19) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 38.325829ms)
Mar 15 22:46:36.172: INFO: (19) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 38.417668ms)
Mar 15 22:46:36.172: INFO: (19) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 37.978074ms)
Mar 15 22:46:36.175: INFO: (19) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 41.422268ms)
Mar 15 22:46:36.175: INFO: (19) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 40.312633ms)
Mar 15 22:46:36.175: INFO: (19) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 40.261932ms)
Mar 15 22:46:36.175: INFO: (19) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 40.601069ms)
STEP: deleting ReplicationController proxy-service-h9fbh in namespace proxy-2733, will wait for the garbage collector to delete the pods 03/15/23 22:46:36.175
Mar 15 22:46:36.233: INFO: Deleting ReplicationController proxy-service-h9fbh took: 4.57578ms
Mar 15 22:46:36.335: INFO: Terminating ReplicationController proxy-service-h9fbh pods took: 102.597159ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 15 22:46:38.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2733" for this suite. 03/15/23 22:46:38.944
------------------------------
• [SLOW TEST] [6.345 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:46:32.614
    Mar 15 22:46:32.614: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename proxy 03/15/23 22:46:32.615
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:32.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:32.631
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 03/15/23 22:46:32.649
    STEP: creating replication controller proxy-service-h9fbh in namespace proxy-2733 03/15/23 22:46:32.649
    I0315 22:46:32.664912      20 runners.go:193] Created replication controller with name: proxy-service-h9fbh, namespace: proxy-2733, replica count: 1
    I0315 22:46:33.716820      20 runners.go:193] proxy-service-h9fbh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0315 22:46:34.717235      20 runners.go:193] proxy-service-h9fbh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0315 22:46:35.717623      20 runners.go:193] proxy-service-h9fbh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 15 22:46:35.720: INFO: setup took 3.085836334s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/15/23 22:46:35.72
    Mar 15 22:46:35.728: INFO: (0) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 7.255352ms)
    Mar 15 22:46:35.736: INFO: (0) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 14.940052ms)
    Mar 15 22:46:35.736: INFO: (0) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 14.696319ms)
    Mar 15 22:46:35.738: INFO: (0) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 16.860491ms)
    Mar 15 22:46:35.740: INFO: (0) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 19.08765ms)
    Mar 15 22:46:35.742: INFO: (0) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 21.098722ms)
    Mar 15 22:46:35.742: INFO: (0) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 20.694917ms)
    Mar 15 22:46:35.742: INFO: (0) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 20.987062ms)
    Mar 15 22:46:35.743: INFO: (0) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 21.826022ms)
    Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 26.115916ms)
    Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 26.272093ms)
    Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 26.630915ms)
    Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 26.493914ms)
    Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 26.876236ms)
    Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 27.175853ms)
    Mar 15 22:46:35.748: INFO: (0) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 27.03387ms)
    Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 12.151161ms)
    Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.278097ms)
    Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 12.528658ms)
    Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 12.731969ms)
    Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 12.687091ms)
    Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.735729ms)
    Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 12.295776ms)
    Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 12.377607ms)
    Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 12.851993ms)
    Mar 15 22:46:35.761: INFO: (1) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 12.955488ms)
    Mar 15 22:46:35.764: INFO: (1) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 15.907601ms)
    Mar 15 22:46:35.764: INFO: (1) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 15.74926ms)
    Mar 15 22:46:35.764: INFO: (1) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 15.742373ms)
    Mar 15 22:46:35.764: INFO: (1) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 15.724912ms)
    Mar 15 22:46:35.765: INFO: (1) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 15.534042ms)
    Mar 15 22:46:35.765: INFO: (1) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 15.655298ms)
    Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 14.16191ms)
    Mar 15 22:46:35.780: INFO: (2) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 14.459435ms)
    Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 14.405017ms)
    Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 14.53631ms)
    Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 14.361079ms)
    Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 14.284853ms)
    Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 14.17295ms)
    Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 14.341824ms)
    Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 13.839316ms)
    Mar 15 22:46:35.779: INFO: (2) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 14.281448ms)
    Mar 15 22:46:35.780: INFO: (2) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 14.524408ms)
    Mar 15 22:46:35.780: INFO: (2) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 14.75995ms)
    Mar 15 22:46:35.784: INFO: (2) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 18.743018ms)
    Mar 15 22:46:35.784: INFO: (2) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 18.977563ms)
    Mar 15 22:46:35.784: INFO: (2) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 18.875664ms)
    Mar 15 22:46:35.784: INFO: (2) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 18.824771ms)
    Mar 15 22:46:35.798: INFO: (3) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 12.864477ms)
    Mar 15 22:46:35.798: INFO: (3) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 13.209609ms)
    Mar 15 22:46:35.798: INFO: (3) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 13.406291ms)
    Mar 15 22:46:35.798: INFO: (3) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 13.901324ms)
    Mar 15 22:46:35.799: INFO: (3) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 13.828278ms)
    Mar 15 22:46:35.799: INFO: (3) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 14.226001ms)
    Mar 15 22:46:35.799: INFO: (3) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 14.341886ms)
    Mar 15 22:46:35.799: INFO: (3) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 14.017214ms)
    Mar 15 22:46:35.799: INFO: (3) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 14.224102ms)
    Mar 15 22:46:35.799: INFO: (3) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 14.349728ms)
    Mar 15 22:46:35.800: INFO: (3) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 15.642983ms)
    Mar 15 22:46:35.801: INFO: (3) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 15.950837ms)
    Mar 15 22:46:35.801: INFO: (3) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 16.346042ms)
    Mar 15 22:46:35.801: INFO: (3) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 16.376373ms)
    Mar 15 22:46:35.801: INFO: (3) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 16.422006ms)
    Mar 15 22:46:35.802: INFO: (3) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 16.834189ms)
    Mar 15 22:46:35.827: INFO: (4) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 25.00411ms)
    Mar 15 22:46:35.827: INFO: (4) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 25.040691ms)
    Mar 15 22:46:35.831: INFO: (4) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 29.380775ms)
    Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 29.66014ms)
    Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 29.248705ms)
    Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 29.375036ms)
    Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 29.828474ms)
    Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 30.398572ms)
    Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 30.643631ms)
    Mar 15 22:46:35.832: INFO: (4) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 30.076754ms)
    Mar 15 22:46:35.833: INFO: (4) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 30.615435ms)
    Mar 15 22:46:35.833: INFO: (4) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 30.754395ms)
    Mar 15 22:46:35.835: INFO: (4) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 33.152886ms)
    Mar 15 22:46:35.848: INFO: (4) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 45.519882ms)
    Mar 15 22:46:35.848: INFO: (4) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 45.399579ms)
    Mar 15 22:46:35.848: INFO: (4) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 45.284301ms)
    Mar 15 22:46:35.866: INFO: (5) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 17.83286ms)
    Mar 15 22:46:35.867: INFO: (5) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 19.261882ms)
    Mar 15 22:46:35.876: INFO: (5) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 27.851602ms)
    Mar 15 22:46:35.876: INFO: (5) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 27.945141ms)
    Mar 15 22:46:35.876: INFO: (5) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 27.746969ms)
    Mar 15 22:46:35.876: INFO: (5) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 28.04731ms)
    Mar 15 22:46:35.876: INFO: (5) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 27.816077ms)
    Mar 15 22:46:35.881: INFO: (5) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 32.587911ms)
    Mar 15 22:46:35.894: INFO: (5) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 46.200806ms)
    Mar 15 22:46:35.894: INFO: (5) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 46.183915ms)
    Mar 15 22:46:35.894: INFO: (5) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 46.403885ms)
    Mar 15 22:46:35.896: INFO: (5) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 47.927396ms)
    Mar 15 22:46:35.896: INFO: (5) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 47.677855ms)
    Mar 15 22:46:35.896: INFO: (5) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 47.727651ms)
    Mar 15 22:46:35.896: INFO: (5) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 47.973552ms)
    Mar 15 22:46:35.896: INFO: (5) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 48.334489ms)
    Mar 15 22:46:35.909: INFO: (6) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 12.549027ms)
    Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.661111ms)
    Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 13.460101ms)
    Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 13.382783ms)
    Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 13.366937ms)
    Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 13.599811ms)
    Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 13.825335ms)
    Mar 15 22:46:35.910: INFO: (6) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 13.407294ms)
    Mar 15 22:46:35.911: INFO: (6) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 14.094588ms)
    Mar 15 22:46:35.911: INFO: (6) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 13.566646ms)
    Mar 15 22:46:35.914: INFO: (6) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 17.912478ms)
    Mar 15 22:46:35.914: INFO: (6) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 17.759022ms)
    Mar 15 22:46:35.915: INFO: (6) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 17.99755ms)
    Mar 15 22:46:35.915: INFO: (6) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 17.855677ms)
    Mar 15 22:46:35.915: INFO: (6) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 17.680815ms)
    Mar 15 22:46:35.915: INFO: (6) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 17.931098ms)
    Mar 15 22:46:35.922: INFO: (7) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 6.698016ms)
    Mar 15 22:46:35.922: INFO: (7) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 7.440639ms)
    Mar 15 22:46:35.928: INFO: (7) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 12.55469ms)
    Mar 15 22:46:35.930: INFO: (7) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 14.165545ms)
    Mar 15 22:46:35.930: INFO: (7) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 14.404837ms)
    Mar 15 22:46:35.930: INFO: (7) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 14.860816ms)
    Mar 15 22:46:35.930: INFO: (7) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 14.947761ms)
    Mar 15 22:46:35.930: INFO: (7) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 14.375139ms)
    Mar 15 22:46:35.930: INFO: (7) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 14.555084ms)
    Mar 15 22:46:35.931: INFO: (7) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 15.018346ms)
    Mar 15 22:46:35.931: INFO: (7) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 15.438317ms)
    Mar 15 22:46:35.934: INFO: (7) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 17.903182ms)
    Mar 15 22:46:35.934: INFO: (7) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 18.645468ms)
    Mar 15 22:46:35.934: INFO: (7) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 18.541477ms)
    Mar 15 22:46:35.934: INFO: (7) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 18.885584ms)
    Mar 15 22:46:35.936: INFO: (7) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 20.779291ms)
    Mar 15 22:46:35.943: INFO: (8) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 6.476045ms)
    Mar 15 22:46:35.948: INFO: (8) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 10.59425ms)
    Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 11.443722ms)
    Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 11.764545ms)
    Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 11.978488ms)
    Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 11.782223ms)
    Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 12.562527ms)
    Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 11.870379ms)
    Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 12.219285ms)
    Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 12.450195ms)
    Mar 15 22:46:35.949: INFO: (8) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 12.336223ms)
    Mar 15 22:46:35.955: INFO: (8) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 18.033601ms)
    Mar 15 22:46:35.956: INFO: (8) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 18.83612ms)
    Mar 15 22:46:35.956: INFO: (8) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 18.440896ms)
    Mar 15 22:46:35.956: INFO: (8) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 19.143315ms)
    Mar 15 22:46:35.956: INFO: (8) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 19.689266ms)
    Mar 15 22:46:35.968: INFO: (9) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 11.126739ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 12.018025ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 11.897687ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 12.403739ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 12.323715ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 11.754183ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 12.021941ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.109407ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 11.773307ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 11.753979ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.006771ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 12.322548ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 12.718382ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 12.879064ms)
    Mar 15 22:46:35.969: INFO: (9) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 12.260842ms)
    Mar 15 22:46:35.970: INFO: (9) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 12.210292ms)
    Mar 15 22:46:35.980: INFO: (10) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 10.559471ms)
    Mar 15 22:46:35.980: INFO: (10) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 10.708606ms)
    Mar 15 22:46:35.982: INFO: (10) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 12.057627ms)
    Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 15.826016ms)
    Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 15.703726ms)
    Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 15.616603ms)
    Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 15.849124ms)
    Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 15.806015ms)
    Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 15.90697ms)
    Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 16.040917ms)
    Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 16.048284ms)
    Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 16.051857ms)
    Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 16.069169ms)
    Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 16.268256ms)
    Mar 15 22:46:35.986: INFO: (10) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 16.646271ms)
    Mar 15 22:46:35.987: INFO: (10) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 16.899261ms)
    Mar 15 22:46:35.998: INFO: (11) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 10.802532ms)
    Mar 15 22:46:35.998: INFO: (11) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 10.679432ms)
    Mar 15 22:46:35.998: INFO: (11) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 11.248188ms)
    Mar 15 22:46:35.999: INFO: (11) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 12.073149ms)
    Mar 15 22:46:35.999: INFO: (11) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 12.323277ms)
    Mar 15 22:46:35.999: INFO: (11) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 12.689409ms)
    Mar 15 22:46:35.999: INFO: (11) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.2784ms)
    Mar 15 22:46:35.999: INFO: (11) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 12.35906ms)
    Mar 15 22:46:35.999: INFO: (11) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 12.549052ms)
    Mar 15 22:46:36.001: INFO: (11) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 14.335522ms)
    Mar 15 22:46:36.001: INFO: (11) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 14.034162ms)
    Mar 15 22:46:36.002: INFO: (11) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 14.616488ms)
    Mar 15 22:46:36.002: INFO: (11) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 14.53136ms)
    Mar 15 22:46:36.002: INFO: (11) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 15.237223ms)
    Mar 15 22:46:36.003: INFO: (11) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 15.510378ms)
    Mar 15 22:46:36.003: INFO: (11) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 15.663269ms)
    Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.285813ms)
    Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 12.058754ms)
    Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 12.495437ms)
    Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.282492ms)
    Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 12.19203ms)
    Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 12.913232ms)
    Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 13.317269ms)
    Mar 15 22:46:36.016: INFO: (12) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 12.871558ms)
    Mar 15 22:46:36.017: INFO: (12) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 13.147372ms)
    Mar 15 22:46:36.017: INFO: (12) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 13.23325ms)
    Mar 15 22:46:36.018: INFO: (12) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 13.987493ms)
    Mar 15 22:46:36.018: INFO: (12) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 14.442191ms)
    Mar 15 22:46:36.018: INFO: (12) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 14.537042ms)
    Mar 15 22:46:36.018: INFO: (12) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 14.405041ms)
    Mar 15 22:46:36.018: INFO: (12) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 14.552766ms)
    Mar 15 22:46:36.018: INFO: (12) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 15.391131ms)
    Mar 15 22:46:36.030: INFO: (13) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 10.848101ms)
    Mar 15 22:46:36.030: INFO: (13) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 11.50063ms)
    Mar 15 22:46:36.030: INFO: (13) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 11.328368ms)
    Mar 15 22:46:36.030: INFO: (13) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 11.871235ms)
    Mar 15 22:46:36.031: INFO: (13) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 11.375488ms)
    Mar 15 22:46:36.031: INFO: (13) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 11.785063ms)
    Mar 15 22:46:36.031: INFO: (13) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 12.49683ms)
    Mar 15 22:46:36.031: INFO: (13) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 12.688888ms)
    Mar 15 22:46:36.032: INFO: (13) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 13.126844ms)
    Mar 15 22:46:36.032: INFO: (13) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 13.50776ms)
    Mar 15 22:46:36.036: INFO: (13) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 16.733938ms)
    Mar 15 22:46:36.036: INFO: (13) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 17.331957ms)
    Mar 15 22:46:36.036: INFO: (13) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 17.437315ms)
    Mar 15 22:46:36.036: INFO: (13) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 17.436982ms)
    Mar 15 22:46:36.036: INFO: (13) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 17.355636ms)
    Mar 15 22:46:36.037: INFO: (13) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 17.512237ms)
    Mar 15 22:46:36.044: INFO: (14) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 6.37884ms)
    Mar 15 22:46:36.044: INFO: (14) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 6.553801ms)
    Mar 15 22:46:36.044: INFO: (14) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 7.237177ms)
    Mar 15 22:46:36.044: INFO: (14) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 7.221506ms)
    Mar 15 22:46:36.045: INFO: (14) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 7.60707ms)
    Mar 15 22:46:36.047: INFO: (14) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 9.449852ms)
    Mar 15 22:46:36.048: INFO: (14) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 10.197348ms)
    Mar 15 22:46:36.048: INFO: (14) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 10.425104ms)
    Mar 15 22:46:36.048: INFO: (14) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 11.379214ms)
    Mar 15 22:46:36.048: INFO: (14) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 10.801485ms)
    Mar 15 22:46:36.049: INFO: (14) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 11.13864ms)
    Mar 15 22:46:36.051: INFO: (14) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 12.799735ms)
    Mar 15 22:46:36.051: INFO: (14) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 13.176868ms)
    Mar 15 22:46:36.051: INFO: (14) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 13.447756ms)
    Mar 15 22:46:36.051: INFO: (14) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 13.222387ms)
    Mar 15 22:46:36.051: INFO: (14) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 13.816811ms)
    Mar 15 22:46:36.059: INFO: (15) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 7.681487ms)
    Mar 15 22:46:36.061: INFO: (15) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 10.013039ms)
    Mar 15 22:46:36.061: INFO: (15) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 10.089613ms)
    Mar 15 22:46:36.061: INFO: (15) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 9.441488ms)
    Mar 15 22:46:36.061: INFO: (15) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 10.170049ms)
    Mar 15 22:46:36.062: INFO: (15) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 10.470247ms)
    Mar 15 22:46:36.062: INFO: (15) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 10.856271ms)
    Mar 15 22:46:36.062: INFO: (15) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 10.963015ms)
    Mar 15 22:46:36.062: INFO: (15) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 10.450844ms)
    Mar 15 22:46:36.062: INFO: (15) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 10.182812ms)
    Mar 15 22:46:36.065: INFO: (15) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 13.185009ms)
    Mar 15 22:46:36.066: INFO: (15) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 14.532425ms)
    Mar 15 22:46:36.066: INFO: (15) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 14.646535ms)
    Mar 15 22:46:36.066: INFO: (15) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 14.73927ms)
    Mar 15 22:46:36.067: INFO: (15) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 15.35423ms)
    Mar 15 22:46:36.067: INFO: (15) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 15.655948ms)
    Mar 15 22:46:36.075: INFO: (16) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 7.345501ms)
    Mar 15 22:46:36.076: INFO: (16) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 7.81884ms)
    Mar 15 22:46:36.076: INFO: (16) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 8.868462ms)
    Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 8.495578ms)
    Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 9.005762ms)
    Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 8.6374ms)
    Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 9.458379ms)
    Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 8.727431ms)
    Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 9.414538ms)
    Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 9.986397ms)
    Mar 15 22:46:36.077: INFO: (16) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 9.489067ms)
    Mar 15 22:46:36.080: INFO: (16) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 12.334371ms)
    Mar 15 22:46:36.080: INFO: (16) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 11.544774ms)
    Mar 15 22:46:36.080: INFO: (16) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 12.800208ms)
    Mar 15 22:46:36.080: INFO: (16) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 12.085462ms)
    Mar 15 22:46:36.080: INFO: (16) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 12.316043ms)
    Mar 15 22:46:36.109: INFO: (17) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 28.221061ms)
    Mar 15 22:46:36.110: INFO: (17) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 28.539009ms)
    Mar 15 22:46:36.111: INFO: (17) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 29.732648ms)
    Mar 15 22:46:36.111: INFO: (17) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 30.790324ms)
    Mar 15 22:46:36.112: INFO: (17) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 30.299139ms)
    Mar 15 22:46:36.112: INFO: (17) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 31.038679ms)
    Mar 15 22:46:36.112: INFO: (17) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 31.407098ms)
    Mar 15 22:46:36.112: INFO: (17) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 31.234443ms)
    Mar 15 22:46:36.112: INFO: (17) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 30.988715ms)
    Mar 15 22:46:36.112: INFO: (17) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 31.61947ms)
    Mar 15 22:46:36.113: INFO: (17) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 31.0275ms)
    Mar 15 22:46:36.113: INFO: (17) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 31.110626ms)
    Mar 15 22:46:36.113: INFO: (17) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 32.305159ms)
    Mar 15 22:46:36.113: INFO: (17) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 31.375627ms)
    Mar 15 22:46:36.113: INFO: (17) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 31.654964ms)
    Mar 15 22:46:36.113: INFO: (17) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 31.77774ms)
    Mar 15 22:46:36.119: INFO: (18) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 5.558286ms)
    Mar 15 22:46:36.119: INFO: (18) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 5.730804ms)
    Mar 15 22:46:36.119: INFO: (18) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 6.052458ms)
    Mar 15 22:46:36.119: INFO: (18) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 5.808576ms)
    Mar 15 22:46:36.120: INFO: (18) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 6.808813ms)
    Mar 15 22:46:36.131: INFO: (18) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 17.341205ms)
    Mar 15 22:46:36.131: INFO: (18) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 17.716698ms)
    Mar 15 22:46:36.132: INFO: (18) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 16.692699ms)
    Mar 15 22:46:36.132: INFO: (18) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 16.892516ms)
    Mar 15 22:46:36.132: INFO: (18) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 18.573535ms)
    Mar 15 22:46:36.132: INFO: (18) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 18.917005ms)
    Mar 15 22:46:36.132: INFO: (18) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 18.787943ms)
    Mar 15 22:46:36.132: INFO: (18) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 18.997446ms)
    Mar 15 22:46:36.133: INFO: (18) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 18.029842ms)
    Mar 15 22:46:36.133: INFO: (18) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 19.704588ms)
    Mar 15 22:46:36.133: INFO: (18) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 18.541951ms)
    Mar 15 22:46:36.163: INFO: (19) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname2/proxy/: bar (200; 28.91346ms)
    Mar 15 22:46:36.163: INFO: (19) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 28.093953ms)
    Mar 15 22:46:36.163: INFO: (19) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname1/proxy/: tls baz (200; 29.158485ms)
    Mar 15 22:46:36.163: INFO: (19) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 28.45166ms)
    Mar 15 22:46:36.163: INFO: (19) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname1/proxy/: foo (200; 29.473096ms)
    Mar 15 22:46:36.163: INFO: (19) /api/v1/namespaces/proxy-2733/services/https:proxy-service-h9fbh:tlsportname2/proxy/: tls qux (200; 29.455086ms)
    Mar 15 22:46:36.164: INFO: (19) /api/v1/namespaces/proxy-2733/services/http:proxy-service-h9fbh:portname1/proxy/: foo (200; 30.274998ms)
    Mar 15 22:46:36.164: INFO: (19) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:162/proxy/: bar (200; 29.778459ms)
    Mar 15 22:46:36.172: INFO: (19) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z/proxy/rewriteme">test</a> (200; 37.585488ms)
    Mar 15 22:46:36.172: INFO: (19) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:462/proxy/: tls qux (200; 38.325829ms)
    Mar 15 22:46:36.172: INFO: (19) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:443/proxy/tlsrewritem... (200; 38.417668ms)
    Mar 15 22:46:36.172: INFO: (19) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">... (200; 37.978074ms)
    Mar 15 22:46:36.175: INFO: (19) /api/v1/namespaces/proxy-2733/pods/https:proxy-service-h9fbh-sj96z:460/proxy/: tls baz (200; 41.422268ms)
    Mar 15 22:46:36.175: INFO: (19) /api/v1/namespaces/proxy-2733/pods/http:proxy-service-h9fbh-sj96z:160/proxy/: foo (200; 40.312633ms)
    Mar 15 22:46:36.175: INFO: (19) /api/v1/namespaces/proxy-2733/services/proxy-service-h9fbh:portname2/proxy/: bar (200; 40.261932ms)
    Mar 15 22:46:36.175: INFO: (19) /api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2733/pods/proxy-service-h9fbh-sj96z:1080/proxy/rewriteme">test<... (200; 40.601069ms)
    STEP: deleting ReplicationController proxy-service-h9fbh in namespace proxy-2733, will wait for the garbage collector to delete the pods 03/15/23 22:46:36.175
    Mar 15 22:46:36.233: INFO: Deleting ReplicationController proxy-service-h9fbh took: 4.57578ms
    Mar 15 22:46:36.335: INFO: Terminating ReplicationController proxy-service-h9fbh pods took: 102.597159ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:46:38.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2733" for this suite. 03/15/23 22:46:38.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:46:38.975
Mar 15 22:46:38.975: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename lease-test 03/15/23 22:46:38.978
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:38.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:39.011
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Mar 15 22:46:39.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-3285" for this suite. 03/15/23 22:46:39.075
------------------------------
• [0.107 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:46:38.975
    Mar 15 22:46:38.975: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename lease-test 03/15/23 22:46:38.978
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:38.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:39.011
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:46:39.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-3285" for this suite. 03/15/23 22:46:39.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:46:39.085
Mar 15 22:46:39.085: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename deployment 03/15/23 22:46:39.086
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:39.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:39.102
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Mar 15 22:46:39.106: INFO: Creating deployment "webserver-deployment"
Mar 15 22:46:39.114: INFO: Waiting for observed generation 1
Mar 15 22:46:41.120: INFO: Waiting for all required pods to come up
Mar 15 22:46:41.125: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 03/15/23 22:46:41.125
Mar 15 22:46:41.125: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xncmc" in namespace "deployment-4365" to be "running"
Mar 15 22:46:41.125: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-l7xr9" in namespace "deployment-4365" to be "running"
Mar 15 22:46:41.125: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-92pkn" in namespace "deployment-4365" to be "running"
Mar 15 22:46:41.126: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fgvpf" in namespace "deployment-4365" to be "running"
Mar 15 22:46:41.126: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-h4fvj" in namespace "deployment-4365" to be "running"
Mar 15 22:46:41.126: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nwswd" in namespace "deployment-4365" to be "running"
Mar 15 22:46:41.129: INFO: Pod "webserver-deployment-7f5969cbc7-nwswd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.618782ms
Mar 15 22:46:41.130: INFO: Pod "webserver-deployment-7f5969cbc7-fgvpf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.942263ms
Mar 15 22:46:41.130: INFO: Pod "webserver-deployment-7f5969cbc7-xncmc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.502517ms
Mar 15 22:46:41.130: INFO: Pod "webserver-deployment-7f5969cbc7-92pkn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.256419ms
Mar 15 22:46:41.130: INFO: Pod "webserver-deployment-7f5969cbc7-l7xr9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.80103ms
Mar 15 22:46:41.131: INFO: Pod "webserver-deployment-7f5969cbc7-h4fvj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825204ms
Mar 15 22:46:43.137: INFO: Pod "webserver-deployment-7f5969cbc7-92pkn": Phase="Running", Reason="", readiness=true. Elapsed: 2.011404105s
Mar 15 22:46:43.137: INFO: Pod "webserver-deployment-7f5969cbc7-92pkn" satisfied condition "running"
Mar 15 22:46:43.137: INFO: Pod "webserver-deployment-7f5969cbc7-xncmc": Phase="Running", Reason="", readiness=true. Elapsed: 2.012131007s
Mar 15 22:46:43.137: INFO: Pod "webserver-deployment-7f5969cbc7-xncmc" satisfied condition "running"
Mar 15 22:46:43.137: INFO: Pod "webserver-deployment-7f5969cbc7-fgvpf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011839244s
Mar 15 22:46:43.137: INFO: Pod "webserver-deployment-7f5969cbc7-h4fvj": Phase="Running", Reason="", readiness=true. Elapsed: 2.011504643s
Mar 15 22:46:43.138: INFO: Pod "webserver-deployment-7f5969cbc7-h4fvj" satisfied condition "running"
Mar 15 22:46:43.138: INFO: Pod "webserver-deployment-7f5969cbc7-l7xr9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012539857s
Mar 15 22:46:43.138: INFO: Pod "webserver-deployment-7f5969cbc7-nwswd": Phase="Running", Reason="", readiness=true. Elapsed: 2.012066495s
Mar 15 22:46:43.138: INFO: Pod "webserver-deployment-7f5969cbc7-nwswd" satisfied condition "running"
Mar 15 22:46:45.135: INFO: Pod "webserver-deployment-7f5969cbc7-fgvpf": Phase="Running", Reason="", readiness=true. Elapsed: 4.009167903s
Mar 15 22:46:45.135: INFO: Pod "webserver-deployment-7f5969cbc7-fgvpf" satisfied condition "running"
Mar 15 22:46:45.135: INFO: Pod "webserver-deployment-7f5969cbc7-l7xr9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009572262s
Mar 15 22:46:45.135: INFO: Pod "webserver-deployment-7f5969cbc7-l7xr9" satisfied condition "running"
Mar 15 22:46:45.135: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 15 22:46:45.143: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 15 22:46:45.154: INFO: Updating deployment webserver-deployment
Mar 15 22:46:45.154: INFO: Waiting for observed generation 2
Mar 15 22:46:47.162: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 15 22:46:47.165: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 15 22:46:47.169: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 15 22:46:47.182: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 15 22:46:47.182: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 15 22:46:47.185: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 15 22:46:47.193: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 15 22:46:47.193: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 15 22:46:47.202: INFO: Updating deployment webserver-deployment
Mar 15 22:46:47.202: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 15 22:46:47.217: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 15 22:46:49.300: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 15 22:46:49.341: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4365  d0dd7a90-b6b4-4751-8b3d-26f407e1fe2f 26548 3 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050f43a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-15 22:46:47 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-03-15 22:46:47 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 15 22:46:49.377: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-4365  1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 26545 3 2023-03-15 22:46:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment d0dd7a90-b6b4-4751-8b3d-26f407e1fe2f 0xc005055e47 0xc005055e48}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0dd7a90-b6b4-4751-8b3d-26f407e1fe2f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005055ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 15 22:46:49.377: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 15 22:46:49.377: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-4365  055f75b8-8dc3-4124-ae3c-a22a2507497b 26537 3 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment d0dd7a90-b6b4-4751-8b3d-26f407e1fe2f 0xc005055d57 0xc005055d58}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0dd7a90-b6b4-4751-8b3d-26f407e1fe2f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005055de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 15 22:46:49.433: INFO: Pod "webserver-deployment-7f5969cbc7-7nggj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7nggj webserver-deployment-7f5969cbc7- deployment-4365  619c58de-91f4-43e1-8422-c3e209a3b1b6 26577 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520a3e7 0xc00520a3e8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ptwrc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ptwrc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.444: INFO: Pod "webserver-deployment-7f5969cbc7-92pkn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-92pkn webserver-deployment-7f5969cbc7- deployment-4365  6789a52d-86d5-4342-a1d8-f9f0550dc17b 26395 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520a5a7 0xc00520a5a8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drfp4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drfp4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:100.96.1.155,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://79718998a4ed0820233a8e2f984d76423658caebf7e42112907f79d60edb9417,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.444: INFO: Pod "webserver-deployment-7f5969cbc7-9d5d4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9d5d4 webserver-deployment-7f5969cbc7- deployment-4365  182c2c3e-a015-4e40-a6fd-e264ca593f7c 26553 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520a787 0xc00520a788}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-shmnv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-shmnv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.476: INFO: Pod "webserver-deployment-7f5969cbc7-cw8rq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cw8rq webserver-deployment-7f5969cbc7- deployment-4365  d72bc020-b2a6-4b39-810f-acd542d74acc 26566 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520a947 0xc00520a948}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qkpv8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qkpv8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.476: INFO: Pod "webserver-deployment-7f5969cbc7-d7bqq" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d7bqq webserver-deployment-7f5969cbc7- deployment-4365  e51d9acb-89b4-4ebc-a287-8f2ed1adfd98 26376 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520ab07 0xc00520ab08}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lh2vk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lh2vk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.187,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e83937ad3dac38de45f62510993bc2250882be59e8c8967474c5dc7625c62adc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.478: INFO: Pod "webserver-deployment-7f5969cbc7-fgvpf" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fgvpf webserver-deployment-7f5969cbc7- deployment-4365  3b7ed3cf-d5f5-4b37-93e3-95a94b90e985 26414 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520ace7 0xc00520ace8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hjbnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hjbnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.250,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d76bfcc4319e81b8fe08d99a23df1556c17b0aae54cae97a8ceb8b5fee40f736,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.478: INFO: Pod "webserver-deployment-7f5969cbc7-gfndz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gfndz webserver-deployment-7f5969cbc7- deployment-4365  f61a9fa5-43c4-4b09-9d37-700066c1ffe3 26574 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520aec7 0xc00520aec8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4fxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4fxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.486: INFO: Pod "webserver-deployment-7f5969cbc7-h45fk" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h45fk webserver-deployment-7f5969cbc7- deployment-4365  6840c144-7f9f-454e-8010-e920dcf47517 26539 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520b087 0xc00520b088}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5r2m8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5r2m8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.487: INFO: Pod "webserver-deployment-7f5969cbc7-h4fvj" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h4fvj webserver-deployment-7f5969cbc7- deployment-4365  194f8f92-892f-4e2e-b07e-18ef2d4e6512 26391 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520b247 0xc00520b248}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7l262,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7l262,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:100.96.1.198,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2296cb1f69faea81141443219c5a068567b250ac26d8bea8f1051a918dd6dbba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.487: INFO: Pod "webserver-deployment-7f5969cbc7-hg8tz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hg8tz webserver-deployment-7f5969cbc7- deployment-4365  e343fcac-fbb5-4fbf-b96a-2024cd04aa15 26546 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520b427 0xc00520b428}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gm6w9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gm6w9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.487: INFO: Pod "webserver-deployment-7f5969cbc7-hx5x6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hx5x6 webserver-deployment-7f5969cbc7- deployment-4365  39c2d48b-97c3-49d2-ab83-3d22f4dddc00 26551 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520b5e7 0xc00520b5e8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x94q2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x94q2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.496: INFO: Pod "webserver-deployment-7f5969cbc7-jz7nd" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jz7nd webserver-deployment-7f5969cbc7- deployment-4365  bb0829be-4674-4773-b614-126cb171d312 26532 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520b7a7 0xc00520b7a8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9wbzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9wbzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.504: INFO: Pod "webserver-deployment-7f5969cbc7-ldv5f" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ldv5f webserver-deployment-7f5969cbc7- deployment-4365  1b45af37-7e6b-4641-a4f1-b791c542e02a 26538 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520b900 0xc00520b901}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qcb9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.504: INFO: Pod "webserver-deployment-7f5969cbc7-lvb62" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lvb62 webserver-deployment-7f5969cbc7- deployment-4365  b41e0278-4d7f-4b83-8df9-0d56cbc50d58 26547 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520bab7 0xc00520bab8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2ml4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2ml4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.505: INFO: Pod "webserver-deployment-7f5969cbc7-ncddb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ncddb webserver-deployment-7f5969cbc7- deployment-4365  fd0daeb7-04ff-4011-8034-107fd19160d5 26378 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520bc77 0xc00520bc78}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vb8wg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vb8wg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.129,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5f2b346efd3b20d9fdeddfb210b555da6609a41a6c3d957b90f6a3cc2e92bd07,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.505: INFO: Pod "webserver-deployment-7f5969cbc7-nwswd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nwswd webserver-deployment-7f5969cbc7- deployment-4365  8a620ca0-ef62-45a8-b19d-aeaa119c0599 26389 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520be57 0xc00520be58}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mggzp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mggzp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:100.96.1.187,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://80a2a20d7080aec4c82818390f7f7b05a6be4e87b4dab616a1ffec49f3eb218d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.508: INFO: Pod "webserver-deployment-7f5969cbc7-stbwq" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-stbwq webserver-deployment-7f5969cbc7- deployment-4365  bc65912f-bed6-4b70-ab1e-d7473f9ba03d 26373 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00528a037 0xc00528a038}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlbrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlbrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.124,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b170326b545d13d88af33ed7ea4ca8696a81f95532709e1bcd3f0ac1d47b26d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.509: INFO: Pod "webserver-deployment-7f5969cbc7-t79b8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t79b8 webserver-deployment-7f5969cbc7- deployment-4365  3d981148-4394-4f5f-9db3-ac64d6c94736 26503 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00528a217 0xc00528a218}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v7dm8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v7dm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.510: INFO: Pod "webserver-deployment-7f5969cbc7-vd2dd" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vd2dd webserver-deployment-7f5969cbc7- deployment-4365  93f29b4f-c906-4873-abf3-54db4406df9c 26524 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00528a3d7 0xc00528a3d8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8t7s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8t7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.516: INFO: Pod "webserver-deployment-7f5969cbc7-xg2k8" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xg2k8 webserver-deployment-7f5969cbc7- deployment-4365  fa2596f2-0068-4d7f-a6a0-0c67a5d817ad 26381 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00528a597 0xc00528a598}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5gkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5gkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.57,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://33347fdfceede88c3179a5cc0092d82b15ed24b86e4f2410b2bf937bbb4623e9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.516: INFO: Pod "webserver-deployment-d9f79cb5-7bgcx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7bgcx webserver-deployment-d9f79cb5- deployment-4365  5abd2a26-0b3f-45f9-a73c-4b9e2546f3af 26523 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528a75f 0xc00528a770}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dd9zm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dd9zm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.530: INFO: Pod "webserver-deployment-d9f79cb5-7rbsx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7rbsx webserver-deployment-d9f79cb5- deployment-4365  66ad46c7-3645-4aa7-b347-f400fa114726 26486 0 2023-03-15 22:46:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528a947 0xc00528a948}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfmtr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfmtr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.81,StartTime:2023-03-15 22:46:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.530: INFO: Pod "webserver-deployment-d9f79cb5-82cpn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-82cpn webserver-deployment-d9f79cb5- deployment-4365  28c716e6-7208-4cf9-92dd-5247a8fd4d56 26527 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528ab3f 0xc00528ab50}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rbpbw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpbw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.530: INFO: Pod "webserver-deployment-d9f79cb5-848rp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-848rp webserver-deployment-d9f79cb5- deployment-4365  8d8580c8-e4d2-49c5-be0a-19148e460f58 26541 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528ad27 0xc00528ad28}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9jdck,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9jdck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.531: INFO: Pod "webserver-deployment-d9f79cb5-8rpb7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8rpb7 webserver-deployment-d9f79cb5- deployment-4365  0b4f193d-05c8-446b-b046-5dcc5410b9f8 26471 0 2023-03-15 22:46:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528ae7f 0xc00528ae90}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5rxml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5rxml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.536: INFO: Pod "webserver-deployment-d9f79cb5-b4s7s" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b4s7s webserver-deployment-d9f79cb5- deployment-4365  51224586-2464-4377-9cd2-303af0499042 26540 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528b067 0xc00528b068}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-754xf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-754xf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.537: INFO: Pod "webserver-deployment-d9f79cb5-fb97v" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fb97v webserver-deployment-d9f79cb5- deployment-4365  69dc14b7-7bac-4a1a-b114-f21b13c16064 26571 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528b247 0xc00528b248}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lwnpn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lwnpn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.538: INFO: Pod "webserver-deployment-d9f79cb5-fb9mb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fb9mb webserver-deployment-d9f79cb5- deployment-4365  c8a12c4b-2966-49a9-a1d3-6d3e7a6962ac 26468 0 2023-03-15 22:46:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528b427 0xc00528b428}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlbwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlbwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:46:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.538: INFO: Pod "webserver-deployment-d9f79cb5-kgf82" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kgf82 webserver-deployment-d9f79cb5- deployment-4365  c76cb1cd-666c-40ca-ac64-e556993d7748 26487 0 2023-03-15 22:46:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528b607 0xc00528b608}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vp7rx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vp7rx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.117,StartTime:2023-03-15 22:46:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.538: INFO: Pod "webserver-deployment-d9f79cb5-ppgs4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ppgs4 webserver-deployment-d9f79cb5- deployment-4365  d6a29052-0ed2-464e-9052-3f5d32e62a1a 26549 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528b817 0xc00528b818}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qpj4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qpj4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.539: INFO: Pod "webserver-deployment-d9f79cb5-qzmtk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qzmtk webserver-deployment-d9f79cb5- deployment-4365  36446502-7d6d-4d6d-97d4-c491a29f912a 26528 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528b9f7 0xc00528b9f8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hr8vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hr8vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.539: INFO: Pod "webserver-deployment-d9f79cb5-sjffb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sjffb webserver-deployment-d9f79cb5- deployment-4365  6a55ef73-3b91-4c65-8b69-9c6767a083de 26580 0 2023-03-15 22:46:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528bb4f 0xc00528bb60}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dh8z5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dh8z5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:100.96.1.223,StartTime:2023-03-15 22:46:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 22:46:49.539: INFO: Pod "webserver-deployment-d9f79cb5-z6vz7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-z6vz7 webserver-deployment-d9f79cb5- deployment-4365  64531f2f-22ca-42b3-b9b0-c9086f5c9360 26550 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528bd67 0xc00528bd68}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s8l64,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s8l64,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 15 22:46:49.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4365" for this suite. 03/15/23 22:46:49.568
------------------------------
• [SLOW TEST] [10.491 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:46:39.085
    Mar 15 22:46:39.085: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename deployment 03/15/23 22:46:39.086
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:39.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:39.102
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Mar 15 22:46:39.106: INFO: Creating deployment "webserver-deployment"
    Mar 15 22:46:39.114: INFO: Waiting for observed generation 1
    Mar 15 22:46:41.120: INFO: Waiting for all required pods to come up
    Mar 15 22:46:41.125: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 03/15/23 22:46:41.125
    Mar 15 22:46:41.125: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xncmc" in namespace "deployment-4365" to be "running"
    Mar 15 22:46:41.125: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-l7xr9" in namespace "deployment-4365" to be "running"
    Mar 15 22:46:41.125: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-92pkn" in namespace "deployment-4365" to be "running"
    Mar 15 22:46:41.126: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fgvpf" in namespace "deployment-4365" to be "running"
    Mar 15 22:46:41.126: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-h4fvj" in namespace "deployment-4365" to be "running"
    Mar 15 22:46:41.126: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nwswd" in namespace "deployment-4365" to be "running"
    Mar 15 22:46:41.129: INFO: Pod "webserver-deployment-7f5969cbc7-nwswd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.618782ms
    Mar 15 22:46:41.130: INFO: Pod "webserver-deployment-7f5969cbc7-fgvpf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.942263ms
    Mar 15 22:46:41.130: INFO: Pod "webserver-deployment-7f5969cbc7-xncmc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.502517ms
    Mar 15 22:46:41.130: INFO: Pod "webserver-deployment-7f5969cbc7-92pkn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.256419ms
    Mar 15 22:46:41.130: INFO: Pod "webserver-deployment-7f5969cbc7-l7xr9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.80103ms
    Mar 15 22:46:41.131: INFO: Pod "webserver-deployment-7f5969cbc7-h4fvj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825204ms
    Mar 15 22:46:43.137: INFO: Pod "webserver-deployment-7f5969cbc7-92pkn": Phase="Running", Reason="", readiness=true. Elapsed: 2.011404105s
    Mar 15 22:46:43.137: INFO: Pod "webserver-deployment-7f5969cbc7-92pkn" satisfied condition "running"
    Mar 15 22:46:43.137: INFO: Pod "webserver-deployment-7f5969cbc7-xncmc": Phase="Running", Reason="", readiness=true. Elapsed: 2.012131007s
    Mar 15 22:46:43.137: INFO: Pod "webserver-deployment-7f5969cbc7-xncmc" satisfied condition "running"
    Mar 15 22:46:43.137: INFO: Pod "webserver-deployment-7f5969cbc7-fgvpf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011839244s
    Mar 15 22:46:43.137: INFO: Pod "webserver-deployment-7f5969cbc7-h4fvj": Phase="Running", Reason="", readiness=true. Elapsed: 2.011504643s
    Mar 15 22:46:43.138: INFO: Pod "webserver-deployment-7f5969cbc7-h4fvj" satisfied condition "running"
    Mar 15 22:46:43.138: INFO: Pod "webserver-deployment-7f5969cbc7-l7xr9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012539857s
    Mar 15 22:46:43.138: INFO: Pod "webserver-deployment-7f5969cbc7-nwswd": Phase="Running", Reason="", readiness=true. Elapsed: 2.012066495s
    Mar 15 22:46:43.138: INFO: Pod "webserver-deployment-7f5969cbc7-nwswd" satisfied condition "running"
    Mar 15 22:46:45.135: INFO: Pod "webserver-deployment-7f5969cbc7-fgvpf": Phase="Running", Reason="", readiness=true. Elapsed: 4.009167903s
    Mar 15 22:46:45.135: INFO: Pod "webserver-deployment-7f5969cbc7-fgvpf" satisfied condition "running"
    Mar 15 22:46:45.135: INFO: Pod "webserver-deployment-7f5969cbc7-l7xr9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009572262s
    Mar 15 22:46:45.135: INFO: Pod "webserver-deployment-7f5969cbc7-l7xr9" satisfied condition "running"
    Mar 15 22:46:45.135: INFO: Waiting for deployment "webserver-deployment" to complete
    Mar 15 22:46:45.143: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Mar 15 22:46:45.154: INFO: Updating deployment webserver-deployment
    Mar 15 22:46:45.154: INFO: Waiting for observed generation 2
    Mar 15 22:46:47.162: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Mar 15 22:46:47.165: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Mar 15 22:46:47.169: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 15 22:46:47.182: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Mar 15 22:46:47.182: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Mar 15 22:46:47.185: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 15 22:46:47.193: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Mar 15 22:46:47.193: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Mar 15 22:46:47.202: INFO: Updating deployment webserver-deployment
    Mar 15 22:46:47.202: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Mar 15 22:46:47.217: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Mar 15 22:46:49.300: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 15 22:46:49.341: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-4365  d0dd7a90-b6b4-4751-8b3d-26f407e1fe2f 26548 3 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050f43a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-15 22:46:47 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-03-15 22:46:47 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Mar 15 22:46:49.377: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-4365  1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 26545 3 2023-03-15 22:46:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment d0dd7a90-b6b4-4751-8b3d-26f407e1fe2f 0xc005055e47 0xc005055e48}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0dd7a90-b6b4-4751-8b3d-26f407e1fe2f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005055ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 15 22:46:49.377: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Mar 15 22:46:49.377: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-4365  055f75b8-8dc3-4124-ae3c-a22a2507497b 26537 3 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment d0dd7a90-b6b4-4751-8b3d-26f407e1fe2f 0xc005055d57 0xc005055d58}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0dd7a90-b6b4-4751-8b3d-26f407e1fe2f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005055de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Mar 15 22:46:49.433: INFO: Pod "webserver-deployment-7f5969cbc7-7nggj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7nggj webserver-deployment-7f5969cbc7- deployment-4365  619c58de-91f4-43e1-8422-c3e209a3b1b6 26577 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520a3e7 0xc00520a3e8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ptwrc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ptwrc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.444: INFO: Pod "webserver-deployment-7f5969cbc7-92pkn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-92pkn webserver-deployment-7f5969cbc7- deployment-4365  6789a52d-86d5-4342-a1d8-f9f0550dc17b 26395 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520a5a7 0xc00520a5a8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drfp4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drfp4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:100.96.1.155,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://79718998a4ed0820233a8e2f984d76423658caebf7e42112907f79d60edb9417,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.444: INFO: Pod "webserver-deployment-7f5969cbc7-9d5d4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9d5d4 webserver-deployment-7f5969cbc7- deployment-4365  182c2c3e-a015-4e40-a6fd-e264ca593f7c 26553 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520a787 0xc00520a788}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-shmnv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-shmnv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.476: INFO: Pod "webserver-deployment-7f5969cbc7-cw8rq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cw8rq webserver-deployment-7f5969cbc7- deployment-4365  d72bc020-b2a6-4b39-810f-acd542d74acc 26566 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520a947 0xc00520a948}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qkpv8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qkpv8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.476: INFO: Pod "webserver-deployment-7f5969cbc7-d7bqq" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d7bqq webserver-deployment-7f5969cbc7- deployment-4365  e51d9acb-89b4-4ebc-a287-8f2ed1adfd98 26376 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520ab07 0xc00520ab08}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lh2vk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lh2vk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.187,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e83937ad3dac38de45f62510993bc2250882be59e8c8967474c5dc7625c62adc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.478: INFO: Pod "webserver-deployment-7f5969cbc7-fgvpf" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fgvpf webserver-deployment-7f5969cbc7- deployment-4365  3b7ed3cf-d5f5-4b37-93e3-95a94b90e985 26414 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520ace7 0xc00520ace8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hjbnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hjbnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.250,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d76bfcc4319e81b8fe08d99a23df1556c17b0aae54cae97a8ceb8b5fee40f736,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.478: INFO: Pod "webserver-deployment-7f5969cbc7-gfndz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gfndz webserver-deployment-7f5969cbc7- deployment-4365  f61a9fa5-43c4-4b09-9d37-700066c1ffe3 26574 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520aec7 0xc00520aec8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4fxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4fxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.486: INFO: Pod "webserver-deployment-7f5969cbc7-h45fk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h45fk webserver-deployment-7f5969cbc7- deployment-4365  6840c144-7f9f-454e-8010-e920dcf47517 26539 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520b087 0xc00520b088}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5r2m8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5r2m8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.487: INFO: Pod "webserver-deployment-7f5969cbc7-h4fvj" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h4fvj webserver-deployment-7f5969cbc7- deployment-4365  194f8f92-892f-4e2e-b07e-18ef2d4e6512 26391 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520b247 0xc00520b248}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7l262,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7l262,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:100.96.1.198,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2296cb1f69faea81141443219c5a068567b250ac26d8bea8f1051a918dd6dbba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.487: INFO: Pod "webserver-deployment-7f5969cbc7-hg8tz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hg8tz webserver-deployment-7f5969cbc7- deployment-4365  e343fcac-fbb5-4fbf-b96a-2024cd04aa15 26546 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520b427 0xc00520b428}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gm6w9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gm6w9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.487: INFO: Pod "webserver-deployment-7f5969cbc7-hx5x6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hx5x6 webserver-deployment-7f5969cbc7- deployment-4365  39c2d48b-97c3-49d2-ab83-3d22f4dddc00 26551 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520b5e7 0xc00520b5e8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x94q2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x94q2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.496: INFO: Pod "webserver-deployment-7f5969cbc7-jz7nd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jz7nd webserver-deployment-7f5969cbc7- deployment-4365  bb0829be-4674-4773-b614-126cb171d312 26532 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520b7a7 0xc00520b7a8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9wbzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9wbzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.504: INFO: Pod "webserver-deployment-7f5969cbc7-ldv5f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ldv5f webserver-deployment-7f5969cbc7- deployment-4365  1b45af37-7e6b-4641-a4f1-b791c542e02a 26538 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520b900 0xc00520b901}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qcb9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.504: INFO: Pod "webserver-deployment-7f5969cbc7-lvb62" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lvb62 webserver-deployment-7f5969cbc7- deployment-4365  b41e0278-4d7f-4b83-8df9-0d56cbc50d58 26547 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520bab7 0xc00520bab8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2ml4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2ml4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.505: INFO: Pod "webserver-deployment-7f5969cbc7-ncddb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ncddb webserver-deployment-7f5969cbc7- deployment-4365  fd0daeb7-04ff-4011-8034-107fd19160d5 26378 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520bc77 0xc00520bc78}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vb8wg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vb8wg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.129,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5f2b346efd3b20d9fdeddfb210b555da6609a41a6c3d957b90f6a3cc2e92bd07,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.505: INFO: Pod "webserver-deployment-7f5969cbc7-nwswd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nwswd webserver-deployment-7f5969cbc7- deployment-4365  8a620ca0-ef62-45a8-b19d-aeaa119c0599 26389 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00520be57 0xc00520be58}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mggzp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mggzp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:100.96.1.187,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://80a2a20d7080aec4c82818390f7f7b05a6be4e87b4dab616a1ffec49f3eb218d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.508: INFO: Pod "webserver-deployment-7f5969cbc7-stbwq" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-stbwq webserver-deployment-7f5969cbc7- deployment-4365  bc65912f-bed6-4b70-ab1e-d7473f9ba03d 26373 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00528a037 0xc00528a038}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlbrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlbrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.124,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b170326b545d13d88af33ed7ea4ca8696a81f95532709e1bcd3f0ac1d47b26d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.509: INFO: Pod "webserver-deployment-7f5969cbc7-t79b8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t79b8 webserver-deployment-7f5969cbc7- deployment-4365  3d981148-4394-4f5f-9db3-ac64d6c94736 26503 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00528a217 0xc00528a218}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v7dm8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v7dm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.510: INFO: Pod "webserver-deployment-7f5969cbc7-vd2dd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vd2dd webserver-deployment-7f5969cbc7- deployment-4365  93f29b4f-c906-4873-abf3-54db4406df9c 26524 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00528a3d7 0xc00528a3d8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8t7s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8t7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.516: INFO: Pod "webserver-deployment-7f5969cbc7-xg2k8" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xg2k8 webserver-deployment-7f5969cbc7- deployment-4365  fa2596f2-0068-4d7f-a6a0-0c67a5d817ad 26381 0 2023-03-15 22:46:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 055f75b8-8dc3-4124-ae3c-a22a2507497b 0xc00528a597 0xc00528a598}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055f75b8-8dc3-4124-ae3c-a22a2507497b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5gkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5gkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.57,StartTime:2023-03-15 22:46:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:46:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://33347fdfceede88c3179a5cc0092d82b15ed24b86e4f2410b2bf937bbb4623e9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.516: INFO: Pod "webserver-deployment-d9f79cb5-7bgcx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7bgcx webserver-deployment-d9f79cb5- deployment-4365  5abd2a26-0b3f-45f9-a73c-4b9e2546f3af 26523 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528a75f 0xc00528a770}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dd9zm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dd9zm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.530: INFO: Pod "webserver-deployment-d9f79cb5-7rbsx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7rbsx webserver-deployment-d9f79cb5- deployment-4365  66ad46c7-3645-4aa7-b347-f400fa114726 26486 0 2023-03-15 22:46:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528a947 0xc00528a948}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfmtr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfmtr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.81,StartTime:2023-03-15 22:46:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.530: INFO: Pod "webserver-deployment-d9f79cb5-82cpn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-82cpn webserver-deployment-d9f79cb5- deployment-4365  28c716e6-7208-4cf9-92dd-5247a8fd4d56 26527 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528ab3f 0xc00528ab50}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rbpbw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpbw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.530: INFO: Pod "webserver-deployment-d9f79cb5-848rp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-848rp webserver-deployment-d9f79cb5- deployment-4365  8d8580c8-e4d2-49c5-be0a-19148e460f58 26541 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528ad27 0xc00528ad28}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9jdck,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9jdck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.531: INFO: Pod "webserver-deployment-d9f79cb5-8rpb7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8rpb7 webserver-deployment-d9f79cb5- deployment-4365  0b4f193d-05c8-446b-b046-5dcc5410b9f8 26471 0 2023-03-15 22:46:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528ae7f 0xc00528ae90}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5rxml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5rxml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.536: INFO: Pod "webserver-deployment-d9f79cb5-b4s7s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b4s7s webserver-deployment-d9f79cb5- deployment-4365  51224586-2464-4377-9cd2-303af0499042 26540 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528b067 0xc00528b068}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-754xf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-754xf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.537: INFO: Pod "webserver-deployment-d9f79cb5-fb97v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fb97v webserver-deployment-d9f79cb5- deployment-4365  69dc14b7-7bac-4a1a-b114-f21b13c16064 26571 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528b247 0xc00528b248}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lwnpn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lwnpn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.538: INFO: Pod "webserver-deployment-d9f79cb5-fb9mb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fb9mb webserver-deployment-d9f79cb5- deployment-4365  c8a12c4b-2966-49a9-a1d3-6d3e7a6962ac 26468 0 2023-03-15 22:46:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528b427 0xc00528b428}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlbwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlbwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:,StartTime:2023-03-15 22:46:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.538: INFO: Pod "webserver-deployment-d9f79cb5-kgf82" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kgf82 webserver-deployment-d9f79cb5- deployment-4365  c76cb1cd-666c-40ca-ac64-e556993d7748 26487 0 2023-03-15 22:46:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528b607 0xc00528b608}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vp7rx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vp7rx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:100.96.2.117,StartTime:2023-03-15 22:46:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.538: INFO: Pod "webserver-deployment-d9f79cb5-ppgs4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ppgs4 webserver-deployment-d9f79cb5- deployment-4365  d6a29052-0ed2-464e-9052-3f5d32e62a1a 26549 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528b817 0xc00528b818}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qpj4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qpj4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.539: INFO: Pod "webserver-deployment-d9f79cb5-qzmtk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qzmtk webserver-deployment-d9f79cb5- deployment-4365  36446502-7d6d-4d6d-97d4-c491a29f912a 26528 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528b9f7 0xc00528b9f8}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hr8vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hr8vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.539: INFO: Pod "webserver-deployment-d9f79cb5-sjffb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sjffb webserver-deployment-d9f79cb5- deployment-4365  6a55ef73-3b91-4c65-8b69-9c6767a083de 26580 0 2023-03-15 22:46:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528bb4f 0xc00528bb60}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dh8z5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dh8z5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0baafb3f4e7bf826e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.60.208,PodIP:100.96.1.223,StartTime:2023-03-15 22:46:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 22:46:49.539: INFO: Pod "webserver-deployment-d9f79cb5-z6vz7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-z6vz7 webserver-deployment-d9f79cb5- deployment-4365  64531f2f-22ca-42b3-b9b0-c9086f5c9360 26550 0 2023-03-15 22:46:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369 0xc00528bd67 0xc00528bd68}] [] [{kube-controller-manager Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c3fbf5e-e4ee-4bdc-a398-37adbc7fc369\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s8l64,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s8l64,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 22:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:46:49.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4365" for this suite. 03/15/23 22:46:49.568
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:46:49.589
Mar 15 22:46:49.589: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename runtimeclass 03/15/23 22:46:49.59
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:49.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:49.614
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Mar 15 22:46:49.641: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7708 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 15 22:46:49.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7708" for this suite. 03/15/23 22:46:49.674
------------------------------
• [0.094 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:46:49.589
    Mar 15 22:46:49.589: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename runtimeclass 03/15/23 22:46:49.59
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:49.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:49.614
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Mar 15 22:46:49.641: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7708 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:46:49.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7708" for this suite. 03/15/23 22:46:49.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:46:49.683
Mar 15 22:46:49.683: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename var-expansion 03/15/23 22:46:49.685
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:49.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:49.729
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 03/15/23 22:46:49.733
Mar 15 22:46:49.751: INFO: Waiting up to 5m0s for pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509" in namespace "var-expansion-4970" to be "Succeeded or Failed"
Mar 15 22:46:49.756: INFO: Pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509": Phase="Pending", Reason="", readiness=false. Elapsed: 4.513734ms
Mar 15 22:46:51.760: INFO: Pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008542065s
Mar 15 22:46:53.764: INFO: Pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012475689s
Mar 15 22:46:55.761: INFO: Pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009605569s
Mar 15 22:46:57.760: INFO: Pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008231211s
STEP: Saw pod success 03/15/23 22:46:57.76
Mar 15 22:46:57.760: INFO: Pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509" satisfied condition "Succeeded or Failed"
Mar 15 22:46:57.763: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509 container dapi-container: <nil>
STEP: delete the pod 03/15/23 22:46:57.774
Mar 15 22:46:57.792: INFO: Waiting for pod var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509 to disappear
Mar 15 22:46:57.800: INFO: Pod var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 15 22:46:57.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4970" for this suite. 03/15/23 22:46:57.809
------------------------------
• [SLOW TEST] [8.138 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:46:49.683
    Mar 15 22:46:49.683: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename var-expansion 03/15/23 22:46:49.685
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:49.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:49.729
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 03/15/23 22:46:49.733
    Mar 15 22:46:49.751: INFO: Waiting up to 5m0s for pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509" in namespace "var-expansion-4970" to be "Succeeded or Failed"
    Mar 15 22:46:49.756: INFO: Pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509": Phase="Pending", Reason="", readiness=false. Elapsed: 4.513734ms
    Mar 15 22:46:51.760: INFO: Pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008542065s
    Mar 15 22:46:53.764: INFO: Pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012475689s
    Mar 15 22:46:55.761: INFO: Pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009605569s
    Mar 15 22:46:57.760: INFO: Pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008231211s
    STEP: Saw pod success 03/15/23 22:46:57.76
    Mar 15 22:46:57.760: INFO: Pod "var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509" satisfied condition "Succeeded or Failed"
    Mar 15 22:46:57.763: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509 container dapi-container: <nil>
    STEP: delete the pod 03/15/23 22:46:57.774
    Mar 15 22:46:57.792: INFO: Waiting for pod var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509 to disappear
    Mar 15 22:46:57.800: INFO: Pod var-expansion-5cce68cf-a7bc-47b1-9b74-74f94f2a2509 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:46:57.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4970" for this suite. 03/15/23 22:46:57.809
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:46:57.823
Mar 15 22:46:57.823: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 22:46:57.824
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:57.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:57.874
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3847 03/15/23 22:46:57.878
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/15/23 22:46:57.93
STEP: creating service externalsvc in namespace services-3847 03/15/23 22:46:57.93
STEP: creating replication controller externalsvc in namespace services-3847 03/15/23 22:46:57.973
I0315 22:46:58.015346      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3847, replica count: 2
I0315 22:47:01.067274      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 03/15/23 22:47:01.07
Mar 15 22:47:01.089: INFO: Creating new exec pod
Mar 15 22:47:01.099: INFO: Waiting up to 5m0s for pod "execpod5w869" in namespace "services-3847" to be "running"
Mar 15 22:47:01.112: INFO: Pod "execpod5w869": Phase="Pending", Reason="", readiness=false. Elapsed: 12.777852ms
Mar 15 22:47:03.117: INFO: Pod "execpod5w869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017813104s
Mar 15 22:47:05.115: INFO: Pod "execpod5w869": Phase="Running", Reason="", readiness=true. Elapsed: 4.016405885s
Mar 15 22:47:05.115: INFO: Pod "execpod5w869" satisfied condition "running"
Mar 15 22:47:05.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-3847 exec execpod5w869 -- /bin/sh -x -c nslookup nodeport-service.services-3847.svc.cluster.local'
Mar 15 22:47:05.353: INFO: stderr: "+ nslookup nodeport-service.services-3847.svc.cluster.local\n"
Mar 15 22:47:05.353: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nnodeport-service.services-3847.svc.cluster.local\tcanonical name = externalsvc.services-3847.svc.cluster.local.\nName:\texternalsvc.services-3847.svc.cluster.local\nAddress: 100.66.160.231\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3847, will wait for the garbage collector to delete the pods 03/15/23 22:47:05.353
Mar 15 22:47:05.412: INFO: Deleting ReplicationController externalsvc took: 4.698423ms
Mar 15 22:47:05.514: INFO: Terminating ReplicationController externalsvc pods took: 101.667583ms
Mar 15 22:47:07.331: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 22:47:07.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3847" for this suite. 03/15/23 22:47:07.345
------------------------------
• [SLOW TEST] [9.530 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:46:57.823
    Mar 15 22:46:57.823: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 22:46:57.824
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:46:57.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:46:57.874
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3847 03/15/23 22:46:57.878
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/15/23 22:46:57.93
    STEP: creating service externalsvc in namespace services-3847 03/15/23 22:46:57.93
    STEP: creating replication controller externalsvc in namespace services-3847 03/15/23 22:46:57.973
    I0315 22:46:58.015346      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3847, replica count: 2
    I0315 22:47:01.067274      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 03/15/23 22:47:01.07
    Mar 15 22:47:01.089: INFO: Creating new exec pod
    Mar 15 22:47:01.099: INFO: Waiting up to 5m0s for pod "execpod5w869" in namespace "services-3847" to be "running"
    Mar 15 22:47:01.112: INFO: Pod "execpod5w869": Phase="Pending", Reason="", readiness=false. Elapsed: 12.777852ms
    Mar 15 22:47:03.117: INFO: Pod "execpod5w869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017813104s
    Mar 15 22:47:05.115: INFO: Pod "execpod5w869": Phase="Running", Reason="", readiness=true. Elapsed: 4.016405885s
    Mar 15 22:47:05.115: INFO: Pod "execpod5w869" satisfied condition "running"
    Mar 15 22:47:05.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-3847 exec execpod5w869 -- /bin/sh -x -c nslookup nodeport-service.services-3847.svc.cluster.local'
    Mar 15 22:47:05.353: INFO: stderr: "+ nslookup nodeport-service.services-3847.svc.cluster.local\n"
    Mar 15 22:47:05.353: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nnodeport-service.services-3847.svc.cluster.local\tcanonical name = externalsvc.services-3847.svc.cluster.local.\nName:\texternalsvc.services-3847.svc.cluster.local\nAddress: 100.66.160.231\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3847, will wait for the garbage collector to delete the pods 03/15/23 22:47:05.353
    Mar 15 22:47:05.412: INFO: Deleting ReplicationController externalsvc took: 4.698423ms
    Mar 15 22:47:05.514: INFO: Terminating ReplicationController externalsvc pods took: 101.667583ms
    Mar 15 22:47:07.331: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:47:07.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3847" for this suite. 03/15/23 22:47:07.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:47:07.355
Mar 15 22:47:07.355: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename ephemeral-containers-test 03/15/23 22:47:07.356
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:47:07.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:47:07.382
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 03/15/23 22:47:07.385
Mar 15 22:47:07.393: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9899" to be "running and ready"
Mar 15 22:47:07.397: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.479328ms
Mar 15 22:47:07.397: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:47:09.402: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008320806s
Mar 15 22:47:09.402: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Mar 15 22:47:09.402: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 03/15/23 22:47:09.405
Mar 15 22:47:09.430: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9899" to be "container debugger running"
Mar 15 22:47:09.433: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.907485ms
Mar 15 22:47:11.437: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007414706s
Mar 15 22:47:13.438: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008166563s
Mar 15 22:47:13.438: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 03/15/23 22:47:13.438
Mar 15 22:47:13.438: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9899 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:47:13.438: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:47:13.439: INFO: ExecWithOptions: Clientset creation
Mar 15 22:47:13.439: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/ephemeral-containers-test-9899/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Mar 15 22:47:13.512: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:47:13.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-9899" for this suite. 03/15/23 22:47:13.52
------------------------------
• [SLOW TEST] [6.170 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:47:07.355
    Mar 15 22:47:07.355: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename ephemeral-containers-test 03/15/23 22:47:07.356
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:47:07.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:47:07.382
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 03/15/23 22:47:07.385
    Mar 15 22:47:07.393: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9899" to be "running and ready"
    Mar 15 22:47:07.397: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.479328ms
    Mar 15 22:47:07.397: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:47:09.402: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008320806s
    Mar 15 22:47:09.402: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Mar 15 22:47:09.402: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 03/15/23 22:47:09.405
    Mar 15 22:47:09.430: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9899" to be "container debugger running"
    Mar 15 22:47:09.433: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.907485ms
    Mar 15 22:47:11.437: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007414706s
    Mar 15 22:47:13.438: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008166563s
    Mar 15 22:47:13.438: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 03/15/23 22:47:13.438
    Mar 15 22:47:13.438: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9899 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:47:13.438: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:47:13.439: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:47:13.439: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/ephemeral-containers-test-9899/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Mar 15 22:47:13.512: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:47:13.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-9899" for this suite. 03/15/23 22:47:13.52
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:47:13.526
Mar 15 22:47:13.526: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename emptydir 03/15/23 22:47:13.526
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:47:13.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:47:13.54
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 03/15/23 22:47:13.543
Mar 15 22:47:13.549: INFO: Waiting up to 5m0s for pod "pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7" in namespace "emptydir-9644" to be "Succeeded or Failed"
Mar 15 22:47:13.556: INFO: Pod "pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073742ms
Mar 15 22:47:15.559: INFO: Pod "pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009447198s
Mar 15 22:47:17.559: INFO: Pod "pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009231711s
STEP: Saw pod success 03/15/23 22:47:17.559
Mar 15 22:47:17.559: INFO: Pod "pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7" satisfied condition "Succeeded or Failed"
Mar 15 22:47:17.562: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7 container test-container: <nil>
STEP: delete the pod 03/15/23 22:47:17.569
Mar 15 22:47:17.586: INFO: Waiting for pod pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7 to disappear
Mar 15 22:47:17.593: INFO: Pod pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 15 22:47:17.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9644" for this suite. 03/15/23 22:47:17.604
------------------------------
• [4.093 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:47:13.526
    Mar 15 22:47:13.526: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename emptydir 03/15/23 22:47:13.526
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:47:13.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:47:13.54
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 03/15/23 22:47:13.543
    Mar 15 22:47:13.549: INFO: Waiting up to 5m0s for pod "pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7" in namespace "emptydir-9644" to be "Succeeded or Failed"
    Mar 15 22:47:13.556: INFO: Pod "pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073742ms
    Mar 15 22:47:15.559: INFO: Pod "pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009447198s
    Mar 15 22:47:17.559: INFO: Pod "pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009231711s
    STEP: Saw pod success 03/15/23 22:47:17.559
    Mar 15 22:47:17.559: INFO: Pod "pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7" satisfied condition "Succeeded or Failed"
    Mar 15 22:47:17.562: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7 container test-container: <nil>
    STEP: delete the pod 03/15/23 22:47:17.569
    Mar 15 22:47:17.586: INFO: Waiting for pod pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7 to disappear
    Mar 15 22:47:17.593: INFO: Pod pod-c79c100e-73f0-4bc2-877e-04c9de26fbe7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:47:17.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9644" for this suite. 03/15/23 22:47:17.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:47:17.619
Mar 15 22:47:17.619: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pods 03/15/23 22:47:17.62
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:47:17.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:47:17.701
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 03/15/23 22:47:17.708
Mar 15 22:47:17.742: INFO: Waiting up to 5m0s for pod "pod-np9dz" in namespace "pods-6638" to be "running"
Mar 15 22:47:17.747: INFO: Pod "pod-np9dz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.542223ms
Mar 15 22:47:19.750: INFO: Pod "pod-np9dz": Phase="Running", Reason="", readiness=true. Elapsed: 2.00740037s
Mar 15 22:47:19.750: INFO: Pod "pod-np9dz" satisfied condition "running"
STEP: patching /status 03/15/23 22:47:19.75
Mar 15 22:47:19.770: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 15 22:47:19.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6638" for this suite. 03/15/23 22:47:19.774
------------------------------
• [2.160 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:47:17.619
    Mar 15 22:47:17.619: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pods 03/15/23 22:47:17.62
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:47:17.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:47:17.701
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 03/15/23 22:47:17.708
    Mar 15 22:47:17.742: INFO: Waiting up to 5m0s for pod "pod-np9dz" in namespace "pods-6638" to be "running"
    Mar 15 22:47:17.747: INFO: Pod "pod-np9dz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.542223ms
    Mar 15 22:47:19.750: INFO: Pod "pod-np9dz": Phase="Running", Reason="", readiness=true. Elapsed: 2.00740037s
    Mar 15 22:47:19.750: INFO: Pod "pod-np9dz" satisfied condition "running"
    STEP: patching /status 03/15/23 22:47:19.75
    Mar 15 22:47:19.770: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:47:19.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6638" for this suite. 03/15/23 22:47:19.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:47:19.792
Mar 15 22:47:19.792: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubelet-test 03/15/23 22:47:19.792
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:47:19.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:47:19.858
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Mar 15 22:47:19.895: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs393d38a3-098d-433f-aef9-0ed763ddf623" in namespace "kubelet-test-6127" to be "running and ready"
Mar 15 22:47:19.907: INFO: Pod "busybox-readonly-fs393d38a3-098d-433f-aef9-0ed763ddf623": Phase="Pending", Reason="", readiness=false. Elapsed: 11.396974ms
Mar 15 22:47:19.907: INFO: The phase of Pod busybox-readonly-fs393d38a3-098d-433f-aef9-0ed763ddf623 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:47:21.913: INFO: Pod "busybox-readonly-fs393d38a3-098d-433f-aef9-0ed763ddf623": Phase="Running", Reason="", readiness=true. Elapsed: 2.017883409s
Mar 15 22:47:21.913: INFO: The phase of Pod busybox-readonly-fs393d38a3-098d-433f-aef9-0ed763ddf623 is Running (Ready = true)
Mar 15 22:47:21.913: INFO: Pod "busybox-readonly-fs393d38a3-098d-433f-aef9-0ed763ddf623" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:47:21.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6127" for this suite. 03/15/23 22:47:21.934
------------------------------
• [2.150 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:47:19.792
    Mar 15 22:47:19.792: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubelet-test 03/15/23 22:47:19.792
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:47:19.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:47:19.858
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Mar 15 22:47:19.895: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs393d38a3-098d-433f-aef9-0ed763ddf623" in namespace "kubelet-test-6127" to be "running and ready"
    Mar 15 22:47:19.907: INFO: Pod "busybox-readonly-fs393d38a3-098d-433f-aef9-0ed763ddf623": Phase="Pending", Reason="", readiness=false. Elapsed: 11.396974ms
    Mar 15 22:47:19.907: INFO: The phase of Pod busybox-readonly-fs393d38a3-098d-433f-aef9-0ed763ddf623 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:47:21.913: INFO: Pod "busybox-readonly-fs393d38a3-098d-433f-aef9-0ed763ddf623": Phase="Running", Reason="", readiness=true. Elapsed: 2.017883409s
    Mar 15 22:47:21.913: INFO: The phase of Pod busybox-readonly-fs393d38a3-098d-433f-aef9-0ed763ddf623 is Running (Ready = true)
    Mar 15 22:47:21.913: INFO: Pod "busybox-readonly-fs393d38a3-098d-433f-aef9-0ed763ddf623" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:47:21.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6127" for this suite. 03/15/23 22:47:21.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:47:21.944
Mar 15 22:47:21.945: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-probe 03/15/23 22:47:21.947
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:47:21.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:47:21.971
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5 in namespace container-probe-2747 03/15/23 22:47:21.978
Mar 15 22:47:21.988: INFO: Waiting up to 5m0s for pod "test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5" in namespace "container-probe-2747" to be "not pending"
Mar 15 22:47:21.998: INFO: Pod "test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.466029ms
Mar 15 22:47:24.008: INFO: Pod "test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.019793196s
Mar 15 22:47:24.008: INFO: Pod "test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5" satisfied condition "not pending"
Mar 15 22:47:24.008: INFO: Started pod test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5 in namespace container-probe-2747
STEP: checking the pod's current state and verifying that restartCount is present 03/15/23 22:47:24.008
Mar 15 22:47:24.014: INFO: Initial restart count of pod test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5 is 0
STEP: deleting the pod 03/15/23 22:51:24.559
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 15 22:51:24.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2747" for this suite. 03/15/23 22:51:24.594
------------------------------
• [SLOW TEST] [242.658 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:47:21.944
    Mar 15 22:47:21.945: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-probe 03/15/23 22:47:21.947
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:47:21.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:47:21.971
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5 in namespace container-probe-2747 03/15/23 22:47:21.978
    Mar 15 22:47:21.988: INFO: Waiting up to 5m0s for pod "test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5" in namespace "container-probe-2747" to be "not pending"
    Mar 15 22:47:21.998: INFO: Pod "test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.466029ms
    Mar 15 22:47:24.008: INFO: Pod "test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.019793196s
    Mar 15 22:47:24.008: INFO: Pod "test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5" satisfied condition "not pending"
    Mar 15 22:47:24.008: INFO: Started pod test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5 in namespace container-probe-2747
    STEP: checking the pod's current state and verifying that restartCount is present 03/15/23 22:47:24.008
    Mar 15 22:47:24.014: INFO: Initial restart count of pod test-webserver-6e1b7249-a7c4-45a7-8676-5baac6cfd8e5 is 0
    STEP: deleting the pod 03/15/23 22:51:24.559
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:51:24.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2747" for this suite. 03/15/23 22:51:24.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:51:24.603
Mar 15 22:51:24.604: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename namespaces 03/15/23 22:51:24.605
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:51:24.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:51:24.631
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 03/15/23 22:51:24.636
STEP: patching the Namespace 03/15/23 22:51:24.654
STEP: get the Namespace and ensuring it has the label 03/15/23 22:51:24.661
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:51:24.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-45" for this suite. 03/15/23 22:51:24.669
STEP: Destroying namespace "nspatchtest-c9bcf6e3-cb54-4754-b718-6683d18235e2-5381" for this suite. 03/15/23 22:51:24.674
------------------------------
• [0.077 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:51:24.603
    Mar 15 22:51:24.604: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename namespaces 03/15/23 22:51:24.605
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:51:24.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:51:24.631
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 03/15/23 22:51:24.636
    STEP: patching the Namespace 03/15/23 22:51:24.654
    STEP: get the Namespace and ensuring it has the label 03/15/23 22:51:24.661
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:51:24.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-45" for this suite. 03/15/23 22:51:24.669
    STEP: Destroying namespace "nspatchtest-c9bcf6e3-cb54-4754-b718-6683d18235e2-5381" for this suite. 03/15/23 22:51:24.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:51:24.682
Mar 15 22:51:24.682: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename daemonsets 03/15/23 22:51:24.684
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:51:24.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:51:24.707
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Mar 15 22:51:24.747: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 03/15/23 22:51:24.757
Mar 15 22:51:24.764: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:51:24.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:51:24.766: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:51:25.771: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:51:25.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:51:25.774: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:51:26.771: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:51:26.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 15 22:51:26.775: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 03/15/23 22:51:26.786
STEP: Check that daemon pods images are updated. 03/15/23 22:51:26.806
Mar 15 22:51:26.810: INFO: Wrong image for pod: daemon-set-4qgqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 15 22:51:26.810: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 15 22:51:26.810: INFO: Wrong image for pod: daemon-set-rrhzc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 15 22:51:26.814: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:51:27.817: INFO: Wrong image for pod: daemon-set-4qgqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 15 22:51:27.817: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 15 22:51:27.821: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:51:28.817: INFO: Wrong image for pod: daemon-set-4qgqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 15 22:51:28.817: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 15 22:51:28.821: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:51:29.818: INFO: Wrong image for pod: daemon-set-4qgqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 15 22:51:29.818: INFO: Pod daemon-set-hp4b2 is not available
Mar 15 22:51:29.818: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 15 22:51:29.825: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:51:30.818: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 15 22:51:30.825: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:51:31.818: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 15 22:51:31.821: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:51:32.817: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 15 22:51:32.817: INFO: Pod daemon-set-spnvb is not available
Mar 15 22:51:32.820: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:51:33.822: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:51:34.818: INFO: Pod daemon-set-7kfcv is not available
Mar 15 22:51:34.824: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 03/15/23 22:51:34.824
Mar 15 22:51:34.830: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:51:34.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 15 22:51:34.837: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/15/23 22:51:34.851
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3034, will wait for the garbage collector to delete the pods 03/15/23 22:51:34.851
Mar 15 22:51:34.920: INFO: Deleting DaemonSet.extensions daemon-set took: 7.122784ms
Mar 15 22:51:35.021: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.732598ms
Mar 15 22:51:37.927: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:51:37.927: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 15 22:51:37.930: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28072"},"items":null}

Mar 15 22:51:37.933: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28072"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:51:37.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3034" for this suite. 03/15/23 22:51:37.957
------------------------------
• [SLOW TEST] [13.298 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:51:24.682
    Mar 15 22:51:24.682: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename daemonsets 03/15/23 22:51:24.684
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:51:24.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:51:24.707
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Mar 15 22:51:24.747: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 03/15/23 22:51:24.757
    Mar 15 22:51:24.764: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:51:24.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:51:24.766: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:51:25.771: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:51:25.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:51:25.774: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:51:26.771: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:51:26.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 15 22:51:26.775: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 03/15/23 22:51:26.786
    STEP: Check that daemon pods images are updated. 03/15/23 22:51:26.806
    Mar 15 22:51:26.810: INFO: Wrong image for pod: daemon-set-4qgqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 15 22:51:26.810: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 15 22:51:26.810: INFO: Wrong image for pod: daemon-set-rrhzc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 15 22:51:26.814: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:51:27.817: INFO: Wrong image for pod: daemon-set-4qgqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 15 22:51:27.817: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 15 22:51:27.821: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:51:28.817: INFO: Wrong image for pod: daemon-set-4qgqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 15 22:51:28.817: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 15 22:51:28.821: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:51:29.818: INFO: Wrong image for pod: daemon-set-4qgqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 15 22:51:29.818: INFO: Pod daemon-set-hp4b2 is not available
    Mar 15 22:51:29.818: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 15 22:51:29.825: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:51:30.818: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 15 22:51:30.825: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:51:31.818: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 15 22:51:31.821: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:51:32.817: INFO: Wrong image for pod: daemon-set-ln7t8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 15 22:51:32.817: INFO: Pod daemon-set-spnvb is not available
    Mar 15 22:51:32.820: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:51:33.822: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:51:34.818: INFO: Pod daemon-set-7kfcv is not available
    Mar 15 22:51:34.824: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 03/15/23 22:51:34.824
    Mar 15 22:51:34.830: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:51:34.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 15 22:51:34.837: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/15/23 22:51:34.851
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3034, will wait for the garbage collector to delete the pods 03/15/23 22:51:34.851
    Mar 15 22:51:34.920: INFO: Deleting DaemonSet.extensions daemon-set took: 7.122784ms
    Mar 15 22:51:35.021: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.732598ms
    Mar 15 22:51:37.927: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:51:37.927: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 15 22:51:37.930: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28072"},"items":null}

    Mar 15 22:51:37.933: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28072"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:51:37.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3034" for this suite. 03/15/23 22:51:37.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:51:37.982
Mar 15 22:51:37.982: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pods 03/15/23 22:51:37.983
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:51:38.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:51:38.053
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Mar 15 22:51:38.058: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: creating the pod 03/15/23 22:51:38.059
STEP: submitting the pod to kubernetes 03/15/23 22:51:38.059
Mar 15 22:51:38.073: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-6371f514-1936-4729-b6b2-1c175d0063c6" in namespace "pods-2867" to be "running and ready"
Mar 15 22:51:38.090: INFO: Pod "pod-exec-websocket-6371f514-1936-4729-b6b2-1c175d0063c6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.669814ms
Mar 15 22:51:38.090: INFO: The phase of Pod pod-exec-websocket-6371f514-1936-4729-b6b2-1c175d0063c6 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:51:40.093: INFO: Pod "pod-exec-websocket-6371f514-1936-4729-b6b2-1c175d0063c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.020094091s
Mar 15 22:51:40.093: INFO: The phase of Pod pod-exec-websocket-6371f514-1936-4729-b6b2-1c175d0063c6 is Running (Ready = true)
Mar 15 22:51:40.093: INFO: Pod "pod-exec-websocket-6371f514-1936-4729-b6b2-1c175d0063c6" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 15 22:51:40.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2867" for this suite. 03/15/23 22:51:40.217
------------------------------
• [2.241 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:51:37.982
    Mar 15 22:51:37.982: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pods 03/15/23 22:51:37.983
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:51:38.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:51:38.053
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Mar 15 22:51:38.058: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: creating the pod 03/15/23 22:51:38.059
    STEP: submitting the pod to kubernetes 03/15/23 22:51:38.059
    Mar 15 22:51:38.073: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-6371f514-1936-4729-b6b2-1c175d0063c6" in namespace "pods-2867" to be "running and ready"
    Mar 15 22:51:38.090: INFO: Pod "pod-exec-websocket-6371f514-1936-4729-b6b2-1c175d0063c6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.669814ms
    Mar 15 22:51:38.090: INFO: The phase of Pod pod-exec-websocket-6371f514-1936-4729-b6b2-1c175d0063c6 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:51:40.093: INFO: Pod "pod-exec-websocket-6371f514-1936-4729-b6b2-1c175d0063c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.020094091s
    Mar 15 22:51:40.093: INFO: The phase of Pod pod-exec-websocket-6371f514-1936-4729-b6b2-1c175d0063c6 is Running (Ready = true)
    Mar 15 22:51:40.093: INFO: Pod "pod-exec-websocket-6371f514-1936-4729-b6b2-1c175d0063c6" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:51:40.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2867" for this suite. 03/15/23 22:51:40.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:51:40.227
Mar 15 22:51:40.227: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename var-expansion 03/15/23 22:51:40.228
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:51:40.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:51:40.246
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 03/15/23 22:51:40.249
STEP: waiting for pod running 03/15/23 22:51:40.256
Mar 15 22:51:40.256: INFO: Waiting up to 2m0s for pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f" in namespace "var-expansion-8888" to be "running"
Mar 15 22:51:40.261: INFO: Pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.412578ms
Mar 15 22:51:42.264: INFO: Pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008033779s
Mar 15 22:51:42.264: INFO: Pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f" satisfied condition "running"
STEP: creating a file in subpath 03/15/23 22:51:42.264
Mar 15 22:51:42.267: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8888 PodName:var-expansion-40df8604-f081-410b-b798-fa4d5b04117f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:51:42.267: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:51:42.268: INFO: ExecWithOptions: Clientset creation
Mar 15 22:51:42.268: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/var-expansion-8888/pods/var-expansion-40df8604-f081-410b-b798-fa4d5b04117f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 03/15/23 22:51:42.339
Mar 15 22:51:42.343: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8888 PodName:var-expansion-40df8604-f081-410b-b798-fa4d5b04117f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 15 22:51:42.343: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 22:51:42.343: INFO: ExecWithOptions: Clientset creation
Mar 15 22:51:42.343: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/var-expansion-8888/pods/var-expansion-40df8604-f081-410b-b798-fa4d5b04117f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 03/15/23 22:51:42.411
Mar 15 22:51:42.925: INFO: Successfully updated pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f"
STEP: waiting for annotated pod running 03/15/23 22:51:42.925
Mar 15 22:51:42.925: INFO: Waiting up to 2m0s for pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f" in namespace "var-expansion-8888" to be "running"
Mar 15 22:51:42.928: INFO: Pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f": Phase="Running", Reason="", readiness=true. Elapsed: 2.938491ms
Mar 15 22:51:42.928: INFO: Pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f" satisfied condition "running"
STEP: deleting the pod gracefully 03/15/23 22:51:42.928
Mar 15 22:51:42.928: INFO: Deleting pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f" in namespace "var-expansion-8888"
Mar 15 22:51:42.937: INFO: Wait up to 5m0s for pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 15 22:52:16.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8888" for this suite. 03/15/23 22:52:16.952
------------------------------
• [SLOW TEST] [36.731 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:51:40.227
    Mar 15 22:51:40.227: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename var-expansion 03/15/23 22:51:40.228
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:51:40.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:51:40.246
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 03/15/23 22:51:40.249
    STEP: waiting for pod running 03/15/23 22:51:40.256
    Mar 15 22:51:40.256: INFO: Waiting up to 2m0s for pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f" in namespace "var-expansion-8888" to be "running"
    Mar 15 22:51:40.261: INFO: Pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.412578ms
    Mar 15 22:51:42.264: INFO: Pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008033779s
    Mar 15 22:51:42.264: INFO: Pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f" satisfied condition "running"
    STEP: creating a file in subpath 03/15/23 22:51:42.264
    Mar 15 22:51:42.267: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8888 PodName:var-expansion-40df8604-f081-410b-b798-fa4d5b04117f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:51:42.267: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:51:42.268: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:51:42.268: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/var-expansion-8888/pods/var-expansion-40df8604-f081-410b-b798-fa4d5b04117f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 03/15/23 22:51:42.339
    Mar 15 22:51:42.343: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8888 PodName:var-expansion-40df8604-f081-410b-b798-fa4d5b04117f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 15 22:51:42.343: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 22:51:42.343: INFO: ExecWithOptions: Clientset creation
    Mar 15 22:51:42.343: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/var-expansion-8888/pods/var-expansion-40df8604-f081-410b-b798-fa4d5b04117f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 03/15/23 22:51:42.411
    Mar 15 22:51:42.925: INFO: Successfully updated pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f"
    STEP: waiting for annotated pod running 03/15/23 22:51:42.925
    Mar 15 22:51:42.925: INFO: Waiting up to 2m0s for pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f" in namespace "var-expansion-8888" to be "running"
    Mar 15 22:51:42.928: INFO: Pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f": Phase="Running", Reason="", readiness=true. Elapsed: 2.938491ms
    Mar 15 22:51:42.928: INFO: Pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f" satisfied condition "running"
    STEP: deleting the pod gracefully 03/15/23 22:51:42.928
    Mar 15 22:51:42.928: INFO: Deleting pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f" in namespace "var-expansion-8888"
    Mar 15 22:51:42.937: INFO: Wait up to 5m0s for pod "var-expansion-40df8604-f081-410b-b798-fa4d5b04117f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:52:16.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8888" for this suite. 03/15/23 22:52:16.952
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:52:16.959
Mar 15 22:52:16.959: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 22:52:16.96
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:52:16.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:52:17
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-5812 03/15/23 22:52:17.006
STEP: creating service affinity-clusterip-transition in namespace services-5812 03/15/23 22:52:17.007
STEP: creating replication controller affinity-clusterip-transition in namespace services-5812 03/15/23 22:52:17.039
I0315 22:52:17.053227      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5812, replica count: 3
I0315 22:52:20.107309      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 22:52:20.112: INFO: Creating new exec pod
Mar 15 22:52:20.117: INFO: Waiting up to 5m0s for pod "execpod-affinitydv4mx" in namespace "services-5812" to be "running"
Mar 15 22:52:20.120: INFO: Pod "execpod-affinitydv4mx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139183ms
Mar 15 22:52:22.127: INFO: Pod "execpod-affinitydv4mx": Phase="Running", Reason="", readiness=true. Elapsed: 2.010371667s
Mar 15 22:52:22.127: INFO: Pod "execpod-affinitydv4mx" satisfied condition "running"
Mar 15 22:52:23.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-5812 exec execpod-affinitydv4mx -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Mar 15 22:52:23.308: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 15 22:52:23.308: INFO: stdout: ""
Mar 15 22:52:23.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-5812 exec execpod-affinitydv4mx -- /bin/sh -x -c nc -v -z -w 2 100.69.142.97 80'
Mar 15 22:52:23.481: INFO: stderr: "+ nc -v -z -w 2 100.69.142.97 80\nConnection to 100.69.142.97 80 port [tcp/http] succeeded!\n"
Mar 15 22:52:23.481: INFO: stdout: ""
Mar 15 22:52:23.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-5812 exec execpod-affinitydv4mx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.69.142.97:80/ ; done'
Mar 15 22:52:23.957: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n"
Mar 15 22:52:23.957: INFO: stdout: "\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-sw4fw\naffinity-clusterip-transition-sw4fw\naffinity-clusterip-transition-z5t6t\naffinity-clusterip-transition-sw4fw\naffinity-clusterip-transition-sw4fw\naffinity-clusterip-transition-z5t6t\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-z5t6t\naffinity-clusterip-transition-z5t6t\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-z5t6t\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-sw4fw\naffinity-clusterip-transition-sw4fw"
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-sw4fw
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-sw4fw
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-z5t6t
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-sw4fw
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-sw4fw
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-z5t6t
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-z5t6t
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-z5t6t
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-z5t6t
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-sw4fw
Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-sw4fw
Mar 15 22:52:23.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-5812 exec execpod-affinitydv4mx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.69.142.97:80/ ; done'
Mar 15 22:52:24.262: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n"
Mar 15 22:52:24.262: INFO: stdout: "\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv"
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
Mar 15 22:52:24.262: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5812, will wait for the garbage collector to delete the pods 03/15/23 22:52:24.276
Mar 15 22:52:24.338: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.274602ms
Mar 15 22:52:24.438: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.337647ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 22:52:26.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5812" for this suite. 03/15/23 22:52:26.871
------------------------------
• [SLOW TEST] [9.920 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:52:16.959
    Mar 15 22:52:16.959: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 22:52:16.96
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:52:16.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:52:17
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-5812 03/15/23 22:52:17.006
    STEP: creating service affinity-clusterip-transition in namespace services-5812 03/15/23 22:52:17.007
    STEP: creating replication controller affinity-clusterip-transition in namespace services-5812 03/15/23 22:52:17.039
    I0315 22:52:17.053227      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5812, replica count: 3
    I0315 22:52:20.107309      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 15 22:52:20.112: INFO: Creating new exec pod
    Mar 15 22:52:20.117: INFO: Waiting up to 5m0s for pod "execpod-affinitydv4mx" in namespace "services-5812" to be "running"
    Mar 15 22:52:20.120: INFO: Pod "execpod-affinitydv4mx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139183ms
    Mar 15 22:52:22.127: INFO: Pod "execpod-affinitydv4mx": Phase="Running", Reason="", readiness=true. Elapsed: 2.010371667s
    Mar 15 22:52:22.127: INFO: Pod "execpod-affinitydv4mx" satisfied condition "running"
    Mar 15 22:52:23.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-5812 exec execpod-affinitydv4mx -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Mar 15 22:52:23.308: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Mar 15 22:52:23.308: INFO: stdout: ""
    Mar 15 22:52:23.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-5812 exec execpod-affinitydv4mx -- /bin/sh -x -c nc -v -z -w 2 100.69.142.97 80'
    Mar 15 22:52:23.481: INFO: stderr: "+ nc -v -z -w 2 100.69.142.97 80\nConnection to 100.69.142.97 80 port [tcp/http] succeeded!\n"
    Mar 15 22:52:23.481: INFO: stdout: ""
    Mar 15 22:52:23.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-5812 exec execpod-affinitydv4mx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.69.142.97:80/ ; done'
    Mar 15 22:52:23.957: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n"
    Mar 15 22:52:23.957: INFO: stdout: "\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-sw4fw\naffinity-clusterip-transition-sw4fw\naffinity-clusterip-transition-z5t6t\naffinity-clusterip-transition-sw4fw\naffinity-clusterip-transition-sw4fw\naffinity-clusterip-transition-z5t6t\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-z5t6t\naffinity-clusterip-transition-z5t6t\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-z5t6t\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-sw4fw\naffinity-clusterip-transition-sw4fw"
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-sw4fw
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-sw4fw
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-z5t6t
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-sw4fw
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-sw4fw
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-z5t6t
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-z5t6t
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-z5t6t
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-z5t6t
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-sw4fw
    Mar 15 22:52:23.957: INFO: Received response from host: affinity-clusterip-transition-sw4fw
    Mar 15 22:52:23.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-5812 exec execpod-affinitydv4mx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.69.142.97:80/ ; done'
    Mar 15 22:52:24.262: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.142.97:80/\n"
    Mar 15 22:52:24.262: INFO: stdout: "\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv\naffinity-clusterip-transition-b8dqv"
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Received response from host: affinity-clusterip-transition-b8dqv
    Mar 15 22:52:24.262: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5812, will wait for the garbage collector to delete the pods 03/15/23 22:52:24.276
    Mar 15 22:52:24.338: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.274602ms
    Mar 15 22:52:24.438: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.337647ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:52:26.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5812" for this suite. 03/15/23 22:52:26.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:52:26.881
Mar 15 22:52:26.881: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename replicaset 03/15/23 22:52:26.882
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:52:26.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:52:26.898
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/15/23 22:52:26.902
Mar 15 22:52:26.911: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5248" to be "running and ready"
Mar 15 22:52:26.916: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.684938ms
Mar 15 22:52:26.916: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:52:28.920: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008645064s
Mar 15 22:52:28.920: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:52:30.920: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.008467474s
Mar 15 22:52:30.920: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Mar 15 22:52:30.920: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 03/15/23 22:52:30.923
STEP: Then the orphan pod is adopted 03/15/23 22:52:30.928
STEP: When the matched label of one of its pods change 03/15/23 22:52:31.936
Mar 15 22:52:31.939: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 03/15/23 22:52:31.954
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:52:32.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5248" for this suite. 03/15/23 22:52:32.975
------------------------------
• [SLOW TEST] [6.101 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:52:26.881
    Mar 15 22:52:26.881: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename replicaset 03/15/23 22:52:26.882
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:52:26.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:52:26.898
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/15/23 22:52:26.902
    Mar 15 22:52:26.911: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5248" to be "running and ready"
    Mar 15 22:52:26.916: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.684938ms
    Mar 15 22:52:26.916: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:52:28.920: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008645064s
    Mar 15 22:52:28.920: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:52:30.920: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.008467474s
    Mar 15 22:52:30.920: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Mar 15 22:52:30.920: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 03/15/23 22:52:30.923
    STEP: Then the orphan pod is adopted 03/15/23 22:52:30.928
    STEP: When the matched label of one of its pods change 03/15/23 22:52:31.936
    Mar 15 22:52:31.939: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/15/23 22:52:31.954
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:52:32.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5248" for this suite. 03/15/23 22:52:32.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:52:32.985
Mar 15 22:52:32.985: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:52:32.99
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:52:33.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:52:33.011
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-ded1d7cf-2389-4d7d-8af2-179507ec4134 03/15/23 22:52:33.022
STEP: Creating configMap with name cm-test-opt-upd-b3767301-2556-4def-817e-9f9c2c7b53b5 03/15/23 22:52:33.032
STEP: Creating the pod 03/15/23 22:52:33.037
Mar 15 22:52:33.050: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f6a706d6-7187-452b-872e-4fd91aee7e29" in namespace "projected-5916" to be "running and ready"
Mar 15 22:52:33.058: INFO: Pod "pod-projected-configmaps-f6a706d6-7187-452b-872e-4fd91aee7e29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025165ms
Mar 15 22:52:33.058: INFO: The phase of Pod pod-projected-configmaps-f6a706d6-7187-452b-872e-4fd91aee7e29 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:52:35.062: INFO: Pod "pod-projected-configmaps-f6a706d6-7187-452b-872e-4fd91aee7e29": Phase="Running", Reason="", readiness=true. Elapsed: 2.012672957s
Mar 15 22:52:35.063: INFO: The phase of Pod pod-projected-configmaps-f6a706d6-7187-452b-872e-4fd91aee7e29 is Running (Ready = true)
Mar 15 22:52:35.063: INFO: Pod "pod-projected-configmaps-f6a706d6-7187-452b-872e-4fd91aee7e29" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-ded1d7cf-2389-4d7d-8af2-179507ec4134 03/15/23 22:52:35.097
STEP: Updating configmap cm-test-opt-upd-b3767301-2556-4def-817e-9f9c2c7b53b5 03/15/23 22:52:35.103
STEP: Creating configMap with name cm-test-opt-create-43709889-49d0-4e7c-b252-61a19792fb3b 03/15/23 22:52:35.108
STEP: waiting to observe update in volume 03/15/23 22:52:35.114
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:54:05.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5916" for this suite. 03/15/23 22:54:05.591
------------------------------
• [SLOW TEST] [92.614 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:52:32.985
    Mar 15 22:52:32.985: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:52:32.99
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:52:33.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:52:33.011
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-ded1d7cf-2389-4d7d-8af2-179507ec4134 03/15/23 22:52:33.022
    STEP: Creating configMap with name cm-test-opt-upd-b3767301-2556-4def-817e-9f9c2c7b53b5 03/15/23 22:52:33.032
    STEP: Creating the pod 03/15/23 22:52:33.037
    Mar 15 22:52:33.050: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f6a706d6-7187-452b-872e-4fd91aee7e29" in namespace "projected-5916" to be "running and ready"
    Mar 15 22:52:33.058: INFO: Pod "pod-projected-configmaps-f6a706d6-7187-452b-872e-4fd91aee7e29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025165ms
    Mar 15 22:52:33.058: INFO: The phase of Pod pod-projected-configmaps-f6a706d6-7187-452b-872e-4fd91aee7e29 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:52:35.062: INFO: Pod "pod-projected-configmaps-f6a706d6-7187-452b-872e-4fd91aee7e29": Phase="Running", Reason="", readiness=true. Elapsed: 2.012672957s
    Mar 15 22:52:35.063: INFO: The phase of Pod pod-projected-configmaps-f6a706d6-7187-452b-872e-4fd91aee7e29 is Running (Ready = true)
    Mar 15 22:52:35.063: INFO: Pod "pod-projected-configmaps-f6a706d6-7187-452b-872e-4fd91aee7e29" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-ded1d7cf-2389-4d7d-8af2-179507ec4134 03/15/23 22:52:35.097
    STEP: Updating configmap cm-test-opt-upd-b3767301-2556-4def-817e-9f9c2c7b53b5 03/15/23 22:52:35.103
    STEP: Creating configMap with name cm-test-opt-create-43709889-49d0-4e7c-b252-61a19792fb3b 03/15/23 22:52:35.108
    STEP: waiting to observe update in volume 03/15/23 22:52:35.114
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:54:05.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5916" for this suite. 03/15/23 22:54:05.591
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:54:05.6
Mar 15 22:54:05.601: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename gc 03/15/23 22:54:05.602
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:05.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:05.637
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 03/15/23 22:54:05.647
STEP: delete the rc 03/15/23 22:54:10.674
STEP: wait for all pods to be garbage collected 03/15/23 22:54:10.68
STEP: Gathering metrics 03/15/23 22:54:15.686
Mar 15 22:54:15.715: INFO: Waiting up to 5m0s for pod "kube-controller-manager-i-00864a1c8c75a434c" in namespace "kube-system" to be "running and ready"
Mar 15 22:54:15.718: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c": Phase="Running", Reason="", readiness=true. Elapsed: 3.587972ms
Mar 15 22:54:15.719: INFO: The phase of Pod kube-controller-manager-i-00864a1c8c75a434c is Running (Ready = true)
Mar 15 22:54:15.719: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c" satisfied condition "running and ready"
Mar 15 22:54:15.784: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 15 22:54:15.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5865" for this suite. 03/15/23 22:54:15.789
------------------------------
• [SLOW TEST] [10.193 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:54:05.6
    Mar 15 22:54:05.601: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename gc 03/15/23 22:54:05.602
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:05.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:05.637
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 03/15/23 22:54:05.647
    STEP: delete the rc 03/15/23 22:54:10.674
    STEP: wait for all pods to be garbage collected 03/15/23 22:54:10.68
    STEP: Gathering metrics 03/15/23 22:54:15.686
    Mar 15 22:54:15.715: INFO: Waiting up to 5m0s for pod "kube-controller-manager-i-00864a1c8c75a434c" in namespace "kube-system" to be "running and ready"
    Mar 15 22:54:15.718: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c": Phase="Running", Reason="", readiness=true. Elapsed: 3.587972ms
    Mar 15 22:54:15.719: INFO: The phase of Pod kube-controller-manager-i-00864a1c8c75a434c is Running (Ready = true)
    Mar 15 22:54:15.719: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c" satisfied condition "running and ready"
    Mar 15 22:54:15.784: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:54:15.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5865" for this suite. 03/15/23 22:54:15.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:54:15.796
Mar 15 22:54:15.796: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename deployment 03/15/23 22:54:15.798
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:15.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:15.812
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Mar 15 22:54:15.827: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 15 22:54:20.830: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/15/23 22:54:20.83
Mar 15 22:54:20.830: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/15/23 22:54:20.843
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 15 22:54:22.863: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1813  e544451e-e2e7-41a9-a26c-cd49ee3d34c4 28913 1 2023-03-15 22:54:20 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-15 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:54:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027e33d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-15 22:54:20 +0000 UTC,LastTransitionTime:2023-03-15 22:54:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-03-15 22:54:22 +0000 UTC,LastTransitionTime:2023-03-15 22:54:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 15 22:54:22.867: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-1813  77a8de29-ebf8-491f-addb-cc61c4e69d0c 28906 1 2023-03-15 22:54:20 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e544451e-e2e7-41a9-a26c-cd49ee3d34c4 0xc0027e37b7 0xc0027e37b8}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e544451e-e2e7-41a9-a26c-cd49ee3d34c4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:54:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027e3868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 15 22:54:22.873: INFO: Pod "test-cleanup-deployment-7698ff6f6b-gx7mk" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-gx7mk test-cleanup-deployment-7698ff6f6b- deployment-1813  07656ca1-839e-4e44-a64f-be1b3d11b6c8 28905 0 2023-03-15 22:54:20 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 77a8de29-ebf8-491f-addb-cc61c4e69d0c 0xc0027e3bf7 0xc0027e3bf8}] [] [{kube-controller-manager Update v1 2023-03-15 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77a8de29-ebf8-491f-addb-cc61c4e69d0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:54:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vrvzx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vrvzx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:54:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:54:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:54:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:54:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.119,StartTime:2023-03-15 22:54:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:54:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://6e962ff28feba3a88f5364b29368528b39e2d9103583f6e41d7fe7e31aef9d1c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 15 22:54:22.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1813" for this suite. 03/15/23 22:54:22.885
------------------------------
• [SLOW TEST] [7.134 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:54:15.796
    Mar 15 22:54:15.796: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename deployment 03/15/23 22:54:15.798
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:15.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:15.812
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Mar 15 22:54:15.827: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Mar 15 22:54:20.830: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/15/23 22:54:20.83
    Mar 15 22:54:20.830: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/15/23 22:54:20.843
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 15 22:54:22.863: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1813  e544451e-e2e7-41a9-a26c-cd49ee3d34c4 28913 1 2023-03-15 22:54:20 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-15 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:54:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027e33d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-15 22:54:20 +0000 UTC,LastTransitionTime:2023-03-15 22:54:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-03-15 22:54:22 +0000 UTC,LastTransitionTime:2023-03-15 22:54:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 15 22:54:22.867: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-1813  77a8de29-ebf8-491f-addb-cc61c4e69d0c 28906 1 2023-03-15 22:54:20 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e544451e-e2e7-41a9-a26c-cd49ee3d34c4 0xc0027e37b7 0xc0027e37b8}] [] [{kube-controller-manager Update apps/v1 2023-03-15 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e544451e-e2e7-41a9-a26c-cd49ee3d34c4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 22:54:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027e3868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 15 22:54:22.873: INFO: Pod "test-cleanup-deployment-7698ff6f6b-gx7mk" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-gx7mk test-cleanup-deployment-7698ff6f6b- deployment-1813  07656ca1-839e-4e44-a64f-be1b3d11b6c8 28905 0 2023-03-15 22:54:20 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 77a8de29-ebf8-491f-addb-cc61c4e69d0c 0xc0027e3bf7 0xc0027e3bf8}] [] [{kube-controller-manager Update v1 2023-03-15 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77a8de29-ebf8-491f-addb-cc61c4e69d0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 22:54:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vrvzx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vrvzx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:54:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:54:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:54:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 22:54:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.119,StartTime:2023-03-15 22:54:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 22:54:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://6e962ff28feba3a88f5364b29368528b39e2d9103583f6e41d7fe7e31aef9d1c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:54:22.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1813" for this suite. 03/15/23 22:54:22.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:54:22.933
Mar 15 22:54:22.933: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename job 03/15/23 22:54:22.934
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:23.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:23.104
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 03/15/23 22:54:23.126
STEP: Ensuring active pods == parallelism 03/15/23 22:54:23.166
STEP: Orphaning one of the Job's Pods 03/15/23 22:54:27.171
Mar 15 22:54:27.686: INFO: Successfully updated pod "adopt-release-jgzcp"
STEP: Checking that the Job readopts the Pod 03/15/23 22:54:27.686
Mar 15 22:54:27.686: INFO: Waiting up to 15m0s for pod "adopt-release-jgzcp" in namespace "job-2235" to be "adopted"
Mar 15 22:54:27.695: INFO: Pod "adopt-release-jgzcp": Phase="Running", Reason="", readiness=true. Elapsed: 8.783429ms
Mar 15 22:54:29.699: INFO: Pod "adopt-release-jgzcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.012054554s
Mar 15 22:54:29.699: INFO: Pod "adopt-release-jgzcp" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 03/15/23 22:54:29.699
Mar 15 22:54:30.217: INFO: Successfully updated pod "adopt-release-jgzcp"
STEP: Checking that the Job releases the Pod 03/15/23 22:54:30.217
Mar 15 22:54:30.217: INFO: Waiting up to 15m0s for pod "adopt-release-jgzcp" in namespace "job-2235" to be "released"
Mar 15 22:54:30.230: INFO: Pod "adopt-release-jgzcp": Phase="Running", Reason="", readiness=true. Elapsed: 12.454895ms
Mar 15 22:54:32.233: INFO: Pod "adopt-release-jgzcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.016180602s
Mar 15 22:54:32.233: INFO: Pod "adopt-release-jgzcp" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 15 22:54:32.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2235" for this suite. 03/15/23 22:54:32.238
------------------------------
• [SLOW TEST] [9.312 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:54:22.933
    Mar 15 22:54:22.933: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename job 03/15/23 22:54:22.934
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:23.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:23.104
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 03/15/23 22:54:23.126
    STEP: Ensuring active pods == parallelism 03/15/23 22:54:23.166
    STEP: Orphaning one of the Job's Pods 03/15/23 22:54:27.171
    Mar 15 22:54:27.686: INFO: Successfully updated pod "adopt-release-jgzcp"
    STEP: Checking that the Job readopts the Pod 03/15/23 22:54:27.686
    Mar 15 22:54:27.686: INFO: Waiting up to 15m0s for pod "adopt-release-jgzcp" in namespace "job-2235" to be "adopted"
    Mar 15 22:54:27.695: INFO: Pod "adopt-release-jgzcp": Phase="Running", Reason="", readiness=true. Elapsed: 8.783429ms
    Mar 15 22:54:29.699: INFO: Pod "adopt-release-jgzcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.012054554s
    Mar 15 22:54:29.699: INFO: Pod "adopt-release-jgzcp" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 03/15/23 22:54:29.699
    Mar 15 22:54:30.217: INFO: Successfully updated pod "adopt-release-jgzcp"
    STEP: Checking that the Job releases the Pod 03/15/23 22:54:30.217
    Mar 15 22:54:30.217: INFO: Waiting up to 15m0s for pod "adopt-release-jgzcp" in namespace "job-2235" to be "released"
    Mar 15 22:54:30.230: INFO: Pod "adopt-release-jgzcp": Phase="Running", Reason="", readiness=true. Elapsed: 12.454895ms
    Mar 15 22:54:32.233: INFO: Pod "adopt-release-jgzcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.016180602s
    Mar 15 22:54:32.233: INFO: Pod "adopt-release-jgzcp" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:54:32.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2235" for this suite. 03/15/23 22:54:32.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:54:32.253
Mar 15 22:54:32.253: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 22:54:32.254
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:32.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:32.274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 03/15/23 22:54:32.277
Mar 15 22:54:32.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4229 cluster-info'
Mar 15 22:54:32.379: INFO: stderr: ""
Mar 15 22:54:32.379: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 22:54:32.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4229" for this suite. 03/15/23 22:54:32.383
------------------------------
• [0.139 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:54:32.253
    Mar 15 22:54:32.253: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 22:54:32.254
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:32.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:32.274
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 03/15/23 22:54:32.277
    Mar 15 22:54:32.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-4229 cluster-info'
    Mar 15 22:54:32.379: INFO: stderr: ""
    Mar 15 22:54:32.379: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:54:32.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4229" for this suite. 03/15/23 22:54:32.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:54:32.397
Mar 15 22:54:32.397: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 22:54:32.4
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:32.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:32.42
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 22:54:32.436
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:54:32.888
STEP: Deploying the webhook pod 03/15/23 22:54:32.896
STEP: Wait for the deployment to be ready 03/15/23 22:54:32.91
Mar 15 22:54:32.922: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 22:54:34.939
STEP: Verifying the service has paired with the endpoint 03/15/23 22:54:34.979
Mar 15 22:54:35.981: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 03/15/23 22:54:36.051
STEP: Creating a configMap that does not comply to the validation webhook rules 03/15/23 22:54:36.101
STEP: Deleting the collection of validation webhooks 03/15/23 22:54:36.152
STEP: Creating a configMap that does not comply to the validation webhook rules 03/15/23 22:54:36.201
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:54:36.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1367" for this suite. 03/15/23 22:54:36.316
STEP: Destroying namespace "webhook-1367-markers" for this suite. 03/15/23 22:54:36.337
------------------------------
• [3.957 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:54:32.397
    Mar 15 22:54:32.397: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 22:54:32.4
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:32.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:32.42
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 22:54:32.436
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:54:32.888
    STEP: Deploying the webhook pod 03/15/23 22:54:32.896
    STEP: Wait for the deployment to be ready 03/15/23 22:54:32.91
    Mar 15 22:54:32.922: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 22:54:34.939
    STEP: Verifying the service has paired with the endpoint 03/15/23 22:54:34.979
    Mar 15 22:54:35.981: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 03/15/23 22:54:36.051
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/15/23 22:54:36.101
    STEP: Deleting the collection of validation webhooks 03/15/23 22:54:36.152
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/15/23 22:54:36.201
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:54:36.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1367" for this suite. 03/15/23 22:54:36.316
    STEP: Destroying namespace "webhook-1367-markers" for this suite. 03/15/23 22:54:36.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:54:36.357
Mar 15 22:54:36.358: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename daemonsets 03/15/23 22:54:36.359
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:36.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:36.375
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 03/15/23 22:54:36.402
STEP: Check that daemon pods launch on every node of the cluster. 03/15/23 22:54:36.408
Mar 15 22:54:36.414: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:54:36.428: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:54:36.429: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:54:37.439: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:54:37.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 15 22:54:37.444: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
Mar 15 22:54:38.433: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 22:54:38.436: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 15 22:54:38.436: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 03/15/23 22:54:38.439
Mar 15 22:54:38.443: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 03/15/23 22:54:38.443
Mar 15 22:54:38.452: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 03/15/23 22:54:38.452
Mar 15 22:54:38.454: INFO: Observed &DaemonSet event: ADDED
Mar 15 22:54:38.455: INFO: Observed &DaemonSet event: MODIFIED
Mar 15 22:54:38.455: INFO: Observed &DaemonSet event: MODIFIED
Mar 15 22:54:38.455: INFO: Observed &DaemonSet event: MODIFIED
Mar 15 22:54:38.455: INFO: Observed &DaemonSet event: MODIFIED
Mar 15 22:54:38.455: INFO: Found daemon set daemon-set in namespace daemonsets-5692 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 15 22:54:38.455: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 03/15/23 22:54:38.455
STEP: watching for the daemon set status to be patched 03/15/23 22:54:38.462
Mar 15 22:54:38.465: INFO: Observed &DaemonSet event: ADDED
Mar 15 22:54:38.465: INFO: Observed &DaemonSet event: MODIFIED
Mar 15 22:54:38.465: INFO: Observed &DaemonSet event: MODIFIED
Mar 15 22:54:38.466: INFO: Observed &DaemonSet event: MODIFIED
Mar 15 22:54:38.466: INFO: Observed &DaemonSet event: MODIFIED
Mar 15 22:54:38.466: INFO: Observed daemon set daemon-set in namespace daemonsets-5692 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 15 22:54:38.466: INFO: Observed &DaemonSet event: MODIFIED
Mar 15 22:54:38.466: INFO: Found daemon set daemon-set in namespace daemonsets-5692 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar 15 22:54:38.466: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/15/23 22:54:38.47
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5692, will wait for the garbage collector to delete the pods 03/15/23 22:54:38.47
Mar 15 22:54:38.528: INFO: Deleting DaemonSet.extensions daemon-set took: 4.563275ms
Mar 15 22:54:38.629: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.756592ms
Mar 15 22:54:41.435: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 15 22:54:41.435: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 15 22:54:41.437: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29156"},"items":null}

Mar 15 22:54:41.441: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29157"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:54:41.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5692" for this suite. 03/15/23 22:54:41.461
------------------------------
• [SLOW TEST] [5.109 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:54:36.357
    Mar 15 22:54:36.358: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename daemonsets 03/15/23 22:54:36.359
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:36.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:36.375
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 03/15/23 22:54:36.402
    STEP: Check that daemon pods launch on every node of the cluster. 03/15/23 22:54:36.408
    Mar 15 22:54:36.414: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:54:36.428: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:54:36.429: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:54:37.439: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:54:37.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 15 22:54:37.444: INFO: Node i-077ee0eb7ec5a02aa is running 0 daemon pod, expected 1
    Mar 15 22:54:38.433: INFO: DaemonSet pods can't tolerate node i-00864a1c8c75a434c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 15 22:54:38.436: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 15 22:54:38.436: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 03/15/23 22:54:38.439
    Mar 15 22:54:38.443: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 03/15/23 22:54:38.443
    Mar 15 22:54:38.452: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 03/15/23 22:54:38.452
    Mar 15 22:54:38.454: INFO: Observed &DaemonSet event: ADDED
    Mar 15 22:54:38.455: INFO: Observed &DaemonSet event: MODIFIED
    Mar 15 22:54:38.455: INFO: Observed &DaemonSet event: MODIFIED
    Mar 15 22:54:38.455: INFO: Observed &DaemonSet event: MODIFIED
    Mar 15 22:54:38.455: INFO: Observed &DaemonSet event: MODIFIED
    Mar 15 22:54:38.455: INFO: Found daemon set daemon-set in namespace daemonsets-5692 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 15 22:54:38.455: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 03/15/23 22:54:38.455
    STEP: watching for the daemon set status to be patched 03/15/23 22:54:38.462
    Mar 15 22:54:38.465: INFO: Observed &DaemonSet event: ADDED
    Mar 15 22:54:38.465: INFO: Observed &DaemonSet event: MODIFIED
    Mar 15 22:54:38.465: INFO: Observed &DaemonSet event: MODIFIED
    Mar 15 22:54:38.466: INFO: Observed &DaemonSet event: MODIFIED
    Mar 15 22:54:38.466: INFO: Observed &DaemonSet event: MODIFIED
    Mar 15 22:54:38.466: INFO: Observed daemon set daemon-set in namespace daemonsets-5692 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 15 22:54:38.466: INFO: Observed &DaemonSet event: MODIFIED
    Mar 15 22:54:38.466: INFO: Found daemon set daemon-set in namespace daemonsets-5692 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Mar 15 22:54:38.466: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/15/23 22:54:38.47
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5692, will wait for the garbage collector to delete the pods 03/15/23 22:54:38.47
    Mar 15 22:54:38.528: INFO: Deleting DaemonSet.extensions daemon-set took: 4.563275ms
    Mar 15 22:54:38.629: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.756592ms
    Mar 15 22:54:41.435: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 15 22:54:41.435: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 15 22:54:41.437: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29156"},"items":null}

    Mar 15 22:54:41.441: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29157"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:54:41.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5692" for this suite. 03/15/23 22:54:41.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:54:41.468
Mar 15 22:54:41.468: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:54:41.469
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:41.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:41.488
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-f51b90c9-474f-47fa-a73a-769e74a84295 03/15/23 22:54:41.494
STEP: Creating a pod to test consume configMaps 03/15/23 22:54:41.5
Mar 15 22:54:41.513: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591" in namespace "configmap-9674" to be "Succeeded or Failed"
Mar 15 22:54:41.518: INFO: Pod "pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591": Phase="Pending", Reason="", readiness=false. Elapsed: 4.453373ms
Mar 15 22:54:43.522: INFO: Pod "pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008230987s
Mar 15 22:54:45.522: INFO: Pod "pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008585907s
STEP: Saw pod success 03/15/23 22:54:45.522
Mar 15 22:54:45.522: INFO: Pod "pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591" satisfied condition "Succeeded or Failed"
Mar 15 22:54:45.525: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591 container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:54:45.534
Mar 15 22:54:45.560: INFO: Waiting for pod pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591 to disappear
Mar 15 22:54:45.564: INFO: Pod pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:54:45.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9674" for this suite. 03/15/23 22:54:45.574
------------------------------
• [4.113 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:54:41.468
    Mar 15 22:54:41.468: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:54:41.469
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:41.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:41.488
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-f51b90c9-474f-47fa-a73a-769e74a84295 03/15/23 22:54:41.494
    STEP: Creating a pod to test consume configMaps 03/15/23 22:54:41.5
    Mar 15 22:54:41.513: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591" in namespace "configmap-9674" to be "Succeeded or Failed"
    Mar 15 22:54:41.518: INFO: Pod "pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591": Phase="Pending", Reason="", readiness=false. Elapsed: 4.453373ms
    Mar 15 22:54:43.522: INFO: Pod "pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008230987s
    Mar 15 22:54:45.522: INFO: Pod "pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008585907s
    STEP: Saw pod success 03/15/23 22:54:45.522
    Mar 15 22:54:45.522: INFO: Pod "pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591" satisfied condition "Succeeded or Failed"
    Mar 15 22:54:45.525: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591 container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:54:45.534
    Mar 15 22:54:45.560: INFO: Waiting for pod pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591 to disappear
    Mar 15 22:54:45.564: INFO: Pod pod-configmaps-fb38d25f-c245-4164-93c1-37cd57bc3591 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:54:45.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9674" for this suite. 03/15/23 22:54:45.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:54:45.583
Mar 15 22:54:45.583: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename custom-resource-definition 03/15/23 22:54:45.584
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:45.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:45.608
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Mar 15 22:54:45.612: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:54:46.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9100" for this suite. 03/15/23 22:54:46.651
------------------------------
• [1.073 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:54:45.583
    Mar 15 22:54:45.583: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename custom-resource-definition 03/15/23 22:54:45.584
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:45.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:45.608
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Mar 15 22:54:45.612: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:54:46.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9100" for this suite. 03/15/23 22:54:46.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:54:46.659
Mar 15 22:54:46.659: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:54:46.661
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:46.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:46.674
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-4837/configmap-test-fe5cfc09-b15f-4380-9ab7-d09ff0d9695b 03/15/23 22:54:46.678
STEP: Creating a pod to test consume configMaps 03/15/23 22:54:46.683
Mar 15 22:54:46.689: INFO: Waiting up to 5m0s for pod "pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048" in namespace "configmap-4837" to be "Succeeded or Failed"
Mar 15 22:54:46.695: INFO: Pod "pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048": Phase="Pending", Reason="", readiness=false. Elapsed: 6.204538ms
Mar 15 22:54:48.699: INFO: Pod "pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010084621s
Mar 15 22:54:50.701: INFO: Pod "pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011829691s
STEP: Saw pod success 03/15/23 22:54:50.701
Mar 15 22:54:50.701: INFO: Pod "pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048" satisfied condition "Succeeded or Failed"
Mar 15 22:54:50.706: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048 container env-test: <nil>
STEP: delete the pod 03/15/23 22:54:50.714
Mar 15 22:54:50.743: INFO: Waiting for pod pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048 to disappear
Mar 15 22:54:50.749: INFO: Pod pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:54:50.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4837" for this suite. 03/15/23 22:54:50.756
------------------------------
• [4.104 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:54:46.659
    Mar 15 22:54:46.659: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:54:46.661
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:46.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:46.674
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-4837/configmap-test-fe5cfc09-b15f-4380-9ab7-d09ff0d9695b 03/15/23 22:54:46.678
    STEP: Creating a pod to test consume configMaps 03/15/23 22:54:46.683
    Mar 15 22:54:46.689: INFO: Waiting up to 5m0s for pod "pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048" in namespace "configmap-4837" to be "Succeeded or Failed"
    Mar 15 22:54:46.695: INFO: Pod "pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048": Phase="Pending", Reason="", readiness=false. Elapsed: 6.204538ms
    Mar 15 22:54:48.699: INFO: Pod "pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010084621s
    Mar 15 22:54:50.701: INFO: Pod "pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011829691s
    STEP: Saw pod success 03/15/23 22:54:50.701
    Mar 15 22:54:50.701: INFO: Pod "pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048" satisfied condition "Succeeded or Failed"
    Mar 15 22:54:50.706: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048 container env-test: <nil>
    STEP: delete the pod 03/15/23 22:54:50.714
    Mar 15 22:54:50.743: INFO: Waiting for pod pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048 to disappear
    Mar 15 22:54:50.749: INFO: Pod pod-configmaps-efaf16b5-f725-4602-a80d-9fe00c767048 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:54:50.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4837" for this suite. 03/15/23 22:54:50.756
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:54:50.764
Mar 15 22:54:50.764: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:54:50.765
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:50.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:50.821
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-cff87af9-79a5-41b7-b45d-905780500efb 03/15/23 22:54:50.845
STEP: Creating configMap with name cm-test-opt-upd-7da6fe5f-fea9-49e5-90d3-a03cbf81bd19 03/15/23 22:54:50.857
STEP: Creating the pod 03/15/23 22:54:50.867
Mar 15 22:54:50.887: INFO: Waiting up to 5m0s for pod "pod-configmaps-49464df1-a1c9-4b66-84f5-1f24107d1924" in namespace "configmap-9784" to be "running and ready"
Mar 15 22:54:50.899: INFO: Pod "pod-configmaps-49464df1-a1c9-4b66-84f5-1f24107d1924": Phase="Pending", Reason="", readiness=false. Elapsed: 12.571295ms
Mar 15 22:54:50.899: INFO: The phase of Pod pod-configmaps-49464df1-a1c9-4b66-84f5-1f24107d1924 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:54:52.903: INFO: Pod "pod-configmaps-49464df1-a1c9-4b66-84f5-1f24107d1924": Phase="Running", Reason="", readiness=true. Elapsed: 2.01677669s
Mar 15 22:54:52.903: INFO: The phase of Pod pod-configmaps-49464df1-a1c9-4b66-84f5-1f24107d1924 is Running (Ready = true)
Mar 15 22:54:52.903: INFO: Pod "pod-configmaps-49464df1-a1c9-4b66-84f5-1f24107d1924" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-cff87af9-79a5-41b7-b45d-905780500efb 03/15/23 22:54:52.922
STEP: Updating configmap cm-test-opt-upd-7da6fe5f-fea9-49e5-90d3-a03cbf81bd19 03/15/23 22:54:52.927
STEP: Creating configMap with name cm-test-opt-create-bfd93d42-48c6-4b4d-963b-1d7918ef5ac5 03/15/23 22:54:52.931
STEP: waiting to observe update in volume 03/15/23 22:54:52.936
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:54:54.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9784" for this suite. 03/15/23 22:54:54.964
------------------------------
• [4.205 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:54:50.764
    Mar 15 22:54:50.764: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:54:50.765
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:50.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:50.821
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-cff87af9-79a5-41b7-b45d-905780500efb 03/15/23 22:54:50.845
    STEP: Creating configMap with name cm-test-opt-upd-7da6fe5f-fea9-49e5-90d3-a03cbf81bd19 03/15/23 22:54:50.857
    STEP: Creating the pod 03/15/23 22:54:50.867
    Mar 15 22:54:50.887: INFO: Waiting up to 5m0s for pod "pod-configmaps-49464df1-a1c9-4b66-84f5-1f24107d1924" in namespace "configmap-9784" to be "running and ready"
    Mar 15 22:54:50.899: INFO: Pod "pod-configmaps-49464df1-a1c9-4b66-84f5-1f24107d1924": Phase="Pending", Reason="", readiness=false. Elapsed: 12.571295ms
    Mar 15 22:54:50.899: INFO: The phase of Pod pod-configmaps-49464df1-a1c9-4b66-84f5-1f24107d1924 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:54:52.903: INFO: Pod "pod-configmaps-49464df1-a1c9-4b66-84f5-1f24107d1924": Phase="Running", Reason="", readiness=true. Elapsed: 2.01677669s
    Mar 15 22:54:52.903: INFO: The phase of Pod pod-configmaps-49464df1-a1c9-4b66-84f5-1f24107d1924 is Running (Ready = true)
    Mar 15 22:54:52.903: INFO: Pod "pod-configmaps-49464df1-a1c9-4b66-84f5-1f24107d1924" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-cff87af9-79a5-41b7-b45d-905780500efb 03/15/23 22:54:52.922
    STEP: Updating configmap cm-test-opt-upd-7da6fe5f-fea9-49e5-90d3-a03cbf81bd19 03/15/23 22:54:52.927
    STEP: Creating configMap with name cm-test-opt-create-bfd93d42-48c6-4b4d-963b-1d7918ef5ac5 03/15/23 22:54:52.931
    STEP: waiting to observe update in volume 03/15/23 22:54:52.936
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:54:54.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9784" for this suite. 03/15/23 22:54:54.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:54:54.976
Mar 15 22:54:54.976: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:54:54.977
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:54.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:54.997
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-992d1f8f-2048-4e74-946f-f29bff5d1dbb 03/15/23 22:54:55.011
STEP: Creating a pod to test consume configMaps 03/15/23 22:54:55.016
Mar 15 22:54:55.028: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a" in namespace "projected-2767" to be "Succeeded or Failed"
Mar 15 22:54:55.037: INFO: Pod "pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.384312ms
Mar 15 22:54:57.041: INFO: Pod "pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012502214s
Mar 15 22:54:59.040: INFO: Pod "pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011803006s
STEP: Saw pod success 03/15/23 22:54:59.04
Mar 15 22:54:59.040: INFO: Pod "pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a" satisfied condition "Succeeded or Failed"
Mar 15 22:54:59.043: INFO: Trying to get logs from node i-077ee0eb7ec5a02aa pod pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:54:59.057
Mar 15 22:54:59.068: INFO: Waiting for pod pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a to disappear
Mar 15 22:54:59.073: INFO: Pod pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:54:59.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2767" for this suite. 03/15/23 22:54:59.078
------------------------------
• [4.117 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:54:54.976
    Mar 15 22:54:54.976: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:54:54.977
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:54.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:54.997
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-992d1f8f-2048-4e74-946f-f29bff5d1dbb 03/15/23 22:54:55.011
    STEP: Creating a pod to test consume configMaps 03/15/23 22:54:55.016
    Mar 15 22:54:55.028: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a" in namespace "projected-2767" to be "Succeeded or Failed"
    Mar 15 22:54:55.037: INFO: Pod "pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.384312ms
    Mar 15 22:54:57.041: INFO: Pod "pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012502214s
    Mar 15 22:54:59.040: INFO: Pod "pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011803006s
    STEP: Saw pod success 03/15/23 22:54:59.04
    Mar 15 22:54:59.040: INFO: Pod "pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a" satisfied condition "Succeeded or Failed"
    Mar 15 22:54:59.043: INFO: Trying to get logs from node i-077ee0eb7ec5a02aa pod pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:54:59.057
    Mar 15 22:54:59.068: INFO: Waiting for pod pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a to disappear
    Mar 15 22:54:59.073: INFO: Pod pod-projected-configmaps-8651cc07-b2f4-4301-8162-accb8ba0be9a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:54:59.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2767" for this suite. 03/15/23 22:54:59.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:54:59.096
Mar 15 22:54:59.096: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-runtime 03/15/23 22:54:59.097
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:59.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:59.136
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 03/15/23 22:54:59.144
STEP: wait for the container to reach Succeeded 03/15/23 22:54:59.154
STEP: get the container status 03/15/23 22:55:03.173
STEP: the container should be terminated 03/15/23 22:55:03.181
STEP: the termination message should be set 03/15/23 22:55:03.181
Mar 15 22:55:03.181: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/15/23 22:55:03.181
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 15 22:55:03.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5109" for this suite. 03/15/23 22:55:03.242
------------------------------
• [4.170 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:54:59.096
    Mar 15 22:54:59.096: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-runtime 03/15/23 22:54:59.097
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:54:59.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:54:59.136
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 03/15/23 22:54:59.144
    STEP: wait for the container to reach Succeeded 03/15/23 22:54:59.154
    STEP: get the container status 03/15/23 22:55:03.173
    STEP: the container should be terminated 03/15/23 22:55:03.181
    STEP: the termination message should be set 03/15/23 22:55:03.181
    Mar 15 22:55:03.181: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/15/23 22:55:03.181
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:55:03.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5109" for this suite. 03/15/23 22:55:03.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:55:03.274
Mar 15 22:55:03.274: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename namespaces 03/15/23 22:55:03.275
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:03.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:03.3
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-qzr7z" 03/15/23 22:55:03.304
Mar 15 22:55:03.329: INFO: Namespace "e2e-ns-qzr7z-7608" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-qzr7z-7608" 03/15/23 22:55:03.329
Mar 15 22:55:03.341: INFO: Namespace "e2e-ns-qzr7z-7608" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-qzr7z-7608" 03/15/23 22:55:03.341
Mar 15 22:55:03.349: INFO: Namespace "e2e-ns-qzr7z-7608" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:55:03.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9764" for this suite. 03/15/23 22:55:03.354
STEP: Destroying namespace "e2e-ns-qzr7z-7608" for this suite. 03/15/23 22:55:03.361
------------------------------
• [0.096 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:55:03.274
    Mar 15 22:55:03.274: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename namespaces 03/15/23 22:55:03.275
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:03.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:03.3
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-qzr7z" 03/15/23 22:55:03.304
    Mar 15 22:55:03.329: INFO: Namespace "e2e-ns-qzr7z-7608" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-qzr7z-7608" 03/15/23 22:55:03.329
    Mar 15 22:55:03.341: INFO: Namespace "e2e-ns-qzr7z-7608" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-qzr7z-7608" 03/15/23 22:55:03.341
    Mar 15 22:55:03.349: INFO: Namespace "e2e-ns-qzr7z-7608" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:55:03.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9764" for this suite. 03/15/23 22:55:03.354
    STEP: Destroying namespace "e2e-ns-qzr7z-7608" for this suite. 03/15/23 22:55:03.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:55:03.374
Mar 15 22:55:03.375: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename replication-controller 03/15/23 22:55:03.376
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:03.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:03.396
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503 03/15/23 22:55:03.406
Mar 15 22:55:03.435: INFO: Pod name my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503: Found 0 pods out of 1
Mar 15 22:55:08.442: INFO: Pod name my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503: Found 1 pods out of 1
Mar 15 22:55:08.442: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503" are running
Mar 15 22:55:08.442: INFO: Waiting up to 5m0s for pod "my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503-6vh9z" in namespace "replication-controller-2892" to be "running"
Mar 15 22:55:08.446: INFO: Pod "my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503-6vh9z": Phase="Running", Reason="", readiness=true. Elapsed: 4.652867ms
Mar 15 22:55:08.446: INFO: Pod "my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503-6vh9z" satisfied condition "running"
Mar 15 22:55:08.446: INFO: Pod "my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503-6vh9z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:55:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:55:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:55:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:55:03 +0000 UTC Reason: Message:}])
Mar 15 22:55:08.446: INFO: Trying to dial the pod
Mar 15 22:55:13.461: INFO: Controller my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503: Got expected result from replica 1 [my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503-6vh9z]: "my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503-6vh9z", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 15 22:55:13.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2892" for this suite. 03/15/23 22:55:13.467
------------------------------
• [SLOW TEST] [10.100 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:55:03.374
    Mar 15 22:55:03.375: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename replication-controller 03/15/23 22:55:03.376
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:03.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:03.396
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503 03/15/23 22:55:03.406
    Mar 15 22:55:03.435: INFO: Pod name my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503: Found 0 pods out of 1
    Mar 15 22:55:08.442: INFO: Pod name my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503: Found 1 pods out of 1
    Mar 15 22:55:08.442: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503" are running
    Mar 15 22:55:08.442: INFO: Waiting up to 5m0s for pod "my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503-6vh9z" in namespace "replication-controller-2892" to be "running"
    Mar 15 22:55:08.446: INFO: Pod "my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503-6vh9z": Phase="Running", Reason="", readiness=true. Elapsed: 4.652867ms
    Mar 15 22:55:08.446: INFO: Pod "my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503-6vh9z" satisfied condition "running"
    Mar 15 22:55:08.446: INFO: Pod "my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503-6vh9z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:55:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:55:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:55:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:55:03 +0000 UTC Reason: Message:}])
    Mar 15 22:55:08.446: INFO: Trying to dial the pod
    Mar 15 22:55:13.461: INFO: Controller my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503: Got expected result from replica 1 [my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503-6vh9z]: "my-hostname-basic-483988cb-3e57-444a-a3f4-d870b9522503-6vh9z", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:55:13.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2892" for this suite. 03/15/23 22:55:13.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:55:13.476
Mar 15 22:55:13.476: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 22:55:13.478
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:13.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:13.495
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 03/15/23 22:55:13.498
Mar 15 22:55:13.508: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8" in namespace "downward-api-7221" to be "Succeeded or Failed"
Mar 15 22:55:13.512: INFO: Pod "downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.451684ms
Mar 15 22:55:15.517: INFO: Pod "downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008793488s
Mar 15 22:55:17.516: INFO: Pod "downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007623309s
STEP: Saw pod success 03/15/23 22:55:17.516
Mar 15 22:55:17.516: INFO: Pod "downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8" satisfied condition "Succeeded or Failed"
Mar 15 22:55:17.520: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8 container client-container: <nil>
STEP: delete the pod 03/15/23 22:55:17.533
Mar 15 22:55:17.551: INFO: Waiting for pod downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8 to disappear
Mar 15 22:55:17.557: INFO: Pod downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 15 22:55:17.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7221" for this suite. 03/15/23 22:55:17.561
------------------------------
• [4.094 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:55:13.476
    Mar 15 22:55:13.476: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 22:55:13.478
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:13.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:13.495
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 03/15/23 22:55:13.498
    Mar 15 22:55:13.508: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8" in namespace "downward-api-7221" to be "Succeeded or Failed"
    Mar 15 22:55:13.512: INFO: Pod "downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.451684ms
    Mar 15 22:55:15.517: INFO: Pod "downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008793488s
    Mar 15 22:55:17.516: INFO: Pod "downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007623309s
    STEP: Saw pod success 03/15/23 22:55:17.516
    Mar 15 22:55:17.516: INFO: Pod "downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8" satisfied condition "Succeeded or Failed"
    Mar 15 22:55:17.520: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8 container client-container: <nil>
    STEP: delete the pod 03/15/23 22:55:17.533
    Mar 15 22:55:17.551: INFO: Waiting for pod downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8 to disappear
    Mar 15 22:55:17.557: INFO: Pod downwardapi-volume-f62f8d78-0ac0-4ceb-bbc4-59860d76bfc8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:55:17.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7221" for this suite. 03/15/23 22:55:17.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:55:17.571
Mar 15 22:55:17.571: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename replicaset 03/15/23 22:55:17.573
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:17.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:17.591
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Mar 15 22:55:17.611: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 15 22:55:22.615: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/15/23 22:55:22.615
STEP: Scaling up "test-rs" replicaset  03/15/23 22:55:22.615
Mar 15 22:55:22.624: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 03/15/23 22:55:22.624
W0315 22:55:22.633163      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 15 22:55:22.637: INFO: observed ReplicaSet test-rs in namespace replicaset-8720 with ReadyReplicas 1, AvailableReplicas 1
Mar 15 22:55:22.685: INFO: observed ReplicaSet test-rs in namespace replicaset-8720 with ReadyReplicas 1, AvailableReplicas 1
Mar 15 22:55:22.741: INFO: observed ReplicaSet test-rs in namespace replicaset-8720 with ReadyReplicas 1, AvailableReplicas 1
Mar 15 22:55:22.758: INFO: observed ReplicaSet test-rs in namespace replicaset-8720 with ReadyReplicas 1, AvailableReplicas 1
Mar 15 22:55:24.489: INFO: observed ReplicaSet test-rs in namespace replicaset-8720 with ReadyReplicas 2, AvailableReplicas 2
Mar 15 22:55:25.280: INFO: observed Replicaset test-rs in namespace replicaset-8720 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:55:25.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8720" for this suite. 03/15/23 22:55:25.293
------------------------------
• [SLOW TEST] [7.735 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:55:17.571
    Mar 15 22:55:17.571: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename replicaset 03/15/23 22:55:17.573
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:17.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:17.591
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Mar 15 22:55:17.611: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 15 22:55:22.615: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/15/23 22:55:22.615
    STEP: Scaling up "test-rs" replicaset  03/15/23 22:55:22.615
    Mar 15 22:55:22.624: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 03/15/23 22:55:22.624
    W0315 22:55:22.633163      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 15 22:55:22.637: INFO: observed ReplicaSet test-rs in namespace replicaset-8720 with ReadyReplicas 1, AvailableReplicas 1
    Mar 15 22:55:22.685: INFO: observed ReplicaSet test-rs in namespace replicaset-8720 with ReadyReplicas 1, AvailableReplicas 1
    Mar 15 22:55:22.741: INFO: observed ReplicaSet test-rs in namespace replicaset-8720 with ReadyReplicas 1, AvailableReplicas 1
    Mar 15 22:55:22.758: INFO: observed ReplicaSet test-rs in namespace replicaset-8720 with ReadyReplicas 1, AvailableReplicas 1
    Mar 15 22:55:24.489: INFO: observed ReplicaSet test-rs in namespace replicaset-8720 with ReadyReplicas 2, AvailableReplicas 2
    Mar 15 22:55:25.280: INFO: observed Replicaset test-rs in namespace replicaset-8720 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:55:25.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8720" for this suite. 03/15/23 22:55:25.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:55:25.318
Mar 15 22:55:25.318: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:55:25.319
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:25.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:25.343
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-c749f31a-3581-4ae4-8f6e-55d1d53162d7 03/15/23 22:55:25.346
STEP: Creating a pod to test consume secrets 03/15/23 22:55:25.351
Mar 15 22:55:25.360: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa" in namespace "projected-854" to be "Succeeded or Failed"
Mar 15 22:55:25.370: INFO: Pod "pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.670244ms
Mar 15 22:55:27.374: INFO: Pod "pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014722872s
Mar 15 22:55:29.373: INFO: Pod "pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013870859s
STEP: Saw pod success 03/15/23 22:55:29.373
Mar 15 22:55:29.374: INFO: Pod "pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa" satisfied condition "Succeeded or Failed"
Mar 15 22:55:29.376: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa container secret-volume-test: <nil>
STEP: delete the pod 03/15/23 22:55:29.396
Mar 15 22:55:29.409: INFO: Waiting for pod pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa to disappear
Mar 15 22:55:29.415: INFO: Pod pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 15 22:55:29.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-854" for this suite. 03/15/23 22:55:29.419
------------------------------
• [4.107 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:55:25.318
    Mar 15 22:55:25.318: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:55:25.319
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:25.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:25.343
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-c749f31a-3581-4ae4-8f6e-55d1d53162d7 03/15/23 22:55:25.346
    STEP: Creating a pod to test consume secrets 03/15/23 22:55:25.351
    Mar 15 22:55:25.360: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa" in namespace "projected-854" to be "Succeeded or Failed"
    Mar 15 22:55:25.370: INFO: Pod "pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.670244ms
    Mar 15 22:55:27.374: INFO: Pod "pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014722872s
    Mar 15 22:55:29.373: INFO: Pod "pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013870859s
    STEP: Saw pod success 03/15/23 22:55:29.373
    Mar 15 22:55:29.374: INFO: Pod "pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa" satisfied condition "Succeeded or Failed"
    Mar 15 22:55:29.376: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa container secret-volume-test: <nil>
    STEP: delete the pod 03/15/23 22:55:29.396
    Mar 15 22:55:29.409: INFO: Waiting for pod pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa to disappear
    Mar 15 22:55:29.415: INFO: Pod pod-projected-secrets-52e31ad6-a763-45d6-9032-45c761ab95fa no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:55:29.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-854" for this suite. 03/15/23 22:55:29.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:55:29.425
Mar 15 22:55:29.425: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 22:55:29.427
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:29.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:29.445
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1773 03/15/23 22:55:29.449
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/15/23 22:55:29.459
STEP: creating service externalsvc in namespace services-1773 03/15/23 22:55:29.46
STEP: creating replication controller externalsvc in namespace services-1773 03/15/23 22:55:29.479
I0315 22:55:29.490956      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1773, replica count: 2
I0315 22:55:32.542669      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 03/15/23 22:55:32.545
Mar 15 22:55:32.567: INFO: Creating new exec pod
Mar 15 22:55:32.586: INFO: Waiting up to 5m0s for pod "execpod9g4tv" in namespace "services-1773" to be "running"
Mar 15 22:55:32.591: INFO: Pod "execpod9g4tv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.948654ms
Mar 15 22:55:34.595: INFO: Pod "execpod9g4tv": Phase="Running", Reason="", readiness=true. Elapsed: 2.008912409s
Mar 15 22:55:34.595: INFO: Pod "execpod9g4tv" satisfied condition "running"
Mar 15 22:55:34.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-1773 exec execpod9g4tv -- /bin/sh -x -c nslookup clusterip-service.services-1773.svc.cluster.local'
Mar 15 22:55:34.795: INFO: stderr: "+ nslookup clusterip-service.services-1773.svc.cluster.local\n"
Mar 15 22:55:34.795: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nclusterip-service.services-1773.svc.cluster.local\tcanonical name = externalsvc.services-1773.svc.cluster.local.\nName:\texternalsvc.services-1773.svc.cluster.local\nAddress: 100.64.221.216\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1773, will wait for the garbage collector to delete the pods 03/15/23 22:55:34.795
Mar 15 22:55:34.854: INFO: Deleting ReplicationController externalsvc took: 5.506648ms
Mar 15 22:55:34.955: INFO: Terminating ReplicationController externalsvc pods took: 101.272725ms
Mar 15 22:55:36.775: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 22:55:36.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1773" for this suite. 03/15/23 22:55:36.804
------------------------------
• [SLOW TEST] [7.384 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:55:29.425
    Mar 15 22:55:29.425: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 22:55:29.427
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:29.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:29.445
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1773 03/15/23 22:55:29.449
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/15/23 22:55:29.459
    STEP: creating service externalsvc in namespace services-1773 03/15/23 22:55:29.46
    STEP: creating replication controller externalsvc in namespace services-1773 03/15/23 22:55:29.479
    I0315 22:55:29.490956      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1773, replica count: 2
    I0315 22:55:32.542669      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 03/15/23 22:55:32.545
    Mar 15 22:55:32.567: INFO: Creating new exec pod
    Mar 15 22:55:32.586: INFO: Waiting up to 5m0s for pod "execpod9g4tv" in namespace "services-1773" to be "running"
    Mar 15 22:55:32.591: INFO: Pod "execpod9g4tv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.948654ms
    Mar 15 22:55:34.595: INFO: Pod "execpod9g4tv": Phase="Running", Reason="", readiness=true. Elapsed: 2.008912409s
    Mar 15 22:55:34.595: INFO: Pod "execpod9g4tv" satisfied condition "running"
    Mar 15 22:55:34.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-1773 exec execpod9g4tv -- /bin/sh -x -c nslookup clusterip-service.services-1773.svc.cluster.local'
    Mar 15 22:55:34.795: INFO: stderr: "+ nslookup clusterip-service.services-1773.svc.cluster.local\n"
    Mar 15 22:55:34.795: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nclusterip-service.services-1773.svc.cluster.local\tcanonical name = externalsvc.services-1773.svc.cluster.local.\nName:\texternalsvc.services-1773.svc.cluster.local\nAddress: 100.64.221.216\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1773, will wait for the garbage collector to delete the pods 03/15/23 22:55:34.795
    Mar 15 22:55:34.854: INFO: Deleting ReplicationController externalsvc took: 5.506648ms
    Mar 15 22:55:34.955: INFO: Terminating ReplicationController externalsvc pods took: 101.272725ms
    Mar 15 22:55:36.775: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:55:36.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1773" for this suite. 03/15/23 22:55:36.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:55:36.813
Mar 15 22:55:36.813: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 22:55:36.813
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:36.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:36.83
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-f1b0a830-a776-488a-8274-805bab40ab63 03/15/23 22:55:36.832
STEP: Creating a pod to test consume configMaps 03/15/23 22:55:36.844
Mar 15 22:55:36.859: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc" in namespace "projected-1478" to be "Succeeded or Failed"
Mar 15 22:55:36.864: INFO: Pod "pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.041001ms
Mar 15 22:55:38.868: INFO: Pod "pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009380139s
Mar 15 22:55:40.867: INFO: Pod "pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008570657s
STEP: Saw pod success 03/15/23 22:55:40.867
Mar 15 22:55:40.868: INFO: Pod "pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc" satisfied condition "Succeeded or Failed"
Mar 15 22:55:40.870: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:55:40.875
Mar 15 22:55:40.889: INFO: Waiting for pod pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc to disappear
Mar 15 22:55:40.892: INFO: Pod pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:55:40.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1478" for this suite. 03/15/23 22:55:40.898
------------------------------
• [4.091 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:55:36.813
    Mar 15 22:55:36.813: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 22:55:36.813
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:36.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:36.83
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-f1b0a830-a776-488a-8274-805bab40ab63 03/15/23 22:55:36.832
    STEP: Creating a pod to test consume configMaps 03/15/23 22:55:36.844
    Mar 15 22:55:36.859: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc" in namespace "projected-1478" to be "Succeeded or Failed"
    Mar 15 22:55:36.864: INFO: Pod "pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.041001ms
    Mar 15 22:55:38.868: INFO: Pod "pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009380139s
    Mar 15 22:55:40.867: INFO: Pod "pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008570657s
    STEP: Saw pod success 03/15/23 22:55:40.867
    Mar 15 22:55:40.868: INFO: Pod "pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc" satisfied condition "Succeeded or Failed"
    Mar 15 22:55:40.870: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:55:40.875
    Mar 15 22:55:40.889: INFO: Waiting for pod pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc to disappear
    Mar 15 22:55:40.892: INFO: Pod pod-projected-configmaps-5d30afb3-5e3b-4dee-be28-94c2ad1557fc no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:55:40.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1478" for this suite. 03/15/23 22:55:40.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:55:40.908
Mar 15 22:55:40.908: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 22:55:40.909
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:40.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:40.926
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Mar 15 22:55:40.929: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/15/23 22:55:42.654
Mar 15 22:55:42.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-4229 --namespace=crd-publish-openapi-4229 create -f -'
Mar 15 22:55:43.463: INFO: stderr: ""
Mar 15 22:55:43.463: INFO: stdout: "e2e-test-crd-publish-openapi-9341-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 15 22:55:43.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-4229 --namespace=crd-publish-openapi-4229 delete e2e-test-crd-publish-openapi-9341-crds test-cr'
Mar 15 22:55:43.563: INFO: stderr: ""
Mar 15 22:55:43.563: INFO: stdout: "e2e-test-crd-publish-openapi-9341-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 15 22:55:43.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-4229 --namespace=crd-publish-openapi-4229 apply -f -'
Mar 15 22:55:43.798: INFO: stderr: ""
Mar 15 22:55:43.798: INFO: stdout: "e2e-test-crd-publish-openapi-9341-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 15 22:55:43.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-4229 --namespace=crd-publish-openapi-4229 delete e2e-test-crd-publish-openapi-9341-crds test-cr'
Mar 15 22:55:43.873: INFO: stderr: ""
Mar 15 22:55:43.873: INFO: stdout: "e2e-test-crd-publish-openapi-9341-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 03/15/23 22:55:43.873
Mar 15 22:55:43.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-4229 explain e2e-test-crd-publish-openapi-9341-crds'
Mar 15 22:55:44.094: INFO: stderr: ""
Mar 15 22:55:44.094: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9341-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:55:46.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4229" for this suite. 03/15/23 22:55:46.285
------------------------------
• [SLOW TEST] [5.383 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:55:40.908
    Mar 15 22:55:40.908: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 22:55:40.909
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:40.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:40.926
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Mar 15 22:55:40.929: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/15/23 22:55:42.654
    Mar 15 22:55:42.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-4229 --namespace=crd-publish-openapi-4229 create -f -'
    Mar 15 22:55:43.463: INFO: stderr: ""
    Mar 15 22:55:43.463: INFO: stdout: "e2e-test-crd-publish-openapi-9341-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 15 22:55:43.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-4229 --namespace=crd-publish-openapi-4229 delete e2e-test-crd-publish-openapi-9341-crds test-cr'
    Mar 15 22:55:43.563: INFO: stderr: ""
    Mar 15 22:55:43.563: INFO: stdout: "e2e-test-crd-publish-openapi-9341-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Mar 15 22:55:43.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-4229 --namespace=crd-publish-openapi-4229 apply -f -'
    Mar 15 22:55:43.798: INFO: stderr: ""
    Mar 15 22:55:43.798: INFO: stdout: "e2e-test-crd-publish-openapi-9341-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 15 22:55:43.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-4229 --namespace=crd-publish-openapi-4229 delete e2e-test-crd-publish-openapi-9341-crds test-cr'
    Mar 15 22:55:43.873: INFO: stderr: ""
    Mar 15 22:55:43.873: INFO: stdout: "e2e-test-crd-publish-openapi-9341-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 03/15/23 22:55:43.873
    Mar 15 22:55:43.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=crd-publish-openapi-4229 explain e2e-test-crd-publish-openapi-9341-crds'
    Mar 15 22:55:44.094: INFO: stderr: ""
    Mar 15 22:55:44.094: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9341-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:55:46.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4229" for this suite. 03/15/23 22:55:46.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:55:46.291
Mar 15 22:55:46.292: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename replication-controller 03/15/23 22:55:46.293
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:46.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:46.312
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 03/15/23 22:55:46.315
Mar 15 22:55:46.321: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8670" to be "running and ready"
Mar 15 22:55:46.328: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 6.948027ms
Mar 15 22:55:46.328: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar 15 22:55:48.333: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.011749636s
Mar 15 22:55:48.333: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Mar 15 22:55:48.333: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 03/15/23 22:55:48.336
STEP: Then the orphan pod is adopted 03/15/23 22:55:48.341
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 15 22:55:49.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8670" for this suite. 03/15/23 22:55:49.351
------------------------------
• [3.065 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:55:46.291
    Mar 15 22:55:46.292: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename replication-controller 03/15/23 22:55:46.293
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:46.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:46.312
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 03/15/23 22:55:46.315
    Mar 15 22:55:46.321: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8670" to be "running and ready"
    Mar 15 22:55:46.328: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 6.948027ms
    Mar 15 22:55:46.328: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 22:55:48.333: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.011749636s
    Mar 15 22:55:48.333: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Mar 15 22:55:48.333: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 03/15/23 22:55:48.336
    STEP: Then the orphan pod is adopted 03/15/23 22:55:48.341
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:55:49.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8670" for this suite. 03/15/23 22:55:49.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:55:49.362
Mar 15 22:55:49.362: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename gc 03/15/23 22:55:49.363
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:49.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:49.382
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 03/15/23 22:55:49.387
STEP: Wait for the Deployment to create new ReplicaSet 03/15/23 22:55:49.4
STEP: delete the deployment 03/15/23 22:55:49.943
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/15/23 22:55:49.956
STEP: Gathering metrics 03/15/23 22:55:50.477
Mar 15 22:55:50.517: INFO: Waiting up to 5m0s for pod "kube-controller-manager-i-00864a1c8c75a434c" in namespace "kube-system" to be "running and ready"
Mar 15 22:55:50.521: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c": Phase="Running", Reason="", readiness=true. Elapsed: 3.897638ms
Mar 15 22:55:50.521: INFO: The phase of Pod kube-controller-manager-i-00864a1c8c75a434c is Running (Ready = true)
Mar 15 22:55:50.521: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c" satisfied condition "running and ready"
Mar 15 22:55:50.624: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 15 22:55:50.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9506" for this suite. 03/15/23 22:55:50.628
------------------------------
• [1.273 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:55:49.362
    Mar 15 22:55:49.362: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename gc 03/15/23 22:55:49.363
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:49.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:49.382
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 03/15/23 22:55:49.387
    STEP: Wait for the Deployment to create new ReplicaSet 03/15/23 22:55:49.4
    STEP: delete the deployment 03/15/23 22:55:49.943
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/15/23 22:55:49.956
    STEP: Gathering metrics 03/15/23 22:55:50.477
    Mar 15 22:55:50.517: INFO: Waiting up to 5m0s for pod "kube-controller-manager-i-00864a1c8c75a434c" in namespace "kube-system" to be "running and ready"
    Mar 15 22:55:50.521: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c": Phase="Running", Reason="", readiness=true. Elapsed: 3.897638ms
    Mar 15 22:55:50.521: INFO: The phase of Pod kube-controller-manager-i-00864a1c8c75a434c is Running (Ready = true)
    Mar 15 22:55:50.521: INFO: Pod "kube-controller-manager-i-00864a1c8c75a434c" satisfied condition "running and ready"
    Mar 15 22:55:50.624: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:55:50.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9506" for this suite. 03/15/23 22:55:50.628
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:55:50.636
Mar 15 22:55:50.636: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename namespaces 03/15/23 22:55:50.637
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:50.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:50.661
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 03/15/23 22:55:50.666
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:50.687
STEP: Creating a service in the namespace 03/15/23 22:55:50.695
STEP: Deleting the namespace 03/15/23 22:55:50.758
STEP: Waiting for the namespace to be removed. 03/15/23 22:55:50.773
STEP: Recreating the namespace 03/15/23 22:55:56.776
STEP: Verifying there is no service in the namespace 03/15/23 22:55:56.789
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:55:56.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8199" for this suite. 03/15/23 22:55:56.797
STEP: Destroying namespace "nsdeletetest-6526" for this suite. 03/15/23 22:55:56.802
Mar 15 22:55:56.804: INFO: Namespace nsdeletetest-6526 was already deleted
STEP: Destroying namespace "nsdeletetest-2367" for this suite. 03/15/23 22:55:56.804
------------------------------
• [SLOW TEST] [6.174 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:55:50.636
    Mar 15 22:55:50.636: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename namespaces 03/15/23 22:55:50.637
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:50.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:50.661
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 03/15/23 22:55:50.666
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:50.687
    STEP: Creating a service in the namespace 03/15/23 22:55:50.695
    STEP: Deleting the namespace 03/15/23 22:55:50.758
    STEP: Waiting for the namespace to be removed. 03/15/23 22:55:50.773
    STEP: Recreating the namespace 03/15/23 22:55:56.776
    STEP: Verifying there is no service in the namespace 03/15/23 22:55:56.789
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:55:56.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8199" for this suite. 03/15/23 22:55:56.797
    STEP: Destroying namespace "nsdeletetest-6526" for this suite. 03/15/23 22:55:56.802
    Mar 15 22:55:56.804: INFO: Namespace nsdeletetest-6526 was already deleted
    STEP: Destroying namespace "nsdeletetest-2367" for this suite. 03/15/23 22:55:56.804
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:55:56.811
Mar 15 22:55:56.811: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename disruption 03/15/23 22:55:56.812
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:56.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:56.825
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 03/15/23 22:55:56.829
STEP: Waiting for the pdb to be processed 03/15/23 22:55:56.839
STEP: updating the pdb 03/15/23 22:55:58.847
STEP: Waiting for the pdb to be processed 03/15/23 22:55:58.855
STEP: patching the pdb 03/15/23 22:55:58.861
STEP: Waiting for the pdb to be processed 03/15/23 22:55:58.87
STEP: Waiting for the pdb to be deleted 03/15/23 22:56:00.882
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 15 22:56:00.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8671" for this suite. 03/15/23 22:56:00.888
------------------------------
• [4.083 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:55:56.811
    Mar 15 22:55:56.811: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename disruption 03/15/23 22:55:56.812
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:55:56.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:55:56.825
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 03/15/23 22:55:56.829
    STEP: Waiting for the pdb to be processed 03/15/23 22:55:56.839
    STEP: updating the pdb 03/15/23 22:55:58.847
    STEP: Waiting for the pdb to be processed 03/15/23 22:55:58.855
    STEP: patching the pdb 03/15/23 22:55:58.861
    STEP: Waiting for the pdb to be processed 03/15/23 22:55:58.87
    STEP: Waiting for the pdb to be deleted 03/15/23 22:56:00.882
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:56:00.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8671" for this suite. 03/15/23 22:56:00.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:56:00.897
Mar 15 22:56:00.897: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename limitrange 03/15/23 22:56:00.898
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:00.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:00.914
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-w6xzh" in namespace "limitrange-5762" 03/15/23 22:56:00.918
STEP: Creating another limitRange in another namespace 03/15/23 22:56:00.926
Mar 15 22:56:00.938: INFO: Namespace "e2e-limitrange-w6xzh-8190" created
Mar 15 22:56:00.938: INFO: Creating LimitRange "e2e-limitrange-w6xzh" in namespace "e2e-limitrange-w6xzh-8190"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-w6xzh" 03/15/23 22:56:00.945
Mar 15 22:56:00.948: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-w6xzh" in "limitrange-5762" namespace 03/15/23 22:56:00.948
Mar 15 22:56:00.955: INFO: LimitRange "e2e-limitrange-w6xzh" has been patched
STEP: Delete LimitRange "e2e-limitrange-w6xzh" by Collection with labelSelector: "e2e-limitrange-w6xzh=patched" 03/15/23 22:56:00.955
STEP: Confirm that the limitRange "e2e-limitrange-w6xzh" has been deleted 03/15/23 22:56:00.961
Mar 15 22:56:00.961: INFO: Requesting list of LimitRange to confirm quantity
Mar 15 22:56:00.965: INFO: Found 0 LimitRange with label "e2e-limitrange-w6xzh=patched"
Mar 15 22:56:00.965: INFO: LimitRange "e2e-limitrange-w6xzh" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-w6xzh" 03/15/23 22:56:00.965
Mar 15 22:56:00.968: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Mar 15 22:56:00.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5762" for this suite. 03/15/23 22:56:00.973
STEP: Destroying namespace "e2e-limitrange-w6xzh-8190" for this suite. 03/15/23 22:56:00.979
------------------------------
• [0.093 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:56:00.897
    Mar 15 22:56:00.897: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename limitrange 03/15/23 22:56:00.898
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:00.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:00.914
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-w6xzh" in namespace "limitrange-5762" 03/15/23 22:56:00.918
    STEP: Creating another limitRange in another namespace 03/15/23 22:56:00.926
    Mar 15 22:56:00.938: INFO: Namespace "e2e-limitrange-w6xzh-8190" created
    Mar 15 22:56:00.938: INFO: Creating LimitRange "e2e-limitrange-w6xzh" in namespace "e2e-limitrange-w6xzh-8190"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-w6xzh" 03/15/23 22:56:00.945
    Mar 15 22:56:00.948: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-w6xzh" in "limitrange-5762" namespace 03/15/23 22:56:00.948
    Mar 15 22:56:00.955: INFO: LimitRange "e2e-limitrange-w6xzh" has been patched
    STEP: Delete LimitRange "e2e-limitrange-w6xzh" by Collection with labelSelector: "e2e-limitrange-w6xzh=patched" 03/15/23 22:56:00.955
    STEP: Confirm that the limitRange "e2e-limitrange-w6xzh" has been deleted 03/15/23 22:56:00.961
    Mar 15 22:56:00.961: INFO: Requesting list of LimitRange to confirm quantity
    Mar 15 22:56:00.965: INFO: Found 0 LimitRange with label "e2e-limitrange-w6xzh=patched"
    Mar 15 22:56:00.965: INFO: LimitRange "e2e-limitrange-w6xzh" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-w6xzh" 03/15/23 22:56:00.965
    Mar 15 22:56:00.968: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:56:00.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5762" for this suite. 03/15/23 22:56:00.973
    STEP: Destroying namespace "e2e-limitrange-w6xzh-8190" for this suite. 03/15/23 22:56:00.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:56:00.996
Mar 15 22:56:00.996: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename replicaset 03/15/23 22:56:00.997
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:01.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:01.015
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Mar 15 22:56:01.018: INFO: Creating ReplicaSet my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32
Mar 15 22:56:01.028: INFO: Pod name my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32: Found 0 pods out of 1
Mar 15 22:56:06.034: INFO: Pod name my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32: Found 1 pods out of 1
Mar 15 22:56:06.034: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32" is running
Mar 15 22:56:06.034: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32-pfx5s" in namespace "replicaset-4441" to be "running"
Mar 15 22:56:06.037: INFO: Pod "my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32-pfx5s": Phase="Running", Reason="", readiness=true. Elapsed: 3.405558ms
Mar 15 22:56:06.037: INFO: Pod "my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32-pfx5s" satisfied condition "running"
Mar 15 22:56:06.037: INFO: Pod "my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32-pfx5s" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:56:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:56:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:56:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:56:01 +0000 UTC Reason: Message:}])
Mar 15 22:56:06.037: INFO: Trying to dial the pod
Mar 15 22:56:11.054: INFO: Controller my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32: Got expected result from replica 1 [my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32-pfx5s]: "my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32-pfx5s", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 15 22:56:11.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4441" for this suite. 03/15/23 22:56:11.058
------------------------------
• [SLOW TEST] [10.069 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:56:00.996
    Mar 15 22:56:00.996: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename replicaset 03/15/23 22:56:00.997
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:01.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:01.015
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Mar 15 22:56:01.018: INFO: Creating ReplicaSet my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32
    Mar 15 22:56:01.028: INFO: Pod name my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32: Found 0 pods out of 1
    Mar 15 22:56:06.034: INFO: Pod name my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32: Found 1 pods out of 1
    Mar 15 22:56:06.034: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32" is running
    Mar 15 22:56:06.034: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32-pfx5s" in namespace "replicaset-4441" to be "running"
    Mar 15 22:56:06.037: INFO: Pod "my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32-pfx5s": Phase="Running", Reason="", readiness=true. Elapsed: 3.405558ms
    Mar 15 22:56:06.037: INFO: Pod "my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32-pfx5s" satisfied condition "running"
    Mar 15 22:56:06.037: INFO: Pod "my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32-pfx5s" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:56:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:56:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:56:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-15 22:56:01 +0000 UTC Reason: Message:}])
    Mar 15 22:56:06.037: INFO: Trying to dial the pod
    Mar 15 22:56:11.054: INFO: Controller my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32: Got expected result from replica 1 [my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32-pfx5s]: "my-hostname-basic-5bb596ad-09eb-4845-8e7b-a4618e48dd32-pfx5s", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:56:11.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4441" for this suite. 03/15/23 22:56:11.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:56:11.065
Mar 15 22:56:11.066: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:56:11.067
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:11.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:11.087
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-d6eaf3f8-ea05-4b65-b65d-f62168a2d63c 03/15/23 22:56:11.094
STEP: Creating a pod to test consume configMaps 03/15/23 22:56:11.101
Mar 15 22:56:11.113: INFO: Waiting up to 5m0s for pod "pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353" in namespace "configmap-962" to be "Succeeded or Failed"
Mar 15 22:56:11.118: INFO: Pod "pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353": Phase="Pending", Reason="", readiness=false. Elapsed: 5.206265ms
Mar 15 22:56:13.127: INFO: Pod "pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014619042s
Mar 15 22:56:15.124: INFO: Pod "pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011472986s
STEP: Saw pod success 03/15/23 22:56:15.124
Mar 15 22:56:15.125: INFO: Pod "pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353" satisfied condition "Succeeded or Failed"
Mar 15 22:56:15.128: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353 container agnhost-container: <nil>
STEP: delete the pod 03/15/23 22:56:15.133
Mar 15 22:56:15.147: INFO: Waiting for pod pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353 to disappear
Mar 15 22:56:15.152: INFO: Pod pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:56:15.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-962" for this suite. 03/15/23 22:56:15.158
------------------------------
• [4.099 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:56:11.065
    Mar 15 22:56:11.066: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:56:11.067
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:11.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:11.087
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-d6eaf3f8-ea05-4b65-b65d-f62168a2d63c 03/15/23 22:56:11.094
    STEP: Creating a pod to test consume configMaps 03/15/23 22:56:11.101
    Mar 15 22:56:11.113: INFO: Waiting up to 5m0s for pod "pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353" in namespace "configmap-962" to be "Succeeded or Failed"
    Mar 15 22:56:11.118: INFO: Pod "pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353": Phase="Pending", Reason="", readiness=false. Elapsed: 5.206265ms
    Mar 15 22:56:13.127: INFO: Pod "pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014619042s
    Mar 15 22:56:15.124: INFO: Pod "pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011472986s
    STEP: Saw pod success 03/15/23 22:56:15.124
    Mar 15 22:56:15.125: INFO: Pod "pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353" satisfied condition "Succeeded or Failed"
    Mar 15 22:56:15.128: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353 container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 22:56:15.133
    Mar 15 22:56:15.147: INFO: Waiting for pod pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353 to disappear
    Mar 15 22:56:15.152: INFO: Pod pod-configmaps-131a5569-7fa0-476a-bbdf-f20f7e8bf353 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:56:15.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-962" for this suite. 03/15/23 22:56:15.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:56:15.173
Mar 15 22:56:15.173: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename svcaccounts 03/15/23 22:56:15.173
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:15.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:15.188
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 03/15/23 22:56:15.192
STEP: watching for the ServiceAccount to be added 03/15/23 22:56:15.201
STEP: patching the ServiceAccount 03/15/23 22:56:15.204
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/15/23 22:56:15.21
STEP: deleting the ServiceAccount 03/15/23 22:56:15.213
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 15 22:56:15.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3682" for this suite. 03/15/23 22:56:15.233
------------------------------
• [0.066 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:56:15.173
    Mar 15 22:56:15.173: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename svcaccounts 03/15/23 22:56:15.173
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:15.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:15.188
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 03/15/23 22:56:15.192
    STEP: watching for the ServiceAccount to be added 03/15/23 22:56:15.201
    STEP: patching the ServiceAccount 03/15/23 22:56:15.204
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/15/23 22:56:15.21
    STEP: deleting the ServiceAccount 03/15/23 22:56:15.213
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:56:15.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3682" for this suite. 03/15/23 22:56:15.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:56:15.24
Mar 15 22:56:15.241: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename resourcequota 03/15/23 22:56:15.241
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:15.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:15.258
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 03/15/23 22:56:15.262
STEP: Creating a ResourceQuota 03/15/23 22:56:20.264
STEP: Ensuring resource quota status is calculated 03/15/23 22:56:20.271
STEP: Creating a Service 03/15/23 22:56:22.275
STEP: Creating a NodePort Service 03/15/23 22:56:22.294
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/15/23 22:56:22.323
STEP: Ensuring resource quota status captures service creation 03/15/23 22:56:22.349
STEP: Deleting Services 03/15/23 22:56:24.353
STEP: Ensuring resource quota status released usage 03/15/23 22:56:24.412
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 15 22:56:26.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6829" for this suite. 03/15/23 22:56:26.436
------------------------------
• [SLOW TEST] [11.202 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:56:15.24
    Mar 15 22:56:15.241: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename resourcequota 03/15/23 22:56:15.241
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:15.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:15.258
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 03/15/23 22:56:15.262
    STEP: Creating a ResourceQuota 03/15/23 22:56:20.264
    STEP: Ensuring resource quota status is calculated 03/15/23 22:56:20.271
    STEP: Creating a Service 03/15/23 22:56:22.275
    STEP: Creating a NodePort Service 03/15/23 22:56:22.294
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/15/23 22:56:22.323
    STEP: Ensuring resource quota status captures service creation 03/15/23 22:56:22.349
    STEP: Deleting Services 03/15/23 22:56:24.353
    STEP: Ensuring resource quota status released usage 03/15/23 22:56:24.412
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:56:26.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6829" for this suite. 03/15/23 22:56:26.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:56:26.45
Mar 15 22:56:26.450: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 22:56:26.453
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:26.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:26.469
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2576 03/15/23 22:56:26.472
STEP: changing the ExternalName service to type=NodePort 03/15/23 22:56:26.478
STEP: creating replication controller externalname-service in namespace services-2576 03/15/23 22:56:26.501
I0315 22:56:26.528411      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2576, replica count: 2
I0315 22:56:29.583608      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0315 22:56:32.584671      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 22:56:32.584: INFO: Creating new exec pod
Mar 15 22:56:32.590: INFO: Waiting up to 5m0s for pod "execpodqjcvd" in namespace "services-2576" to be "running"
Mar 15 22:56:32.594: INFO: Pod "execpodqjcvd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.780554ms
Mar 15 22:56:34.601: INFO: Pod "execpodqjcvd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010688462s
Mar 15 22:56:34.601: INFO: Pod "execpodqjcvd" satisfied condition "running"
Mar 15 22:56:35.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2576 exec execpodqjcvd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Mar 15 22:56:35.802: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 15 22:56:35.802: INFO: stdout: ""
Mar 15 22:56:35.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2576 exec execpodqjcvd -- /bin/sh -x -c nc -v -z -w 2 100.66.40.101 80'
Mar 15 22:56:36.001: INFO: stderr: "+ nc -v -z -w 2 100.66.40.101 80\nConnection to 100.66.40.101 80 port [tcp/http] succeeded!\n"
Mar 15 22:56:36.001: INFO: stdout: ""
Mar 15 22:56:36.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2576 exec execpodqjcvd -- /bin/sh -x -c nc -v -z -w 2 172.20.60.208 30148'
Mar 15 22:56:36.186: INFO: stderr: "+ nc -v -z -w 2 172.20.60.208 30148\nConnection to 172.20.60.208 30148 port [tcp/*] succeeded!\n"
Mar 15 22:56:36.186: INFO: stdout: ""
Mar 15 22:56:36.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2576 exec execpodqjcvd -- /bin/sh -x -c nc -v -z -w 2 172.20.75.105 30148'
Mar 15 22:56:36.336: INFO: stderr: "+ nc -v -z -w 2 172.20.75.105 30148\nConnection to 172.20.75.105 30148 port [tcp/*] succeeded!\n"
Mar 15 22:56:36.336: INFO: stdout: ""
Mar 15 22:56:36.336: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 22:56:36.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2576" for this suite. 03/15/23 22:56:36.369
------------------------------
• [SLOW TEST] [9.929 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:56:26.45
    Mar 15 22:56:26.450: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 22:56:26.453
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:26.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:26.469
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2576 03/15/23 22:56:26.472
    STEP: changing the ExternalName service to type=NodePort 03/15/23 22:56:26.478
    STEP: creating replication controller externalname-service in namespace services-2576 03/15/23 22:56:26.501
    I0315 22:56:26.528411      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2576, replica count: 2
    I0315 22:56:29.583608      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0315 22:56:32.584671      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 15 22:56:32.584: INFO: Creating new exec pod
    Mar 15 22:56:32.590: INFO: Waiting up to 5m0s for pod "execpodqjcvd" in namespace "services-2576" to be "running"
    Mar 15 22:56:32.594: INFO: Pod "execpodqjcvd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.780554ms
    Mar 15 22:56:34.601: INFO: Pod "execpodqjcvd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010688462s
    Mar 15 22:56:34.601: INFO: Pod "execpodqjcvd" satisfied condition "running"
    Mar 15 22:56:35.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2576 exec execpodqjcvd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Mar 15 22:56:35.802: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 15 22:56:35.802: INFO: stdout: ""
    Mar 15 22:56:35.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2576 exec execpodqjcvd -- /bin/sh -x -c nc -v -z -w 2 100.66.40.101 80'
    Mar 15 22:56:36.001: INFO: stderr: "+ nc -v -z -w 2 100.66.40.101 80\nConnection to 100.66.40.101 80 port [tcp/http] succeeded!\n"
    Mar 15 22:56:36.001: INFO: stdout: ""
    Mar 15 22:56:36.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2576 exec execpodqjcvd -- /bin/sh -x -c nc -v -z -w 2 172.20.60.208 30148'
    Mar 15 22:56:36.186: INFO: stderr: "+ nc -v -z -w 2 172.20.60.208 30148\nConnection to 172.20.60.208 30148 port [tcp/*] succeeded!\n"
    Mar 15 22:56:36.186: INFO: stdout: ""
    Mar 15 22:56:36.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2576 exec execpodqjcvd -- /bin/sh -x -c nc -v -z -w 2 172.20.75.105 30148'
    Mar 15 22:56:36.336: INFO: stderr: "+ nc -v -z -w 2 172.20.75.105 30148\nConnection to 172.20.75.105 30148 port [tcp/*] succeeded!\n"
    Mar 15 22:56:36.336: INFO: stdout: ""
    Mar 15 22:56:36.336: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:56:36.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2576" for this suite. 03/15/23 22:56:36.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:56:36.381
Mar 15 22:56:36.381: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename subpath 03/15/23 22:56:36.384
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:36.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:36.409
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/15/23 22:56:36.419
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-g6qw 03/15/23 22:56:36.43
STEP: Creating a pod to test atomic-volume-subpath 03/15/23 22:56:36.43
Mar 15 22:56:36.439: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-g6qw" in namespace "subpath-5177" to be "Succeeded or Failed"
Mar 15 22:56:36.447: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.577881ms
Mar 15 22:56:38.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 2.012621657s
Mar 15 22:56:40.453: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 4.014387922s
Mar 15 22:56:42.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 6.012099328s
Mar 15 22:56:44.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 8.012234009s
Mar 15 22:56:46.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 10.01274143s
Mar 15 22:56:48.452: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 12.013784725s
Mar 15 22:56:50.454: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 14.015451145s
Mar 15 22:56:52.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 16.012614306s
Mar 15 22:56:54.452: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 18.013226349s
Mar 15 22:56:56.454: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 20.015587194s
Mar 15 22:56:58.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=false. Elapsed: 22.012494238s
Mar 15 22:57:00.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012529869s
STEP: Saw pod success 03/15/23 22:57:00.451
Mar 15 22:57:00.451: INFO: Pod "pod-subpath-test-configmap-g6qw" satisfied condition "Succeeded or Failed"
Mar 15 22:57:00.459: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-subpath-test-configmap-g6qw container test-container-subpath-configmap-g6qw: <nil>
STEP: delete the pod 03/15/23 22:57:00.477
Mar 15 22:57:00.492: INFO: Waiting for pod pod-subpath-test-configmap-g6qw to disappear
Mar 15 22:57:00.496: INFO: Pod pod-subpath-test-configmap-g6qw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-g6qw 03/15/23 22:57:00.496
Mar 15 22:57:00.497: INFO: Deleting pod "pod-subpath-test-configmap-g6qw" in namespace "subpath-5177"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 15 22:57:00.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5177" for this suite. 03/15/23 22:57:00.503
------------------------------
• [SLOW TEST] [24.128 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:56:36.381
    Mar 15 22:56:36.381: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename subpath 03/15/23 22:56:36.384
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:56:36.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:56:36.409
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/15/23 22:56:36.419
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-g6qw 03/15/23 22:56:36.43
    STEP: Creating a pod to test atomic-volume-subpath 03/15/23 22:56:36.43
    Mar 15 22:56:36.439: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-g6qw" in namespace "subpath-5177" to be "Succeeded or Failed"
    Mar 15 22:56:36.447: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.577881ms
    Mar 15 22:56:38.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 2.012621657s
    Mar 15 22:56:40.453: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 4.014387922s
    Mar 15 22:56:42.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 6.012099328s
    Mar 15 22:56:44.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 8.012234009s
    Mar 15 22:56:46.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 10.01274143s
    Mar 15 22:56:48.452: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 12.013784725s
    Mar 15 22:56:50.454: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 14.015451145s
    Mar 15 22:56:52.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 16.012614306s
    Mar 15 22:56:54.452: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 18.013226349s
    Mar 15 22:56:56.454: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=true. Elapsed: 20.015587194s
    Mar 15 22:56:58.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Running", Reason="", readiness=false. Elapsed: 22.012494238s
    Mar 15 22:57:00.451: INFO: Pod "pod-subpath-test-configmap-g6qw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012529869s
    STEP: Saw pod success 03/15/23 22:57:00.451
    Mar 15 22:57:00.451: INFO: Pod "pod-subpath-test-configmap-g6qw" satisfied condition "Succeeded or Failed"
    Mar 15 22:57:00.459: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod pod-subpath-test-configmap-g6qw container test-container-subpath-configmap-g6qw: <nil>
    STEP: delete the pod 03/15/23 22:57:00.477
    Mar 15 22:57:00.492: INFO: Waiting for pod pod-subpath-test-configmap-g6qw to disappear
    Mar 15 22:57:00.496: INFO: Pod pod-subpath-test-configmap-g6qw no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-g6qw 03/15/23 22:57:00.496
    Mar 15 22:57:00.497: INFO: Deleting pod "pod-subpath-test-configmap-g6qw" in namespace "subpath-5177"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:57:00.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5177" for this suite. 03/15/23 22:57:00.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:57:00.518
Mar 15 22:57:00.518: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 22:57:00.519
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:57:00.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:57:00.537
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-bd8705a0-e815-488f-b7a8-24ebcfa47b72 03/15/23 22:57:00.545
STEP: Creating the pod 03/15/23 22:57:00.549
Mar 15 22:57:00.558: INFO: Waiting up to 5m0s for pod "pod-configmaps-be79ba98-d63a-4329-b59c-d9a3efcc0560" in namespace "configmap-8278" to be "running"
Mar 15 22:57:00.570: INFO: Pod "pod-configmaps-be79ba98-d63a-4329-b59c-d9a3efcc0560": Phase="Pending", Reason="", readiness=false. Elapsed: 11.779133ms
Mar 15 22:57:02.578: INFO: Pod "pod-configmaps-be79ba98-d63a-4329-b59c-d9a3efcc0560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019147895s
Mar 15 22:57:04.574: INFO: Pod "pod-configmaps-be79ba98-d63a-4329-b59c-d9a3efcc0560": Phase="Running", Reason="", readiness=false. Elapsed: 4.015366998s
Mar 15 22:57:04.574: INFO: Pod "pod-configmaps-be79ba98-d63a-4329-b59c-d9a3efcc0560" satisfied condition "running"
STEP: Waiting for pod with text data 03/15/23 22:57:04.574
STEP: Waiting for pod with binary data 03/15/23 22:57:04.579
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 22:57:04.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8278" for this suite. 03/15/23 22:57:04.591
------------------------------
• [4.082 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:57:00.518
    Mar 15 22:57:00.518: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 22:57:00.519
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:57:00.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:57:00.537
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-bd8705a0-e815-488f-b7a8-24ebcfa47b72 03/15/23 22:57:00.545
    STEP: Creating the pod 03/15/23 22:57:00.549
    Mar 15 22:57:00.558: INFO: Waiting up to 5m0s for pod "pod-configmaps-be79ba98-d63a-4329-b59c-d9a3efcc0560" in namespace "configmap-8278" to be "running"
    Mar 15 22:57:00.570: INFO: Pod "pod-configmaps-be79ba98-d63a-4329-b59c-d9a3efcc0560": Phase="Pending", Reason="", readiness=false. Elapsed: 11.779133ms
    Mar 15 22:57:02.578: INFO: Pod "pod-configmaps-be79ba98-d63a-4329-b59c-d9a3efcc0560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019147895s
    Mar 15 22:57:04.574: INFO: Pod "pod-configmaps-be79ba98-d63a-4329-b59c-d9a3efcc0560": Phase="Running", Reason="", readiness=false. Elapsed: 4.015366998s
    Mar 15 22:57:04.574: INFO: Pod "pod-configmaps-be79ba98-d63a-4329-b59c-d9a3efcc0560" satisfied condition "running"
    STEP: Waiting for pod with text data 03/15/23 22:57:04.574
    STEP: Waiting for pod with binary data 03/15/23 22:57:04.579
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:57:04.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8278" for this suite. 03/15/23 22:57:04.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:57:04.602
Mar 15 22:57:04.602: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 22:57:04.603
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:57:04.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:57:04.617
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 22:57:04.639
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:57:04.955
STEP: Deploying the webhook pod 03/15/23 22:57:04.962
STEP: Wait for the deployment to be ready 03/15/23 22:57:04.992
Mar 15 22:57:05.008: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 22:57:07.017
STEP: Verifying the service has paired with the endpoint 03/15/23 22:57:07.027
Mar 15 22:57:08.029: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 03/15/23 22:57:08.033
STEP: Creating a custom resource definition that should be denied by the webhook 03/15/23 22:57:08.049
Mar 15 22:57:08.050: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 22:57:08.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6508" for this suite. 03/15/23 22:57:08.123
STEP: Destroying namespace "webhook-6508-markers" for this suite. 03/15/23 22:57:08.133
------------------------------
• [3.550 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:57:04.602
    Mar 15 22:57:04.602: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 22:57:04.603
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:57:04.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:57:04.617
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 22:57:04.639
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 22:57:04.955
    STEP: Deploying the webhook pod 03/15/23 22:57:04.962
    STEP: Wait for the deployment to be ready 03/15/23 22:57:04.992
    Mar 15 22:57:05.008: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 22:57:07.017
    STEP: Verifying the service has paired with the endpoint 03/15/23 22:57:07.027
    Mar 15 22:57:08.029: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 03/15/23 22:57:08.033
    STEP: Creating a custom resource definition that should be denied by the webhook 03/15/23 22:57:08.049
    Mar 15 22:57:08.050: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:57:08.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6508" for this suite. 03/15/23 22:57:08.123
    STEP: Destroying namespace "webhook-6508-markers" for this suite. 03/15/23 22:57:08.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:57:08.165
Mar 15 22:57:08.165: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 22:57:08.166
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:57:08.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:57:08.202
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-2109 03/15/23 22:57:08.206
STEP: creating service affinity-nodeport-transition in namespace services-2109 03/15/23 22:57:08.206
STEP: creating replication controller affinity-nodeport-transition in namespace services-2109 03/15/23 22:57:08.224
I0315 22:57:08.229862      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2109, replica count: 3
I0315 22:57:11.282442      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 22:57:11.290: INFO: Creating new exec pod
Mar 15 22:57:11.295: INFO: Waiting up to 5m0s for pod "execpod-affinitytqm5t" in namespace "services-2109" to be "running"
Mar 15 22:57:11.300: INFO: Pod "execpod-affinitytqm5t": Phase="Pending", Reason="", readiness=false. Elapsed: 5.406341ms
Mar 15 22:57:13.305: INFO: Pod "execpod-affinitytqm5t": Phase="Running", Reason="", readiness=true. Elapsed: 2.010435736s
Mar 15 22:57:13.305: INFO: Pod "execpod-affinitytqm5t" satisfied condition "running"
Mar 15 22:57:14.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2109 exec execpod-affinitytqm5t -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Mar 15 22:57:14.553: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 15 22:57:14.553: INFO: stdout: ""
Mar 15 22:57:14.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2109 exec execpod-affinitytqm5t -- /bin/sh -x -c nc -v -z -w 2 100.66.164.15 80'
Mar 15 22:57:14.766: INFO: stderr: "+ nc -v -z -w 2 100.66.164.15 80\nConnection to 100.66.164.15 80 port [tcp/http] succeeded!\n"
Mar 15 22:57:14.766: INFO: stdout: ""
Mar 15 22:57:14.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2109 exec execpod-affinitytqm5t -- /bin/sh -x -c nc -v -z -w 2 172.20.126.23 32118'
Mar 15 22:57:14.973: INFO: stderr: "+ nc -v -z -w 2 172.20.126.23 32118\nConnection to 172.20.126.23 32118 port [tcp/*] succeeded!\n"
Mar 15 22:57:14.973: INFO: stdout: ""
Mar 15 22:57:14.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2109 exec execpod-affinitytqm5t -- /bin/sh -x -c nc -v -z -w 2 172.20.75.105 32118'
Mar 15 22:57:15.186: INFO: stderr: "+ nc -v -z -w 2 172.20.75.105 32118\nConnection to 172.20.75.105 32118 port [tcp/*] succeeded!\n"
Mar 15 22:57:15.186: INFO: stdout: ""
Mar 15 22:57:15.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2109 exec execpod-affinitytqm5t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.75.105:32118/ ; done'
Mar 15 22:57:15.525: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n"
Mar 15 22:57:15.525: INFO: stdout: "\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-7n2vq\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-7n2vq\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-7n2vq\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl"
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-7n2vq
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-7n2vq
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-7n2vq
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2109 exec execpod-affinitytqm5t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.75.105:32118/ ; done'
Mar 15 22:57:15.864: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n"
Mar 15 22:57:15.864: INFO: stdout: "\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl"
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
Mar 15 22:57:15.864: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2109, will wait for the garbage collector to delete the pods 03/15/23 22:57:15.889
Mar 15 22:57:15.949: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.950491ms
Mar 15 22:57:16.050: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.830377ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 22:57:18.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2109" for this suite. 03/15/23 22:57:18.307
------------------------------
• [SLOW TEST] [10.152 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:57:08.165
    Mar 15 22:57:08.165: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 22:57:08.166
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:57:08.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:57:08.202
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-2109 03/15/23 22:57:08.206
    STEP: creating service affinity-nodeport-transition in namespace services-2109 03/15/23 22:57:08.206
    STEP: creating replication controller affinity-nodeport-transition in namespace services-2109 03/15/23 22:57:08.224
    I0315 22:57:08.229862      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2109, replica count: 3
    I0315 22:57:11.282442      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 15 22:57:11.290: INFO: Creating new exec pod
    Mar 15 22:57:11.295: INFO: Waiting up to 5m0s for pod "execpod-affinitytqm5t" in namespace "services-2109" to be "running"
    Mar 15 22:57:11.300: INFO: Pod "execpod-affinitytqm5t": Phase="Pending", Reason="", readiness=false. Elapsed: 5.406341ms
    Mar 15 22:57:13.305: INFO: Pod "execpod-affinitytqm5t": Phase="Running", Reason="", readiness=true. Elapsed: 2.010435736s
    Mar 15 22:57:13.305: INFO: Pod "execpod-affinitytqm5t" satisfied condition "running"
    Mar 15 22:57:14.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2109 exec execpod-affinitytqm5t -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Mar 15 22:57:14.553: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Mar 15 22:57:14.553: INFO: stdout: ""
    Mar 15 22:57:14.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2109 exec execpod-affinitytqm5t -- /bin/sh -x -c nc -v -z -w 2 100.66.164.15 80'
    Mar 15 22:57:14.766: INFO: stderr: "+ nc -v -z -w 2 100.66.164.15 80\nConnection to 100.66.164.15 80 port [tcp/http] succeeded!\n"
    Mar 15 22:57:14.766: INFO: stdout: ""
    Mar 15 22:57:14.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2109 exec execpod-affinitytqm5t -- /bin/sh -x -c nc -v -z -w 2 172.20.126.23 32118'
    Mar 15 22:57:14.973: INFO: stderr: "+ nc -v -z -w 2 172.20.126.23 32118\nConnection to 172.20.126.23 32118 port [tcp/*] succeeded!\n"
    Mar 15 22:57:14.973: INFO: stdout: ""
    Mar 15 22:57:14.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2109 exec execpod-affinitytqm5t -- /bin/sh -x -c nc -v -z -w 2 172.20.75.105 32118'
    Mar 15 22:57:15.186: INFO: stderr: "+ nc -v -z -w 2 172.20.75.105 32118\nConnection to 172.20.75.105 32118 port [tcp/*] succeeded!\n"
    Mar 15 22:57:15.186: INFO: stdout: ""
    Mar 15 22:57:15.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2109 exec execpod-affinitytqm5t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.75.105:32118/ ; done'
    Mar 15 22:57:15.525: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n"
    Mar 15 22:57:15.525: INFO: stdout: "\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-7n2vq\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-7n2vq\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-qvcn2\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-7n2vq\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl"
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-7n2vq
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-7n2vq
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-qvcn2
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-7n2vq
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.525: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-2109 exec execpod-affinitytqm5t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.75.105:32118/ ; done'
    Mar 15 22:57:15.864: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.75.105:32118/\n"
    Mar 15 22:57:15.864: INFO: stdout: "\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl\naffinity-nodeport-transition-nf9xl"
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Received response from host: affinity-nodeport-transition-nf9xl
    Mar 15 22:57:15.864: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2109, will wait for the garbage collector to delete the pods 03/15/23 22:57:15.889
    Mar 15 22:57:15.949: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.950491ms
    Mar 15 22:57:16.050: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.830377ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:57:18.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2109" for this suite. 03/15/23 22:57:18.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:57:18.318
Mar 15 22:57:18.318: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename limitrange 03/15/23 22:57:18.319
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:57:18.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:57:18.336
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 03/15/23 22:57:18.339
STEP: Setting up watch 03/15/23 22:57:18.339
STEP: Submitting a LimitRange 03/15/23 22:57:18.443
STEP: Verifying LimitRange creation was observed 03/15/23 22:57:18.449
STEP: Fetching the LimitRange to ensure it has proper values 03/15/23 22:57:18.449
Mar 15 22:57:18.452: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 15 22:57:18.452: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 03/15/23 22:57:18.452
STEP: Ensuring Pod has resource requirements applied from LimitRange 03/15/23 22:57:18.457
Mar 15 22:57:18.460: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 15 22:57:18.461: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 03/15/23 22:57:18.461
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/15/23 22:57:18.47
Mar 15 22:57:18.477: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 15 22:57:18.477: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 03/15/23 22:57:18.477
STEP: Failing to create a Pod with more than max resources 03/15/23 22:57:18.48
STEP: Updating a LimitRange 03/15/23 22:57:18.483
STEP: Verifying LimitRange updating is effective 03/15/23 22:57:18.488
STEP: Creating a Pod with less than former min resources 03/15/23 22:57:20.492
STEP: Failing to create a Pod with more than max resources 03/15/23 22:57:20.51
STEP: Deleting a LimitRange 03/15/23 22:57:20.514
STEP: Verifying the LimitRange was deleted 03/15/23 22:57:20.526
Mar 15 22:57:25.532: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 03/15/23 22:57:25.532
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Mar 15 22:57:25.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-769" for this suite. 03/15/23 22:57:25.544
------------------------------
• [SLOW TEST] [7.238 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:57:18.318
    Mar 15 22:57:18.318: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename limitrange 03/15/23 22:57:18.319
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:57:18.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:57:18.336
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 03/15/23 22:57:18.339
    STEP: Setting up watch 03/15/23 22:57:18.339
    STEP: Submitting a LimitRange 03/15/23 22:57:18.443
    STEP: Verifying LimitRange creation was observed 03/15/23 22:57:18.449
    STEP: Fetching the LimitRange to ensure it has proper values 03/15/23 22:57:18.449
    Mar 15 22:57:18.452: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 15 22:57:18.452: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 03/15/23 22:57:18.452
    STEP: Ensuring Pod has resource requirements applied from LimitRange 03/15/23 22:57:18.457
    Mar 15 22:57:18.460: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 15 22:57:18.461: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 03/15/23 22:57:18.461
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/15/23 22:57:18.47
    Mar 15 22:57:18.477: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Mar 15 22:57:18.477: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 03/15/23 22:57:18.477
    STEP: Failing to create a Pod with more than max resources 03/15/23 22:57:18.48
    STEP: Updating a LimitRange 03/15/23 22:57:18.483
    STEP: Verifying LimitRange updating is effective 03/15/23 22:57:18.488
    STEP: Creating a Pod with less than former min resources 03/15/23 22:57:20.492
    STEP: Failing to create a Pod with more than max resources 03/15/23 22:57:20.51
    STEP: Deleting a LimitRange 03/15/23 22:57:20.514
    STEP: Verifying the LimitRange was deleted 03/15/23 22:57:20.526
    Mar 15 22:57:25.532: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 03/15/23 22:57:25.532
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Mar 15 22:57:25.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-769" for this suite. 03/15/23 22:57:25.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 22:57:25.56
Mar 15 22:57:25.560: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename var-expansion 03/15/23 22:57:25.561
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:57:25.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:57:25.577
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 03/15/23 22:57:25.58
Mar 15 22:57:25.595: INFO: Waiting up to 2m0s for pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0" in namespace "var-expansion-2435" to be "running"
Mar 15 22:57:25.598: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.350621ms
Mar 15 22:57:27.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00731692s
Mar 15 22:57:29.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007571776s
Mar 15 22:57:31.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006947272s
Mar 15 22:57:33.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008288188s
Mar 15 22:57:35.606: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010666094s
Mar 15 22:57:37.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008060123s
Mar 15 22:57:39.604: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008542209s
Mar 15 22:57:41.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006556935s
Mar 15 22:57:43.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007994708s
Mar 15 22:57:45.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008094648s
Mar 15 22:57:47.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007674445s
Mar 15 22:57:49.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007852253s
Mar 15 22:57:51.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008270248s
Mar 15 22:57:53.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007028088s
Mar 15 22:57:55.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007904864s
Mar 15 22:57:57.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007231778s
Mar 15 22:57:59.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006826754s
Mar 15 22:58:01.620: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025223984s
Mar 15 22:58:03.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007970667s
Mar 15 22:58:05.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00784471s
Mar 15 22:58:07.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006805992s
Mar 15 22:58:09.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007512438s
Mar 15 22:58:11.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 46.00766042s
Mar 15 22:58:13.607: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011540698s
Mar 15 22:58:15.614: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 50.019308403s
Mar 15 22:58:17.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007196401s
Mar 15 22:58:19.605: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009957659s
Mar 15 22:58:21.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006916961s
Mar 15 22:58:23.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007903807s
Mar 15 22:58:25.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007799917s
Mar 15 22:58:27.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00704902s
Mar 15 22:58:29.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007221388s
Mar 15 22:58:31.604: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008495274s
Mar 15 22:58:33.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008368837s
Mar 15 22:58:35.607: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012430165s
Mar 15 22:58:37.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00689389s
Mar 15 22:58:39.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007754705s
Mar 15 22:58:41.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007598872s
Mar 15 22:58:43.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007254003s
Mar 15 22:58:45.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007820778s
Mar 15 22:58:47.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006896754s
Mar 15 22:58:49.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008131824s
Mar 15 22:58:51.604: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009298572s
Mar 15 22:58:53.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006733059s
Mar 15 22:58:55.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.006963701s
Mar 15 22:58:57.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007261001s
Mar 15 22:58:59.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007013285s
Mar 15 22:59:01.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007228395s
Mar 15 22:59:03.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.007853566s
Mar 15 22:59:05.607: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.012128782s
Mar 15 22:59:07.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007615244s
Mar 15 22:59:09.606: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010617296s
Mar 15 22:59:11.605: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009597324s
Mar 15 22:59:13.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007594626s
Mar 15 22:59:15.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007285058s
Mar 15 22:59:17.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008166913s
Mar 15 22:59:19.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006713632s
Mar 15 22:59:21.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006506277s
Mar 15 22:59:23.606: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010443626s
Mar 15 22:59:25.607: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012260438s
Mar 15 22:59:25.612: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016952381s
STEP: updating the pod 03/15/23 22:59:25.612
Mar 15 22:59:26.127: INFO: Successfully updated pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0"
STEP: waiting for pod running 03/15/23 22:59:26.127
Mar 15 22:59:26.127: INFO: Waiting up to 2m0s for pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0" in namespace "var-expansion-2435" to be "running"
Mar 15 22:59:26.134: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.41666ms
Mar 15 22:59:28.137: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.009659586s
Mar 15 22:59:28.137: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0" satisfied condition "running"
STEP: deleting the pod gracefully 03/15/23 22:59:28.137
Mar 15 22:59:28.137: INFO: Deleting pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0" in namespace "var-expansion-2435"
Mar 15 22:59:28.153: INFO: Wait up to 5m0s for pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 15 23:00:00.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2435" for this suite. 03/15/23 23:00:00.199
------------------------------
• [SLOW TEST] [154.663 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 22:57:25.56
    Mar 15 22:57:25.560: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename var-expansion 03/15/23 22:57:25.561
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 22:57:25.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 22:57:25.577
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 03/15/23 22:57:25.58
    Mar 15 22:57:25.595: INFO: Waiting up to 2m0s for pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0" in namespace "var-expansion-2435" to be "running"
    Mar 15 22:57:25.598: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.350621ms
    Mar 15 22:57:27.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00731692s
    Mar 15 22:57:29.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007571776s
    Mar 15 22:57:31.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006947272s
    Mar 15 22:57:33.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008288188s
    Mar 15 22:57:35.606: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010666094s
    Mar 15 22:57:37.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008060123s
    Mar 15 22:57:39.604: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008542209s
    Mar 15 22:57:41.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006556935s
    Mar 15 22:57:43.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007994708s
    Mar 15 22:57:45.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008094648s
    Mar 15 22:57:47.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007674445s
    Mar 15 22:57:49.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007852253s
    Mar 15 22:57:51.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008270248s
    Mar 15 22:57:53.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007028088s
    Mar 15 22:57:55.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007904864s
    Mar 15 22:57:57.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007231778s
    Mar 15 22:57:59.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006826754s
    Mar 15 22:58:01.620: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025223984s
    Mar 15 22:58:03.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007970667s
    Mar 15 22:58:05.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00784471s
    Mar 15 22:58:07.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006805992s
    Mar 15 22:58:09.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007512438s
    Mar 15 22:58:11.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 46.00766042s
    Mar 15 22:58:13.607: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011540698s
    Mar 15 22:58:15.614: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 50.019308403s
    Mar 15 22:58:17.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007196401s
    Mar 15 22:58:19.605: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009957659s
    Mar 15 22:58:21.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006916961s
    Mar 15 22:58:23.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007903807s
    Mar 15 22:58:25.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007799917s
    Mar 15 22:58:27.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00704902s
    Mar 15 22:58:29.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007221388s
    Mar 15 22:58:31.604: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008495274s
    Mar 15 22:58:33.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008368837s
    Mar 15 22:58:35.607: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012430165s
    Mar 15 22:58:37.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00689389s
    Mar 15 22:58:39.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007754705s
    Mar 15 22:58:41.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007598872s
    Mar 15 22:58:43.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007254003s
    Mar 15 22:58:45.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007820778s
    Mar 15 22:58:47.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006896754s
    Mar 15 22:58:49.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008131824s
    Mar 15 22:58:51.604: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009298572s
    Mar 15 22:58:53.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006733059s
    Mar 15 22:58:55.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.006963701s
    Mar 15 22:58:57.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007261001s
    Mar 15 22:58:59.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007013285s
    Mar 15 22:59:01.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007228395s
    Mar 15 22:59:03.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.007853566s
    Mar 15 22:59:05.607: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.012128782s
    Mar 15 22:59:07.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007615244s
    Mar 15 22:59:09.606: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010617296s
    Mar 15 22:59:11.605: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009597324s
    Mar 15 22:59:13.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007594626s
    Mar 15 22:59:15.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007285058s
    Mar 15 22:59:17.603: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008166913s
    Mar 15 22:59:19.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006713632s
    Mar 15 22:59:21.602: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006506277s
    Mar 15 22:59:23.606: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010443626s
    Mar 15 22:59:25.607: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012260438s
    Mar 15 22:59:25.612: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016952381s
    STEP: updating the pod 03/15/23 22:59:25.612
    Mar 15 22:59:26.127: INFO: Successfully updated pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0"
    STEP: waiting for pod running 03/15/23 22:59:26.127
    Mar 15 22:59:26.127: INFO: Waiting up to 2m0s for pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0" in namespace "var-expansion-2435" to be "running"
    Mar 15 22:59:26.134: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.41666ms
    Mar 15 22:59:28.137: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.009659586s
    Mar 15 22:59:28.137: INFO: Pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0" satisfied condition "running"
    STEP: deleting the pod gracefully 03/15/23 22:59:28.137
    Mar 15 22:59:28.137: INFO: Deleting pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0" in namespace "var-expansion-2435"
    Mar 15 22:59:28.153: INFO: Wait up to 5m0s for pod "var-expansion-b66afa7b-ed02-451a-8c17-a45540b078a0" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:00:00.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2435" for this suite. 03/15/23 23:00:00.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:00:00.226
Mar 15 23:00:00.226: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename svcaccounts 03/15/23 23:00:00.243
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:00.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:00.324
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Mar 15 23:00:00.402: INFO: created pod
Mar 15 23:00:00.402: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4646" to be "Succeeded or Failed"
Mar 15 23:00:00.441: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 38.965488ms
Mar 15 23:00:02.445: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.043037489s
Mar 15 23:00:04.464: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.062862106s
Mar 15 23:00:06.445: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043923217s
STEP: Saw pod success 03/15/23 23:00:06.446
Mar 15 23:00:06.446: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar 15 23:00:36.449: INFO: polling logs
Mar 15 23:00:36.473: INFO: Pod logs: 
I0315 23:00:01.585796       1 log.go:198] OK: Got token
I0315 23:00:01.585844       1 log.go:198] validating with in-cluster discovery
I0315 23:00:01.586303       1 log.go:198] OK: got issuer https://api.internal.1-26-amd64-3e1df960.ba26.prod-build-pdx.kops-ci.model-rocket.aws.dev
I0315 23:00:01.586340       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.internal.1-26-amd64-3e1df960.ba26.prod-build-pdx.kops-ci.model-rocket.aws.dev", Subject:"system:serviceaccount:svcaccounts-4646:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1678921800, NotBefore:1678921200, IssuedAt:1678921200, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4646", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"89fe5980-cf67-41ab-a827-bcc89b4ece37"}}}
I0315 23:00:01.636226       1 log.go:198] OK: Constructed OIDC provider for issuer https://api.internal.1-26-amd64-3e1df960.ba26.prod-build-pdx.kops-ci.model-rocket.aws.dev
I0315 23:00:01.638241       1 log.go:198] OK: Validated signature on JWT
I0315 23:00:01.638355       1 log.go:198] OK: Got valid claims from token!
I0315 23:00:01.638384       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.internal.1-26-amd64-3e1df960.ba26.prod-build-pdx.kops-ci.model-rocket.aws.dev", Subject:"system:serviceaccount:svcaccounts-4646:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1678921800, NotBefore:1678921200, IssuedAt:1678921200, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4646", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"89fe5980-cf67-41ab-a827-bcc89b4ece37"}}}

Mar 15 23:00:36.473: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 15 23:00:36.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4646" for this suite. 03/15/23 23:00:36.494
------------------------------
• [SLOW TEST] [36.280 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:00:00.226
    Mar 15 23:00:00.226: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename svcaccounts 03/15/23 23:00:00.243
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:00.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:00.324
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Mar 15 23:00:00.402: INFO: created pod
    Mar 15 23:00:00.402: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4646" to be "Succeeded or Failed"
    Mar 15 23:00:00.441: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 38.965488ms
    Mar 15 23:00:02.445: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.043037489s
    Mar 15 23:00:04.464: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.062862106s
    Mar 15 23:00:06.445: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043923217s
    STEP: Saw pod success 03/15/23 23:00:06.446
    Mar 15 23:00:06.446: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Mar 15 23:00:36.449: INFO: polling logs
    Mar 15 23:00:36.473: INFO: Pod logs: 
    I0315 23:00:01.585796       1 log.go:198] OK: Got token
    I0315 23:00:01.585844       1 log.go:198] validating with in-cluster discovery
    I0315 23:00:01.586303       1 log.go:198] OK: got issuer https://api.internal.1-26-amd64-3e1df960.ba26.prod-build-pdx.kops-ci.model-rocket.aws.dev
    I0315 23:00:01.586340       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.internal.1-26-amd64-3e1df960.ba26.prod-build-pdx.kops-ci.model-rocket.aws.dev", Subject:"system:serviceaccount:svcaccounts-4646:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1678921800, NotBefore:1678921200, IssuedAt:1678921200, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4646", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"89fe5980-cf67-41ab-a827-bcc89b4ece37"}}}
    I0315 23:00:01.636226       1 log.go:198] OK: Constructed OIDC provider for issuer https://api.internal.1-26-amd64-3e1df960.ba26.prod-build-pdx.kops-ci.model-rocket.aws.dev
    I0315 23:00:01.638241       1 log.go:198] OK: Validated signature on JWT
    I0315 23:00:01.638355       1 log.go:198] OK: Got valid claims from token!
    I0315 23:00:01.638384       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.internal.1-26-amd64-3e1df960.ba26.prod-build-pdx.kops-ci.model-rocket.aws.dev", Subject:"system:serviceaccount:svcaccounts-4646:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1678921800, NotBefore:1678921200, IssuedAt:1678921200, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4646", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"89fe5980-cf67-41ab-a827-bcc89b4ece37"}}}

    Mar 15 23:00:36.473: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:00:36.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4646" for this suite. 03/15/23 23:00:36.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:00:36.506
Mar 15 23:00:36.506: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename proxy 03/15/23 23:00:36.507
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:36.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:36.529
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Mar 15 23:00:36.532: INFO: Creating pod...
Mar 15 23:00:36.541: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-992" to be "running"
Mar 15 23:00:36.549: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.453874ms
Mar 15 23:00:38.558: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.016999431s
Mar 15 23:00:38.558: INFO: Pod "agnhost" satisfied condition "running"
Mar 15 23:00:38.558: INFO: Creating service...
Mar 15 23:00:38.576: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/DELETE
Mar 15 23:00:38.596: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 15 23:00:38.597: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/GET
Mar 15 23:00:38.605: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 15 23:00:38.605: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/HEAD
Mar 15 23:00:38.611: INFO: http.Client request:HEAD | StatusCode:200
Mar 15 23:00:38.612: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/OPTIONS
Mar 15 23:00:38.618: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 15 23:00:38.618: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/PATCH
Mar 15 23:00:38.623: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 15 23:00:38.623: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/POST
Mar 15 23:00:38.627: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 15 23:00:38.627: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/PUT
Mar 15 23:00:38.634: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 15 23:00:38.634: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/DELETE
Mar 15 23:00:38.643: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 15 23:00:38.643: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/GET
Mar 15 23:00:38.650: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 15 23:00:38.650: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/HEAD
Mar 15 23:00:38.656: INFO: http.Client request:HEAD | StatusCode:200
Mar 15 23:00:38.656: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/OPTIONS
Mar 15 23:00:38.661: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 15 23:00:38.662: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/PATCH
Mar 15 23:00:38.672: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 15 23:00:38.672: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/POST
Mar 15 23:00:38.678: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 15 23:00:38.679: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/PUT
Mar 15 23:00:38.692: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 15 23:00:38.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-992" for this suite. 03/15/23 23:00:38.704
------------------------------
• [2.208 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:00:36.506
    Mar 15 23:00:36.506: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename proxy 03/15/23 23:00:36.507
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:36.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:36.529
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Mar 15 23:00:36.532: INFO: Creating pod...
    Mar 15 23:00:36.541: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-992" to be "running"
    Mar 15 23:00:36.549: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.453874ms
    Mar 15 23:00:38.558: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.016999431s
    Mar 15 23:00:38.558: INFO: Pod "agnhost" satisfied condition "running"
    Mar 15 23:00:38.558: INFO: Creating service...
    Mar 15 23:00:38.576: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/DELETE
    Mar 15 23:00:38.596: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 15 23:00:38.597: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/GET
    Mar 15 23:00:38.605: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 15 23:00:38.605: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/HEAD
    Mar 15 23:00:38.611: INFO: http.Client request:HEAD | StatusCode:200
    Mar 15 23:00:38.612: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/OPTIONS
    Mar 15 23:00:38.618: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 15 23:00:38.618: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/PATCH
    Mar 15 23:00:38.623: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 15 23:00:38.623: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/POST
    Mar 15 23:00:38.627: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 15 23:00:38.627: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/pods/agnhost/proxy/some/path/with/PUT
    Mar 15 23:00:38.634: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 15 23:00:38.634: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/DELETE
    Mar 15 23:00:38.643: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 15 23:00:38.643: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/GET
    Mar 15 23:00:38.650: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 15 23:00:38.650: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/HEAD
    Mar 15 23:00:38.656: INFO: http.Client request:HEAD | StatusCode:200
    Mar 15 23:00:38.656: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/OPTIONS
    Mar 15 23:00:38.661: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 15 23:00:38.662: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/PATCH
    Mar 15 23:00:38.672: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 15 23:00:38.672: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/POST
    Mar 15 23:00:38.678: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 15 23:00:38.679: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-992/services/test-service/proxy/some/path/with/PUT
    Mar 15 23:00:38.692: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:00:38.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-992" for this suite. 03/15/23 23:00:38.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:00:38.726
Mar 15 23:00:38.726: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename svcaccounts 03/15/23 23:00:38.743
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:38.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:38.792
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Mar 15 23:00:38.813: INFO: Waiting up to 5m0s for pod "pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26" in namespace "svcaccounts-63" to be "running"
Mar 15 23:00:38.818: INFO: Pod "pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26": Phase="Pending", Reason="", readiness=false. Elapsed: 5.393858ms
Mar 15 23:00:40.822: INFO: Pod "pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26": Phase="Running", Reason="", readiness=true. Elapsed: 2.008738694s
Mar 15 23:00:40.822: INFO: Pod "pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26" satisfied condition "running"
STEP: reading a file in the container 03/15/23 23:00:40.822
Mar 15 23:00:40.822: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-63 pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 03/15/23 23:00:41.042
Mar 15 23:00:41.042: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-63 pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 03/15/23 23:00:41.247
Mar 15 23:00:41.247: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-63 pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar 15 23:00:41.488: INFO: Got root ca configmap in namespace "svcaccounts-63"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 15 23:00:41.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-63" for this suite. 03/15/23 23:00:41.501
------------------------------
• [2.783 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:00:38.726
    Mar 15 23:00:38.726: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename svcaccounts 03/15/23 23:00:38.743
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:38.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:38.792
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Mar 15 23:00:38.813: INFO: Waiting up to 5m0s for pod "pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26" in namespace "svcaccounts-63" to be "running"
    Mar 15 23:00:38.818: INFO: Pod "pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26": Phase="Pending", Reason="", readiness=false. Elapsed: 5.393858ms
    Mar 15 23:00:40.822: INFO: Pod "pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26": Phase="Running", Reason="", readiness=true. Elapsed: 2.008738694s
    Mar 15 23:00:40.822: INFO: Pod "pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26" satisfied condition "running"
    STEP: reading a file in the container 03/15/23 23:00:40.822
    Mar 15 23:00:40.822: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-63 pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 03/15/23 23:00:41.042
    Mar 15 23:00:41.042: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-63 pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 03/15/23 23:00:41.247
    Mar 15 23:00:41.247: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-63 pod-service-account-91b2ce0b-4c0b-4d01-acc5-def86df7ba26 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Mar 15 23:00:41.488: INFO: Got root ca configmap in namespace "svcaccounts-63"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:00:41.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-63" for this suite. 03/15/23 23:00:41.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:00:41.51
Mar 15 23:00:41.511: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename pods 03/15/23 23:00:41.511
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:41.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:41.532
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 03/15/23 23:00:41.539
STEP: submitting the pod to kubernetes 03/15/23 23:00:41.539
Mar 15 23:00:41.549: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19" in namespace "pods-7006" to be "running and ready"
Mar 15 23:00:41.555: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19": Phase="Pending", Reason="", readiness=false. Elapsed: 5.955229ms
Mar 15 23:00:41.555: INFO: The phase of Pod pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 23:00:43.559: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19": Phase="Running", Reason="", readiness=true. Elapsed: 2.009998648s
Mar 15 23:00:43.559: INFO: The phase of Pod pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19 is Running (Ready = true)
Mar 15 23:00:43.559: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/15/23 23:00:43.561
STEP: updating the pod 03/15/23 23:00:43.564
Mar 15 23:00:44.077: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19"
Mar 15 23:00:44.077: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19" in namespace "pods-7006" to be "terminated with reason DeadlineExceeded"
Mar 15 23:00:44.080: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19": Phase="Running", Reason="", readiness=true. Elapsed: 3.437226ms
Mar 15 23:00:46.086: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19": Phase="Running", Reason="", readiness=true. Elapsed: 2.009100324s
Mar 15 23:00:48.085: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007890875s
Mar 15 23:00:48.085: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 15 23:00:48.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7006" for this suite. 03/15/23 23:00:48.093
------------------------------
• [SLOW TEST] [6.588 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:00:41.51
    Mar 15 23:00:41.511: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename pods 03/15/23 23:00:41.511
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:41.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:41.532
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 03/15/23 23:00:41.539
    STEP: submitting the pod to kubernetes 03/15/23 23:00:41.539
    Mar 15 23:00:41.549: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19" in namespace "pods-7006" to be "running and ready"
    Mar 15 23:00:41.555: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19": Phase="Pending", Reason="", readiness=false. Elapsed: 5.955229ms
    Mar 15 23:00:41.555: INFO: The phase of Pod pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19 is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 23:00:43.559: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19": Phase="Running", Reason="", readiness=true. Elapsed: 2.009998648s
    Mar 15 23:00:43.559: INFO: The phase of Pod pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19 is Running (Ready = true)
    Mar 15 23:00:43.559: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/15/23 23:00:43.561
    STEP: updating the pod 03/15/23 23:00:43.564
    Mar 15 23:00:44.077: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19"
    Mar 15 23:00:44.077: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19" in namespace "pods-7006" to be "terminated with reason DeadlineExceeded"
    Mar 15 23:00:44.080: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19": Phase="Running", Reason="", readiness=true. Elapsed: 3.437226ms
    Mar 15 23:00:46.086: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19": Phase="Running", Reason="", readiness=true. Elapsed: 2.009100324s
    Mar 15 23:00:48.085: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007890875s
    Mar 15 23:00:48.085: INFO: Pod "pod-update-activedeadlineseconds-ce695928-850a-454f-936f-cd32ae191a19" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:00:48.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7006" for this suite. 03/15/23 23:00:48.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:00:48.1
Mar 15 23:00:48.100: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 23:00:48.101
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:48.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:48.13
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Mar 15 23:00:48.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-5549 version'
Mar 15 23:00:48.196: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar 15 23:00:48.196: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.2\", GitCommit:\"fc04e732bb3e7198d2fa44efa5457c7c6f8c0f5b\", GitTreeState:\"clean\", BuildDate:\"2023-02-22T13:39:03Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.2-eks-b106822\", GitCommit:\"b10682298156c2741918844e724713543bfc375d\", GitTreeState:\"archive\", BuildDate:\"2023-02-22T13:32:21Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 23:00:48.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5549" for this suite. 03/15/23 23:00:48.2
------------------------------
• [0.107 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:00:48.1
    Mar 15 23:00:48.100: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 23:00:48.101
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:48.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:48.13
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Mar 15 23:00:48.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-5549 version'
    Mar 15 23:00:48.196: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Mar 15 23:00:48.196: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.2\", GitCommit:\"fc04e732bb3e7198d2fa44efa5457c7c6f8c0f5b\", GitTreeState:\"clean\", BuildDate:\"2023-02-22T13:39:03Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.2-eks-b106822\", GitCommit:\"b10682298156c2741918844e724713543bfc375d\", GitTreeState:\"archive\", BuildDate:\"2023-02-22T13:32:21Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:00:48.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5549" for this suite. 03/15/23 23:00:48.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:00:48.207
Mar 15 23:00:48.207: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename services 03/15/23 23:00:48.208
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:48.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:48.252
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-6064 03/15/23 23:00:48.264
STEP: creating service affinity-clusterip in namespace services-6064 03/15/23 23:00:48.264
STEP: creating replication controller affinity-clusterip in namespace services-6064 03/15/23 23:00:48.285
I0315 23:00:48.301220      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6064, replica count: 3
I0315 23:00:51.354628      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 23:00:51.362: INFO: Creating new exec pod
Mar 15 23:00:51.369: INFO: Waiting up to 5m0s for pod "execpod-affinityjhh8l" in namespace "services-6064" to be "running"
Mar 15 23:00:51.375: INFO: Pod "execpod-affinityjhh8l": Phase="Pending", Reason="", readiness=false. Elapsed: 5.57519ms
Mar 15 23:00:53.382: INFO: Pod "execpod-affinityjhh8l": Phase="Running", Reason="", readiness=true. Elapsed: 2.012205443s
Mar 15 23:00:53.382: INFO: Pod "execpod-affinityjhh8l" satisfied condition "running"
Mar 15 23:00:54.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-6064 exec execpod-affinityjhh8l -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Mar 15 23:00:54.538: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 15 23:00:54.538: INFO: stdout: ""
Mar 15 23:00:54.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-6064 exec execpod-affinityjhh8l -- /bin/sh -x -c nc -v -z -w 2 100.64.40.161 80'
Mar 15 23:00:54.699: INFO: stderr: "+ nc -v -z -w 2 100.64.40.161 80\nConnection to 100.64.40.161 80 port [tcp/http] succeeded!\n"
Mar 15 23:00:54.699: INFO: stdout: ""
Mar 15 23:00:54.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-6064 exec execpod-affinityjhh8l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.64.40.161:80/ ; done'
Mar 15 23:00:54.917: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n"
Mar 15 23:00:54.917: INFO: stdout: "\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb"
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
Mar 15 23:00:54.917: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6064, will wait for the garbage collector to delete the pods 03/15/23 23:00:54.931
Mar 15 23:00:54.993: INFO: Deleting ReplicationController affinity-clusterip took: 5.175508ms
Mar 15 23:00:55.093: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.154046ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 15 23:00:57.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6064" for this suite. 03/15/23 23:00:57.315
------------------------------
• [SLOW TEST] [9.113 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:00:48.207
    Mar 15 23:00:48.207: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename services 03/15/23 23:00:48.208
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:48.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:48.252
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-6064 03/15/23 23:00:48.264
    STEP: creating service affinity-clusterip in namespace services-6064 03/15/23 23:00:48.264
    STEP: creating replication controller affinity-clusterip in namespace services-6064 03/15/23 23:00:48.285
    I0315 23:00:48.301220      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6064, replica count: 3
    I0315 23:00:51.354628      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 15 23:00:51.362: INFO: Creating new exec pod
    Mar 15 23:00:51.369: INFO: Waiting up to 5m0s for pod "execpod-affinityjhh8l" in namespace "services-6064" to be "running"
    Mar 15 23:00:51.375: INFO: Pod "execpod-affinityjhh8l": Phase="Pending", Reason="", readiness=false. Elapsed: 5.57519ms
    Mar 15 23:00:53.382: INFO: Pod "execpod-affinityjhh8l": Phase="Running", Reason="", readiness=true. Elapsed: 2.012205443s
    Mar 15 23:00:53.382: INFO: Pod "execpod-affinityjhh8l" satisfied condition "running"
    Mar 15 23:00:54.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-6064 exec execpod-affinityjhh8l -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Mar 15 23:00:54.538: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Mar 15 23:00:54.538: INFO: stdout: ""
    Mar 15 23:00:54.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-6064 exec execpod-affinityjhh8l -- /bin/sh -x -c nc -v -z -w 2 100.64.40.161 80'
    Mar 15 23:00:54.699: INFO: stderr: "+ nc -v -z -w 2 100.64.40.161 80\nConnection to 100.64.40.161 80 port [tcp/http] succeeded!\n"
    Mar 15 23:00:54.699: INFO: stdout: ""
    Mar 15 23:00:54.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=services-6064 exec execpod-affinityjhh8l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.64.40.161:80/ ; done'
    Mar 15 23:00:54.917: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.40.161:80/\n"
    Mar 15 23:00:54.917: INFO: stdout: "\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb\naffinity-clusterip-rgljb"
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Received response from host: affinity-clusterip-rgljb
    Mar 15 23:00:54.917: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-6064, will wait for the garbage collector to delete the pods 03/15/23 23:00:54.931
    Mar 15 23:00:54.993: INFO: Deleting ReplicationController affinity-clusterip took: 5.175508ms
    Mar 15 23:00:55.093: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.154046ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:00:57.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6064" for this suite. 03/15/23 23:00:57.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:00:57.321
Mar 15 23:00:57.322: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename dns 03/15/23 23:00:57.323
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:57.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:57.34
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 03/15/23 23:00:57.343
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9036.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local; sleep 1; done
 03/15/23 23:00:57.348
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9036.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local; sleep 1; done
 03/15/23 23:00:57.348
STEP: creating a pod to probe DNS 03/15/23 23:00:57.349
STEP: submitting the pod to kubernetes 03/15/23 23:00:57.349
Mar 15 23:00:57.359: INFO: Waiting up to 15m0s for pod "dns-test-c10fc8ad-dace-4db6-a811-60bfee010c74" in namespace "dns-9036" to be "running"
Mar 15 23:00:57.364: INFO: Pod "dns-test-c10fc8ad-dace-4db6-a811-60bfee010c74": Phase="Pending", Reason="", readiness=false. Elapsed: 4.604936ms
Mar 15 23:00:59.367: INFO: Pod "dns-test-c10fc8ad-dace-4db6-a811-60bfee010c74": Phase="Running", Reason="", readiness=true. Elapsed: 2.008356424s
Mar 15 23:00:59.368: INFO: Pod "dns-test-c10fc8ad-dace-4db6-a811-60bfee010c74" satisfied condition "running"
STEP: retrieving the pod 03/15/23 23:00:59.368
STEP: looking for the results for each expected name from probers 03/15/23 23:00:59.371
Mar 15 23:00:59.378: INFO: DNS probes using dns-test-c10fc8ad-dace-4db6-a811-60bfee010c74 succeeded

STEP: deleting the pod 03/15/23 23:00:59.378
STEP: changing the externalName to bar.example.com 03/15/23 23:00:59.397
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9036.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local; sleep 1; done
 03/15/23 23:00:59.424
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9036.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local; sleep 1; done
 03/15/23 23:00:59.424
STEP: creating a second pod to probe DNS 03/15/23 23:00:59.424
STEP: submitting the pod to kubernetes 03/15/23 23:00:59.424
Mar 15 23:00:59.449: INFO: Waiting up to 15m0s for pod "dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b" in namespace "dns-9036" to be "running"
Mar 15 23:00:59.462: INFO: Pod "dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.612949ms
Mar 15 23:01:01.466: INFO: Pod "dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016956836s
Mar 15 23:01:01.466: INFO: Pod "dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b" satisfied condition "running"
STEP: retrieving the pod 03/15/23 23:01:01.466
STEP: looking for the results for each expected name from probers 03/15/23 23:01:01.497
Mar 15 23:01:01.530: INFO: File wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 23:01:01.562: INFO: File jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 23:01:01.562: INFO: Lookups using dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b failed for: [wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local]

Mar 15 23:01:06.567: INFO: File wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 23:01:06.572: INFO: File jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 23:01:06.572: INFO: Lookups using dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b failed for: [wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local]

Mar 15 23:01:11.569: INFO: File wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 23:01:11.573: INFO: File jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 23:01:11.573: INFO: Lookups using dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b failed for: [wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local]

Mar 15 23:01:16.571: INFO: File wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 23:01:16.575: INFO: File jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 23:01:16.575: INFO: Lookups using dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b failed for: [wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local]

Mar 15 23:01:21.568: INFO: File wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 23:01:21.572: INFO: File jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 23:01:21.572: INFO: Lookups using dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b failed for: [wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local]

Mar 15 23:01:26.568: INFO: File wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 23:01:26.571: INFO: File jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 23:01:26.571: INFO: Lookups using dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b failed for: [wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local]

Mar 15 23:01:31.569: INFO: DNS probes using dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b succeeded

STEP: deleting the pod 03/15/23 23:01:31.569
STEP: changing the service to type=ClusterIP 03/15/23 23:01:31.583
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9036.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local; sleep 1; done
 03/15/23 23:01:31.669
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9036.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local; sleep 1; done
 03/15/23 23:01:31.669
STEP: creating a third pod to probe DNS 03/15/23 23:01:31.669
STEP: submitting the pod to kubernetes 03/15/23 23:01:31.68
Mar 15 23:01:31.697: INFO: Waiting up to 15m0s for pod "dns-test-175f26a6-35cc-4bfb-86ee-5d296169aa9f" in namespace "dns-9036" to be "running"
Mar 15 23:01:31.708: INFO: Pod "dns-test-175f26a6-35cc-4bfb-86ee-5d296169aa9f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.280363ms
Mar 15 23:01:33.711: INFO: Pod "dns-test-175f26a6-35cc-4bfb-86ee-5d296169aa9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.014204784s
Mar 15 23:01:33.711: INFO: Pod "dns-test-175f26a6-35cc-4bfb-86ee-5d296169aa9f" satisfied condition "running"
STEP: retrieving the pod 03/15/23 23:01:33.712
STEP: looking for the results for each expected name from probers 03/15/23 23:01:33.714
Mar 15 23:01:33.723: INFO: DNS probes using dns-test-175f26a6-35cc-4bfb-86ee-5d296169aa9f succeeded

STEP: deleting the pod 03/15/23 23:01:33.723
STEP: deleting the test externalName service 03/15/23 23:01:33.737
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 15 23:01:33.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9036" for this suite. 03/15/23 23:01:33.772
------------------------------
• [SLOW TEST] [36.466 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:00:57.321
    Mar 15 23:00:57.322: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename dns 03/15/23 23:00:57.323
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:00:57.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:00:57.34
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 03/15/23 23:00:57.343
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9036.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local; sleep 1; done
     03/15/23 23:00:57.348
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9036.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local; sleep 1; done
     03/15/23 23:00:57.348
    STEP: creating a pod to probe DNS 03/15/23 23:00:57.349
    STEP: submitting the pod to kubernetes 03/15/23 23:00:57.349
    Mar 15 23:00:57.359: INFO: Waiting up to 15m0s for pod "dns-test-c10fc8ad-dace-4db6-a811-60bfee010c74" in namespace "dns-9036" to be "running"
    Mar 15 23:00:57.364: INFO: Pod "dns-test-c10fc8ad-dace-4db6-a811-60bfee010c74": Phase="Pending", Reason="", readiness=false. Elapsed: 4.604936ms
    Mar 15 23:00:59.367: INFO: Pod "dns-test-c10fc8ad-dace-4db6-a811-60bfee010c74": Phase="Running", Reason="", readiness=true. Elapsed: 2.008356424s
    Mar 15 23:00:59.368: INFO: Pod "dns-test-c10fc8ad-dace-4db6-a811-60bfee010c74" satisfied condition "running"
    STEP: retrieving the pod 03/15/23 23:00:59.368
    STEP: looking for the results for each expected name from probers 03/15/23 23:00:59.371
    Mar 15 23:00:59.378: INFO: DNS probes using dns-test-c10fc8ad-dace-4db6-a811-60bfee010c74 succeeded

    STEP: deleting the pod 03/15/23 23:00:59.378
    STEP: changing the externalName to bar.example.com 03/15/23 23:00:59.397
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9036.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local; sleep 1; done
     03/15/23 23:00:59.424
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9036.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local; sleep 1; done
     03/15/23 23:00:59.424
    STEP: creating a second pod to probe DNS 03/15/23 23:00:59.424
    STEP: submitting the pod to kubernetes 03/15/23 23:00:59.424
    Mar 15 23:00:59.449: INFO: Waiting up to 15m0s for pod "dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b" in namespace "dns-9036" to be "running"
    Mar 15 23:00:59.462: INFO: Pod "dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.612949ms
    Mar 15 23:01:01.466: INFO: Pod "dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016956836s
    Mar 15 23:01:01.466: INFO: Pod "dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b" satisfied condition "running"
    STEP: retrieving the pod 03/15/23 23:01:01.466
    STEP: looking for the results for each expected name from probers 03/15/23 23:01:01.497
    Mar 15 23:01:01.530: INFO: File wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 15 23:01:01.562: INFO: File jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 15 23:01:01.562: INFO: Lookups using dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b failed for: [wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local]

    Mar 15 23:01:06.567: INFO: File wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 15 23:01:06.572: INFO: File jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 15 23:01:06.572: INFO: Lookups using dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b failed for: [wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local]

    Mar 15 23:01:11.569: INFO: File wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 15 23:01:11.573: INFO: File jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 15 23:01:11.573: INFO: Lookups using dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b failed for: [wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local]

    Mar 15 23:01:16.571: INFO: File wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 15 23:01:16.575: INFO: File jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 15 23:01:16.575: INFO: Lookups using dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b failed for: [wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local]

    Mar 15 23:01:21.568: INFO: File wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 15 23:01:21.572: INFO: File jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 15 23:01:21.572: INFO: Lookups using dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b failed for: [wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local]

    Mar 15 23:01:26.568: INFO: File wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 15 23:01:26.571: INFO: File jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local from pod  dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 15 23:01:26.571: INFO: Lookups using dns-9036/dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b failed for: [wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local]

    Mar 15 23:01:31.569: INFO: DNS probes using dns-test-6bfeecfe-e953-4eae-8d59-1b4b208def0b succeeded

    STEP: deleting the pod 03/15/23 23:01:31.569
    STEP: changing the service to type=ClusterIP 03/15/23 23:01:31.583
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9036.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9036.svc.cluster.local; sleep 1; done
     03/15/23 23:01:31.669
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9036.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9036.svc.cluster.local; sleep 1; done
     03/15/23 23:01:31.669
    STEP: creating a third pod to probe DNS 03/15/23 23:01:31.669
    STEP: submitting the pod to kubernetes 03/15/23 23:01:31.68
    Mar 15 23:01:31.697: INFO: Waiting up to 15m0s for pod "dns-test-175f26a6-35cc-4bfb-86ee-5d296169aa9f" in namespace "dns-9036" to be "running"
    Mar 15 23:01:31.708: INFO: Pod "dns-test-175f26a6-35cc-4bfb-86ee-5d296169aa9f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.280363ms
    Mar 15 23:01:33.711: INFO: Pod "dns-test-175f26a6-35cc-4bfb-86ee-5d296169aa9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.014204784s
    Mar 15 23:01:33.711: INFO: Pod "dns-test-175f26a6-35cc-4bfb-86ee-5d296169aa9f" satisfied condition "running"
    STEP: retrieving the pod 03/15/23 23:01:33.712
    STEP: looking for the results for each expected name from probers 03/15/23 23:01:33.714
    Mar 15 23:01:33.723: INFO: DNS probes using dns-test-175f26a6-35cc-4bfb-86ee-5d296169aa9f succeeded

    STEP: deleting the pod 03/15/23 23:01:33.723
    STEP: deleting the test externalName service 03/15/23 23:01:33.737
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:01:33.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9036" for this suite. 03/15/23 23:01:33.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:01:33.797
Mar 15 23:01:33.798: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename custom-resource-definition 03/15/23 23:01:33.799
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:01:33.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:01:33.819
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Mar 15 23:01:33.824: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 23:01:34.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2313" for this suite. 03/15/23 23:01:34.378
------------------------------
• [0.596 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:01:33.797
    Mar 15 23:01:33.798: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename custom-resource-definition 03/15/23 23:01:33.799
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:01:33.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:01:33.819
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Mar 15 23:01:33.824: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:01:34.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2313" for this suite. 03/15/23 23:01:34.378
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:01:34.396
Mar 15 23:01:34.396: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename taint-single-pod 03/15/23 23:01:34.397
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:01:34.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:01:34.456
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Mar 15 23:01:34.465: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 15 23:02:34.504: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Mar 15 23:02:34.509: INFO: Starting informer...
STEP: Starting pod... 03/15/23 23:02:34.509
Mar 15 23:02:34.737: INFO: Pod is running on i-0faaf83f00b43c88c. Tainting Node
STEP: Trying to apply a taint on the Node 03/15/23 23:02:34.737
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/15/23 23:02:34.753
STEP: Waiting short time to make sure Pod is queued for deletion 03/15/23 23:02:34.763
Mar 15 23:02:34.763: INFO: Pod wasn't evicted. Proceeding
Mar 15 23:02:34.763: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/15/23 23:02:34.778
STEP: Waiting some time to make sure that toleration time passed. 03/15/23 23:02:34.789
Mar 15 23:03:49.792: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 23:03:49.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-5094" for this suite. 03/15/23 23:03:49.796
------------------------------
• [SLOW TEST] [135.406 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:01:34.396
    Mar 15 23:01:34.396: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename taint-single-pod 03/15/23 23:01:34.397
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:01:34.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:01:34.456
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Mar 15 23:01:34.465: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 15 23:02:34.504: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Mar 15 23:02:34.509: INFO: Starting informer...
    STEP: Starting pod... 03/15/23 23:02:34.509
    Mar 15 23:02:34.737: INFO: Pod is running on i-0faaf83f00b43c88c. Tainting Node
    STEP: Trying to apply a taint on the Node 03/15/23 23:02:34.737
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/15/23 23:02:34.753
    STEP: Waiting short time to make sure Pod is queued for deletion 03/15/23 23:02:34.763
    Mar 15 23:02:34.763: INFO: Pod wasn't evicted. Proceeding
    Mar 15 23:02:34.763: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/15/23 23:02:34.778
    STEP: Waiting some time to make sure that toleration time passed. 03/15/23 23:02:34.789
    Mar 15 23:03:49.792: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:03:49.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-5094" for this suite. 03/15/23 23:03:49.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:03:49.809
Mar 15 23:03:49.809: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 23:03:49.812
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:03:49.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:03:49.832
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 23:03:49.849
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 23:03:50.342
STEP: Deploying the webhook pod 03/15/23 23:03:50.349
STEP: Wait for the deployment to be ready 03/15/23 23:03:50.372
Mar 15 23:03:50.398: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 23:03:52.409
STEP: Verifying the service has paired with the endpoint 03/15/23 23:03:52.421
Mar 15 23:03:53.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/15/23 23:03:53.425
STEP: Registering slow webhook via the AdmissionRegistration API 03/15/23 23:03:53.426
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/15/23 23:03:53.442
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/15/23 23:03:54.469
STEP: Registering slow webhook via the AdmissionRegistration API 03/15/23 23:03:54.469
STEP: Having no error when timeout is longer than webhook latency 03/15/23 23:03:55.502
STEP: Registering slow webhook via the AdmissionRegistration API 03/15/23 23:03:55.502
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/15/23 23:04:00.536
STEP: Registering slow webhook via the AdmissionRegistration API 03/15/23 23:04:00.536
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 23:04:05.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4555" for this suite. 03/15/23 23:04:05.713
STEP: Destroying namespace "webhook-4555-markers" for this suite. 03/15/23 23:04:05.741
------------------------------
• [SLOW TEST] [15.982 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:03:49.809
    Mar 15 23:03:49.809: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 23:03:49.812
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:03:49.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:03:49.832
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 23:03:49.849
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 23:03:50.342
    STEP: Deploying the webhook pod 03/15/23 23:03:50.349
    STEP: Wait for the deployment to be ready 03/15/23 23:03:50.372
    Mar 15 23:03:50.398: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 23:03:52.409
    STEP: Verifying the service has paired with the endpoint 03/15/23 23:03:52.421
    Mar 15 23:03:53.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/15/23 23:03:53.425
    STEP: Registering slow webhook via the AdmissionRegistration API 03/15/23 23:03:53.426
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/15/23 23:03:53.442
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/15/23 23:03:54.469
    STEP: Registering slow webhook via the AdmissionRegistration API 03/15/23 23:03:54.469
    STEP: Having no error when timeout is longer than webhook latency 03/15/23 23:03:55.502
    STEP: Registering slow webhook via the AdmissionRegistration API 03/15/23 23:03:55.502
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/15/23 23:04:00.536
    STEP: Registering slow webhook via the AdmissionRegistration API 03/15/23 23:04:00.536
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:04:05.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4555" for this suite. 03/15/23 23:04:05.713
    STEP: Destroying namespace "webhook-4555-markers" for this suite. 03/15/23 23:04:05.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:04:05.794
Mar 15 23:04:05.794: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename sched-pred 03/15/23 23:04:05.795
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:04:05.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:04:05.889
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 15 23:04:05.893: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 15 23:04:05.915: INFO: Waiting for terminating namespaces to be deleted...
Mar 15 23:04:05.922: INFO: 
Logging pods the apiserver thinks is on node i-077ee0eb7ec5a02aa before test
Mar 15 23:04:05.941: INFO: etcd1 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.941: INFO: 	Container etcd1 ready: true, restart count 0
Mar 15 23:04:05.941: INFO: cilium-hvkmm from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.941: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 15 23:04:05.941: INFO: coredns-85dfcfb87f-p4x25 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.941: INFO: 	Container coredns ready: true, restart count 0
Mar 15 23:04:05.941: INFO: coredns-autoscaler-7cb5c5b969-tn76l from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.941: INFO: 	Container autoscaler ready: true, restart count 0
Mar 15 23:04:05.941: INFO: ebs-csi-node-9zwns from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
Mar 15 23:04:05.941: INFO: 	Container ebs-plugin ready: true, restart count 0
Mar 15 23:04:05.941: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 23:04:05.941: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 23:04:05.941: INFO: kube-proxy-i-077ee0eb7ec5a02aa from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.941: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 15 23:04:05.941: INFO: metrics-server-79b7f8b6d9-g4qd4 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.941: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 23:04:05.941: INFO: sonobuoy-e2e-job-cc297e458b6c49af from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 23:04:05.941: INFO: 	Container e2e ready: true, restart count 0
Mar 15 23:04:05.941: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 23:04:05.941: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-hkt9j from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 23:04:05.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 23:04:05.942: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 23:04:05.942: INFO: 
Logging pods the apiserver thinks is on node i-0baafb3f4e7bf826e before test
Mar 15 23:04:05.961: INFO: csi-mockplugin-85d5674684-nwzd7 from default started at 2023-03-15 21:42:36 +0000 UTC (7 container statuses recorded)
Mar 15 23:04:05.961: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 15 23:04:05.961: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 15 23:04:05.961: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 15 23:04:05.961: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 15 23:04:05.961: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 23:04:05.961: INFO: 	Container mock-driver ready: true, restart count 0
Mar 15 23:04:05.961: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 23:04:05.961: INFO: etcd2 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.961: INFO: 	Container etcd2 ready: true, restart count 0
Mar 15 23:04:05.961: INFO: web-server from default started at 2023-03-15 21:42:51 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.961: INFO: 	Container web-server ready: true, restart count 0
Mar 15 23:04:05.961: INFO: cilium-8q84b from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.961: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 15 23:04:05.962: INFO: coredns-85dfcfb87f-9x682 from kube-system started at 2023-03-15 22:21:30 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.962: INFO: 	Container coredns ready: true, restart count 0
Mar 15 23:04:05.962: INFO: ebs-csi-node-8vkhc from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
Mar 15 23:04:05.962: INFO: 	Container ebs-plugin ready: true, restart count 0
Mar 15 23:04:05.962: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 23:04:05.962: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 23:04:05.962: INFO: kube-proxy-i-0baafb3f4e7bf826e from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.962: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 15 23:04:05.962: INFO: metrics-server-79b7f8b6d9-dgk5h from kube-system started at 2023-03-15 21:41:14 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.962: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 23:04:05.962: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-49vvc from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 23:04:05.962: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 23:04:05.962: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 23:04:05.962: INFO: 
Logging pods the apiserver thinks is on node i-0faaf83f00b43c88c before test
Mar 15 23:04:05.988: INFO: cilium-nht5t from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.988: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 15 23:04:05.988: INFO: ebs-csi-node-kkqjj from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
Mar 15 23:04:05.988: INFO: 	Container ebs-plugin ready: true, restart count 0
Mar 15 23:04:05.988: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 15 23:04:05.988: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 15 23:04:05.988: INFO: kube-proxy-i-0faaf83f00b43c88c from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.988: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 15 23:04:05.988: INFO: sonobuoy from sonobuoy started at 2023-03-15 21:43:05 +0000 UTC (1 container statuses recorded)
Mar 15 23:04:05.988: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 15 23:04:05.988: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-dc6rq from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
Mar 15 23:04:05.988: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 23:04:05.988: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/15/23 23:04:05.988
Mar 15 23:04:06.011: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6582" to be "running"
Mar 15 23:04:06.020: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.762776ms
Mar 15 23:04:08.024: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013111901s
Mar 15 23:04:08.024: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/15/23 23:04:08.028
STEP: Trying to apply a random label on the found node. 03/15/23 23:04:08.052
STEP: verifying the node has the label kubernetes.io/e2e-aa49009e-7f70-465f-9703-cce6d8b4f991 95 03/15/23 23:04:08.075
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/15/23 23:04:08.084
Mar 15 23:04:08.089: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6582" to be "not pending"
Mar 15 23:04:08.097: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.962166ms
Mar 15 23:04:10.101: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011602948s
Mar 15 23:04:10.101: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.20.126.23 on the node which pod4 resides and expect not scheduled 03/15/23 23:04:10.101
Mar 15 23:04:10.109: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6582" to be "not pending"
Mar 15 23:04:10.117: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.12712ms
Mar 15 23:04:12.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012256945s
Mar 15 23:04:14.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012496946s
Mar 15 23:04:16.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012564785s
Mar 15 23:04:18.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012010941s
Mar 15 23:04:20.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01372469s
Mar 15 23:04:22.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014033496s
Mar 15 23:04:24.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01160072s
Mar 15 23:04:26.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.012942698s
Mar 15 23:04:28.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012776743s
Mar 15 23:04:30.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018124749s
Mar 15 23:04:32.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011678207s
Mar 15 23:04:34.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01189893s
Mar 15 23:04:36.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.013166042s
Mar 15 23:04:38.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.012450873s
Mar 15 23:04:40.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.01182548s
Mar 15 23:04:42.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.012513247s
Mar 15 23:04:44.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011423035s
Mar 15 23:04:46.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012970004s
Mar 15 23:04:48.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.013027319s
Mar 15 23:04:50.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.018045816s
Mar 15 23:04:52.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.011725982s
Mar 15 23:04:54.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.012884136s
Mar 15 23:04:56.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.014594988s
Mar 15 23:04:58.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.012659263s
Mar 15 23:05:00.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011787702s
Mar 15 23:05:02.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011827386s
Mar 15 23:05:04.126: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.017053571s
Mar 15 23:05:06.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015198623s
Mar 15 23:05:08.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.013175947s
Mar 15 23:05:10.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.013501836s
Mar 15 23:05:12.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.016325407s
Mar 15 23:05:14.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.012246913s
Mar 15 23:05:16.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012975747s
Mar 15 23:05:18.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.012189481s
Mar 15 23:05:20.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011688489s
Mar 15 23:05:22.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.024372575s
Mar 15 23:05:24.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.013688963s
Mar 15 23:05:26.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013477646s
Mar 15 23:05:28.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011426497s
Mar 15 23:05:30.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012927468s
Mar 15 23:05:32.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01251376s
Mar 15 23:05:34.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012805889s
Mar 15 23:05:36.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.011976259s
Mar 15 23:05:38.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.013792581s
Mar 15 23:05:40.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.015085124s
Mar 15 23:05:42.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.016749542s
Mar 15 23:05:44.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.013995671s
Mar 15 23:05:46.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.015379908s
Mar 15 23:05:48.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013609435s
Mar 15 23:05:50.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.015061764s
Mar 15 23:05:52.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012974796s
Mar 15 23:05:54.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012549239s
Mar 15 23:05:56.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013609312s
Mar 15 23:05:58.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011872246s
Mar 15 23:06:00.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013599811s
Mar 15 23:06:02.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.034018685s
Mar 15 23:06:04.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011454322s
Mar 15 23:06:06.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.013486004s
Mar 15 23:06:08.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.011985973s
Mar 15 23:06:10.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013429484s
Mar 15 23:06:12.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.016088631s
Mar 15 23:06:14.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.013056446s
Mar 15 23:06:16.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.022583145s
Mar 15 23:06:18.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.016652619s
Mar 15 23:06:20.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.011564069s
Mar 15 23:06:22.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.012197196s
Mar 15 23:06:24.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.012253477s
Mar 15 23:06:26.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.013141537s
Mar 15 23:06:28.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.011803901s
Mar 15 23:06:30.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.012992494s
Mar 15 23:06:32.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.012478546s
Mar 15 23:06:34.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.011292586s
Mar 15 23:06:36.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.01151815s
Mar 15 23:06:38.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.011510742s
Mar 15 23:06:40.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.012291594s
Mar 15 23:06:42.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.019320967s
Mar 15 23:06:44.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.012365655s
Mar 15 23:06:46.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.017926111s
Mar 15 23:06:48.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.011765741s
Mar 15 23:06:50.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.012767857s
Mar 15 23:06:52.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.014192803s
Mar 15 23:06:54.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.012978486s
Mar 15 23:06:56.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.012111722s
Mar 15 23:06:58.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.018519757s
Mar 15 23:07:00.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.013273254s
Mar 15 23:07:02.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.018900272s
Mar 15 23:07:04.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.018083053s
Mar 15 23:07:06.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.012985471s
Mar 15 23:07:08.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.012073158s
Mar 15 23:07:10.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.012049755s
Mar 15 23:07:12.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.015197862s
Mar 15 23:07:14.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.011788992s
Mar 15 23:07:16.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.014610057s
Mar 15 23:07:18.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.011208287s
Mar 15 23:07:20.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.01170636s
Mar 15 23:07:22.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.016455806s
Mar 15 23:07:24.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.01374829s
Mar 15 23:07:26.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.011446846s
Mar 15 23:07:28.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.021640624s
Mar 15 23:07:30.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.011581376s
Mar 15 23:07:32.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.012458917s
Mar 15 23:07:34.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.01343056s
Mar 15 23:07:36.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.011517585s
Mar 15 23:07:38.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.013264287s
Mar 15 23:07:40.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.011331557s
Mar 15 23:07:42.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.011634308s
Mar 15 23:07:44.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.012900514s
Mar 15 23:07:46.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.011564516s
Mar 15 23:07:48.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.016044972s
Mar 15 23:07:50.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.012697025s
Mar 15 23:07:52.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.013081315s
Mar 15 23:07:54.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.011649636s
Mar 15 23:07:56.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.012698112s
Mar 15 23:07:58.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.012637277s
Mar 15 23:08:00.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.011816751s
Mar 15 23:08:02.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.014296048s
Mar 15 23:08:04.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.011944734s
Mar 15 23:08:06.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.013161029s
Mar 15 23:08:08.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.011830535s
Mar 15 23:08:10.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.011871241s
Mar 15 23:08:12.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.012105507s
Mar 15 23:08:14.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.013499601s
Mar 15 23:08:16.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.013026971s
Mar 15 23:08:18.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.013347789s
Mar 15 23:08:20.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.012948778s
Mar 15 23:08:22.126: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.017581117s
Mar 15 23:08:24.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.01172261s
Mar 15 23:08:26.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.012638721s
Mar 15 23:08:28.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.012235203s
Mar 15 23:08:30.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.011800707s
Mar 15 23:08:32.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.014862644s
Mar 15 23:08:34.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.011565545s
Mar 15 23:08:36.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.012450543s
Mar 15 23:08:38.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.0114265s
Mar 15 23:08:40.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.013981797s
Mar 15 23:08:42.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.011434614s
Mar 15 23:08:44.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.013146229s
Mar 15 23:08:46.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.013287603s
Mar 15 23:08:48.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.022512801s
Mar 15 23:08:50.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.01226168s
Mar 15 23:08:52.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.012597958s
Mar 15 23:08:54.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.011329035s
Mar 15 23:08:56.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.012730043s
Mar 15 23:08:58.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.012190389s
Mar 15 23:09:00.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.019659601s
Mar 15 23:09:02.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.011869412s
Mar 15 23:09:04.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.012728244s
Mar 15 23:09:06.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013009674s
Mar 15 23:09:08.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012053315s
Mar 15 23:09:10.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.013343427s
Mar 15 23:09:10.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.016036353s
STEP: removing the label kubernetes.io/e2e-aa49009e-7f70-465f-9703-cce6d8b4f991 off the node i-0faaf83f00b43c88c 03/15/23 23:09:10.125
STEP: verifying the node doesn't have the label kubernetes.io/e2e-aa49009e-7f70-465f-9703-cce6d8b4f991 03/15/23 23:09:10.143
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 15 23:09:10.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6582" for this suite. 03/15/23 23:09:10.166
------------------------------
• [SLOW TEST] [304.380 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:04:05.794
    Mar 15 23:04:05.794: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename sched-pred 03/15/23 23:04:05.795
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:04:05.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:04:05.889
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 15 23:04:05.893: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 15 23:04:05.915: INFO: Waiting for terminating namespaces to be deleted...
    Mar 15 23:04:05.922: INFO: 
    Logging pods the apiserver thinks is on node i-077ee0eb7ec5a02aa before test
    Mar 15 23:04:05.941: INFO: etcd1 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.941: INFO: 	Container etcd1 ready: true, restart count 0
    Mar 15 23:04:05.941: INFO: cilium-hvkmm from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.941: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 15 23:04:05.941: INFO: coredns-85dfcfb87f-p4x25 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.941: INFO: 	Container coredns ready: true, restart count 0
    Mar 15 23:04:05.941: INFO: coredns-autoscaler-7cb5c5b969-tn76l from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.941: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 15 23:04:05.941: INFO: ebs-csi-node-9zwns from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
    Mar 15 23:04:05.941: INFO: 	Container ebs-plugin ready: true, restart count 0
    Mar 15 23:04:05.941: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 23:04:05.941: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 23:04:05.941: INFO: kube-proxy-i-077ee0eb7ec5a02aa from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.941: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 15 23:04:05.941: INFO: metrics-server-79b7f8b6d9-g4qd4 from kube-system started at 2023-03-15 21:41:11 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.941: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 15 23:04:05.941: INFO: sonobuoy-e2e-job-cc297e458b6c49af from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 23:04:05.941: INFO: 	Container e2e ready: true, restart count 0
    Mar 15 23:04:05.941: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 23:04:05.941: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-hkt9j from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 23:04:05.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 23:04:05.942: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 15 23:04:05.942: INFO: 
    Logging pods the apiserver thinks is on node i-0baafb3f4e7bf826e before test
    Mar 15 23:04:05.961: INFO: csi-mockplugin-85d5674684-nwzd7 from default started at 2023-03-15 21:42:36 +0000 UTC (7 container statuses recorded)
    Mar 15 23:04:05.961: INFO: 	Container csi-attacher ready: true, restart count 0
    Mar 15 23:04:05.961: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar 15 23:04:05.961: INFO: 	Container csi-resizer ready: true, restart count 0
    Mar 15 23:04:05.961: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Mar 15 23:04:05.961: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 23:04:05.961: INFO: 	Container mock-driver ready: true, restart count 0
    Mar 15 23:04:05.961: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 23:04:05.961: INFO: etcd2 from default started at 2023-03-15 21:42:59 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.961: INFO: 	Container etcd2 ready: true, restart count 0
    Mar 15 23:04:05.961: INFO: web-server from default started at 2023-03-15 21:42:51 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.961: INFO: 	Container web-server ready: true, restart count 0
    Mar 15 23:04:05.961: INFO: cilium-8q84b from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.961: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 15 23:04:05.962: INFO: coredns-85dfcfb87f-9x682 from kube-system started at 2023-03-15 22:21:30 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.962: INFO: 	Container coredns ready: true, restart count 0
    Mar 15 23:04:05.962: INFO: ebs-csi-node-8vkhc from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
    Mar 15 23:04:05.962: INFO: 	Container ebs-plugin ready: true, restart count 0
    Mar 15 23:04:05.962: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 23:04:05.962: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 23:04:05.962: INFO: kube-proxy-i-0baafb3f4e7bf826e from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.962: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 15 23:04:05.962: INFO: metrics-server-79b7f8b6d9-dgk5h from kube-system started at 2023-03-15 21:41:14 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.962: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 15 23:04:05.962: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-49vvc from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 23:04:05.962: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 23:04:05.962: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 15 23:04:05.962: INFO: 
    Logging pods the apiserver thinks is on node i-0faaf83f00b43c88c before test
    Mar 15 23:04:05.988: INFO: cilium-nht5t from kube-system started at 2023-03-15 21:41:01 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.988: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 15 23:04:05.988: INFO: ebs-csi-node-kkqjj from kube-system started at 2023-03-15 21:41:01 +0000 UTC (3 container statuses recorded)
    Mar 15 23:04:05.988: INFO: 	Container ebs-plugin ready: true, restart count 0
    Mar 15 23:04:05.988: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 15 23:04:05.988: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 15 23:04:05.988: INFO: kube-proxy-i-0faaf83f00b43c88c from kube-system started at 2023-03-15 21:41:00 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.988: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 15 23:04:05.988: INFO: sonobuoy from sonobuoy started at 2023-03-15 21:43:05 +0000 UTC (1 container statuses recorded)
    Mar 15 23:04:05.988: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 15 23:04:05.988: INFO: sonobuoy-systemd-logs-daemon-set-33374ecb2340468f-dc6rq from sonobuoy started at 2023-03-15 21:43:08 +0000 UTC (2 container statuses recorded)
    Mar 15 23:04:05.988: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 15 23:04:05.988: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/15/23 23:04:05.988
    Mar 15 23:04:06.011: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6582" to be "running"
    Mar 15 23:04:06.020: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.762776ms
    Mar 15 23:04:08.024: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013111901s
    Mar 15 23:04:08.024: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/15/23 23:04:08.028
    STEP: Trying to apply a random label on the found node. 03/15/23 23:04:08.052
    STEP: verifying the node has the label kubernetes.io/e2e-aa49009e-7f70-465f-9703-cce6d8b4f991 95 03/15/23 23:04:08.075
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/15/23 23:04:08.084
    Mar 15 23:04:08.089: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6582" to be "not pending"
    Mar 15 23:04:08.097: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.962166ms
    Mar 15 23:04:10.101: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011602948s
    Mar 15 23:04:10.101: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.20.126.23 on the node which pod4 resides and expect not scheduled 03/15/23 23:04:10.101
    Mar 15 23:04:10.109: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6582" to be "not pending"
    Mar 15 23:04:10.117: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.12712ms
    Mar 15 23:04:12.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012256945s
    Mar 15 23:04:14.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012496946s
    Mar 15 23:04:16.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012564785s
    Mar 15 23:04:18.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012010941s
    Mar 15 23:04:20.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01372469s
    Mar 15 23:04:22.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014033496s
    Mar 15 23:04:24.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01160072s
    Mar 15 23:04:26.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.012942698s
    Mar 15 23:04:28.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012776743s
    Mar 15 23:04:30.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018124749s
    Mar 15 23:04:32.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011678207s
    Mar 15 23:04:34.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01189893s
    Mar 15 23:04:36.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.013166042s
    Mar 15 23:04:38.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.012450873s
    Mar 15 23:04:40.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.01182548s
    Mar 15 23:04:42.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.012513247s
    Mar 15 23:04:44.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011423035s
    Mar 15 23:04:46.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012970004s
    Mar 15 23:04:48.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.013027319s
    Mar 15 23:04:50.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.018045816s
    Mar 15 23:04:52.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.011725982s
    Mar 15 23:04:54.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.012884136s
    Mar 15 23:04:56.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.014594988s
    Mar 15 23:04:58.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.012659263s
    Mar 15 23:05:00.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011787702s
    Mar 15 23:05:02.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011827386s
    Mar 15 23:05:04.126: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.017053571s
    Mar 15 23:05:06.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015198623s
    Mar 15 23:05:08.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.013175947s
    Mar 15 23:05:10.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.013501836s
    Mar 15 23:05:12.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.016325407s
    Mar 15 23:05:14.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.012246913s
    Mar 15 23:05:16.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012975747s
    Mar 15 23:05:18.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.012189481s
    Mar 15 23:05:20.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011688489s
    Mar 15 23:05:22.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.024372575s
    Mar 15 23:05:24.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.013688963s
    Mar 15 23:05:26.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013477646s
    Mar 15 23:05:28.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011426497s
    Mar 15 23:05:30.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012927468s
    Mar 15 23:05:32.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01251376s
    Mar 15 23:05:34.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012805889s
    Mar 15 23:05:36.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.011976259s
    Mar 15 23:05:38.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.013792581s
    Mar 15 23:05:40.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.015085124s
    Mar 15 23:05:42.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.016749542s
    Mar 15 23:05:44.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.013995671s
    Mar 15 23:05:46.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.015379908s
    Mar 15 23:05:48.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013609435s
    Mar 15 23:05:50.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.015061764s
    Mar 15 23:05:52.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012974796s
    Mar 15 23:05:54.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012549239s
    Mar 15 23:05:56.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013609312s
    Mar 15 23:05:58.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011872246s
    Mar 15 23:06:00.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013599811s
    Mar 15 23:06:02.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.034018685s
    Mar 15 23:06:04.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011454322s
    Mar 15 23:06:06.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.013486004s
    Mar 15 23:06:08.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.011985973s
    Mar 15 23:06:10.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013429484s
    Mar 15 23:06:12.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.016088631s
    Mar 15 23:06:14.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.013056446s
    Mar 15 23:06:16.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.022583145s
    Mar 15 23:06:18.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.016652619s
    Mar 15 23:06:20.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.011564069s
    Mar 15 23:06:22.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.012197196s
    Mar 15 23:06:24.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.012253477s
    Mar 15 23:06:26.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.013141537s
    Mar 15 23:06:28.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.011803901s
    Mar 15 23:06:30.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.012992494s
    Mar 15 23:06:32.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.012478546s
    Mar 15 23:06:34.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.011292586s
    Mar 15 23:06:36.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.01151815s
    Mar 15 23:06:38.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.011510742s
    Mar 15 23:06:40.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.012291594s
    Mar 15 23:06:42.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.019320967s
    Mar 15 23:06:44.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.012365655s
    Mar 15 23:06:46.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.017926111s
    Mar 15 23:06:48.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.011765741s
    Mar 15 23:06:50.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.012767857s
    Mar 15 23:06:52.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.014192803s
    Mar 15 23:06:54.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.012978486s
    Mar 15 23:06:56.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.012111722s
    Mar 15 23:06:58.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.018519757s
    Mar 15 23:07:00.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.013273254s
    Mar 15 23:07:02.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.018900272s
    Mar 15 23:07:04.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.018083053s
    Mar 15 23:07:06.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.012985471s
    Mar 15 23:07:08.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.012073158s
    Mar 15 23:07:10.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.012049755s
    Mar 15 23:07:12.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.015197862s
    Mar 15 23:07:14.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.011788992s
    Mar 15 23:07:16.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.014610057s
    Mar 15 23:07:18.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.011208287s
    Mar 15 23:07:20.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.01170636s
    Mar 15 23:07:22.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.016455806s
    Mar 15 23:07:24.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.01374829s
    Mar 15 23:07:26.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.011446846s
    Mar 15 23:07:28.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.021640624s
    Mar 15 23:07:30.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.011581376s
    Mar 15 23:07:32.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.012458917s
    Mar 15 23:07:34.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.01343056s
    Mar 15 23:07:36.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.011517585s
    Mar 15 23:07:38.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.013264287s
    Mar 15 23:07:40.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.011331557s
    Mar 15 23:07:42.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.011634308s
    Mar 15 23:07:44.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.012900514s
    Mar 15 23:07:46.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.011564516s
    Mar 15 23:07:48.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.016044972s
    Mar 15 23:07:50.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.012697025s
    Mar 15 23:07:52.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.013081315s
    Mar 15 23:07:54.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.011649636s
    Mar 15 23:07:56.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.012698112s
    Mar 15 23:07:58.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.012637277s
    Mar 15 23:08:00.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.011816751s
    Mar 15 23:08:02.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.014296048s
    Mar 15 23:08:04.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.011944734s
    Mar 15 23:08:06.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.013161029s
    Mar 15 23:08:08.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.011830535s
    Mar 15 23:08:10.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.011871241s
    Mar 15 23:08:12.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.012105507s
    Mar 15 23:08:14.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.013499601s
    Mar 15 23:08:16.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.013026971s
    Mar 15 23:08:18.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.013347789s
    Mar 15 23:08:20.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.012948778s
    Mar 15 23:08:22.126: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.017581117s
    Mar 15 23:08:24.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.01172261s
    Mar 15 23:08:26.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.012638721s
    Mar 15 23:08:28.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.012235203s
    Mar 15 23:08:30.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.011800707s
    Mar 15 23:08:32.124: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.014862644s
    Mar 15 23:08:34.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.011565545s
    Mar 15 23:08:36.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.012450543s
    Mar 15 23:08:38.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.0114265s
    Mar 15 23:08:40.123: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.013981797s
    Mar 15 23:08:42.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.011434614s
    Mar 15 23:08:44.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.013146229s
    Mar 15 23:08:46.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.013287603s
    Mar 15 23:08:48.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.022512801s
    Mar 15 23:08:50.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.01226168s
    Mar 15 23:08:52.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.012597958s
    Mar 15 23:08:54.120: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.011329035s
    Mar 15 23:08:56.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.012730043s
    Mar 15 23:08:58.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.012190389s
    Mar 15 23:09:00.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.019659601s
    Mar 15 23:09:02.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.011869412s
    Mar 15 23:09:04.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.012728244s
    Mar 15 23:09:06.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013009674s
    Mar 15 23:09:08.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012053315s
    Mar 15 23:09:10.122: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.013343427s
    Mar 15 23:09:10.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.016036353s
    STEP: removing the label kubernetes.io/e2e-aa49009e-7f70-465f-9703-cce6d8b4f991 off the node i-0faaf83f00b43c88c 03/15/23 23:09:10.125
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-aa49009e-7f70-465f-9703-cce6d8b4f991 03/15/23 23:09:10.143
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:09:10.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6582" for this suite. 03/15/23 23:09:10.166
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:09:10.175
Mar 15 23:09:10.175: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename container-lifecycle-hook 03/15/23 23:09:10.176
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:09:10.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:09:10.196
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/15/23 23:09:10.203
Mar 15 23:09:10.225: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7607" to be "running and ready"
Mar 15 23:09:10.229: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.327985ms
Mar 15 23:09:10.229: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 15 23:09:12.232: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006825593s
Mar 15 23:09:12.232: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 15 23:09:12.232: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 03/15/23 23:09:12.236
Mar 15 23:09:12.243: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7607" to be "running and ready"
Mar 15 23:09:12.247: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.826715ms
Mar 15 23:09:12.247: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 15 23:09:14.261: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018141488s
Mar 15 23:09:14.261: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Mar 15 23:09:14.261: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/15/23 23:09:14.264
Mar 15 23:09:14.280: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 15 23:09:14.284: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 15 23:09:16.285: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 15 23:09:16.289: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 15 23:09:18.284: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 15 23:09:18.288: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 03/15/23 23:09:18.288
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 15 23:09:18.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7607" for this suite. 03/15/23 23:09:18.313
------------------------------
• [SLOW TEST] [8.144 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:09:10.175
    Mar 15 23:09:10.175: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/15/23 23:09:10.176
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:09:10.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:09:10.196
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/15/23 23:09:10.203
    Mar 15 23:09:10.225: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7607" to be "running and ready"
    Mar 15 23:09:10.229: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.327985ms
    Mar 15 23:09:10.229: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 23:09:12.232: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006825593s
    Mar 15 23:09:12.232: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 15 23:09:12.232: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 03/15/23 23:09:12.236
    Mar 15 23:09:12.243: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7607" to be "running and ready"
    Mar 15 23:09:12.247: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.826715ms
    Mar 15 23:09:12.247: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 15 23:09:14.261: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018141488s
    Mar 15 23:09:14.261: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Mar 15 23:09:14.261: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/15/23 23:09:14.264
    Mar 15 23:09:14.280: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 15 23:09:14.284: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 15 23:09:16.285: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 15 23:09:16.289: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 15 23:09:18.284: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 15 23:09:18.288: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 03/15/23 23:09:18.288
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:09:18.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7607" for this suite. 03/15/23 23:09:18.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:09:18.32
Mar 15 23:09:18.320: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename kubectl 03/15/23 23:09:18.321
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:09:18.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:09:18.336
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 03/15/23 23:09:18.34
Mar 15 23:09:18.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 create -f -'
Mar 15 23:09:19.116: INFO: stderr: ""
Mar 15 23:09:19.116: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/15/23 23:09:19.116
Mar 15 23:09:19.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 23:09:19.207: INFO: stderr: ""
Mar 15 23:09:19.207: INFO: stdout: "update-demo-nautilus-jwngs update-demo-nautilus-phz66 "
Mar 15 23:09:19.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-jwngs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 23:09:19.282: INFO: stderr: ""
Mar 15 23:09:19.283: INFO: stdout: ""
Mar 15 23:09:19.283: INFO: update-demo-nautilus-jwngs is created but not running
Mar 15 23:09:24.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 23:09:24.373: INFO: stderr: ""
Mar 15 23:09:24.373: INFO: stdout: "update-demo-nautilus-jwngs update-demo-nautilus-phz66 "
Mar 15 23:09:24.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-jwngs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 23:09:24.463: INFO: stderr: ""
Mar 15 23:09:24.463: INFO: stdout: "true"
Mar 15 23:09:24.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-jwngs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 23:09:24.550: INFO: stderr: ""
Mar 15 23:09:24.550: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 15 23:09:24.550: INFO: validating pod update-demo-nautilus-jwngs
Mar 15 23:09:24.555: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 23:09:24.555: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 23:09:24.555: INFO: update-demo-nautilus-jwngs is verified up and running
Mar 15 23:09:24.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 23:09:24.679: INFO: stderr: ""
Mar 15 23:09:24.679: INFO: stdout: "true"
Mar 15 23:09:24.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 23:09:24.782: INFO: stderr: ""
Mar 15 23:09:24.782: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 15 23:09:24.782: INFO: validating pod update-demo-nautilus-phz66
Mar 15 23:09:24.787: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 23:09:24.787: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 23:09:24.787: INFO: update-demo-nautilus-phz66 is verified up and running
STEP: scaling down the replication controller 03/15/23 23:09:24.787
Mar 15 23:09:24.789: INFO: scanned /root for discovery docs: <nil>
Mar 15 23:09:24.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 15 23:09:25.879: INFO: stderr: ""
Mar 15 23:09:25.879: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/15/23 23:09:25.879
Mar 15 23:09:25.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 23:09:25.967: INFO: stderr: ""
Mar 15 23:09:25.967: INFO: stdout: "update-demo-nautilus-phz66 "
Mar 15 23:09:25.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 23:09:26.053: INFO: stderr: ""
Mar 15 23:09:26.053: INFO: stdout: "true"
Mar 15 23:09:26.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 23:09:26.141: INFO: stderr: ""
Mar 15 23:09:26.141: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 15 23:09:26.141: INFO: validating pod update-demo-nautilus-phz66
Mar 15 23:09:26.145: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 23:09:26.145: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 23:09:26.145: INFO: update-demo-nautilus-phz66 is verified up and running
STEP: scaling up the replication controller 03/15/23 23:09:26.145
Mar 15 23:09:26.146: INFO: scanned /root for discovery docs: <nil>
Mar 15 23:09:26.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 15 23:09:27.250: INFO: stderr: ""
Mar 15 23:09:27.250: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/15/23 23:09:27.25
Mar 15 23:09:27.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 23:09:27.332: INFO: stderr: ""
Mar 15 23:09:27.333: INFO: stdout: "update-demo-nautilus-phz66 update-demo-nautilus-pw27n "
Mar 15 23:09:27.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 23:09:27.426: INFO: stderr: ""
Mar 15 23:09:27.426: INFO: stdout: "true"
Mar 15 23:09:27.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 23:09:27.505: INFO: stderr: ""
Mar 15 23:09:27.505: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 15 23:09:27.505: INFO: validating pod update-demo-nautilus-phz66
Mar 15 23:09:27.517: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 23:09:27.517: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 23:09:27.517: INFO: update-demo-nautilus-phz66 is verified up and running
Mar 15 23:09:27.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-pw27n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 23:09:27.588: INFO: stderr: ""
Mar 15 23:09:27.589: INFO: stdout: ""
Mar 15 23:09:27.589: INFO: update-demo-nautilus-pw27n is created but not running
Mar 15 23:09:32.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 23:09:32.741: INFO: stderr: ""
Mar 15 23:09:32.741: INFO: stdout: "update-demo-nautilus-phz66 update-demo-nautilus-pw27n "
Mar 15 23:09:32.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 23:09:32.867: INFO: stderr: ""
Mar 15 23:09:32.867: INFO: stdout: "true"
Mar 15 23:09:32.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 23:09:32.955: INFO: stderr: ""
Mar 15 23:09:32.955: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 15 23:09:32.955: INFO: validating pod update-demo-nautilus-phz66
Mar 15 23:09:32.964: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 23:09:32.964: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 23:09:32.964: INFO: update-demo-nautilus-phz66 is verified up and running
Mar 15 23:09:32.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-pw27n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 23:09:33.036: INFO: stderr: ""
Mar 15 23:09:33.036: INFO: stdout: "true"
Mar 15 23:09:33.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-pw27n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 23:09:33.106: INFO: stderr: ""
Mar 15 23:09:33.106: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 15 23:09:33.106: INFO: validating pod update-demo-nautilus-pw27n
Mar 15 23:09:33.110: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 23:09:33.110: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 23:09:33.110: INFO: update-demo-nautilus-pw27n is verified up and running
STEP: using delete to clean up resources 03/15/23 23:09:33.11
Mar 15 23:09:33.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 delete --grace-period=0 --force -f -'
Mar 15 23:09:33.181: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 23:09:33.181: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 15 23:09:33.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get rc,svc -l name=update-demo --no-headers'
Mar 15 23:09:33.283: INFO: stderr: "No resources found in kubectl-7996 namespace.\n"
Mar 15 23:09:33.283: INFO: stdout: ""
Mar 15 23:09:33.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 15 23:09:33.367: INFO: stderr: ""
Mar 15 23:09:33.367: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 15 23:09:33.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7996" for this suite. 03/15/23 23:09:33.371
------------------------------
• [SLOW TEST] [15.056 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:09:18.32
    Mar 15 23:09:18.320: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename kubectl 03/15/23 23:09:18.321
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:09:18.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:09:18.336
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 03/15/23 23:09:18.34
    Mar 15 23:09:18.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 create -f -'
    Mar 15 23:09:19.116: INFO: stderr: ""
    Mar 15 23:09:19.116: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/15/23 23:09:19.116
    Mar 15 23:09:19.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 15 23:09:19.207: INFO: stderr: ""
    Mar 15 23:09:19.207: INFO: stdout: "update-demo-nautilus-jwngs update-demo-nautilus-phz66 "
    Mar 15 23:09:19.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-jwngs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 15 23:09:19.282: INFO: stderr: ""
    Mar 15 23:09:19.283: INFO: stdout: ""
    Mar 15 23:09:19.283: INFO: update-demo-nautilus-jwngs is created but not running
    Mar 15 23:09:24.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 15 23:09:24.373: INFO: stderr: ""
    Mar 15 23:09:24.373: INFO: stdout: "update-demo-nautilus-jwngs update-demo-nautilus-phz66 "
    Mar 15 23:09:24.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-jwngs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 15 23:09:24.463: INFO: stderr: ""
    Mar 15 23:09:24.463: INFO: stdout: "true"
    Mar 15 23:09:24.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-jwngs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 15 23:09:24.550: INFO: stderr: ""
    Mar 15 23:09:24.550: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 15 23:09:24.550: INFO: validating pod update-demo-nautilus-jwngs
    Mar 15 23:09:24.555: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 15 23:09:24.555: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 15 23:09:24.555: INFO: update-demo-nautilus-jwngs is verified up and running
    Mar 15 23:09:24.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 15 23:09:24.679: INFO: stderr: ""
    Mar 15 23:09:24.679: INFO: stdout: "true"
    Mar 15 23:09:24.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 15 23:09:24.782: INFO: stderr: ""
    Mar 15 23:09:24.782: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 15 23:09:24.782: INFO: validating pod update-demo-nautilus-phz66
    Mar 15 23:09:24.787: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 15 23:09:24.787: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 15 23:09:24.787: INFO: update-demo-nautilus-phz66 is verified up and running
    STEP: scaling down the replication controller 03/15/23 23:09:24.787
    Mar 15 23:09:24.789: INFO: scanned /root for discovery docs: <nil>
    Mar 15 23:09:24.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Mar 15 23:09:25.879: INFO: stderr: ""
    Mar 15 23:09:25.879: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/15/23 23:09:25.879
    Mar 15 23:09:25.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 15 23:09:25.967: INFO: stderr: ""
    Mar 15 23:09:25.967: INFO: stdout: "update-demo-nautilus-phz66 "
    Mar 15 23:09:25.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 15 23:09:26.053: INFO: stderr: ""
    Mar 15 23:09:26.053: INFO: stdout: "true"
    Mar 15 23:09:26.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 15 23:09:26.141: INFO: stderr: ""
    Mar 15 23:09:26.141: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 15 23:09:26.141: INFO: validating pod update-demo-nautilus-phz66
    Mar 15 23:09:26.145: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 15 23:09:26.145: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 15 23:09:26.145: INFO: update-demo-nautilus-phz66 is verified up and running
    STEP: scaling up the replication controller 03/15/23 23:09:26.145
    Mar 15 23:09:26.146: INFO: scanned /root for discovery docs: <nil>
    Mar 15 23:09:26.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Mar 15 23:09:27.250: INFO: stderr: ""
    Mar 15 23:09:27.250: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/15/23 23:09:27.25
    Mar 15 23:09:27.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 15 23:09:27.332: INFO: stderr: ""
    Mar 15 23:09:27.333: INFO: stdout: "update-demo-nautilus-phz66 update-demo-nautilus-pw27n "
    Mar 15 23:09:27.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 15 23:09:27.426: INFO: stderr: ""
    Mar 15 23:09:27.426: INFO: stdout: "true"
    Mar 15 23:09:27.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 15 23:09:27.505: INFO: stderr: ""
    Mar 15 23:09:27.505: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 15 23:09:27.505: INFO: validating pod update-demo-nautilus-phz66
    Mar 15 23:09:27.517: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 15 23:09:27.517: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 15 23:09:27.517: INFO: update-demo-nautilus-phz66 is verified up and running
    Mar 15 23:09:27.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-pw27n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 15 23:09:27.588: INFO: stderr: ""
    Mar 15 23:09:27.589: INFO: stdout: ""
    Mar 15 23:09:27.589: INFO: update-demo-nautilus-pw27n is created but not running
    Mar 15 23:09:32.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 15 23:09:32.741: INFO: stderr: ""
    Mar 15 23:09:32.741: INFO: stdout: "update-demo-nautilus-phz66 update-demo-nautilus-pw27n "
    Mar 15 23:09:32.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 15 23:09:32.867: INFO: stderr: ""
    Mar 15 23:09:32.867: INFO: stdout: "true"
    Mar 15 23:09:32.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-phz66 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 15 23:09:32.955: INFO: stderr: ""
    Mar 15 23:09:32.955: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 15 23:09:32.955: INFO: validating pod update-demo-nautilus-phz66
    Mar 15 23:09:32.964: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 15 23:09:32.964: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 15 23:09:32.964: INFO: update-demo-nautilus-phz66 is verified up and running
    Mar 15 23:09:32.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-pw27n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 15 23:09:33.036: INFO: stderr: ""
    Mar 15 23:09:33.036: INFO: stdout: "true"
    Mar 15 23:09:33.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods update-demo-nautilus-pw27n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 15 23:09:33.106: INFO: stderr: ""
    Mar 15 23:09:33.106: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 15 23:09:33.106: INFO: validating pod update-demo-nautilus-pw27n
    Mar 15 23:09:33.110: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 15 23:09:33.110: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 15 23:09:33.110: INFO: update-demo-nautilus-pw27n is verified up and running
    STEP: using delete to clean up resources 03/15/23 23:09:33.11
    Mar 15 23:09:33.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 delete --grace-period=0 --force -f -'
    Mar 15 23:09:33.181: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 15 23:09:33.181: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 15 23:09:33.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get rc,svc -l name=update-demo --no-headers'
    Mar 15 23:09:33.283: INFO: stderr: "No resources found in kubectl-7996 namespace.\n"
    Mar 15 23:09:33.283: INFO: stdout: ""
    Mar 15 23:09:33.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=kubectl-7996 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 15 23:09:33.367: INFO: stderr: ""
    Mar 15 23:09:33.367: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:09:33.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7996" for this suite. 03/15/23 23:09:33.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:09:33.378
Mar 15 23:09:33.378: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 23:09:33.379
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:09:33.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:09:33.394
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 23:09:33.421
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 23:09:33.874
STEP: Deploying the webhook pod 03/15/23 23:09:33.881
STEP: Wait for the deployment to be ready 03/15/23 23:09:33.893
Mar 15 23:09:33.904: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 23:09:35.919
STEP: Verifying the service has paired with the endpoint 03/15/23 23:09:35.932
Mar 15 23:09:36.934: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Mar 15 23:09:36.937: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7484-crds.webhook.example.com via the AdmissionRegistration API 03/15/23 23:09:37.451
STEP: Creating a custom resource while v1 is storage version 03/15/23 23:09:37.468
STEP: Patching Custom Resource Definition to set v2 as storage 03/15/23 23:09:39.52
STEP: Patching the custom resource while v2 is storage version 03/15/23 23:09:39.528
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 23:09:40.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3051" for this suite. 03/15/23 23:09:40.227
STEP: Destroying namespace "webhook-3051-markers" for this suite. 03/15/23 23:09:40.264
------------------------------
• [SLOW TEST] [6.942 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:09:33.378
    Mar 15 23:09:33.378: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 23:09:33.379
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:09:33.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:09:33.394
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 23:09:33.421
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 23:09:33.874
    STEP: Deploying the webhook pod 03/15/23 23:09:33.881
    STEP: Wait for the deployment to be ready 03/15/23 23:09:33.893
    Mar 15 23:09:33.904: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 23:09:35.919
    STEP: Verifying the service has paired with the endpoint 03/15/23 23:09:35.932
    Mar 15 23:09:36.934: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Mar 15 23:09:36.937: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7484-crds.webhook.example.com via the AdmissionRegistration API 03/15/23 23:09:37.451
    STEP: Creating a custom resource while v1 is storage version 03/15/23 23:09:37.468
    STEP: Patching Custom Resource Definition to set v2 as storage 03/15/23 23:09:39.52
    STEP: Patching the custom resource while v2 is storage version 03/15/23 23:09:39.528
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:09:40.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3051" for this suite. 03/15/23 23:09:40.227
    STEP: Destroying namespace "webhook-3051-markers" for this suite. 03/15/23 23:09:40.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:09:40.361
Mar 15 23:09:40.362: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 23:09:40.369
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:09:40.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:09:40.484
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 23:09:40.614
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 23:09:41.613
STEP: Deploying the webhook pod 03/15/23 23:09:41.618
STEP: Wait for the deployment to be ready 03/15/23 23:09:41.631
Mar 15 23:09:41.648: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 23:09:43.657
STEP: Verifying the service has paired with the endpoint 03/15/23 23:09:43.666
Mar 15 23:09:44.667: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 03/15/23 23:09:44.67
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/15/23 23:09:44.673
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/15/23 23:09:44.673
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/15/23 23:09:44.673
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/15/23 23:09:44.674
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/15/23 23:09:44.675
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/15/23 23:09:44.676
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 23:09:44.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-466" for this suite. 03/15/23 23:09:44.749
STEP: Destroying namespace "webhook-466-markers" for this suite. 03/15/23 23:09:44.767
------------------------------
• [4.424 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:09:40.361
    Mar 15 23:09:40.362: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 23:09:40.369
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:09:40.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:09:40.484
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 23:09:40.614
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 23:09:41.613
    STEP: Deploying the webhook pod 03/15/23 23:09:41.618
    STEP: Wait for the deployment to be ready 03/15/23 23:09:41.631
    Mar 15 23:09:41.648: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 23:09:43.657
    STEP: Verifying the service has paired with the endpoint 03/15/23 23:09:43.666
    Mar 15 23:09:44.667: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 03/15/23 23:09:44.67
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/15/23 23:09:44.673
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/15/23 23:09:44.673
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/15/23 23:09:44.673
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/15/23 23:09:44.674
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/15/23 23:09:44.675
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/15/23 23:09:44.676
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:09:44.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-466" for this suite. 03/15/23 23:09:44.749
    STEP: Destroying namespace "webhook-466-markers" for this suite. 03/15/23 23:09:44.767
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:09:44.786
Mar 15 23:09:44.786: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename deployment 03/15/23 23:09:44.787
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:09:44.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:09:44.802
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Mar 15 23:09:44.810: INFO: Creating simple deployment test-new-deployment
Mar 15 23:09:44.840: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 03/15/23 23:09:46.852
STEP: updating a scale subresource 03/15/23 23:09:46.856
STEP: verifying the deployment Spec.Replicas was modified 03/15/23 23:09:46.861
STEP: Patch a scale subresource 03/15/23 23:09:46.864
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 15 23:09:46.899: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1794  9efd6af6-bd0b-44c2-bdf4-74792bfb9d69 33922 3 2023-03-15 23:09:44 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-15 23:09:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 23:09:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032c5938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-15 23:09:46 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-03-15 23:09:46 +0000 UTC,LastTransitionTime:2023-03-15 23:09:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 15 23:09:46.908: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-1794  93677530-9275-4e72-963a-bbca8b8d311f 33926 2 2023-03-15 23:09:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 9efd6af6-bd0b-44c2-bdf4-74792bfb9d69 0xc0032c5d17 0xc0032c5d18}] [] [{kube-controller-manager Update apps/v1 2023-03-15 23:09:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9efd6af6-bd0b-44c2-bdf4-74792bfb9d69\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 23:09:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032c5da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 15 23:09:46.919: INFO: Pod "test-new-deployment-7f5969cbc7-4jg78" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-4jg78 test-new-deployment-7f5969cbc7- deployment-1794  edb181d2-bba8-472b-a358-6e72df1a11fa 33915 0 2023-03-15 23:09:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 93677530-9275-4e72-963a-bbca8b8d311f 0xc002f01417 0xc002f01418}] [] [{kube-controller-manager Update v1 2023-03-15 23:09:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93677530-9275-4e72-963a-bbca8b8d311f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 23:09:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9bj54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9bj54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.57,StartTime:2023-03-15 23:09:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 23:09:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://851d17e105f9826d3dd72916b6a815759e4f5f01af7c7ff23e92ba57c3c233fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 23:09:46.919: INFO: Pod "test-new-deployment-7f5969cbc7-gnn8n" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-gnn8n test-new-deployment-7f5969cbc7- deployment-1794  05e397a1-1fe2-4c37-b41a-7210133aae79 33927 0 2023-03-15 23:09:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 93677530-9275-4e72-963a-bbca8b8d311f 0xc002f015f0 0xc002f015f1}] [] [{kube-controller-manager Update v1 2023-03-15 23:09:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93677530-9275-4e72-963a-bbca8b8d311f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 23:09:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6qxvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6qxvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 23:09:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 15 23:09:46.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1794" for this suite. 03/15/23 23:09:46.924
------------------------------
• [2.160 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:09:44.786
    Mar 15 23:09:44.786: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename deployment 03/15/23 23:09:44.787
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:09:44.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:09:44.802
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Mar 15 23:09:44.810: INFO: Creating simple deployment test-new-deployment
    Mar 15 23:09:44.840: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 03/15/23 23:09:46.852
    STEP: updating a scale subresource 03/15/23 23:09:46.856
    STEP: verifying the deployment Spec.Replicas was modified 03/15/23 23:09:46.861
    STEP: Patch a scale subresource 03/15/23 23:09:46.864
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 15 23:09:46.899: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-1794  9efd6af6-bd0b-44c2-bdf4-74792bfb9d69 33922 3 2023-03-15 23:09:44 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-15 23:09:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 23:09:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032c5938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-15 23:09:46 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-03-15 23:09:46 +0000 UTC,LastTransitionTime:2023-03-15 23:09:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 15 23:09:46.908: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-1794  93677530-9275-4e72-963a-bbca8b8d311f 33926 2 2023-03-15 23:09:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 9efd6af6-bd0b-44c2-bdf4-74792bfb9d69 0xc0032c5d17 0xc0032c5d18}] [] [{kube-controller-manager Update apps/v1 2023-03-15 23:09:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9efd6af6-bd0b-44c2-bdf4-74792bfb9d69\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-15 23:09:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032c5da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 15 23:09:46.919: INFO: Pod "test-new-deployment-7f5969cbc7-4jg78" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-4jg78 test-new-deployment-7f5969cbc7- deployment-1794  edb181d2-bba8-472b-a358-6e72df1a11fa 33915 0 2023-03-15 23:09:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 93677530-9275-4e72-963a-bbca8b8d311f 0xc002f01417 0xc002f01418}] [] [{kube-controller-manager Update v1 2023-03-15 23:09:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93677530-9275-4e72-963a-bbca8b8d311f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 23:09:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9bj54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9bj54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-0faaf83f00b43c88c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.126.23,PodIP:100.96.3.57,StartTime:2023-03-15 23:09:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-15 23:09:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://851d17e105f9826d3dd72916b6a815759e4f5f01af7c7ff23e92ba57c3c233fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 15 23:09:46.919: INFO: Pod "test-new-deployment-7f5969cbc7-gnn8n" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-gnn8n test-new-deployment-7f5969cbc7- deployment-1794  05e397a1-1fe2-4c37-b41a-7210133aae79 33927 0 2023-03-15 23:09:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 93677530-9275-4e72-963a-bbca8b8d311f 0xc002f015f0 0xc002f015f1}] [] [{kube-controller-manager Update v1 2023-03-15 23:09:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93677530-9275-4e72-963a-bbca8b8d311f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-15 23:09:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6qxvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6qxvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:i-077ee0eb7ec5a02aa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-15 23:09:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.20.75.105,PodIP:,StartTime:2023-03-15 23:09:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:09:46.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1794" for this suite. 03/15/23 23:09:46.924
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:09:46.947
Mar 15 23:09:46.947: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 23:09:46.948
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:09:47.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:09:47.049
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/15/23 23:09:47.054
Mar 15 23:09:47.055: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/15/23 23:09:53.602
Mar 15 23:09:53.602: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
Mar 15 23:09:55.291: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 23:10:02.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4601" for this suite. 03/15/23 23:10:02.653
------------------------------
• [SLOW TEST] [15.715 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:09:46.947
    Mar 15 23:09:46.947: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename crd-publish-openapi 03/15/23 23:09:46.948
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:09:47.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:09:47.049
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/15/23 23:09:47.054
    Mar 15 23:09:47.055: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/15/23 23:09:53.602
    Mar 15 23:09:53.602: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    Mar 15 23:09:55.291: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:10:02.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4601" for this suite. 03/15/23 23:10:02.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:10:02.667
Mar 15 23:10:02.667: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename statefulset 03/15/23 23:10:02.668
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:10:02.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:10:02.71
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4902 03/15/23 23:10:02.714
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 03/15/23 23:10:02.719
Mar 15 23:10:02.732: INFO: Found 0 stateful pods, waiting for 3
Mar 15 23:10:12.736: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 23:10:12.736: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 23:10:12.736: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 23:10:12.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-4902 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 23:10:12.973: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 23:10:12.973: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 23:10:12.973: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/15/23 23:10:22.987
Mar 15 23:10:23.014: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/15/23 23:10:23.014
STEP: Updating Pods in reverse ordinal order 03/15/23 23:10:33.034
Mar 15 23:10:33.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-4902 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 23:10:33.221: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 23:10:33.221: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 23:10:33.221: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 03/15/23 23:10:53.239
Mar 15 23:10:53.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-4902 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 23:10:53.549: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 23:10:53.549: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 23:10:53.549: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 23:11:03.584: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 03/15/23 23:11:13.598
Mar 15 23:11:13.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-4902 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 23:11:13.779: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 23:11:13.779: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 23:11:13.779: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 15 23:11:23.799: INFO: Deleting all statefulset in ns statefulset-4902
Mar 15 23:11:23.801: INFO: Scaling statefulset ss2 to 0
Mar 15 23:11:33.827: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 23:11:33.831: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 15 23:11:33.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4902" for this suite. 03/15/23 23:11:33.886
------------------------------
• [SLOW TEST] [91.227 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:10:02.667
    Mar 15 23:10:02.667: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename statefulset 03/15/23 23:10:02.668
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:10:02.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:10:02.71
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4902 03/15/23 23:10:02.714
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 03/15/23 23:10:02.719
    Mar 15 23:10:02.732: INFO: Found 0 stateful pods, waiting for 3
    Mar 15 23:10:12.736: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 15 23:10:12.736: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 15 23:10:12.736: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Mar 15 23:10:12.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-4902 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 15 23:10:12.973: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 15 23:10:12.973: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 15 23:10:12.973: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/15/23 23:10:22.987
    Mar 15 23:10:23.014: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/15/23 23:10:23.014
    STEP: Updating Pods in reverse ordinal order 03/15/23 23:10:33.034
    Mar 15 23:10:33.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-4902 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 15 23:10:33.221: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 15 23:10:33.221: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 15 23:10:33.221: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 03/15/23 23:10:53.239
    Mar 15 23:10:53.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-4902 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 15 23:10:53.549: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 15 23:10:53.549: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 15 23:10:53.549: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 15 23:11:03.584: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 03/15/23 23:11:13.598
    Mar 15 23:11:13.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-551247953 --namespace=statefulset-4902 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 15 23:11:13.779: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 15 23:11:13.779: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 15 23:11:13.779: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 15 23:11:23.799: INFO: Deleting all statefulset in ns statefulset-4902
    Mar 15 23:11:23.801: INFO: Scaling statefulset ss2 to 0
    Mar 15 23:11:33.827: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 15 23:11:33.831: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:11:33.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4902" for this suite. 03/15/23 23:11:33.886
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:11:33.896
Mar 15 23:11:33.896: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename svc-latency 03/15/23 23:11:33.897
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:11:33.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:11:33.916
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Mar 15 23:11:33.920: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5511 03/15/23 23:11:33.921
I0315 23:11:33.927016      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5511, replica count: 1
I0315 23:11:34.977725      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0315 23:11:35.978737      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 23:11:36.095: INFO: Created: latency-svc-wz9qn
Mar 15 23:11:36.110: INFO: Got endpoints: latency-svc-wz9qn [30.763619ms]
Mar 15 23:11:36.137: INFO: Created: latency-svc-l7cx6
Mar 15 23:11:36.164: INFO: Got endpoints: latency-svc-l7cx6 [53.025134ms]
Mar 15 23:11:36.166: INFO: Created: latency-svc-lsgct
Mar 15 23:11:36.166: INFO: Got endpoints: latency-svc-lsgct [55.25733ms]
Mar 15 23:11:36.183: INFO: Created: latency-svc-nwdl9
Mar 15 23:11:36.206: INFO: Created: latency-svc-fx4cv
Mar 15 23:11:36.220: INFO: Got endpoints: latency-svc-nwdl9 [108.310366ms]
Mar 15 23:11:36.230: INFO: Got endpoints: latency-svc-fx4cv [118.055116ms]
Mar 15 23:11:36.237: INFO: Created: latency-svc-pnjxf
Mar 15 23:11:36.245: INFO: Created: latency-svc-zz8ml
Mar 15 23:11:36.269: INFO: Got endpoints: latency-svc-pnjxf [155.741677ms]
Mar 15 23:11:36.291: INFO: Got endpoints: latency-svc-zz8ml [177.299326ms]
Mar 15 23:11:36.318: INFO: Created: latency-svc-zmpwb
Mar 15 23:11:36.323: INFO: Created: latency-svc-jz2f5
Mar 15 23:11:36.339: INFO: Created: latency-svc-pjdz9
Mar 15 23:11:36.363: INFO: Got endpoints: latency-svc-zmpwb [249.825903ms]
Mar 15 23:11:36.372: INFO: Got endpoints: latency-svc-jz2f5 [258.697054ms]
Mar 15 23:11:36.374: INFO: Got endpoints: latency-svc-pjdz9 [260.064559ms]
Mar 15 23:11:36.467: INFO: Created: latency-svc-5kc6k
Mar 15 23:11:36.483: INFO: Created: latency-svc-48kjr
Mar 15 23:11:36.483: INFO: Created: latency-svc-jwbpz
Mar 15 23:11:36.484: INFO: Created: latency-svc-5hb5f
Mar 15 23:11:36.485: INFO: Created: latency-svc-4tjfk
Mar 15 23:11:36.485: INFO: Created: latency-svc-kwp8r
Mar 15 23:11:36.485: INFO: Created: latency-svc-dg6rm
Mar 15 23:11:36.487: INFO: Created: latency-svc-bc88w
Mar 15 23:11:36.488: INFO: Got endpoints: latency-svc-5kc6k [373.876492ms]
Mar 15 23:11:36.489: INFO: Created: latency-svc-dcmqm
Mar 15 23:11:36.493: INFO: Created: latency-svc-cc4x7
Mar 15 23:11:36.493: INFO: Created: latency-svc-czgbp
Mar 15 23:11:36.493: INFO: Created: latency-svc-sbqlb
Mar 15 23:11:36.493: INFO: Created: latency-svc-s5jfb
Mar 15 23:11:36.494: INFO: Created: latency-svc-k2f5t
Mar 15 23:11:36.497: INFO: Created: latency-svc-lc7t2
Mar 15 23:11:36.499: INFO: Got endpoints: latency-svc-lc7t2 [131.003393ms]
Mar 15 23:11:36.523: INFO: Got endpoints: latency-svc-jwbpz [151.117256ms]
Mar 15 23:11:36.543: INFO: Created: latency-svc-l66gj
Mar 15 23:11:36.550: INFO: Got endpoints: latency-svc-5hb5f [437.273362ms]
Mar 15 23:11:36.551: INFO: Got endpoints: latency-svc-dg6rm [383.383315ms]
Mar 15 23:11:36.551: INFO: Got endpoints: latency-svc-4tjfk [439.646703ms]
Mar 15 23:11:36.552: INFO: Got endpoints: latency-svc-48kjr [177.916532ms]
Mar 15 23:11:36.552: INFO: Got endpoints: latency-svc-kwp8r [387.467013ms]
Mar 15 23:11:36.564: INFO: Got endpoints: latency-svc-bc88w [342.345004ms]
Mar 15 23:11:36.575: INFO: Got endpoints: latency-svc-dcmqm [342.970368ms]
Mar 15 23:11:36.575: INFO: Got endpoints: latency-svc-czgbp [462.408378ms]
Mar 15 23:11:36.576: INFO: Got endpoints: latency-svc-k2f5t [307.225025ms]
Mar 15 23:11:36.576: INFO: Got endpoints: latency-svc-s5jfb [282.929097ms]
Mar 15 23:11:36.580: INFO: Got endpoints: latency-svc-sbqlb [467.023917ms]
Mar 15 23:11:36.604: INFO: Got endpoints: latency-svc-cc4x7 [490.832509ms]
Mar 15 23:11:36.605: INFO: Got endpoints: latency-svc-l66gj [117.012681ms]
Mar 15 23:11:36.737: INFO: Created: latency-svc-qqkt2
Mar 15 23:11:36.766: INFO: Created: latency-svc-lcph7
Mar 15 23:11:36.766: INFO: Created: latency-svc-znvb9
Mar 15 23:11:36.766: INFO: Created: latency-svc-5nxzr
Mar 15 23:11:36.774: INFO: Got endpoints: latency-svc-qqkt2 [223.757794ms]
Mar 15 23:11:36.778: INFO: Created: latency-svc-dvds8
Mar 15 23:11:36.778: INFO: Created: latency-svc-7vf2r
Mar 15 23:11:36.778: INFO: Created: latency-svc-8fn42
Mar 15 23:11:36.778: INFO: Created: latency-svc-ks86s
Mar 15 23:11:36.778: INFO: Created: latency-svc-lhlbh
Mar 15 23:11:36.778: INFO: Got endpoints: latency-svc-lhlbh [203.457784ms]
Mar 15 23:11:36.779: INFO: Created: latency-svc-d4zpb
Mar 15 23:11:36.779: INFO: Created: latency-svc-f5q6m
Mar 15 23:11:36.779: INFO: Created: latency-svc-9zmnq
Mar 15 23:11:36.794: INFO: Created: latency-svc-4pj57
Mar 15 23:11:36.795: INFO: Got endpoints: latency-svc-d4zpb [230.650996ms]
Mar 15 23:11:36.795: INFO: Got endpoints: latency-svc-5nxzr [190.053275ms]
Mar 15 23:11:36.796: INFO: Got endpoints: latency-svc-ks86s [220.382683ms]
Mar 15 23:11:36.796: INFO: Created: latency-svc-6dm4m
Mar 15 23:11:36.812: INFO: Got endpoints: latency-svc-6dm4m [288.357146ms]
Mar 15 23:11:36.812: INFO: Got endpoints: latency-svc-8fn42 [235.345698ms]
Mar 15 23:11:36.812: INFO: Got endpoints: latency-svc-7vf2r [260.883283ms]
Mar 15 23:11:36.812: INFO: Got endpoints: latency-svc-4pj57 [235.535659ms]
Mar 15 23:11:36.813: INFO: Created: latency-svc-r4qtf
Mar 15 23:11:36.813: INFO: Got endpoints: latency-svc-r4qtf [261.455219ms]
Mar 15 23:11:36.824: INFO: Got endpoints: latency-svc-dvds8 [325.074162ms]
Mar 15 23:11:36.827: INFO: Got endpoints: latency-svc-lcph7 [223.091192ms]
Mar 15 23:11:36.835: INFO: Created: latency-svc-b54r7
Mar 15 23:11:36.838: INFO: Got endpoints: latency-svc-9zmnq [286.089709ms]
Mar 15 23:11:36.838: INFO: Got endpoints: latency-svc-f5q6m [285.838164ms]
Mar 15 23:11:36.842: INFO: Got endpoints: latency-svc-znvb9 [261.552718ms]
Mar 15 23:11:36.849: INFO: Got endpoints: latency-svc-b54r7 [75.309985ms]
Mar 15 23:11:37.021: INFO: Created: latency-svc-vm8dr
Mar 15 23:11:37.022: INFO: Created: latency-svc-42sln
Mar 15 23:11:37.022: INFO: Created: latency-svc-mvgrs
Mar 15 23:11:37.023: INFO: Created: latency-svc-8wlv2
Mar 15 23:11:37.023: INFO: Created: latency-svc-qlpgq
Mar 15 23:11:37.023: INFO: Created: latency-svc-jbw8q
Mar 15 23:11:37.024: INFO: Created: latency-svc-lbz7b
Mar 15 23:11:37.024: INFO: Created: latency-svc-x2nrk
Mar 15 23:11:37.024: INFO: Created: latency-svc-vmpb5
Mar 15 23:11:37.024: INFO: Created: latency-svc-fmv49
Mar 15 23:11:37.026: INFO: Created: latency-svc-s527t
Mar 15 23:11:37.026: INFO: Created: latency-svc-f9wvr
Mar 15 23:11:37.027: INFO: Created: latency-svc-ddtwx
Mar 15 23:11:37.027: INFO: Created: latency-svc-rrn8m
Mar 15 23:11:37.028: INFO: Created: latency-svc-t6nq4
Mar 15 23:11:37.036: INFO: Got endpoints: latency-svc-mvgrs [194.378521ms]
Mar 15 23:11:37.140: INFO: Got endpoints: latency-svc-8wlv2 [297.767572ms]
Mar 15 23:11:37.144: INFO: Got endpoints: latency-svc-vm8dr [332.005715ms]
Mar 15 23:11:37.197: INFO: Created: latency-svc-flgq8
Mar 15 23:11:37.197: INFO: Got endpoints: latency-svc-jbw8q [418.537505ms]
Mar 15 23:11:37.199: INFO: Got endpoints: latency-svc-qlpgq [349.091732ms]
Mar 15 23:11:37.199: INFO: Got endpoints: latency-svc-vmpb5 [402.577464ms]
Mar 15 23:11:37.199: INFO: Got endpoints: latency-svc-x2nrk [403.461445ms]
Mar 15 23:11:37.199: INFO: Got endpoints: latency-svc-lbz7b [403.884972ms]
Mar 15 23:11:37.223: INFO: Got endpoints: latency-svc-s527t [409.97011ms]
Mar 15 23:11:37.226: INFO: Created: latency-svc-kvlq6
Mar 15 23:11:37.226: INFO: Got endpoints: latency-svc-f9wvr [413.822037ms]
Mar 15 23:11:37.240: INFO: Created: latency-svc-zpcwh
Mar 15 23:11:37.255: INFO: Got endpoints: latency-svc-rrn8m [442.402608ms]
Mar 15 23:11:37.273: INFO: Created: latency-svc-czljb
Mar 15 23:11:37.293: INFO: Created: latency-svc-8k82j
Mar 15 23:11:37.304: INFO: Created: latency-svc-j6lbj
Mar 15 23:11:37.306: INFO: Got endpoints: latency-svc-t6nq4 [493.984181ms]
Mar 15 23:11:37.330: INFO: Created: latency-svc-9pqtb
Mar 15 23:11:37.334: INFO: Created: latency-svc-5j7gd
Mar 15 23:11:37.357: INFO: Created: latency-svc-7l2nk
Mar 15 23:11:37.368: INFO: Got endpoints: latency-svc-ddtwx [543.422416ms]
Mar 15 23:11:37.381: INFO: Created: latency-svc-85mcs
Mar 15 23:11:37.391: INFO: Created: latency-svc-k48rc
Mar 15 23:11:37.402: INFO: Created: latency-svc-4b7td
Mar 15 23:11:37.404: INFO: Got endpoints: latency-svc-42sln [576.42173ms]
Mar 15 23:11:37.417: INFO: Created: latency-svc-sqmp5
Mar 15 23:11:37.420: INFO: Created: latency-svc-xn5zr
Mar 15 23:11:37.449: INFO: Got endpoints: latency-svc-fmv49 [607.269603ms]
Mar 15 23:11:37.461: INFO: Created: latency-svc-mjv9p
Mar 15 23:11:37.503: INFO: Got endpoints: latency-svc-flgq8 [465.812405ms]
Mar 15 23:11:37.515: INFO: Created: latency-svc-28rt2
Mar 15 23:11:37.551: INFO: Got endpoints: latency-svc-kvlq6 [410.479314ms]
Mar 15 23:11:37.564: INFO: Created: latency-svc-cp79l
Mar 15 23:11:37.601: INFO: Got endpoints: latency-svc-zpcwh [456.771739ms]
Mar 15 23:11:37.615: INFO: Created: latency-svc-n4lk7
Mar 15 23:11:37.649: INFO: Got endpoints: latency-svc-czljb [450.740353ms]
Mar 15 23:11:37.665: INFO: Created: latency-svc-484zb
Mar 15 23:11:37.702: INFO: Got endpoints: latency-svc-8k82j [503.055009ms]
Mar 15 23:11:37.714: INFO: Created: latency-svc-h7xqx
Mar 15 23:11:37.749: INFO: Got endpoints: latency-svc-j6lbj [549.800303ms]
Mar 15 23:11:37.762: INFO: Created: latency-svc-8fcrr
Mar 15 23:11:37.802: INFO: Got endpoints: latency-svc-9pqtb [602.593107ms]
Mar 15 23:11:37.813: INFO: Created: latency-svc-ph985
Mar 15 23:11:37.852: INFO: Got endpoints: latency-svc-5j7gd [650.359275ms]
Mar 15 23:11:37.865: INFO: Created: latency-svc-vl46z
Mar 15 23:11:37.901: INFO: Got endpoints: latency-svc-7l2nk [678.420527ms]
Mar 15 23:11:37.915: INFO: Created: latency-svc-cg58c
Mar 15 23:11:37.952: INFO: Got endpoints: latency-svc-85mcs [723.236231ms]
Mar 15 23:11:37.963: INFO: Created: latency-svc-zmnh6
Mar 15 23:11:38.001: INFO: Got endpoints: latency-svc-k48rc [746.096182ms]
Mar 15 23:11:38.014: INFO: Created: latency-svc-qq9t4
Mar 15 23:11:38.051: INFO: Got endpoints: latency-svc-4b7td [744.630569ms]
Mar 15 23:11:38.072: INFO: Created: latency-svc-wxq8h
Mar 15 23:11:38.102: INFO: Got endpoints: latency-svc-sqmp5 [733.381434ms]
Mar 15 23:11:38.122: INFO: Created: latency-svc-wrrn8
Mar 15 23:11:38.155: INFO: Got endpoints: latency-svc-xn5zr [750.802944ms]
Mar 15 23:11:38.173: INFO: Created: latency-svc-5b7j4
Mar 15 23:11:38.203: INFO: Got endpoints: latency-svc-mjv9p [753.454772ms]
Mar 15 23:11:38.215: INFO: Created: latency-svc-857sm
Mar 15 23:11:38.251: INFO: Got endpoints: latency-svc-28rt2 [747.583071ms]
Mar 15 23:11:38.268: INFO: Created: latency-svc-dhjnq
Mar 15 23:11:38.303: INFO: Got endpoints: latency-svc-cp79l [752.454508ms]
Mar 15 23:11:38.315: INFO: Created: latency-svc-2z577
Mar 15 23:11:38.351: INFO: Got endpoints: latency-svc-n4lk7 [749.989825ms]
Mar 15 23:11:38.368: INFO: Created: latency-svc-9nhm5
Mar 15 23:11:38.401: INFO: Got endpoints: latency-svc-484zb [751.726075ms]
Mar 15 23:11:38.416: INFO: Created: latency-svc-qcg5b
Mar 15 23:11:38.452: INFO: Got endpoints: latency-svc-h7xqx [750.253348ms]
Mar 15 23:11:38.469: INFO: Created: latency-svc-7hj6c
Mar 15 23:11:38.503: INFO: Got endpoints: latency-svc-8fcrr [753.195716ms]
Mar 15 23:11:38.515: INFO: Created: latency-svc-29mwc
Mar 15 23:11:38.550: INFO: Got endpoints: latency-svc-ph985 [748.259645ms]
Mar 15 23:11:38.566: INFO: Created: latency-svc-7qvb8
Mar 15 23:11:38.600: INFO: Got endpoints: latency-svc-vl46z [748.063092ms]
Mar 15 23:11:38.612: INFO: Created: latency-svc-jwppq
Mar 15 23:11:38.652: INFO: Got endpoints: latency-svc-cg58c [750.123326ms]
Mar 15 23:11:38.664: INFO: Created: latency-svc-wxxzx
Mar 15 23:11:38.700: INFO: Got endpoints: latency-svc-zmnh6 [747.7086ms]
Mar 15 23:11:38.714: INFO: Created: latency-svc-4hdcc
Mar 15 23:11:38.751: INFO: Got endpoints: latency-svc-qq9t4 [749.958625ms]
Mar 15 23:11:38.764: INFO: Created: latency-svc-pdskp
Mar 15 23:11:38.801: INFO: Got endpoints: latency-svc-wxq8h [749.993142ms]
Mar 15 23:11:38.813: INFO: Created: latency-svc-n7bq5
Mar 15 23:11:38.849: INFO: Got endpoints: latency-svc-wrrn8 [747.581197ms]
Mar 15 23:11:38.863: INFO: Created: latency-svc-q4qn5
Mar 15 23:11:38.903: INFO: Got endpoints: latency-svc-5b7j4 [748.255846ms]
Mar 15 23:11:38.921: INFO: Created: latency-svc-6b2j7
Mar 15 23:11:38.955: INFO: Got endpoints: latency-svc-857sm [751.690824ms]
Mar 15 23:11:38.968: INFO: Created: latency-svc-xzmf9
Mar 15 23:11:39.001: INFO: Got endpoints: latency-svc-dhjnq [749.69827ms]
Mar 15 23:11:39.017: INFO: Created: latency-svc-4jlqx
Mar 15 23:11:39.050: INFO: Got endpoints: latency-svc-2z577 [746.715142ms]
Mar 15 23:11:39.065: INFO: Created: latency-svc-d88vp
Mar 15 23:11:39.104: INFO: Got endpoints: latency-svc-9nhm5 [752.331151ms]
Mar 15 23:11:39.119: INFO: Created: latency-svc-k86w5
Mar 15 23:11:39.156: INFO: Got endpoints: latency-svc-qcg5b [754.894072ms]
Mar 15 23:11:39.174: INFO: Created: latency-svc-2ds9w
Mar 15 23:11:39.209: INFO: Got endpoints: latency-svc-7hj6c [756.196369ms]
Mar 15 23:11:39.220: INFO: Created: latency-svc-95w5w
Mar 15 23:11:39.254: INFO: Got endpoints: latency-svc-29mwc [751.361765ms]
Mar 15 23:11:39.274: INFO: Created: latency-svc-ktn6l
Mar 15 23:11:39.302: INFO: Got endpoints: latency-svc-7qvb8 [752.008734ms]
Mar 15 23:11:39.319: INFO: Created: latency-svc-dzf4d
Mar 15 23:11:39.356: INFO: Got endpoints: latency-svc-jwppq [756.070314ms]
Mar 15 23:11:39.370: INFO: Created: latency-svc-642jg
Mar 15 23:11:39.401: INFO: Got endpoints: latency-svc-wxxzx [749.268941ms]
Mar 15 23:11:39.427: INFO: Created: latency-svc-8lfmh
Mar 15 23:11:39.452: INFO: Got endpoints: latency-svc-4hdcc [751.992561ms]
Mar 15 23:11:39.465: INFO: Created: latency-svc-g8889
Mar 15 23:11:39.502: INFO: Got endpoints: latency-svc-pdskp [751.173139ms]
Mar 15 23:11:39.517: INFO: Created: latency-svc-bm4jn
Mar 15 23:11:39.551: INFO: Got endpoints: latency-svc-n7bq5 [750.171373ms]
Mar 15 23:11:39.561: INFO: Created: latency-svc-2d7km
Mar 15 23:11:39.600: INFO: Got endpoints: latency-svc-q4qn5 [751.190758ms]
Mar 15 23:11:39.616: INFO: Created: latency-svc-6s4z5
Mar 15 23:11:39.650: INFO: Got endpoints: latency-svc-6b2j7 [746.918473ms]
Mar 15 23:11:39.664: INFO: Created: latency-svc-wq926
Mar 15 23:11:39.700: INFO: Got endpoints: latency-svc-xzmf9 [745.414142ms]
Mar 15 23:11:39.714: INFO: Created: latency-svc-zzk7f
Mar 15 23:11:39.802: INFO: Got endpoints: latency-svc-4jlqx [800.832589ms]
Mar 15 23:11:39.813: INFO: Created: latency-svc-7vmmp
Mar 15 23:11:39.853: INFO: Got endpoints: latency-svc-d88vp [802.799804ms]
Mar 15 23:11:39.864: INFO: Created: latency-svc-pl9rl
Mar 15 23:11:39.900: INFO: Got endpoints: latency-svc-k86w5 [796.662303ms]
Mar 15 23:11:39.913: INFO: Created: latency-svc-22jkv
Mar 15 23:11:39.951: INFO: Got endpoints: latency-svc-2ds9w [795.464081ms]
Mar 15 23:11:39.965: INFO: Created: latency-svc-qndrl
Mar 15 23:11:40.002: INFO: Got endpoints: latency-svc-95w5w [793.277823ms]
Mar 15 23:11:40.013: INFO: Created: latency-svc-gfmgs
Mar 15 23:11:40.051: INFO: Got endpoints: latency-svc-ktn6l [796.370146ms]
Mar 15 23:11:40.066: INFO: Created: latency-svc-2kwqp
Mar 15 23:11:40.105: INFO: Got endpoints: latency-svc-dzf4d [803.151984ms]
Mar 15 23:11:40.127: INFO: Created: latency-svc-86nm6
Mar 15 23:11:40.151: INFO: Got endpoints: latency-svc-642jg [795.024617ms]
Mar 15 23:11:40.175: INFO: Created: latency-svc-6rm8b
Mar 15 23:11:40.200: INFO: Got endpoints: latency-svc-8lfmh [799.005128ms]
Mar 15 23:11:40.217: INFO: Created: latency-svc-9qv9z
Mar 15 23:11:40.251: INFO: Got endpoints: latency-svc-g8889 [799.423073ms]
Mar 15 23:11:40.271: INFO: Created: latency-svc-4vprn
Mar 15 23:11:40.305: INFO: Got endpoints: latency-svc-bm4jn [802.611905ms]
Mar 15 23:11:40.317: INFO: Created: latency-svc-wntqc
Mar 15 23:11:40.352: INFO: Got endpoints: latency-svc-2d7km [800.905118ms]
Mar 15 23:11:40.369: INFO: Created: latency-svc-ffjb8
Mar 15 23:11:40.400: INFO: Got endpoints: latency-svc-6s4z5 [799.263412ms]
Mar 15 23:11:40.413: INFO: Created: latency-svc-f87h6
Mar 15 23:11:40.450: INFO: Got endpoints: latency-svc-wq926 [799.540292ms]
Mar 15 23:11:40.466: INFO: Created: latency-svc-csm9n
Mar 15 23:11:40.504: INFO: Got endpoints: latency-svc-zzk7f [804.284998ms]
Mar 15 23:11:40.517: INFO: Created: latency-svc-2z4bh
Mar 15 23:11:40.553: INFO: Got endpoints: latency-svc-7vmmp [751.145376ms]
Mar 15 23:11:40.566: INFO: Created: latency-svc-m2jps
Mar 15 23:11:40.603: INFO: Got endpoints: latency-svc-pl9rl [749.689445ms]
Mar 15 23:11:40.615: INFO: Created: latency-svc-tmh8d
Mar 15 23:11:40.652: INFO: Got endpoints: latency-svc-22jkv [751.7349ms]
Mar 15 23:11:40.665: INFO: Created: latency-svc-pmx6p
Mar 15 23:11:40.701: INFO: Got endpoints: latency-svc-qndrl [749.659142ms]
Mar 15 23:11:40.716: INFO: Created: latency-svc-k6swf
Mar 15 23:11:40.752: INFO: Got endpoints: latency-svc-gfmgs [749.986603ms]
Mar 15 23:11:40.765: INFO: Created: latency-svc-b4g46
Mar 15 23:11:40.804: INFO: Got endpoints: latency-svc-2kwqp [753.090559ms]
Mar 15 23:11:40.816: INFO: Created: latency-svc-vmg2h
Mar 15 23:11:40.855: INFO: Got endpoints: latency-svc-86nm6 [749.14655ms]
Mar 15 23:11:40.876: INFO: Created: latency-svc-qj2ks
Mar 15 23:11:40.903: INFO: Got endpoints: latency-svc-6rm8b [751.762256ms]
Mar 15 23:11:40.917: INFO: Created: latency-svc-4jgll
Mar 15 23:11:40.951: INFO: Got endpoints: latency-svc-9qv9z [750.669623ms]
Mar 15 23:11:40.964: INFO: Created: latency-svc-6wxh6
Mar 15 23:11:41.004: INFO: Got endpoints: latency-svc-4vprn [752.824234ms]
Mar 15 23:11:41.046: INFO: Created: latency-svc-69s2j
Mar 15 23:11:41.053: INFO: Got endpoints: latency-svc-wntqc [748.234242ms]
Mar 15 23:11:41.092: INFO: Created: latency-svc-rq7q5
Mar 15 23:11:41.103: INFO: Got endpoints: latency-svc-ffjb8 [750.126499ms]
Mar 15 23:11:41.121: INFO: Created: latency-svc-h66gc
Mar 15 23:11:41.152: INFO: Got endpoints: latency-svc-f87h6 [752.353152ms]
Mar 15 23:11:41.183: INFO: Created: latency-svc-zghhv
Mar 15 23:11:41.207: INFO: Got endpoints: latency-svc-csm9n [757.151068ms]
Mar 15 23:11:41.225: INFO: Created: latency-svc-whnhm
Mar 15 23:11:41.253: INFO: Got endpoints: latency-svc-2z4bh [748.039011ms]
Mar 15 23:11:41.272: INFO: Created: latency-svc-c2vwp
Mar 15 23:11:41.302: INFO: Got endpoints: latency-svc-m2jps [748.878911ms]
Mar 15 23:11:41.318: INFO: Created: latency-svc-p9mfz
Mar 15 23:11:41.358: INFO: Got endpoints: latency-svc-tmh8d [755.763666ms]
Mar 15 23:11:41.380: INFO: Created: latency-svc-vngzr
Mar 15 23:11:41.400: INFO: Got endpoints: latency-svc-pmx6p [747.765129ms]
Mar 15 23:11:41.414: INFO: Created: latency-svc-ctw7s
Mar 15 23:11:41.450: INFO: Got endpoints: latency-svc-k6swf [748.67697ms]
Mar 15 23:11:41.469: INFO: Created: latency-svc-wwvjs
Mar 15 23:11:41.501: INFO: Got endpoints: latency-svc-b4g46 [748.433274ms]
Mar 15 23:11:41.515: INFO: Created: latency-svc-wz82c
Mar 15 23:11:41.550: INFO: Got endpoints: latency-svc-vmg2h [746.527521ms]
Mar 15 23:11:41.563: INFO: Created: latency-svc-6c897
Mar 15 23:11:41.600: INFO: Got endpoints: latency-svc-qj2ks [745.441223ms]
Mar 15 23:11:41.613: INFO: Created: latency-svc-hrr6h
Mar 15 23:11:41.660: INFO: Got endpoints: latency-svc-4jgll [757.277487ms]
Mar 15 23:11:41.680: INFO: Created: latency-svc-wr8lf
Mar 15 23:11:41.702: INFO: Got endpoints: latency-svc-6wxh6 [751.541158ms]
Mar 15 23:11:41.722: INFO: Created: latency-svc-czhtl
Mar 15 23:11:41.760: INFO: Got endpoints: latency-svc-69s2j [755.248219ms]
Mar 15 23:11:41.771: INFO: Created: latency-svc-b27q5
Mar 15 23:11:41.803: INFO: Got endpoints: latency-svc-rq7q5 [750.129221ms]
Mar 15 23:11:41.816: INFO: Created: latency-svc-4nlmv
Mar 15 23:11:41.851: INFO: Got endpoints: latency-svc-h66gc [747.426846ms]
Mar 15 23:11:41.866: INFO: Created: latency-svc-7pbvk
Mar 15 23:11:41.900: INFO: Got endpoints: latency-svc-zghhv [746.905522ms]
Mar 15 23:11:41.912: INFO: Created: latency-svc-8ts6f
Mar 15 23:11:41.957: INFO: Got endpoints: latency-svc-whnhm [749.698334ms]
Mar 15 23:11:41.968: INFO: Created: latency-svc-qdw6c
Mar 15 23:11:42.000: INFO: Got endpoints: latency-svc-c2vwp [747.514938ms]
Mar 15 23:11:42.013: INFO: Created: latency-svc-khj88
Mar 15 23:11:42.051: INFO: Got endpoints: latency-svc-p9mfz [748.829918ms]
Mar 15 23:11:42.065: INFO: Created: latency-svc-rf7mx
Mar 15 23:11:42.103: INFO: Got endpoints: latency-svc-vngzr [744.277219ms]
Mar 15 23:11:42.119: INFO: Created: latency-svc-mj5jg
Mar 15 23:11:42.157: INFO: Got endpoints: latency-svc-ctw7s [756.962451ms]
Mar 15 23:11:42.172: INFO: Created: latency-svc-6p5cz
Mar 15 23:11:42.206: INFO: Got endpoints: latency-svc-wwvjs [756.238012ms]
Mar 15 23:11:42.218: INFO: Created: latency-svc-9wgh2
Mar 15 23:11:42.251: INFO: Got endpoints: latency-svc-wz82c [750.406073ms]
Mar 15 23:11:42.263: INFO: Created: latency-svc-8xbt9
Mar 15 23:11:42.303: INFO: Got endpoints: latency-svc-6c897 [752.526276ms]
Mar 15 23:11:42.318: INFO: Created: latency-svc-5ghrv
Mar 15 23:11:42.353: INFO: Got endpoints: latency-svc-hrr6h [752.6436ms]
Mar 15 23:11:42.365: INFO: Created: latency-svc-mz6wg
Mar 15 23:11:42.399: INFO: Got endpoints: latency-svc-wr8lf [738.172235ms]
Mar 15 23:11:42.419: INFO: Created: latency-svc-kss59
Mar 15 23:11:42.453: INFO: Got endpoints: latency-svc-czhtl [750.6589ms]
Mar 15 23:11:42.465: INFO: Created: latency-svc-t9rqr
Mar 15 23:11:42.504: INFO: Got endpoints: latency-svc-b27q5 [744.861746ms]
Mar 15 23:11:42.528: INFO: Created: latency-svc-ccv42
Mar 15 23:11:42.554: INFO: Got endpoints: latency-svc-4nlmv [751.041077ms]
Mar 15 23:11:42.581: INFO: Created: latency-svc-k9zdg
Mar 15 23:11:42.599: INFO: Got endpoints: latency-svc-7pbvk [748.613365ms]
Mar 15 23:11:42.613: INFO: Created: latency-svc-g6nzd
Mar 15 23:11:42.654: INFO: Got endpoints: latency-svc-8ts6f [754.772798ms]
Mar 15 23:11:42.666: INFO: Created: latency-svc-sdrv2
Mar 15 23:11:42.705: INFO: Got endpoints: latency-svc-qdw6c [747.751969ms]
Mar 15 23:11:42.718: INFO: Created: latency-svc-c2whl
Mar 15 23:11:42.749: INFO: Got endpoints: latency-svc-khj88 [748.898505ms]
Mar 15 23:11:42.764: INFO: Created: latency-svc-7v8w2
Mar 15 23:11:42.799: INFO: Got endpoints: latency-svc-rf7mx [747.542326ms]
Mar 15 23:11:42.817: INFO: Created: latency-svc-9kdqf
Mar 15 23:11:42.860: INFO: Got endpoints: latency-svc-mj5jg [755.966804ms]
Mar 15 23:11:42.889: INFO: Created: latency-svc-fphbp
Mar 15 23:11:42.916: INFO: Got endpoints: latency-svc-6p5cz [758.885015ms]
Mar 15 23:11:42.950: INFO: Created: latency-svc-jr2ft
Mar 15 23:11:42.952: INFO: Got endpoints: latency-svc-9wgh2 [745.354892ms]
Mar 15 23:11:43.005: INFO: Created: latency-svc-758gr
Mar 15 23:11:43.009: INFO: Got endpoints: latency-svc-8xbt9 [757.646586ms]
Mar 15 23:11:43.028: INFO: Created: latency-svc-l7h2v
Mar 15 23:11:43.057: INFO: Got endpoints: latency-svc-5ghrv [753.572558ms]
Mar 15 23:11:43.070: INFO: Created: latency-svc-vfwk6
Mar 15 23:11:43.109: INFO: Got endpoints: latency-svc-mz6wg [755.51401ms]
Mar 15 23:11:43.125: INFO: Created: latency-svc-hm6mh
Mar 15 23:11:43.152: INFO: Got endpoints: latency-svc-kss59 [752.719744ms]
Mar 15 23:11:43.172: INFO: Created: latency-svc-lw8zk
Mar 15 23:11:43.202: INFO: Got endpoints: latency-svc-t9rqr [748.745385ms]
Mar 15 23:11:43.215: INFO: Created: latency-svc-vll7f
Mar 15 23:11:43.252: INFO: Got endpoints: latency-svc-ccv42 [747.322834ms]
Mar 15 23:11:43.263: INFO: Created: latency-svc-sndgg
Mar 15 23:11:43.301: INFO: Got endpoints: latency-svc-k9zdg [746.554322ms]
Mar 15 23:11:43.313: INFO: Created: latency-svc-5sk5p
Mar 15 23:11:43.353: INFO: Got endpoints: latency-svc-g6nzd [753.728669ms]
Mar 15 23:11:43.365: INFO: Created: latency-svc-6sq6m
Mar 15 23:11:43.401: INFO: Got endpoints: latency-svc-sdrv2 [746.300715ms]
Mar 15 23:11:43.415: INFO: Created: latency-svc-2jqj9
Mar 15 23:11:43.451: INFO: Got endpoints: latency-svc-c2whl [746.307698ms]
Mar 15 23:11:43.465: INFO: Created: latency-svc-86hwl
Mar 15 23:11:43.502: INFO: Got endpoints: latency-svc-7v8w2 [752.678494ms]
Mar 15 23:11:43.514: INFO: Created: latency-svc-g9lwc
Mar 15 23:11:43.552: INFO: Got endpoints: latency-svc-9kdqf [752.705999ms]
Mar 15 23:11:43.563: INFO: Created: latency-svc-497dl
Mar 15 23:11:43.602: INFO: Got endpoints: latency-svc-fphbp [741.925549ms]
Mar 15 23:11:43.612: INFO: Created: latency-svc-7z2l4
Mar 15 23:11:43.650: INFO: Got endpoints: latency-svc-jr2ft [733.227663ms]
Mar 15 23:11:43.665: INFO: Created: latency-svc-9gjd9
Mar 15 23:11:43.699: INFO: Got endpoints: latency-svc-758gr [747.270954ms]
Mar 15 23:11:43.713: INFO: Created: latency-svc-9vkd9
Mar 15 23:11:43.749: INFO: Got endpoints: latency-svc-l7h2v [740.407156ms]
Mar 15 23:11:43.762: INFO: Created: latency-svc-wdtft
Mar 15 23:11:43.804: INFO: Got endpoints: latency-svc-vfwk6 [746.346849ms]
Mar 15 23:11:43.818: INFO: Created: latency-svc-wpphb
Mar 15 23:11:43.851: INFO: Got endpoints: latency-svc-hm6mh [741.518102ms]
Mar 15 23:11:43.865: INFO: Created: latency-svc-rhnt7
Mar 15 23:11:43.902: INFO: Got endpoints: latency-svc-lw8zk [749.247278ms]
Mar 15 23:11:43.918: INFO: Created: latency-svc-hnmmg
Mar 15 23:11:43.951: INFO: Got endpoints: latency-svc-vll7f [748.907652ms]
Mar 15 23:11:43.965: INFO: Created: latency-svc-gktjr
Mar 15 23:11:44.003: INFO: Got endpoints: latency-svc-sndgg [751.319607ms]
Mar 15 23:11:44.053: INFO: Got endpoints: latency-svc-5sk5p [751.783913ms]
Mar 15 23:11:44.102: INFO: Got endpoints: latency-svc-6sq6m [748.558256ms]
Mar 15 23:11:44.157: INFO: Got endpoints: latency-svc-2jqj9 [756.755098ms]
Mar 15 23:11:44.206: INFO: Got endpoints: latency-svc-86hwl [754.433227ms]
Mar 15 23:11:44.268: INFO: Got endpoints: latency-svc-g9lwc [765.66492ms]
Mar 15 23:11:44.300: INFO: Got endpoints: latency-svc-497dl [748.747788ms]
Mar 15 23:11:44.350: INFO: Got endpoints: latency-svc-7z2l4 [748.203989ms]
Mar 15 23:11:44.400: INFO: Got endpoints: latency-svc-9gjd9 [750.440576ms]
Mar 15 23:11:44.452: INFO: Got endpoints: latency-svc-9vkd9 [752.317549ms]
Mar 15 23:11:44.507: INFO: Got endpoints: latency-svc-wdtft [757.98199ms]
Mar 15 23:11:44.549: INFO: Got endpoints: latency-svc-wpphb [745.26896ms]
Mar 15 23:11:44.603: INFO: Got endpoints: latency-svc-rhnt7 [751.957831ms]
Mar 15 23:11:44.650: INFO: Got endpoints: latency-svc-hnmmg [748.65129ms]
Mar 15 23:11:44.701: INFO: Got endpoints: latency-svc-gktjr [749.265072ms]
Mar 15 23:11:44.701: INFO: Latencies: [53.025134ms 55.25733ms 75.309985ms 108.310366ms 117.012681ms 118.055116ms 131.003393ms 151.117256ms 155.741677ms 177.299326ms 177.916532ms 190.053275ms 194.378521ms 203.457784ms 220.382683ms 223.091192ms 223.757794ms 230.650996ms 235.345698ms 235.535659ms 249.825903ms 258.697054ms 260.064559ms 260.883283ms 261.455219ms 261.552718ms 282.929097ms 285.838164ms 286.089709ms 288.357146ms 297.767572ms 307.225025ms 325.074162ms 332.005715ms 342.345004ms 342.970368ms 349.091732ms 373.876492ms 383.383315ms 387.467013ms 402.577464ms 403.461445ms 403.884972ms 409.97011ms 410.479314ms 413.822037ms 418.537505ms 437.273362ms 439.646703ms 442.402608ms 450.740353ms 456.771739ms 462.408378ms 465.812405ms 467.023917ms 490.832509ms 493.984181ms 503.055009ms 543.422416ms 549.800303ms 576.42173ms 602.593107ms 607.269603ms 650.359275ms 678.420527ms 723.236231ms 733.227663ms 733.381434ms 738.172235ms 740.407156ms 741.518102ms 741.925549ms 744.277219ms 744.630569ms 744.861746ms 745.26896ms 745.354892ms 745.414142ms 745.441223ms 746.096182ms 746.300715ms 746.307698ms 746.346849ms 746.527521ms 746.554322ms 746.715142ms 746.905522ms 746.918473ms 747.270954ms 747.322834ms 747.426846ms 747.514938ms 747.542326ms 747.581197ms 747.583071ms 747.7086ms 747.751969ms 747.765129ms 748.039011ms 748.063092ms 748.203989ms 748.234242ms 748.255846ms 748.259645ms 748.433274ms 748.558256ms 748.613365ms 748.65129ms 748.67697ms 748.745385ms 748.747788ms 748.829918ms 748.878911ms 748.898505ms 748.907652ms 749.14655ms 749.247278ms 749.265072ms 749.268941ms 749.659142ms 749.689445ms 749.69827ms 749.698334ms 749.958625ms 749.986603ms 749.989825ms 749.993142ms 750.123326ms 750.126499ms 750.129221ms 750.171373ms 750.253348ms 750.406073ms 750.440576ms 750.6589ms 750.669623ms 750.802944ms 751.041077ms 751.145376ms 751.173139ms 751.190758ms 751.319607ms 751.361765ms 751.541158ms 751.690824ms 751.726075ms 751.7349ms 751.762256ms 751.783913ms 751.957831ms 751.992561ms 752.008734ms 752.317549ms 752.331151ms 752.353152ms 752.454508ms 752.526276ms 752.6436ms 752.678494ms 752.705999ms 752.719744ms 752.824234ms 753.090559ms 753.195716ms 753.454772ms 753.572558ms 753.728669ms 754.433227ms 754.772798ms 754.894072ms 755.248219ms 755.51401ms 755.763666ms 755.966804ms 756.070314ms 756.196369ms 756.238012ms 756.755098ms 756.962451ms 757.151068ms 757.277487ms 757.646586ms 757.98199ms 758.885015ms 765.66492ms 793.277823ms 795.024617ms 795.464081ms 796.370146ms 796.662303ms 799.005128ms 799.263412ms 799.423073ms 799.540292ms 800.832589ms 800.905118ms 802.611905ms 802.799804ms 803.151984ms 804.284998ms]
Mar 15 23:11:44.702: INFO: 50 %ile: 748.203989ms
Mar 15 23:11:44.702: INFO: 90 %ile: 757.277487ms
Mar 15 23:11:44.702: INFO: 99 %ile: 803.151984ms
Mar 15 23:11:44.702: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Mar 15 23:11:44.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-5511" for this suite. 03/15/23 23:11:44.711
------------------------------
• [SLOW TEST] [10.821 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:11:33.896
    Mar 15 23:11:33.896: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename svc-latency 03/15/23 23:11:33.897
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:11:33.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:11:33.916
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Mar 15 23:11:33.920: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5511 03/15/23 23:11:33.921
    I0315 23:11:33.927016      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5511, replica count: 1
    I0315 23:11:34.977725      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0315 23:11:35.978737      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 15 23:11:36.095: INFO: Created: latency-svc-wz9qn
    Mar 15 23:11:36.110: INFO: Got endpoints: latency-svc-wz9qn [30.763619ms]
    Mar 15 23:11:36.137: INFO: Created: latency-svc-l7cx6
    Mar 15 23:11:36.164: INFO: Got endpoints: latency-svc-l7cx6 [53.025134ms]
    Mar 15 23:11:36.166: INFO: Created: latency-svc-lsgct
    Mar 15 23:11:36.166: INFO: Got endpoints: latency-svc-lsgct [55.25733ms]
    Mar 15 23:11:36.183: INFO: Created: latency-svc-nwdl9
    Mar 15 23:11:36.206: INFO: Created: latency-svc-fx4cv
    Mar 15 23:11:36.220: INFO: Got endpoints: latency-svc-nwdl9 [108.310366ms]
    Mar 15 23:11:36.230: INFO: Got endpoints: latency-svc-fx4cv [118.055116ms]
    Mar 15 23:11:36.237: INFO: Created: latency-svc-pnjxf
    Mar 15 23:11:36.245: INFO: Created: latency-svc-zz8ml
    Mar 15 23:11:36.269: INFO: Got endpoints: latency-svc-pnjxf [155.741677ms]
    Mar 15 23:11:36.291: INFO: Got endpoints: latency-svc-zz8ml [177.299326ms]
    Mar 15 23:11:36.318: INFO: Created: latency-svc-zmpwb
    Mar 15 23:11:36.323: INFO: Created: latency-svc-jz2f5
    Mar 15 23:11:36.339: INFO: Created: latency-svc-pjdz9
    Mar 15 23:11:36.363: INFO: Got endpoints: latency-svc-zmpwb [249.825903ms]
    Mar 15 23:11:36.372: INFO: Got endpoints: latency-svc-jz2f5 [258.697054ms]
    Mar 15 23:11:36.374: INFO: Got endpoints: latency-svc-pjdz9 [260.064559ms]
    Mar 15 23:11:36.467: INFO: Created: latency-svc-5kc6k
    Mar 15 23:11:36.483: INFO: Created: latency-svc-48kjr
    Mar 15 23:11:36.483: INFO: Created: latency-svc-jwbpz
    Mar 15 23:11:36.484: INFO: Created: latency-svc-5hb5f
    Mar 15 23:11:36.485: INFO: Created: latency-svc-4tjfk
    Mar 15 23:11:36.485: INFO: Created: latency-svc-kwp8r
    Mar 15 23:11:36.485: INFO: Created: latency-svc-dg6rm
    Mar 15 23:11:36.487: INFO: Created: latency-svc-bc88w
    Mar 15 23:11:36.488: INFO: Got endpoints: latency-svc-5kc6k [373.876492ms]
    Mar 15 23:11:36.489: INFO: Created: latency-svc-dcmqm
    Mar 15 23:11:36.493: INFO: Created: latency-svc-cc4x7
    Mar 15 23:11:36.493: INFO: Created: latency-svc-czgbp
    Mar 15 23:11:36.493: INFO: Created: latency-svc-sbqlb
    Mar 15 23:11:36.493: INFO: Created: latency-svc-s5jfb
    Mar 15 23:11:36.494: INFO: Created: latency-svc-k2f5t
    Mar 15 23:11:36.497: INFO: Created: latency-svc-lc7t2
    Mar 15 23:11:36.499: INFO: Got endpoints: latency-svc-lc7t2 [131.003393ms]
    Mar 15 23:11:36.523: INFO: Got endpoints: latency-svc-jwbpz [151.117256ms]
    Mar 15 23:11:36.543: INFO: Created: latency-svc-l66gj
    Mar 15 23:11:36.550: INFO: Got endpoints: latency-svc-5hb5f [437.273362ms]
    Mar 15 23:11:36.551: INFO: Got endpoints: latency-svc-dg6rm [383.383315ms]
    Mar 15 23:11:36.551: INFO: Got endpoints: latency-svc-4tjfk [439.646703ms]
    Mar 15 23:11:36.552: INFO: Got endpoints: latency-svc-48kjr [177.916532ms]
    Mar 15 23:11:36.552: INFO: Got endpoints: latency-svc-kwp8r [387.467013ms]
    Mar 15 23:11:36.564: INFO: Got endpoints: latency-svc-bc88w [342.345004ms]
    Mar 15 23:11:36.575: INFO: Got endpoints: latency-svc-dcmqm [342.970368ms]
    Mar 15 23:11:36.575: INFO: Got endpoints: latency-svc-czgbp [462.408378ms]
    Mar 15 23:11:36.576: INFO: Got endpoints: latency-svc-k2f5t [307.225025ms]
    Mar 15 23:11:36.576: INFO: Got endpoints: latency-svc-s5jfb [282.929097ms]
    Mar 15 23:11:36.580: INFO: Got endpoints: latency-svc-sbqlb [467.023917ms]
    Mar 15 23:11:36.604: INFO: Got endpoints: latency-svc-cc4x7 [490.832509ms]
    Mar 15 23:11:36.605: INFO: Got endpoints: latency-svc-l66gj [117.012681ms]
    Mar 15 23:11:36.737: INFO: Created: latency-svc-qqkt2
    Mar 15 23:11:36.766: INFO: Created: latency-svc-lcph7
    Mar 15 23:11:36.766: INFO: Created: latency-svc-znvb9
    Mar 15 23:11:36.766: INFO: Created: latency-svc-5nxzr
    Mar 15 23:11:36.774: INFO: Got endpoints: latency-svc-qqkt2 [223.757794ms]
    Mar 15 23:11:36.778: INFO: Created: latency-svc-dvds8
    Mar 15 23:11:36.778: INFO: Created: latency-svc-7vf2r
    Mar 15 23:11:36.778: INFO: Created: latency-svc-8fn42
    Mar 15 23:11:36.778: INFO: Created: latency-svc-ks86s
    Mar 15 23:11:36.778: INFO: Created: latency-svc-lhlbh
    Mar 15 23:11:36.778: INFO: Got endpoints: latency-svc-lhlbh [203.457784ms]
    Mar 15 23:11:36.779: INFO: Created: latency-svc-d4zpb
    Mar 15 23:11:36.779: INFO: Created: latency-svc-f5q6m
    Mar 15 23:11:36.779: INFO: Created: latency-svc-9zmnq
    Mar 15 23:11:36.794: INFO: Created: latency-svc-4pj57
    Mar 15 23:11:36.795: INFO: Got endpoints: latency-svc-d4zpb [230.650996ms]
    Mar 15 23:11:36.795: INFO: Got endpoints: latency-svc-5nxzr [190.053275ms]
    Mar 15 23:11:36.796: INFO: Got endpoints: latency-svc-ks86s [220.382683ms]
    Mar 15 23:11:36.796: INFO: Created: latency-svc-6dm4m
    Mar 15 23:11:36.812: INFO: Got endpoints: latency-svc-6dm4m [288.357146ms]
    Mar 15 23:11:36.812: INFO: Got endpoints: latency-svc-8fn42 [235.345698ms]
    Mar 15 23:11:36.812: INFO: Got endpoints: latency-svc-7vf2r [260.883283ms]
    Mar 15 23:11:36.812: INFO: Got endpoints: latency-svc-4pj57 [235.535659ms]
    Mar 15 23:11:36.813: INFO: Created: latency-svc-r4qtf
    Mar 15 23:11:36.813: INFO: Got endpoints: latency-svc-r4qtf [261.455219ms]
    Mar 15 23:11:36.824: INFO: Got endpoints: latency-svc-dvds8 [325.074162ms]
    Mar 15 23:11:36.827: INFO: Got endpoints: latency-svc-lcph7 [223.091192ms]
    Mar 15 23:11:36.835: INFO: Created: latency-svc-b54r7
    Mar 15 23:11:36.838: INFO: Got endpoints: latency-svc-9zmnq [286.089709ms]
    Mar 15 23:11:36.838: INFO: Got endpoints: latency-svc-f5q6m [285.838164ms]
    Mar 15 23:11:36.842: INFO: Got endpoints: latency-svc-znvb9 [261.552718ms]
    Mar 15 23:11:36.849: INFO: Got endpoints: latency-svc-b54r7 [75.309985ms]
    Mar 15 23:11:37.021: INFO: Created: latency-svc-vm8dr
    Mar 15 23:11:37.022: INFO: Created: latency-svc-42sln
    Mar 15 23:11:37.022: INFO: Created: latency-svc-mvgrs
    Mar 15 23:11:37.023: INFO: Created: latency-svc-8wlv2
    Mar 15 23:11:37.023: INFO: Created: latency-svc-qlpgq
    Mar 15 23:11:37.023: INFO: Created: latency-svc-jbw8q
    Mar 15 23:11:37.024: INFO: Created: latency-svc-lbz7b
    Mar 15 23:11:37.024: INFO: Created: latency-svc-x2nrk
    Mar 15 23:11:37.024: INFO: Created: latency-svc-vmpb5
    Mar 15 23:11:37.024: INFO: Created: latency-svc-fmv49
    Mar 15 23:11:37.026: INFO: Created: latency-svc-s527t
    Mar 15 23:11:37.026: INFO: Created: latency-svc-f9wvr
    Mar 15 23:11:37.027: INFO: Created: latency-svc-ddtwx
    Mar 15 23:11:37.027: INFO: Created: latency-svc-rrn8m
    Mar 15 23:11:37.028: INFO: Created: latency-svc-t6nq4
    Mar 15 23:11:37.036: INFO: Got endpoints: latency-svc-mvgrs [194.378521ms]
    Mar 15 23:11:37.140: INFO: Got endpoints: latency-svc-8wlv2 [297.767572ms]
    Mar 15 23:11:37.144: INFO: Got endpoints: latency-svc-vm8dr [332.005715ms]
    Mar 15 23:11:37.197: INFO: Created: latency-svc-flgq8
    Mar 15 23:11:37.197: INFO: Got endpoints: latency-svc-jbw8q [418.537505ms]
    Mar 15 23:11:37.199: INFO: Got endpoints: latency-svc-qlpgq [349.091732ms]
    Mar 15 23:11:37.199: INFO: Got endpoints: latency-svc-vmpb5 [402.577464ms]
    Mar 15 23:11:37.199: INFO: Got endpoints: latency-svc-x2nrk [403.461445ms]
    Mar 15 23:11:37.199: INFO: Got endpoints: latency-svc-lbz7b [403.884972ms]
    Mar 15 23:11:37.223: INFO: Got endpoints: latency-svc-s527t [409.97011ms]
    Mar 15 23:11:37.226: INFO: Created: latency-svc-kvlq6
    Mar 15 23:11:37.226: INFO: Got endpoints: latency-svc-f9wvr [413.822037ms]
    Mar 15 23:11:37.240: INFO: Created: latency-svc-zpcwh
    Mar 15 23:11:37.255: INFO: Got endpoints: latency-svc-rrn8m [442.402608ms]
    Mar 15 23:11:37.273: INFO: Created: latency-svc-czljb
    Mar 15 23:11:37.293: INFO: Created: latency-svc-8k82j
    Mar 15 23:11:37.304: INFO: Created: latency-svc-j6lbj
    Mar 15 23:11:37.306: INFO: Got endpoints: latency-svc-t6nq4 [493.984181ms]
    Mar 15 23:11:37.330: INFO: Created: latency-svc-9pqtb
    Mar 15 23:11:37.334: INFO: Created: latency-svc-5j7gd
    Mar 15 23:11:37.357: INFO: Created: latency-svc-7l2nk
    Mar 15 23:11:37.368: INFO: Got endpoints: latency-svc-ddtwx [543.422416ms]
    Mar 15 23:11:37.381: INFO: Created: latency-svc-85mcs
    Mar 15 23:11:37.391: INFO: Created: latency-svc-k48rc
    Mar 15 23:11:37.402: INFO: Created: latency-svc-4b7td
    Mar 15 23:11:37.404: INFO: Got endpoints: latency-svc-42sln [576.42173ms]
    Mar 15 23:11:37.417: INFO: Created: latency-svc-sqmp5
    Mar 15 23:11:37.420: INFO: Created: latency-svc-xn5zr
    Mar 15 23:11:37.449: INFO: Got endpoints: latency-svc-fmv49 [607.269603ms]
    Mar 15 23:11:37.461: INFO: Created: latency-svc-mjv9p
    Mar 15 23:11:37.503: INFO: Got endpoints: latency-svc-flgq8 [465.812405ms]
    Mar 15 23:11:37.515: INFO: Created: latency-svc-28rt2
    Mar 15 23:11:37.551: INFO: Got endpoints: latency-svc-kvlq6 [410.479314ms]
    Mar 15 23:11:37.564: INFO: Created: latency-svc-cp79l
    Mar 15 23:11:37.601: INFO: Got endpoints: latency-svc-zpcwh [456.771739ms]
    Mar 15 23:11:37.615: INFO: Created: latency-svc-n4lk7
    Mar 15 23:11:37.649: INFO: Got endpoints: latency-svc-czljb [450.740353ms]
    Mar 15 23:11:37.665: INFO: Created: latency-svc-484zb
    Mar 15 23:11:37.702: INFO: Got endpoints: latency-svc-8k82j [503.055009ms]
    Mar 15 23:11:37.714: INFO: Created: latency-svc-h7xqx
    Mar 15 23:11:37.749: INFO: Got endpoints: latency-svc-j6lbj [549.800303ms]
    Mar 15 23:11:37.762: INFO: Created: latency-svc-8fcrr
    Mar 15 23:11:37.802: INFO: Got endpoints: latency-svc-9pqtb [602.593107ms]
    Mar 15 23:11:37.813: INFO: Created: latency-svc-ph985
    Mar 15 23:11:37.852: INFO: Got endpoints: latency-svc-5j7gd [650.359275ms]
    Mar 15 23:11:37.865: INFO: Created: latency-svc-vl46z
    Mar 15 23:11:37.901: INFO: Got endpoints: latency-svc-7l2nk [678.420527ms]
    Mar 15 23:11:37.915: INFO: Created: latency-svc-cg58c
    Mar 15 23:11:37.952: INFO: Got endpoints: latency-svc-85mcs [723.236231ms]
    Mar 15 23:11:37.963: INFO: Created: latency-svc-zmnh6
    Mar 15 23:11:38.001: INFO: Got endpoints: latency-svc-k48rc [746.096182ms]
    Mar 15 23:11:38.014: INFO: Created: latency-svc-qq9t4
    Mar 15 23:11:38.051: INFO: Got endpoints: latency-svc-4b7td [744.630569ms]
    Mar 15 23:11:38.072: INFO: Created: latency-svc-wxq8h
    Mar 15 23:11:38.102: INFO: Got endpoints: latency-svc-sqmp5 [733.381434ms]
    Mar 15 23:11:38.122: INFO: Created: latency-svc-wrrn8
    Mar 15 23:11:38.155: INFO: Got endpoints: latency-svc-xn5zr [750.802944ms]
    Mar 15 23:11:38.173: INFO: Created: latency-svc-5b7j4
    Mar 15 23:11:38.203: INFO: Got endpoints: latency-svc-mjv9p [753.454772ms]
    Mar 15 23:11:38.215: INFO: Created: latency-svc-857sm
    Mar 15 23:11:38.251: INFO: Got endpoints: latency-svc-28rt2 [747.583071ms]
    Mar 15 23:11:38.268: INFO: Created: latency-svc-dhjnq
    Mar 15 23:11:38.303: INFO: Got endpoints: latency-svc-cp79l [752.454508ms]
    Mar 15 23:11:38.315: INFO: Created: latency-svc-2z577
    Mar 15 23:11:38.351: INFO: Got endpoints: latency-svc-n4lk7 [749.989825ms]
    Mar 15 23:11:38.368: INFO: Created: latency-svc-9nhm5
    Mar 15 23:11:38.401: INFO: Got endpoints: latency-svc-484zb [751.726075ms]
    Mar 15 23:11:38.416: INFO: Created: latency-svc-qcg5b
    Mar 15 23:11:38.452: INFO: Got endpoints: latency-svc-h7xqx [750.253348ms]
    Mar 15 23:11:38.469: INFO: Created: latency-svc-7hj6c
    Mar 15 23:11:38.503: INFO: Got endpoints: latency-svc-8fcrr [753.195716ms]
    Mar 15 23:11:38.515: INFO: Created: latency-svc-29mwc
    Mar 15 23:11:38.550: INFO: Got endpoints: latency-svc-ph985 [748.259645ms]
    Mar 15 23:11:38.566: INFO: Created: latency-svc-7qvb8
    Mar 15 23:11:38.600: INFO: Got endpoints: latency-svc-vl46z [748.063092ms]
    Mar 15 23:11:38.612: INFO: Created: latency-svc-jwppq
    Mar 15 23:11:38.652: INFO: Got endpoints: latency-svc-cg58c [750.123326ms]
    Mar 15 23:11:38.664: INFO: Created: latency-svc-wxxzx
    Mar 15 23:11:38.700: INFO: Got endpoints: latency-svc-zmnh6 [747.7086ms]
    Mar 15 23:11:38.714: INFO: Created: latency-svc-4hdcc
    Mar 15 23:11:38.751: INFO: Got endpoints: latency-svc-qq9t4 [749.958625ms]
    Mar 15 23:11:38.764: INFO: Created: latency-svc-pdskp
    Mar 15 23:11:38.801: INFO: Got endpoints: latency-svc-wxq8h [749.993142ms]
    Mar 15 23:11:38.813: INFO: Created: latency-svc-n7bq5
    Mar 15 23:11:38.849: INFO: Got endpoints: latency-svc-wrrn8 [747.581197ms]
    Mar 15 23:11:38.863: INFO: Created: latency-svc-q4qn5
    Mar 15 23:11:38.903: INFO: Got endpoints: latency-svc-5b7j4 [748.255846ms]
    Mar 15 23:11:38.921: INFO: Created: latency-svc-6b2j7
    Mar 15 23:11:38.955: INFO: Got endpoints: latency-svc-857sm [751.690824ms]
    Mar 15 23:11:38.968: INFO: Created: latency-svc-xzmf9
    Mar 15 23:11:39.001: INFO: Got endpoints: latency-svc-dhjnq [749.69827ms]
    Mar 15 23:11:39.017: INFO: Created: latency-svc-4jlqx
    Mar 15 23:11:39.050: INFO: Got endpoints: latency-svc-2z577 [746.715142ms]
    Mar 15 23:11:39.065: INFO: Created: latency-svc-d88vp
    Mar 15 23:11:39.104: INFO: Got endpoints: latency-svc-9nhm5 [752.331151ms]
    Mar 15 23:11:39.119: INFO: Created: latency-svc-k86w5
    Mar 15 23:11:39.156: INFO: Got endpoints: latency-svc-qcg5b [754.894072ms]
    Mar 15 23:11:39.174: INFO: Created: latency-svc-2ds9w
    Mar 15 23:11:39.209: INFO: Got endpoints: latency-svc-7hj6c [756.196369ms]
    Mar 15 23:11:39.220: INFO: Created: latency-svc-95w5w
    Mar 15 23:11:39.254: INFO: Got endpoints: latency-svc-29mwc [751.361765ms]
    Mar 15 23:11:39.274: INFO: Created: latency-svc-ktn6l
    Mar 15 23:11:39.302: INFO: Got endpoints: latency-svc-7qvb8 [752.008734ms]
    Mar 15 23:11:39.319: INFO: Created: latency-svc-dzf4d
    Mar 15 23:11:39.356: INFO: Got endpoints: latency-svc-jwppq [756.070314ms]
    Mar 15 23:11:39.370: INFO: Created: latency-svc-642jg
    Mar 15 23:11:39.401: INFO: Got endpoints: latency-svc-wxxzx [749.268941ms]
    Mar 15 23:11:39.427: INFO: Created: latency-svc-8lfmh
    Mar 15 23:11:39.452: INFO: Got endpoints: latency-svc-4hdcc [751.992561ms]
    Mar 15 23:11:39.465: INFO: Created: latency-svc-g8889
    Mar 15 23:11:39.502: INFO: Got endpoints: latency-svc-pdskp [751.173139ms]
    Mar 15 23:11:39.517: INFO: Created: latency-svc-bm4jn
    Mar 15 23:11:39.551: INFO: Got endpoints: latency-svc-n7bq5 [750.171373ms]
    Mar 15 23:11:39.561: INFO: Created: latency-svc-2d7km
    Mar 15 23:11:39.600: INFO: Got endpoints: latency-svc-q4qn5 [751.190758ms]
    Mar 15 23:11:39.616: INFO: Created: latency-svc-6s4z5
    Mar 15 23:11:39.650: INFO: Got endpoints: latency-svc-6b2j7 [746.918473ms]
    Mar 15 23:11:39.664: INFO: Created: latency-svc-wq926
    Mar 15 23:11:39.700: INFO: Got endpoints: latency-svc-xzmf9 [745.414142ms]
    Mar 15 23:11:39.714: INFO: Created: latency-svc-zzk7f
    Mar 15 23:11:39.802: INFO: Got endpoints: latency-svc-4jlqx [800.832589ms]
    Mar 15 23:11:39.813: INFO: Created: latency-svc-7vmmp
    Mar 15 23:11:39.853: INFO: Got endpoints: latency-svc-d88vp [802.799804ms]
    Mar 15 23:11:39.864: INFO: Created: latency-svc-pl9rl
    Mar 15 23:11:39.900: INFO: Got endpoints: latency-svc-k86w5 [796.662303ms]
    Mar 15 23:11:39.913: INFO: Created: latency-svc-22jkv
    Mar 15 23:11:39.951: INFO: Got endpoints: latency-svc-2ds9w [795.464081ms]
    Mar 15 23:11:39.965: INFO: Created: latency-svc-qndrl
    Mar 15 23:11:40.002: INFO: Got endpoints: latency-svc-95w5w [793.277823ms]
    Mar 15 23:11:40.013: INFO: Created: latency-svc-gfmgs
    Mar 15 23:11:40.051: INFO: Got endpoints: latency-svc-ktn6l [796.370146ms]
    Mar 15 23:11:40.066: INFO: Created: latency-svc-2kwqp
    Mar 15 23:11:40.105: INFO: Got endpoints: latency-svc-dzf4d [803.151984ms]
    Mar 15 23:11:40.127: INFO: Created: latency-svc-86nm6
    Mar 15 23:11:40.151: INFO: Got endpoints: latency-svc-642jg [795.024617ms]
    Mar 15 23:11:40.175: INFO: Created: latency-svc-6rm8b
    Mar 15 23:11:40.200: INFO: Got endpoints: latency-svc-8lfmh [799.005128ms]
    Mar 15 23:11:40.217: INFO: Created: latency-svc-9qv9z
    Mar 15 23:11:40.251: INFO: Got endpoints: latency-svc-g8889 [799.423073ms]
    Mar 15 23:11:40.271: INFO: Created: latency-svc-4vprn
    Mar 15 23:11:40.305: INFO: Got endpoints: latency-svc-bm4jn [802.611905ms]
    Mar 15 23:11:40.317: INFO: Created: latency-svc-wntqc
    Mar 15 23:11:40.352: INFO: Got endpoints: latency-svc-2d7km [800.905118ms]
    Mar 15 23:11:40.369: INFO: Created: latency-svc-ffjb8
    Mar 15 23:11:40.400: INFO: Got endpoints: latency-svc-6s4z5 [799.263412ms]
    Mar 15 23:11:40.413: INFO: Created: latency-svc-f87h6
    Mar 15 23:11:40.450: INFO: Got endpoints: latency-svc-wq926 [799.540292ms]
    Mar 15 23:11:40.466: INFO: Created: latency-svc-csm9n
    Mar 15 23:11:40.504: INFO: Got endpoints: latency-svc-zzk7f [804.284998ms]
    Mar 15 23:11:40.517: INFO: Created: latency-svc-2z4bh
    Mar 15 23:11:40.553: INFO: Got endpoints: latency-svc-7vmmp [751.145376ms]
    Mar 15 23:11:40.566: INFO: Created: latency-svc-m2jps
    Mar 15 23:11:40.603: INFO: Got endpoints: latency-svc-pl9rl [749.689445ms]
    Mar 15 23:11:40.615: INFO: Created: latency-svc-tmh8d
    Mar 15 23:11:40.652: INFO: Got endpoints: latency-svc-22jkv [751.7349ms]
    Mar 15 23:11:40.665: INFO: Created: latency-svc-pmx6p
    Mar 15 23:11:40.701: INFO: Got endpoints: latency-svc-qndrl [749.659142ms]
    Mar 15 23:11:40.716: INFO: Created: latency-svc-k6swf
    Mar 15 23:11:40.752: INFO: Got endpoints: latency-svc-gfmgs [749.986603ms]
    Mar 15 23:11:40.765: INFO: Created: latency-svc-b4g46
    Mar 15 23:11:40.804: INFO: Got endpoints: latency-svc-2kwqp [753.090559ms]
    Mar 15 23:11:40.816: INFO: Created: latency-svc-vmg2h
    Mar 15 23:11:40.855: INFO: Got endpoints: latency-svc-86nm6 [749.14655ms]
    Mar 15 23:11:40.876: INFO: Created: latency-svc-qj2ks
    Mar 15 23:11:40.903: INFO: Got endpoints: latency-svc-6rm8b [751.762256ms]
    Mar 15 23:11:40.917: INFO: Created: latency-svc-4jgll
    Mar 15 23:11:40.951: INFO: Got endpoints: latency-svc-9qv9z [750.669623ms]
    Mar 15 23:11:40.964: INFO: Created: latency-svc-6wxh6
    Mar 15 23:11:41.004: INFO: Got endpoints: latency-svc-4vprn [752.824234ms]
    Mar 15 23:11:41.046: INFO: Created: latency-svc-69s2j
    Mar 15 23:11:41.053: INFO: Got endpoints: latency-svc-wntqc [748.234242ms]
    Mar 15 23:11:41.092: INFO: Created: latency-svc-rq7q5
    Mar 15 23:11:41.103: INFO: Got endpoints: latency-svc-ffjb8 [750.126499ms]
    Mar 15 23:11:41.121: INFO: Created: latency-svc-h66gc
    Mar 15 23:11:41.152: INFO: Got endpoints: latency-svc-f87h6 [752.353152ms]
    Mar 15 23:11:41.183: INFO: Created: latency-svc-zghhv
    Mar 15 23:11:41.207: INFO: Got endpoints: latency-svc-csm9n [757.151068ms]
    Mar 15 23:11:41.225: INFO: Created: latency-svc-whnhm
    Mar 15 23:11:41.253: INFO: Got endpoints: latency-svc-2z4bh [748.039011ms]
    Mar 15 23:11:41.272: INFO: Created: latency-svc-c2vwp
    Mar 15 23:11:41.302: INFO: Got endpoints: latency-svc-m2jps [748.878911ms]
    Mar 15 23:11:41.318: INFO: Created: latency-svc-p9mfz
    Mar 15 23:11:41.358: INFO: Got endpoints: latency-svc-tmh8d [755.763666ms]
    Mar 15 23:11:41.380: INFO: Created: latency-svc-vngzr
    Mar 15 23:11:41.400: INFO: Got endpoints: latency-svc-pmx6p [747.765129ms]
    Mar 15 23:11:41.414: INFO: Created: latency-svc-ctw7s
    Mar 15 23:11:41.450: INFO: Got endpoints: latency-svc-k6swf [748.67697ms]
    Mar 15 23:11:41.469: INFO: Created: latency-svc-wwvjs
    Mar 15 23:11:41.501: INFO: Got endpoints: latency-svc-b4g46 [748.433274ms]
    Mar 15 23:11:41.515: INFO: Created: latency-svc-wz82c
    Mar 15 23:11:41.550: INFO: Got endpoints: latency-svc-vmg2h [746.527521ms]
    Mar 15 23:11:41.563: INFO: Created: latency-svc-6c897
    Mar 15 23:11:41.600: INFO: Got endpoints: latency-svc-qj2ks [745.441223ms]
    Mar 15 23:11:41.613: INFO: Created: latency-svc-hrr6h
    Mar 15 23:11:41.660: INFO: Got endpoints: latency-svc-4jgll [757.277487ms]
    Mar 15 23:11:41.680: INFO: Created: latency-svc-wr8lf
    Mar 15 23:11:41.702: INFO: Got endpoints: latency-svc-6wxh6 [751.541158ms]
    Mar 15 23:11:41.722: INFO: Created: latency-svc-czhtl
    Mar 15 23:11:41.760: INFO: Got endpoints: latency-svc-69s2j [755.248219ms]
    Mar 15 23:11:41.771: INFO: Created: latency-svc-b27q5
    Mar 15 23:11:41.803: INFO: Got endpoints: latency-svc-rq7q5 [750.129221ms]
    Mar 15 23:11:41.816: INFO: Created: latency-svc-4nlmv
    Mar 15 23:11:41.851: INFO: Got endpoints: latency-svc-h66gc [747.426846ms]
    Mar 15 23:11:41.866: INFO: Created: latency-svc-7pbvk
    Mar 15 23:11:41.900: INFO: Got endpoints: latency-svc-zghhv [746.905522ms]
    Mar 15 23:11:41.912: INFO: Created: latency-svc-8ts6f
    Mar 15 23:11:41.957: INFO: Got endpoints: latency-svc-whnhm [749.698334ms]
    Mar 15 23:11:41.968: INFO: Created: latency-svc-qdw6c
    Mar 15 23:11:42.000: INFO: Got endpoints: latency-svc-c2vwp [747.514938ms]
    Mar 15 23:11:42.013: INFO: Created: latency-svc-khj88
    Mar 15 23:11:42.051: INFO: Got endpoints: latency-svc-p9mfz [748.829918ms]
    Mar 15 23:11:42.065: INFO: Created: latency-svc-rf7mx
    Mar 15 23:11:42.103: INFO: Got endpoints: latency-svc-vngzr [744.277219ms]
    Mar 15 23:11:42.119: INFO: Created: latency-svc-mj5jg
    Mar 15 23:11:42.157: INFO: Got endpoints: latency-svc-ctw7s [756.962451ms]
    Mar 15 23:11:42.172: INFO: Created: latency-svc-6p5cz
    Mar 15 23:11:42.206: INFO: Got endpoints: latency-svc-wwvjs [756.238012ms]
    Mar 15 23:11:42.218: INFO: Created: latency-svc-9wgh2
    Mar 15 23:11:42.251: INFO: Got endpoints: latency-svc-wz82c [750.406073ms]
    Mar 15 23:11:42.263: INFO: Created: latency-svc-8xbt9
    Mar 15 23:11:42.303: INFO: Got endpoints: latency-svc-6c897 [752.526276ms]
    Mar 15 23:11:42.318: INFO: Created: latency-svc-5ghrv
    Mar 15 23:11:42.353: INFO: Got endpoints: latency-svc-hrr6h [752.6436ms]
    Mar 15 23:11:42.365: INFO: Created: latency-svc-mz6wg
    Mar 15 23:11:42.399: INFO: Got endpoints: latency-svc-wr8lf [738.172235ms]
    Mar 15 23:11:42.419: INFO: Created: latency-svc-kss59
    Mar 15 23:11:42.453: INFO: Got endpoints: latency-svc-czhtl [750.6589ms]
    Mar 15 23:11:42.465: INFO: Created: latency-svc-t9rqr
    Mar 15 23:11:42.504: INFO: Got endpoints: latency-svc-b27q5 [744.861746ms]
    Mar 15 23:11:42.528: INFO: Created: latency-svc-ccv42
    Mar 15 23:11:42.554: INFO: Got endpoints: latency-svc-4nlmv [751.041077ms]
    Mar 15 23:11:42.581: INFO: Created: latency-svc-k9zdg
    Mar 15 23:11:42.599: INFO: Got endpoints: latency-svc-7pbvk [748.613365ms]
    Mar 15 23:11:42.613: INFO: Created: latency-svc-g6nzd
    Mar 15 23:11:42.654: INFO: Got endpoints: latency-svc-8ts6f [754.772798ms]
    Mar 15 23:11:42.666: INFO: Created: latency-svc-sdrv2
    Mar 15 23:11:42.705: INFO: Got endpoints: latency-svc-qdw6c [747.751969ms]
    Mar 15 23:11:42.718: INFO: Created: latency-svc-c2whl
    Mar 15 23:11:42.749: INFO: Got endpoints: latency-svc-khj88 [748.898505ms]
    Mar 15 23:11:42.764: INFO: Created: latency-svc-7v8w2
    Mar 15 23:11:42.799: INFO: Got endpoints: latency-svc-rf7mx [747.542326ms]
    Mar 15 23:11:42.817: INFO: Created: latency-svc-9kdqf
    Mar 15 23:11:42.860: INFO: Got endpoints: latency-svc-mj5jg [755.966804ms]
    Mar 15 23:11:42.889: INFO: Created: latency-svc-fphbp
    Mar 15 23:11:42.916: INFO: Got endpoints: latency-svc-6p5cz [758.885015ms]
    Mar 15 23:11:42.950: INFO: Created: latency-svc-jr2ft
    Mar 15 23:11:42.952: INFO: Got endpoints: latency-svc-9wgh2 [745.354892ms]
    Mar 15 23:11:43.005: INFO: Created: latency-svc-758gr
    Mar 15 23:11:43.009: INFO: Got endpoints: latency-svc-8xbt9 [757.646586ms]
    Mar 15 23:11:43.028: INFO: Created: latency-svc-l7h2v
    Mar 15 23:11:43.057: INFO: Got endpoints: latency-svc-5ghrv [753.572558ms]
    Mar 15 23:11:43.070: INFO: Created: latency-svc-vfwk6
    Mar 15 23:11:43.109: INFO: Got endpoints: latency-svc-mz6wg [755.51401ms]
    Mar 15 23:11:43.125: INFO: Created: latency-svc-hm6mh
    Mar 15 23:11:43.152: INFO: Got endpoints: latency-svc-kss59 [752.719744ms]
    Mar 15 23:11:43.172: INFO: Created: latency-svc-lw8zk
    Mar 15 23:11:43.202: INFO: Got endpoints: latency-svc-t9rqr [748.745385ms]
    Mar 15 23:11:43.215: INFO: Created: latency-svc-vll7f
    Mar 15 23:11:43.252: INFO: Got endpoints: latency-svc-ccv42 [747.322834ms]
    Mar 15 23:11:43.263: INFO: Created: latency-svc-sndgg
    Mar 15 23:11:43.301: INFO: Got endpoints: latency-svc-k9zdg [746.554322ms]
    Mar 15 23:11:43.313: INFO: Created: latency-svc-5sk5p
    Mar 15 23:11:43.353: INFO: Got endpoints: latency-svc-g6nzd [753.728669ms]
    Mar 15 23:11:43.365: INFO: Created: latency-svc-6sq6m
    Mar 15 23:11:43.401: INFO: Got endpoints: latency-svc-sdrv2 [746.300715ms]
    Mar 15 23:11:43.415: INFO: Created: latency-svc-2jqj9
    Mar 15 23:11:43.451: INFO: Got endpoints: latency-svc-c2whl [746.307698ms]
    Mar 15 23:11:43.465: INFO: Created: latency-svc-86hwl
    Mar 15 23:11:43.502: INFO: Got endpoints: latency-svc-7v8w2 [752.678494ms]
    Mar 15 23:11:43.514: INFO: Created: latency-svc-g9lwc
    Mar 15 23:11:43.552: INFO: Got endpoints: latency-svc-9kdqf [752.705999ms]
    Mar 15 23:11:43.563: INFO: Created: latency-svc-497dl
    Mar 15 23:11:43.602: INFO: Got endpoints: latency-svc-fphbp [741.925549ms]
    Mar 15 23:11:43.612: INFO: Created: latency-svc-7z2l4
    Mar 15 23:11:43.650: INFO: Got endpoints: latency-svc-jr2ft [733.227663ms]
    Mar 15 23:11:43.665: INFO: Created: latency-svc-9gjd9
    Mar 15 23:11:43.699: INFO: Got endpoints: latency-svc-758gr [747.270954ms]
    Mar 15 23:11:43.713: INFO: Created: latency-svc-9vkd9
    Mar 15 23:11:43.749: INFO: Got endpoints: latency-svc-l7h2v [740.407156ms]
    Mar 15 23:11:43.762: INFO: Created: latency-svc-wdtft
    Mar 15 23:11:43.804: INFO: Got endpoints: latency-svc-vfwk6 [746.346849ms]
    Mar 15 23:11:43.818: INFO: Created: latency-svc-wpphb
    Mar 15 23:11:43.851: INFO: Got endpoints: latency-svc-hm6mh [741.518102ms]
    Mar 15 23:11:43.865: INFO: Created: latency-svc-rhnt7
    Mar 15 23:11:43.902: INFO: Got endpoints: latency-svc-lw8zk [749.247278ms]
    Mar 15 23:11:43.918: INFO: Created: latency-svc-hnmmg
    Mar 15 23:11:43.951: INFO: Got endpoints: latency-svc-vll7f [748.907652ms]
    Mar 15 23:11:43.965: INFO: Created: latency-svc-gktjr
    Mar 15 23:11:44.003: INFO: Got endpoints: latency-svc-sndgg [751.319607ms]
    Mar 15 23:11:44.053: INFO: Got endpoints: latency-svc-5sk5p [751.783913ms]
    Mar 15 23:11:44.102: INFO: Got endpoints: latency-svc-6sq6m [748.558256ms]
    Mar 15 23:11:44.157: INFO: Got endpoints: latency-svc-2jqj9 [756.755098ms]
    Mar 15 23:11:44.206: INFO: Got endpoints: latency-svc-86hwl [754.433227ms]
    Mar 15 23:11:44.268: INFO: Got endpoints: latency-svc-g9lwc [765.66492ms]
    Mar 15 23:11:44.300: INFO: Got endpoints: latency-svc-497dl [748.747788ms]
    Mar 15 23:11:44.350: INFO: Got endpoints: latency-svc-7z2l4 [748.203989ms]
    Mar 15 23:11:44.400: INFO: Got endpoints: latency-svc-9gjd9 [750.440576ms]
    Mar 15 23:11:44.452: INFO: Got endpoints: latency-svc-9vkd9 [752.317549ms]
    Mar 15 23:11:44.507: INFO: Got endpoints: latency-svc-wdtft [757.98199ms]
    Mar 15 23:11:44.549: INFO: Got endpoints: latency-svc-wpphb [745.26896ms]
    Mar 15 23:11:44.603: INFO: Got endpoints: latency-svc-rhnt7 [751.957831ms]
    Mar 15 23:11:44.650: INFO: Got endpoints: latency-svc-hnmmg [748.65129ms]
    Mar 15 23:11:44.701: INFO: Got endpoints: latency-svc-gktjr [749.265072ms]
    Mar 15 23:11:44.701: INFO: Latencies: [53.025134ms 55.25733ms 75.309985ms 108.310366ms 117.012681ms 118.055116ms 131.003393ms 151.117256ms 155.741677ms 177.299326ms 177.916532ms 190.053275ms 194.378521ms 203.457784ms 220.382683ms 223.091192ms 223.757794ms 230.650996ms 235.345698ms 235.535659ms 249.825903ms 258.697054ms 260.064559ms 260.883283ms 261.455219ms 261.552718ms 282.929097ms 285.838164ms 286.089709ms 288.357146ms 297.767572ms 307.225025ms 325.074162ms 332.005715ms 342.345004ms 342.970368ms 349.091732ms 373.876492ms 383.383315ms 387.467013ms 402.577464ms 403.461445ms 403.884972ms 409.97011ms 410.479314ms 413.822037ms 418.537505ms 437.273362ms 439.646703ms 442.402608ms 450.740353ms 456.771739ms 462.408378ms 465.812405ms 467.023917ms 490.832509ms 493.984181ms 503.055009ms 543.422416ms 549.800303ms 576.42173ms 602.593107ms 607.269603ms 650.359275ms 678.420527ms 723.236231ms 733.227663ms 733.381434ms 738.172235ms 740.407156ms 741.518102ms 741.925549ms 744.277219ms 744.630569ms 744.861746ms 745.26896ms 745.354892ms 745.414142ms 745.441223ms 746.096182ms 746.300715ms 746.307698ms 746.346849ms 746.527521ms 746.554322ms 746.715142ms 746.905522ms 746.918473ms 747.270954ms 747.322834ms 747.426846ms 747.514938ms 747.542326ms 747.581197ms 747.583071ms 747.7086ms 747.751969ms 747.765129ms 748.039011ms 748.063092ms 748.203989ms 748.234242ms 748.255846ms 748.259645ms 748.433274ms 748.558256ms 748.613365ms 748.65129ms 748.67697ms 748.745385ms 748.747788ms 748.829918ms 748.878911ms 748.898505ms 748.907652ms 749.14655ms 749.247278ms 749.265072ms 749.268941ms 749.659142ms 749.689445ms 749.69827ms 749.698334ms 749.958625ms 749.986603ms 749.989825ms 749.993142ms 750.123326ms 750.126499ms 750.129221ms 750.171373ms 750.253348ms 750.406073ms 750.440576ms 750.6589ms 750.669623ms 750.802944ms 751.041077ms 751.145376ms 751.173139ms 751.190758ms 751.319607ms 751.361765ms 751.541158ms 751.690824ms 751.726075ms 751.7349ms 751.762256ms 751.783913ms 751.957831ms 751.992561ms 752.008734ms 752.317549ms 752.331151ms 752.353152ms 752.454508ms 752.526276ms 752.6436ms 752.678494ms 752.705999ms 752.719744ms 752.824234ms 753.090559ms 753.195716ms 753.454772ms 753.572558ms 753.728669ms 754.433227ms 754.772798ms 754.894072ms 755.248219ms 755.51401ms 755.763666ms 755.966804ms 756.070314ms 756.196369ms 756.238012ms 756.755098ms 756.962451ms 757.151068ms 757.277487ms 757.646586ms 757.98199ms 758.885015ms 765.66492ms 793.277823ms 795.024617ms 795.464081ms 796.370146ms 796.662303ms 799.005128ms 799.263412ms 799.423073ms 799.540292ms 800.832589ms 800.905118ms 802.611905ms 802.799804ms 803.151984ms 804.284998ms]
    Mar 15 23:11:44.702: INFO: 50 %ile: 748.203989ms
    Mar 15 23:11:44.702: INFO: 90 %ile: 757.277487ms
    Mar 15 23:11:44.702: INFO: 99 %ile: 803.151984ms
    Mar 15 23:11:44.702: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:11:44.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-5511" for this suite. 03/15/23 23:11:44.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:11:44.721
Mar 15 23:11:44.721: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename webhook 03/15/23 23:11:44.722
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:11:44.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:11:44.737
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/15/23 23:11:44.75
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 23:11:45.484
STEP: Deploying the webhook pod 03/15/23 23:11:45.507
STEP: Wait for the deployment to be ready 03/15/23 23:11:45.52
Mar 15 23:11:45.529: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/15/23 23:11:47.547
STEP: Verifying the service has paired with the endpoint 03/15/23 23:11:47.558
Mar 15 23:11:48.558: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Mar 15 23:11:48.561: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/15/23 23:11:49.07
STEP: Creating a custom resource that should be denied by the webhook 03/15/23 23:11:49.086
STEP: Creating a custom resource whose deletion would be denied by the webhook 03/15/23 23:11:51.118
STEP: Updating the custom resource with disallowed data should be denied 03/15/23 23:11:51.129
STEP: Deleting the custom resource should be denied 03/15/23 23:11:51.143
STEP: Remove the offending key and value from the custom resource data 03/15/23 23:11:51.153
STEP: Deleting the updated custom resource should be successful 03/15/23 23:11:51.166
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 15 23:11:51.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1108" for this suite. 03/15/23 23:11:51.771
STEP: Destroying namespace "webhook-1108-markers" for this suite. 03/15/23 23:11:51.783
------------------------------
• [SLOW TEST] [7.075 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:11:44.721
    Mar 15 23:11:44.721: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename webhook 03/15/23 23:11:44.722
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:11:44.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:11:44.737
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/15/23 23:11:44.75
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/15/23 23:11:45.484
    STEP: Deploying the webhook pod 03/15/23 23:11:45.507
    STEP: Wait for the deployment to be ready 03/15/23 23:11:45.52
    Mar 15 23:11:45.529: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/15/23 23:11:47.547
    STEP: Verifying the service has paired with the endpoint 03/15/23 23:11:47.558
    Mar 15 23:11:48.558: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Mar 15 23:11:48.561: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/15/23 23:11:49.07
    STEP: Creating a custom resource that should be denied by the webhook 03/15/23 23:11:49.086
    STEP: Creating a custom resource whose deletion would be denied by the webhook 03/15/23 23:11:51.118
    STEP: Updating the custom resource with disallowed data should be denied 03/15/23 23:11:51.129
    STEP: Deleting the custom resource should be denied 03/15/23 23:11:51.143
    STEP: Remove the offending key and value from the custom resource data 03/15/23 23:11:51.153
    STEP: Deleting the updated custom resource should be successful 03/15/23 23:11:51.166
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:11:51.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1108" for this suite. 03/15/23 23:11:51.771
    STEP: Destroying namespace "webhook-1108-markers" for this suite. 03/15/23 23:11:51.783
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:11:51.801
Mar 15 23:11:51.801: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename projected 03/15/23 23:11:51.802
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:11:51.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:11:51.846
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 03/15/23 23:11:51.862
Mar 15 23:11:51.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e" in namespace "projected-4413" to be "Succeeded or Failed"
Mar 15 23:11:51.911: INFO: Pod "downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014157ms
Mar 15 23:11:53.916: INFO: Pod "downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022862019s
Mar 15 23:11:55.919: INFO: Pod "downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025069691s
STEP: Saw pod success 03/15/23 23:11:55.921
Mar 15 23:11:55.921: INFO: Pod "downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e" satisfied condition "Succeeded or Failed"
Mar 15 23:11:55.925: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e container client-container: <nil>
STEP: delete the pod 03/15/23 23:11:55.937
Mar 15 23:11:55.948: INFO: Waiting for pod downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e to disappear
Mar 15 23:11:55.950: INFO: Pod downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 15 23:11:55.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4413" for this suite. 03/15/23 23:11:55.955
------------------------------
• [4.158 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:11:51.801
    Mar 15 23:11:51.801: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename projected 03/15/23 23:11:51.802
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:11:51.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:11:51.846
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 03/15/23 23:11:51.862
    Mar 15 23:11:51.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e" in namespace "projected-4413" to be "Succeeded or Failed"
    Mar 15 23:11:51.911: INFO: Pod "downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014157ms
    Mar 15 23:11:53.916: INFO: Pod "downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022862019s
    Mar 15 23:11:55.919: INFO: Pod "downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025069691s
    STEP: Saw pod success 03/15/23 23:11:55.921
    Mar 15 23:11:55.921: INFO: Pod "downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e" satisfied condition "Succeeded or Failed"
    Mar 15 23:11:55.925: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e container client-container: <nil>
    STEP: delete the pod 03/15/23 23:11:55.937
    Mar 15 23:11:55.948: INFO: Waiting for pod downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e to disappear
    Mar 15 23:11:55.950: INFO: Pod downwardapi-volume-0fadec82-4cea-4b98-8c80-33cc45fd894e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:11:55.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4413" for this suite. 03/15/23 23:11:55.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:11:55.961
Mar 15 23:11:55.961: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename configmap 03/15/23 23:11:55.962
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:11:55.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:11:55.98
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-02c53ade-7605-4078-8af7-a80487af72ca 03/15/23 23:11:55.983
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 15 23:11:55.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3450" for this suite. 03/15/23 23:11:55.994
------------------------------
• [0.038 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:11:55.961
    Mar 15 23:11:55.961: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename configmap 03/15/23 23:11:55.962
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:11:55.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:11:55.98
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-02c53ade-7605-4078-8af7-a80487af72ca 03/15/23 23:11:55.983
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:11:55.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3450" for this suite. 03/15/23 23:11:55.994
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:11:56.002
Mar 15 23:11:56.002: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename job 03/15/23 23:11:56.003
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:11:56.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:11:56.022
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 03/15/23 23:11:56.027
STEP: Ensuring job reaches completions 03/15/23 23:11:56.033
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 15 23:12:08.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5203" for this suite. 03/15/23 23:12:08.041
------------------------------
• [SLOW TEST] [12.052 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:11:56.002
    Mar 15 23:11:56.002: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename job 03/15/23 23:11:56.003
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:11:56.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:11:56.022
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 03/15/23 23:11:56.027
    STEP: Ensuring job reaches completions 03/15/23 23:11:56.033
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:12:08.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5203" for this suite. 03/15/23 23:12:08.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:12:08.055
Mar 15 23:12:08.055: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename dns 03/15/23 23:12:08.056
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:12:08.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:12:08.08
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 03/15/23 23:12:08.084
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 03/15/23 23:12:08.095
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 03/15/23 23:12:08.095
STEP: creating a pod to probe DNS 03/15/23 23:12:08.095
STEP: submitting the pod to kubernetes 03/15/23 23:12:08.096
Mar 15 23:12:08.107: INFO: Waiting up to 15m0s for pod "dns-test-13439323-0ab7-494f-8082-146d693d1815" in namespace "dns-6781" to be "running"
Mar 15 23:12:08.111: INFO: Pod "dns-test-13439323-0ab7-494f-8082-146d693d1815": Phase="Pending", Reason="", readiness=false. Elapsed: 3.447287ms
Mar 15 23:12:10.115: INFO: Pod "dns-test-13439323-0ab7-494f-8082-146d693d1815": Phase="Running", Reason="", readiness=true. Elapsed: 2.007411291s
Mar 15 23:12:10.115: INFO: Pod "dns-test-13439323-0ab7-494f-8082-146d693d1815" satisfied condition "running"
STEP: retrieving the pod 03/15/23 23:12:10.115
STEP: looking for the results for each expected name from probers 03/15/23 23:12:10.118
Mar 15 23:12:10.152: INFO: DNS probes using dns-6781/dns-test-13439323-0ab7-494f-8082-146d693d1815 succeeded

STEP: deleting the pod 03/15/23 23:12:10.152
STEP: deleting the test headless service 03/15/23 23:12:10.213
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 15 23:12:10.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6781" for this suite. 03/15/23 23:12:10.286
------------------------------
• [2.255 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:12:08.055
    Mar 15 23:12:08.055: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename dns 03/15/23 23:12:08.056
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:12:08.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:12:08.08
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 03/15/23 23:12:08.084
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     03/15/23 23:12:08.095
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6781.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     03/15/23 23:12:08.095
    STEP: creating a pod to probe DNS 03/15/23 23:12:08.095
    STEP: submitting the pod to kubernetes 03/15/23 23:12:08.096
    Mar 15 23:12:08.107: INFO: Waiting up to 15m0s for pod "dns-test-13439323-0ab7-494f-8082-146d693d1815" in namespace "dns-6781" to be "running"
    Mar 15 23:12:08.111: INFO: Pod "dns-test-13439323-0ab7-494f-8082-146d693d1815": Phase="Pending", Reason="", readiness=false. Elapsed: 3.447287ms
    Mar 15 23:12:10.115: INFO: Pod "dns-test-13439323-0ab7-494f-8082-146d693d1815": Phase="Running", Reason="", readiness=true. Elapsed: 2.007411291s
    Mar 15 23:12:10.115: INFO: Pod "dns-test-13439323-0ab7-494f-8082-146d693d1815" satisfied condition "running"
    STEP: retrieving the pod 03/15/23 23:12:10.115
    STEP: looking for the results for each expected name from probers 03/15/23 23:12:10.118
    Mar 15 23:12:10.152: INFO: DNS probes using dns-6781/dns-test-13439323-0ab7-494f-8082-146d693d1815 succeeded

    STEP: deleting the pod 03/15/23 23:12:10.152
    STEP: deleting the test headless service 03/15/23 23:12:10.213
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:12:10.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6781" for this suite. 03/15/23 23:12:10.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:12:10.312
Mar 15 23:12:10.313: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename statefulset 03/15/23 23:12:10.314
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:12:10.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:12:10.34
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7602 03/15/23 23:12:10.345
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 03/15/23 23:12:10.352
Mar 15 23:12:10.387: INFO: Found 0 stateful pods, waiting for 3
Mar 15 23:12:20.391: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 23:12:20.391: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 23:12:20.391: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/15/23 23:12:20.401
Mar 15 23:12:20.424: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/15/23 23:12:20.424
STEP: Not applying an update when the partition is greater than the number of replicas 03/15/23 23:12:30.446
STEP: Performing a canary update 03/15/23 23:12:30.446
Mar 15 23:12:30.468: INFO: Updating stateful set ss2
Mar 15 23:12:30.478: INFO: Waiting for Pod statefulset-7602/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 03/15/23 23:12:40.521
Mar 15 23:12:40.698: INFO: Found 1 stateful pods, waiting for 3
Mar 15 23:12:50.702: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 23:12:50.702: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 23:12:50.702: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 03/15/23 23:12:50.707
Mar 15 23:12:50.727: INFO: Updating stateful set ss2
Mar 15 23:12:50.739: INFO: Waiting for Pod statefulset-7602/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Mar 15 23:13:00.771: INFO: Updating stateful set ss2
Mar 15 23:13:00.789: INFO: Waiting for StatefulSet statefulset-7602/ss2 to complete update
Mar 15 23:13:00.789: INFO: Waiting for Pod statefulset-7602/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 15 23:13:10.797: INFO: Deleting all statefulset in ns statefulset-7602
Mar 15 23:13:10.801: INFO: Scaling statefulset ss2 to 0
Mar 15 23:13:20.817: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 23:13:20.820: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 15 23:13:20.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7602" for this suite. 03/15/23 23:13:20.844
------------------------------
• [SLOW TEST] [70.541 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:12:10.312
    Mar 15 23:12:10.313: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename statefulset 03/15/23 23:12:10.314
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:12:10.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:12:10.34
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7602 03/15/23 23:12:10.345
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 03/15/23 23:12:10.352
    Mar 15 23:12:10.387: INFO: Found 0 stateful pods, waiting for 3
    Mar 15 23:12:20.391: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 15 23:12:20.391: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 15 23:12:20.391: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/15/23 23:12:20.401
    Mar 15 23:12:20.424: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/15/23 23:12:20.424
    STEP: Not applying an update when the partition is greater than the number of replicas 03/15/23 23:12:30.446
    STEP: Performing a canary update 03/15/23 23:12:30.446
    Mar 15 23:12:30.468: INFO: Updating stateful set ss2
    Mar 15 23:12:30.478: INFO: Waiting for Pod statefulset-7602/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 03/15/23 23:12:40.521
    Mar 15 23:12:40.698: INFO: Found 1 stateful pods, waiting for 3
    Mar 15 23:12:50.702: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 15 23:12:50.702: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 15 23:12:50.702: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 03/15/23 23:12:50.707
    Mar 15 23:12:50.727: INFO: Updating stateful set ss2
    Mar 15 23:12:50.739: INFO: Waiting for Pod statefulset-7602/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Mar 15 23:13:00.771: INFO: Updating stateful set ss2
    Mar 15 23:13:00.789: INFO: Waiting for StatefulSet statefulset-7602/ss2 to complete update
    Mar 15 23:13:00.789: INFO: Waiting for Pod statefulset-7602/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 15 23:13:10.797: INFO: Deleting all statefulset in ns statefulset-7602
    Mar 15 23:13:10.801: INFO: Scaling statefulset ss2 to 0
    Mar 15 23:13:20.817: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 15 23:13:20.820: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:13:20.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7602" for this suite. 03/15/23 23:13:20.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:13:20.88
Mar 15 23:13:20.880: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename downward-api 03/15/23 23:13:20.881
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:13:20.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:13:20.906
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 03/15/23 23:13:20.909
Mar 15 23:13:20.919: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1" in namespace "downward-api-2331" to be "Succeeded or Failed"
Mar 15 23:13:20.924: INFO: Pod "downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.842493ms
Mar 15 23:13:22.927: INFO: Pod "downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007784788s
Mar 15 23:13:24.927: INFO: Pod "downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007775926s
STEP: Saw pod success 03/15/23 23:13:24.927
Mar 15 23:13:24.927: INFO: Pod "downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1" satisfied condition "Succeeded or Failed"
Mar 15 23:13:24.929: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1 container client-container: <nil>
STEP: delete the pod 03/15/23 23:13:24.942
Mar 15 23:13:24.956: INFO: Waiting for pod downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1 to disappear
Mar 15 23:13:24.959: INFO: Pod downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 15 23:13:24.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2331" for this suite. 03/15/23 23:13:24.963
------------------------------
• [4.087 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:13:20.88
    Mar 15 23:13:20.880: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename downward-api 03/15/23 23:13:20.881
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:13:20.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:13:20.906
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 03/15/23 23:13:20.909
    Mar 15 23:13:20.919: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1" in namespace "downward-api-2331" to be "Succeeded or Failed"
    Mar 15 23:13:20.924: INFO: Pod "downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.842493ms
    Mar 15 23:13:22.927: INFO: Pod "downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007784788s
    Mar 15 23:13:24.927: INFO: Pod "downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007775926s
    STEP: Saw pod success 03/15/23 23:13:24.927
    Mar 15 23:13:24.927: INFO: Pod "downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1" satisfied condition "Succeeded or Failed"
    Mar 15 23:13:24.929: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1 container client-container: <nil>
    STEP: delete the pod 03/15/23 23:13:24.942
    Mar 15 23:13:24.956: INFO: Waiting for pod downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1 to disappear
    Mar 15 23:13:24.959: INFO: Pod downwardapi-volume-3a4b7b18-ab8e-45e1-96da-c3ab84c6eab1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:13:24.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2331" for this suite. 03/15/23 23:13:24.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:13:24.97
Mar 15 23:13:24.970: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename svcaccounts 03/15/23 23:13:24.971
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:13:24.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:13:24.99
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  03/15/23 23:13:24.994
Mar 15 23:13:25.008: INFO: Waiting up to 5m0s for pod "test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9" in namespace "svcaccounts-3026" to be "Succeeded or Failed"
Mar 15 23:13:25.017: INFO: Pod "test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.491211ms
Mar 15 23:13:27.020: INFO: Pod "test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012348397s
Mar 15 23:13:29.022: INFO: Pod "test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013672185s
STEP: Saw pod success 03/15/23 23:13:29.022
Mar 15 23:13:29.022: INFO: Pod "test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9" satisfied condition "Succeeded or Failed"
Mar 15 23:13:29.024: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9 container agnhost-container: <nil>
STEP: delete the pod 03/15/23 23:13:29.029
Mar 15 23:13:29.041: INFO: Waiting for pod test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9 to disappear
Mar 15 23:13:29.044: INFO: Pod test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 15 23:13:29.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3026" for this suite. 03/15/23 23:13:29.051
------------------------------
• [4.085 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:13:24.97
    Mar 15 23:13:24.970: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename svcaccounts 03/15/23 23:13:24.971
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:13:24.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:13:24.99
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  03/15/23 23:13:24.994
    Mar 15 23:13:25.008: INFO: Waiting up to 5m0s for pod "test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9" in namespace "svcaccounts-3026" to be "Succeeded or Failed"
    Mar 15 23:13:25.017: INFO: Pod "test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.491211ms
    Mar 15 23:13:27.020: INFO: Pod "test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012348397s
    Mar 15 23:13:29.022: INFO: Pod "test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013672185s
    STEP: Saw pod success 03/15/23 23:13:29.022
    Mar 15 23:13:29.022: INFO: Pod "test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9" satisfied condition "Succeeded or Failed"
    Mar 15 23:13:29.024: INFO: Trying to get logs from node i-0faaf83f00b43c88c pod test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9 container agnhost-container: <nil>
    STEP: delete the pod 03/15/23 23:13:29.029
    Mar 15 23:13:29.041: INFO: Waiting for pod test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9 to disappear
    Mar 15 23:13:29.044: INFO: Pod test-pod-a05d81fb-4d30-4df0-848e-02cfa3e5ddc9 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:13:29.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3026" for this suite. 03/15/23 23:13:29.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/15/23 23:13:29.06
Mar 15 23:13:29.060: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
STEP: Building a namespace api object, basename cronjob 03/15/23 23:13:29.061
STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:13:29.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:13:29.079
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 03/15/23 23:13:29.082
STEP: Ensuring a job is scheduled 03/15/23 23:13:29.087
STEP: Ensuring exactly one is scheduled 03/15/23 23:14:01.09
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/15/23 23:14:01.096
STEP: Ensuring no more jobs are scheduled 03/15/23 23:14:01.099
STEP: Removing cronjob 03/15/23 23:19:01.105
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 15 23:19:01.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3520" for this suite. 03/15/23 23:19:01.116
------------------------------
• [SLOW TEST] [332.070 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/15/23 23:13:29.06
    Mar 15 23:13:29.060: INFO: >>> kubeConfig: /tmp/kubeconfig-551247953
    STEP: Building a namespace api object, basename cronjob 03/15/23 23:13:29.061
    STEP: Waiting for a default service account to be provisioned in namespace 03/15/23 23:13:29.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/15/23 23:13:29.079
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 03/15/23 23:13:29.082
    STEP: Ensuring a job is scheduled 03/15/23 23:13:29.087
    STEP: Ensuring exactly one is scheduled 03/15/23 23:14:01.09
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/15/23 23:14:01.096
    STEP: Ensuring no more jobs are scheduled 03/15/23 23:14:01.099
    STEP: Removing cronjob 03/15/23 23:19:01.105
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 15 23:19:01.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3520" for this suite. 03/15/23 23:19:01.116
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Mar 15 23:19:01.132: INFO: Running AfterSuite actions on node 1
Mar 15 23:19:01.132: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Mar 15 23:19:01.132: INFO: Running AfterSuite actions on node 1
    Mar 15 23:19:01.132: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.164 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5739.224 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h35m39.662756998s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

