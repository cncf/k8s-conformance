I0330 13:08:41.590547      23 e2e.go:126] Starting e2e run "2220ea2c-1613-43b9-bb5d-81b7f052acb2" on Ginkgo node 1
Mar 30 13:08:41.601: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1680181721 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Mar 30 13:08:41.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:08:41.677: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0330 13:08:41.677781      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar 30 13:08:41.687: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 30 13:08:41.714: INFO: 41 / 41 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 30 13:08:41.714: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
Mar 30 13:08:41.714: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 30 13:08:41.720: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-controller-manager' (0 seconds elapsed)
Mar 30 13:08:41.720: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'csi-plugin' (0 seconds elapsed)
Mar 30 13:08:41.720: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Mar 30 13:08:41.720: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-windows' (0 seconds elapsed)
Mar 30 13:08:41.720: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-master' (0 seconds elapsed)
Mar 30 13:08:41.720: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker' (0 seconds elapsed)
Mar 30 13:08:41.720: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-windows' (0 seconds elapsed)
Mar 30 13:08:41.720: INFO: e2e test version: v1.26.3
Mar 30 13:08:41.721: INFO: kube-apiserver version: v1.26.3-aliyun.1
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Mar 30 13:08:41.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:08:41.723: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.047 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Mar 30 13:08:41.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:08:41.677: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0330 13:08:41.677781      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Mar 30 13:08:41.687: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Mar 30 13:08:41.714: INFO: 41 / 41 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Mar 30 13:08:41.714: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
    Mar 30 13:08:41.714: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Mar 30 13:08:41.720: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-controller-manager' (0 seconds elapsed)
    Mar 30 13:08:41.720: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'csi-plugin' (0 seconds elapsed)
    Mar 30 13:08:41.720: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
    Mar 30 13:08:41.720: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-windows' (0 seconds elapsed)
    Mar 30 13:08:41.720: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-master' (0 seconds elapsed)
    Mar 30 13:08:41.720: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker' (0 seconds elapsed)
    Mar 30 13:08:41.720: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-windows' (0 seconds elapsed)
    Mar 30 13:08:41.720: INFO: e2e test version: v1.26.3
    Mar 30 13:08:41.721: INFO: kube-apiserver version: v1.26.3-aliyun.1
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Mar 30 13:08:41.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:08:41.723: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:08:41.74
Mar 30 13:08:41.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pod-network-test 03/30/23 13:08:41.74
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:08:41.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:08:41.749
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-4785 03/30/23 13:08:41.751
STEP: creating a selector 03/30/23 13:08:41.751
STEP: Creating the service pods in kubernetes 03/30/23 13:08:41.751
Mar 30 13:08:41.751: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 30 13:08:41.763: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4785" to be "running and ready"
Mar 30 13:08:41.765: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.664844ms
Mar 30 13:08:41.765: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:08:43.767: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003754699s
Mar 30 13:08:43.767: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:08:45.768: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004830231s
Mar 30 13:08:45.768: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:08:47.767: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003566957s
Mar 30 13:08:47.767: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:08:49.768: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004868315s
Mar 30 13:08:49.768: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:08:51.768: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004609599s
Mar 30 13:08:51.768: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:08:53.768: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004876968s
Mar 30 13:08:53.768: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:08:55.767: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.003884325s
Mar 30 13:08:55.767: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:08:57.768: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004742695s
Mar 30 13:08:57.768: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:08:59.767: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004095847s
Mar 30 13:08:59.767: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:09:01.768: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004320134s
Mar 30 13:09:01.768: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:09:03.768: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.004910069s
Mar 30 13:09:03.768: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:09:05.768: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 24.005077704s
Mar 30 13:09:05.768: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:09:07.767: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 26.003576564s
Mar 30 13:09:07.767: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:09:09.768: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 28.004756137s
Mar 30 13:09:09.768: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:09:11.767: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 30.004241948s
Mar 30 13:09:11.767: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:09:13.768: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 32.005128236s
Mar 30 13:09:13.768: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 30 13:09:13.768: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 30 13:09:13.770: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4785" to be "running and ready"
Mar 30 13:09:13.772: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 1.510578ms
Mar 30 13:09:13.772: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 30 13:09:15.774: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.003749158s
Mar 30 13:09:15.774: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 30 13:09:17.774: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.003646597s
Mar 30 13:09:17.774: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 30 13:09:19.774: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.003834788s
Mar 30 13:09:19.774: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 30 13:09:21.774: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.004216386s
Mar 30 13:09:21.774: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 30 13:09:23.775: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.004953787s
Mar 30 13:09:23.775: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 30 13:09:23.775: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 30 13:09:23.777: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4785" to be "running and ready"
Mar 30 13:09:23.778: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.368501ms
Mar 30 13:09:23.778: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 30 13:09:23.778: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/30/23 13:09:23.779
Mar 30 13:09:23.784: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4785" to be "running"
Mar 30 13:09:23.785: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26734ms
Mar 30 13:09:25.788: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00402929s
Mar 30 13:09:25.788: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 30 13:09:25.790: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 30 13:09:25.790: INFO: Breadth first check of 10.29.1.69 on host 192.168.0.3...
Mar 30 13:09:25.792: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.29.1.7:9080/dial?request=hostname&protocol=http&host=10.29.1.69&port=8083&tries=1'] Namespace:pod-network-test-4785 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:09:25.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:09:25.792: INFO: ExecWithOptions: Clientset creation
Mar 30 13:09:25.792: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-4785/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.29.1.7%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.29.1.69%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 30 13:09:25.836: INFO: Waiting for responses: map[]
Mar 30 13:09:25.836: INFO: reached 10.29.1.69 after 0/1 tries
Mar 30 13:09:25.836: INFO: Breadth first check of 10.29.0.200 on host 192.168.0.4...
Mar 30 13:09:25.838: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.29.1.7:9080/dial?request=hostname&protocol=http&host=10.29.0.200&port=8083&tries=1'] Namespace:pod-network-test-4785 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:09:25.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:09:25.838: INFO: ExecWithOptions: Clientset creation
Mar 30 13:09:25.838: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-4785/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.29.1.7%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.29.0.200%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 30 13:09:25.881: INFO: Waiting for responses: map[]
Mar 30 13:09:25.881: INFO: reached 10.29.0.200 after 0/1 tries
Mar 30 13:09:25.881: INFO: Breadth first check of 10.29.1.6 on host 192.168.0.5...
Mar 30 13:09:25.883: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.29.1.7:9080/dial?request=hostname&protocol=http&host=10.29.1.6&port=8083&tries=1'] Namespace:pod-network-test-4785 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:09:25.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:09:25.883: INFO: ExecWithOptions: Clientset creation
Mar 30 13:09:25.883: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-4785/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.29.1.7%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.29.1.6%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 30 13:09:25.927: INFO: Waiting for responses: map[]
Mar 30 13:09:25.927: INFO: reached 10.29.1.6 after 0/1 tries
Mar 30 13:09:25.927: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 30 13:09:25.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4785" for this suite. 03/30/23 13:09:25.93
------------------------------
â€¢ [SLOW TEST] [44.193 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:08:41.74
    Mar 30 13:08:41.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pod-network-test 03/30/23 13:08:41.74
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:08:41.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:08:41.749
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-4785 03/30/23 13:08:41.751
    STEP: creating a selector 03/30/23 13:08:41.751
    STEP: Creating the service pods in kubernetes 03/30/23 13:08:41.751
    Mar 30 13:08:41.751: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 30 13:08:41.763: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4785" to be "running and ready"
    Mar 30 13:08:41.765: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.664844ms
    Mar 30 13:08:41.765: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:08:43.767: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003754699s
    Mar 30 13:08:43.767: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:08:45.768: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004830231s
    Mar 30 13:08:45.768: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:08:47.767: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003566957s
    Mar 30 13:08:47.767: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:08:49.768: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004868315s
    Mar 30 13:08:49.768: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:08:51.768: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004609599s
    Mar 30 13:08:51.768: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:08:53.768: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004876968s
    Mar 30 13:08:53.768: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:08:55.767: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.003884325s
    Mar 30 13:08:55.767: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:08:57.768: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004742695s
    Mar 30 13:08:57.768: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:08:59.767: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004095847s
    Mar 30 13:08:59.767: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:09:01.768: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004320134s
    Mar 30 13:09:01.768: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:09:03.768: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.004910069s
    Mar 30 13:09:03.768: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:09:05.768: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 24.005077704s
    Mar 30 13:09:05.768: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:09:07.767: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 26.003576564s
    Mar 30 13:09:07.767: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:09:09.768: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 28.004756137s
    Mar 30 13:09:09.768: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:09:11.767: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 30.004241948s
    Mar 30 13:09:11.767: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:09:13.768: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 32.005128236s
    Mar 30 13:09:13.768: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 30 13:09:13.768: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 30 13:09:13.770: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4785" to be "running and ready"
    Mar 30 13:09:13.772: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 1.510578ms
    Mar 30 13:09:13.772: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 30 13:09:15.774: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.003749158s
    Mar 30 13:09:15.774: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 30 13:09:17.774: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.003646597s
    Mar 30 13:09:17.774: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 30 13:09:19.774: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.003834788s
    Mar 30 13:09:19.774: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 30 13:09:21.774: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.004216386s
    Mar 30 13:09:21.774: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 30 13:09:23.775: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.004953787s
    Mar 30 13:09:23.775: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 30 13:09:23.775: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 30 13:09:23.777: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4785" to be "running and ready"
    Mar 30 13:09:23.778: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.368501ms
    Mar 30 13:09:23.778: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 30 13:09:23.778: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/30/23 13:09:23.779
    Mar 30 13:09:23.784: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4785" to be "running"
    Mar 30 13:09:23.785: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26734ms
    Mar 30 13:09:25.788: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00402929s
    Mar 30 13:09:25.788: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 30 13:09:25.790: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 30 13:09:25.790: INFO: Breadth first check of 10.29.1.69 on host 192.168.0.3...
    Mar 30 13:09:25.792: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.29.1.7:9080/dial?request=hostname&protocol=http&host=10.29.1.69&port=8083&tries=1'] Namespace:pod-network-test-4785 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:09:25.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:09:25.792: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:09:25.792: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-4785/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.29.1.7%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.29.1.69%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 30 13:09:25.836: INFO: Waiting for responses: map[]
    Mar 30 13:09:25.836: INFO: reached 10.29.1.69 after 0/1 tries
    Mar 30 13:09:25.836: INFO: Breadth first check of 10.29.0.200 on host 192.168.0.4...
    Mar 30 13:09:25.838: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.29.1.7:9080/dial?request=hostname&protocol=http&host=10.29.0.200&port=8083&tries=1'] Namespace:pod-network-test-4785 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:09:25.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:09:25.838: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:09:25.838: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-4785/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.29.1.7%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.29.0.200%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 30 13:09:25.881: INFO: Waiting for responses: map[]
    Mar 30 13:09:25.881: INFO: reached 10.29.0.200 after 0/1 tries
    Mar 30 13:09:25.881: INFO: Breadth first check of 10.29.1.6 on host 192.168.0.5...
    Mar 30 13:09:25.883: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.29.1.7:9080/dial?request=hostname&protocol=http&host=10.29.1.6&port=8083&tries=1'] Namespace:pod-network-test-4785 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:09:25.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:09:25.883: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:09:25.883: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-4785/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.29.1.7%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.29.1.6%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 30 13:09:25.927: INFO: Waiting for responses: map[]
    Mar 30 13:09:25.927: INFO: reached 10.29.1.6 after 0/1 tries
    Mar 30 13:09:25.927: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:09:25.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4785" for this suite. 03/30/23 13:09:25.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:09:25.933
Mar 30 13:09:25.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename deployment 03/30/23 13:09:25.934
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:09:25.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:09:25.942
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Mar 30 13:09:25.947: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 30 13:09:30.953: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/30/23 13:09:30.953
Mar 30 13:09:30.953: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 30 13:09:32.973: INFO: Creating deployment "test-rollover-deployment"
Mar 30 13:09:32.977: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 30 13:09:35.455: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 30 13:09:35.458: INFO: Ensure that both replica sets have 1 created replica
Mar 30 13:09:35.462: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 30 13:09:35.466: INFO: Updating deployment test-rollover-deployment
Mar 30 13:09:35.466: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 30 13:09:37.470: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 30 13:09:37.473: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 30 13:09:37.476: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 13:09:37.476: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:09:39.481: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 13:09:39.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:09:41.480: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 13:09:41.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:09:43.481: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 13:09:43.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:09:45.481: INFO: all replica sets need to contain the pod-template-hash label
Mar 30 13:09:45.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:09:47.480: INFO: 
Mar 30 13:09:47.480: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 30 13:09:47.485: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8843  b2e2b438-a54e-4944-ab15-d56504fa61cc 53537 2 2023-03-30 13:09:32 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-30 13:09:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:09:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00295fcb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-30 13:09:33 +0000 UTC,LastTransitionTime:2023-03-30 13:09:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-03-30 13:09:46 +0000 UTC,LastTransitionTime:2023-03-30 13:09:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 30 13:09:47.486: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8843  2a64eb20-941b-4a24-85f7-df65524e8209 53527 2 2023-03-30 13:09:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b2e2b438-a54e-4944-ab15-d56504fa61cc 0xc0033c4187 0xc0033c4188}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:09:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e2b438-a54e-4944-ab15-d56504fa61cc\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:09:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033c4238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 30 13:09:47.486: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 30 13:09:47.486: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8843  b2b88921-1e48-48f6-bf96-e72513a9823e 53536 2 2023-03-30 13:09:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b2e2b438-a54e-4944-ab15-d56504fa61cc 0xc0033c4057 0xc0033c4058}] [] [{e2e.test Update apps/v1 2023-03-30 13:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:09:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e2b438-a54e-4944-ab15-d56504fa61cc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:09:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0033c4118 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 13:09:47.486: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8843  f91ec9ec-5b10-4048-936b-c2535a6bd497 53460 2 2023-03-30 13:09:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b2e2b438-a54e-4944-ab15-d56504fa61cc 0xc0033c42a7 0xc0033c42a8}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:09:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e2b438-a54e-4944-ab15-d56504fa61cc\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:09:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033c4358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 13:09:47.488: INFO: Pod "test-rollover-deployment-6c6df9974f-46gl7" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-46gl7 test-rollover-deployment-6c6df9974f- deployment-8843  a9646c9c-a838-4457-beb2-7dd588ef080a 53476 0 2023-03-30 13:09:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 2a64eb20-941b-4a24-85f7-df65524e8209 0xc0033c48a7 0xc0033c48a8}] [] [{kube-controller-manager Update v1 2023-03-30 13:09:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a64eb20-941b-4a24-85f7-df65524e8209\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:09:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pjcv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pjcv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:09:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:09:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:09:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:09:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:10.29.1.71,StartTime:2023-03-30 13:09:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:09:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://96be2ad242cfd9769107e5feb7cfb443461ee4b9f8ff6b5970dd3c2f2d876eef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 30 13:09:47.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8843" for this suite. 03/30/23 13:09:47.49
------------------------------
â€¢ [SLOW TEST] [21.560 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:09:25.933
    Mar 30 13:09:25.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename deployment 03/30/23 13:09:25.934
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:09:25.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:09:25.942
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Mar 30 13:09:25.947: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Mar 30 13:09:30.953: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/30/23 13:09:30.953
    Mar 30 13:09:30.953: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Mar 30 13:09:32.973: INFO: Creating deployment "test-rollover-deployment"
    Mar 30 13:09:32.977: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Mar 30 13:09:35.455: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Mar 30 13:09:35.458: INFO: Ensure that both replica sets have 1 created replica
    Mar 30 13:09:35.462: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Mar 30 13:09:35.466: INFO: Updating deployment test-rollover-deployment
    Mar 30 13:09:35.466: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Mar 30 13:09:37.470: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Mar 30 13:09:37.473: INFO: Make sure deployment "test-rollover-deployment" is complete
    Mar 30 13:09:37.476: INFO: all replica sets need to contain the pod-template-hash label
    Mar 30 13:09:37.476: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:09:39.481: INFO: all replica sets need to contain the pod-template-hash label
    Mar 30 13:09:39.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:09:41.480: INFO: all replica sets need to contain the pod-template-hash label
    Mar 30 13:09:41.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:09:43.481: INFO: all replica sets need to contain the pod-template-hash label
    Mar 30 13:09:43.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:09:45.481: INFO: all replica sets need to contain the pod-template-hash label
    Mar 30 13:09:45.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 9, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 9, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:09:47.480: INFO: 
    Mar 30 13:09:47.480: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 30 13:09:47.485: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8843  b2e2b438-a54e-4944-ab15-d56504fa61cc 53537 2 2023-03-30 13:09:32 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-30 13:09:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:09:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00295fcb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-30 13:09:33 +0000 UTC,LastTransitionTime:2023-03-30 13:09:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-03-30 13:09:46 +0000 UTC,LastTransitionTime:2023-03-30 13:09:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 30 13:09:47.486: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8843  2a64eb20-941b-4a24-85f7-df65524e8209 53527 2 2023-03-30 13:09:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b2e2b438-a54e-4944-ab15-d56504fa61cc 0xc0033c4187 0xc0033c4188}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:09:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e2b438-a54e-4944-ab15-d56504fa61cc\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:09:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033c4238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 13:09:47.486: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Mar 30 13:09:47.486: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8843  b2b88921-1e48-48f6-bf96-e72513a9823e 53536 2 2023-03-30 13:09:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b2e2b438-a54e-4944-ab15-d56504fa61cc 0xc0033c4057 0xc0033c4058}] [] [{e2e.test Update apps/v1 2023-03-30 13:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:09:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e2b438-a54e-4944-ab15-d56504fa61cc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:09:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0033c4118 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 13:09:47.486: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8843  f91ec9ec-5b10-4048-936b-c2535a6bd497 53460 2 2023-03-30 13:09:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b2e2b438-a54e-4944-ab15-d56504fa61cc 0xc0033c42a7 0xc0033c42a8}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:09:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e2b438-a54e-4944-ab15-d56504fa61cc\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:09:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033c4358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 13:09:47.488: INFO: Pod "test-rollover-deployment-6c6df9974f-46gl7" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-46gl7 test-rollover-deployment-6c6df9974f- deployment-8843  a9646c9c-a838-4457-beb2-7dd588ef080a 53476 0 2023-03-30 13:09:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 2a64eb20-941b-4a24-85f7-df65524e8209 0xc0033c48a7 0xc0033c48a8}] [] [{kube-controller-manager Update v1 2023-03-30 13:09:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a64eb20-941b-4a24-85f7-df65524e8209\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:09:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pjcv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pjcv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:09:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:09:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:09:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:09:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:10.29.1.71,StartTime:2023-03-30 13:09:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:09:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://96be2ad242cfd9769107e5feb7cfb443461ee4b9f8ff6b5970dd3c2f2d876eef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:09:47.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8843" for this suite. 03/30/23 13:09:47.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:09:47.494
Mar 30 13:09:47.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 13:09:47.494
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:09:47.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:09:47.502
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8434 03/30/23 13:09:47.504
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/30/23 13:09:47.51
STEP: creating service externalsvc in namespace services-8434 03/30/23 13:09:47.51
STEP: creating replication controller externalsvc in namespace services-8434 03/30/23 13:09:47.515
I0330 13:09:47.518264      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8434, replica count: 2
I0330 13:09:50.569552      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 03/30/23 13:09:50.571
Mar 30 13:09:50.579: INFO: Creating new exec pod
Mar 30 13:09:50.584: INFO: Waiting up to 5m0s for pod "execpodgbtrr" in namespace "services-8434" to be "running"
Mar 30 13:09:50.585: INFO: Pod "execpodgbtrr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.534782ms
Mar 30 13:09:52.588: INFO: Pod "execpodgbtrr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004294316s
Mar 30 13:09:52.588: INFO: Pod "execpodgbtrr" satisfied condition "running"
Mar 30 13:09:52.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-8434 exec execpodgbtrr -- /bin/sh -x -c nslookup nodeport-service.services-8434.svc.cluster.local'
Mar 30 13:09:52.711: INFO: stderr: "+ nslookup nodeport-service.services-8434.svc.cluster.local\n"
Mar 30 13:09:52.711: INFO: stdout: "Server:\t\t172.16.0.10\nAddress:\t172.16.0.10#53\n\nnodeport-service.services-8434.svc.cluster.local\tcanonical name = externalsvc.services-8434.svc.cluster.local.\nName:\texternalsvc.services-8434.svc.cluster.local\nAddress: 172.16.93.164\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8434, will wait for the garbage collector to delete the pods 03/30/23 13:09:52.711
Mar 30 13:09:52.766: INFO: Deleting ReplicationController externalsvc took: 2.686651ms
Mar 30 13:09:52.867: INFO: Terminating ReplicationController externalsvc pods took: 100.435566ms
Mar 30 13:09:54.375: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 13:09:54.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8434" for this suite. 03/30/23 13:09:54.383
------------------------------
â€¢ [SLOW TEST] [6.892 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:09:47.494
    Mar 30 13:09:47.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 13:09:47.494
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:09:47.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:09:47.502
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8434 03/30/23 13:09:47.504
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/30/23 13:09:47.51
    STEP: creating service externalsvc in namespace services-8434 03/30/23 13:09:47.51
    STEP: creating replication controller externalsvc in namespace services-8434 03/30/23 13:09:47.515
    I0330 13:09:47.518264      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8434, replica count: 2
    I0330 13:09:50.569552      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 03/30/23 13:09:50.571
    Mar 30 13:09:50.579: INFO: Creating new exec pod
    Mar 30 13:09:50.584: INFO: Waiting up to 5m0s for pod "execpodgbtrr" in namespace "services-8434" to be "running"
    Mar 30 13:09:50.585: INFO: Pod "execpodgbtrr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.534782ms
    Mar 30 13:09:52.588: INFO: Pod "execpodgbtrr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004294316s
    Mar 30 13:09:52.588: INFO: Pod "execpodgbtrr" satisfied condition "running"
    Mar 30 13:09:52.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-8434 exec execpodgbtrr -- /bin/sh -x -c nslookup nodeport-service.services-8434.svc.cluster.local'
    Mar 30 13:09:52.711: INFO: stderr: "+ nslookup nodeport-service.services-8434.svc.cluster.local\n"
    Mar 30 13:09:52.711: INFO: stdout: "Server:\t\t172.16.0.10\nAddress:\t172.16.0.10#53\n\nnodeport-service.services-8434.svc.cluster.local\tcanonical name = externalsvc.services-8434.svc.cluster.local.\nName:\texternalsvc.services-8434.svc.cluster.local\nAddress: 172.16.93.164\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8434, will wait for the garbage collector to delete the pods 03/30/23 13:09:52.711
    Mar 30 13:09:52.766: INFO: Deleting ReplicationController externalsvc took: 2.686651ms
    Mar 30 13:09:52.867: INFO: Terminating ReplicationController externalsvc pods took: 100.435566ms
    Mar 30 13:09:54.375: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:09:54.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8434" for this suite. 03/30/23 13:09:54.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:09:54.388
Mar 30 13:09:54.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 13:09:54.388
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:09:54.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:09:54.397
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 13:09:54.406
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:09:54.861
STEP: Deploying the webhook pod 03/30/23 13:09:54.864
STEP: Wait for the deployment to be ready 03/30/23 13:09:54.87
Mar 30 13:09:54.873: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/30/23 13:09:56.879
STEP: Verifying the service has paired with the endpoint 03/30/23 13:09:56.883
Mar 30 13:09:57.884: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/30/23 13:09:57.885
STEP: create a pod that should be updated by the webhook 03/30/23 13:09:57.895
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:09:57.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2057" for this suite. 03/30/23 13:09:57.926
STEP: Destroying namespace "webhook-2057-markers" for this suite. 03/30/23 13:09:57.933
------------------------------
â€¢ [3.549 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:09:54.388
    Mar 30 13:09:54.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 13:09:54.388
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:09:54.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:09:54.397
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 13:09:54.406
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:09:54.861
    STEP: Deploying the webhook pod 03/30/23 13:09:54.864
    STEP: Wait for the deployment to be ready 03/30/23 13:09:54.87
    Mar 30 13:09:54.873: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/30/23 13:09:56.879
    STEP: Verifying the service has paired with the endpoint 03/30/23 13:09:56.883
    Mar 30 13:09:57.884: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/30/23 13:09:57.885
    STEP: create a pod that should be updated by the webhook 03/30/23 13:09:57.895
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:09:57.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2057" for this suite. 03/30/23 13:09:57.926
    STEP: Destroying namespace "webhook-2057-markers" for this suite. 03/30/23 13:09:57.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:09:57.937
Mar 30 13:09:57.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename gc 03/30/23 13:09:57.938
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:09:57.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:09:57.946
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 03/30/23 13:09:57.948
STEP: Wait for the Deployment to create new ReplicaSet 03/30/23 13:09:57.951
STEP: delete the deployment 03/30/23 13:09:58.456
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/30/23 13:09:58.459
STEP: Gathering metrics 03/30/23 13:09:58.968
Mar 30 13:09:58.983: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cn-hongkong.192.168.0.252" in namespace "kube-system" to be "running and ready"
Mar 30 13:09:58.984: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252": Phase="Running", Reason="", readiness=true. Elapsed: 1.42061ms
Mar 30 13:09:58.984: INFO: The phase of Pod kube-controller-manager-cn-hongkong.192.168.0.252 is Running (Ready = true)
Mar 30 13:09:58.984: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252" satisfied condition "running and ready"
Mar 30 13:09:59.020: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 30 13:09:59.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4955" for this suite. 03/30/23 13:09:59.022
------------------------------
â€¢ [1.088 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:09:57.937
    Mar 30 13:09:57.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename gc 03/30/23 13:09:57.938
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:09:57.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:09:57.946
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 03/30/23 13:09:57.948
    STEP: Wait for the Deployment to create new ReplicaSet 03/30/23 13:09:57.951
    STEP: delete the deployment 03/30/23 13:09:58.456
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/30/23 13:09:58.459
    STEP: Gathering metrics 03/30/23 13:09:58.968
    Mar 30 13:09:58.983: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cn-hongkong.192.168.0.252" in namespace "kube-system" to be "running and ready"
    Mar 30 13:09:58.984: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252": Phase="Running", Reason="", readiness=true. Elapsed: 1.42061ms
    Mar 30 13:09:58.984: INFO: The phase of Pod kube-controller-manager-cn-hongkong.192.168.0.252 is Running (Ready = true)
    Mar 30 13:09:58.984: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252" satisfied condition "running and ready"
    Mar 30 13:09:59.020: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:09:59.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4955" for this suite. 03/30/23 13:09:59.022
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:09:59.026
Mar 30 13:09:59.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pods 03/30/23 13:09:59.026
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:09:59.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:09:59.034
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 03/30/23 13:09:59.035
STEP: submitting the pod to kubernetes 03/30/23 13:09:59.035
Mar 30 13:09:59.039: INFO: Waiting up to 5m0s for pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959" in namespace "pods-6012" to be "running and ready"
Mar 30 13:09:59.040: INFO: Pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959": Phase="Pending", Reason="", readiness=false. Elapsed: 1.407754ms
Mar 30 13:09:59.040: INFO: The phase of Pod pod-update-9bdf993b-22b1-486a-ae47-40beed78c959 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:10:01.043: INFO: Pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959": Phase="Running", Reason="", readiness=true. Elapsed: 2.004175262s
Mar 30 13:10:01.043: INFO: The phase of Pod pod-update-9bdf993b-22b1-486a-ae47-40beed78c959 is Running (Ready = true)
Mar 30 13:10:01.043: INFO: Pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/30/23 13:10:01.044
STEP: updating the pod 03/30/23 13:10:01.046
Mar 30 13:10:01.551: INFO: Successfully updated pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959"
Mar 30 13:10:01.551: INFO: Waiting up to 5m0s for pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959" in namespace "pods-6012" to be "running"
Mar 30 13:10:01.553: INFO: Pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959": Phase="Running", Reason="", readiness=true. Elapsed: 1.514833ms
Mar 30 13:10:01.553: INFO: Pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 03/30/23 13:10:01.553
Mar 30 13:10:01.554: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 30 13:10:01.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6012" for this suite. 03/30/23 13:10:01.557
------------------------------
â€¢ [2.534 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:09:59.026
    Mar 30 13:09:59.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pods 03/30/23 13:09:59.026
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:09:59.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:09:59.034
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 03/30/23 13:09:59.035
    STEP: submitting the pod to kubernetes 03/30/23 13:09:59.035
    Mar 30 13:09:59.039: INFO: Waiting up to 5m0s for pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959" in namespace "pods-6012" to be "running and ready"
    Mar 30 13:09:59.040: INFO: Pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959": Phase="Pending", Reason="", readiness=false. Elapsed: 1.407754ms
    Mar 30 13:09:59.040: INFO: The phase of Pod pod-update-9bdf993b-22b1-486a-ae47-40beed78c959 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:10:01.043: INFO: Pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959": Phase="Running", Reason="", readiness=true. Elapsed: 2.004175262s
    Mar 30 13:10:01.043: INFO: The phase of Pod pod-update-9bdf993b-22b1-486a-ae47-40beed78c959 is Running (Ready = true)
    Mar 30 13:10:01.043: INFO: Pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/30/23 13:10:01.044
    STEP: updating the pod 03/30/23 13:10:01.046
    Mar 30 13:10:01.551: INFO: Successfully updated pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959"
    Mar 30 13:10:01.551: INFO: Waiting up to 5m0s for pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959" in namespace "pods-6012" to be "running"
    Mar 30 13:10:01.553: INFO: Pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959": Phase="Running", Reason="", readiness=true. Elapsed: 1.514833ms
    Mar 30 13:10:01.553: INFO: Pod "pod-update-9bdf993b-22b1-486a-ae47-40beed78c959" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 03/30/23 13:10:01.553
    Mar 30 13:10:01.554: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:10:01.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6012" for this suite. 03/30/23 13:10:01.557
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:10:01.56
Mar 30 13:10:01.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename podtemplate 03/30/23 13:10:01.561
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:10:01.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:10:01.569
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 03/30/23 13:10:01.571
STEP: Replace a pod template 03/30/23 13:10:01.573
Mar 30 13:10:01.577: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 30 13:10:01.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6613" for this suite. 03/30/23 13:10:01.579
------------------------------
â€¢ [0.021 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:10:01.56
    Mar 30 13:10:01.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename podtemplate 03/30/23 13:10:01.561
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:10:01.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:10:01.569
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 03/30/23 13:10:01.571
    STEP: Replace a pod template 03/30/23 13:10:01.573
    Mar 30 13:10:01.577: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:10:01.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6613" for this suite. 03/30/23 13:10:01.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:10:01.583
Mar 30 13:10:01.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename security-context-test 03/30/23 13:10:01.583
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:10:01.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:10:01.591
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Mar 30 13:10:01.595: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0f048f2b-6644-45fa-9b1b-378a9f39f92c" in namespace "security-context-test-6460" to be "Succeeded or Failed"
Mar 30 13:10:01.597: INFO: Pod "alpine-nnp-false-0f048f2b-6644-45fa-9b1b-378a9f39f92c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.41648ms
Mar 30 13:10:03.599: INFO: Pod "alpine-nnp-false-0f048f2b-6644-45fa-9b1b-378a9f39f92c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003486888s
Mar 30 13:10:05.599: INFO: Pod "alpine-nnp-false-0f048f2b-6644-45fa-9b1b-378a9f39f92c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003377188s
Mar 30 13:10:07.599: INFO: Pod "alpine-nnp-false-0f048f2b-6644-45fa-9b1b-378a9f39f92c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003686363s
Mar 30 13:10:07.599: INFO: Pod "alpine-nnp-false-0f048f2b-6644-45fa-9b1b-378a9f39f92c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 30 13:10:07.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6460" for this suite. 03/30/23 13:10:07.611
------------------------------
â€¢ [SLOW TEST] [6.031 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:10:01.583
    Mar 30 13:10:01.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename security-context-test 03/30/23 13:10:01.583
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:10:01.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:10:01.591
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Mar 30 13:10:01.595: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0f048f2b-6644-45fa-9b1b-378a9f39f92c" in namespace "security-context-test-6460" to be "Succeeded or Failed"
    Mar 30 13:10:01.597: INFO: Pod "alpine-nnp-false-0f048f2b-6644-45fa-9b1b-378a9f39f92c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.41648ms
    Mar 30 13:10:03.599: INFO: Pod "alpine-nnp-false-0f048f2b-6644-45fa-9b1b-378a9f39f92c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003486888s
    Mar 30 13:10:05.599: INFO: Pod "alpine-nnp-false-0f048f2b-6644-45fa-9b1b-378a9f39f92c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003377188s
    Mar 30 13:10:07.599: INFO: Pod "alpine-nnp-false-0f048f2b-6644-45fa-9b1b-378a9f39f92c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003686363s
    Mar 30 13:10:07.599: INFO: Pod "alpine-nnp-false-0f048f2b-6644-45fa-9b1b-378a9f39f92c" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:10:07.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6460" for this suite. 03/30/23 13:10:07.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:10:07.614
Mar 30 13:10:07.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename init-container 03/30/23 13:10:07.615
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:10:07.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:10:07.623
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 03/30/23 13:10:07.624
Mar 30 13:10:07.624: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:10:13.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4733" for this suite. 03/30/23 13:10:13.032
------------------------------
â€¢ [SLOW TEST] [5.421 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:10:07.614
    Mar 30 13:10:07.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename init-container 03/30/23 13:10:07.615
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:10:07.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:10:07.623
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 03/30/23 13:10:07.624
    Mar 30 13:10:07.624: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:10:13.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4733" for this suite. 03/30/23 13:10:13.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:10:13.036
Mar 30 13:10:13.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename proxy 03/30/23 13:10:13.036
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:10:13.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:10:13.044
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Mar 30 13:10:13.045: INFO: Creating pod...
Mar 30 13:10:13.049: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1412" to be "running"
Mar 30 13:10:13.050: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.332067ms
Mar 30 13:10:15.053: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004250735s
Mar 30 13:10:15.053: INFO: Pod "agnhost" satisfied condition "running"
Mar 30 13:10:15.053: INFO: Creating service...
Mar 30 13:10:15.058: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/DELETE
Mar 30 13:10:15.060: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 30 13:10:15.060: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/GET
Mar 30 13:10:15.062: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 30 13:10:15.062: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/HEAD
Mar 30 13:10:15.064: INFO: http.Client request:HEAD | StatusCode:200
Mar 30 13:10:15.064: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/OPTIONS
Mar 30 13:10:15.066: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 30 13:10:15.066: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/PATCH
Mar 30 13:10:15.068: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 30 13:10:15.068: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/POST
Mar 30 13:10:15.070: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 30 13:10:15.070: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/PUT
Mar 30 13:10:15.071: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 30 13:10:15.071: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/DELETE
Mar 30 13:10:15.074: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 30 13:10:15.074: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/GET
Mar 30 13:10:15.076: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 30 13:10:15.076: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/HEAD
Mar 30 13:10:15.078: INFO: http.Client request:HEAD | StatusCode:200
Mar 30 13:10:15.078: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/OPTIONS
Mar 30 13:10:15.081: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 30 13:10:15.081: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/PATCH
Mar 30 13:10:15.083: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 30 13:10:15.083: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/POST
Mar 30 13:10:15.086: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 30 13:10:15.086: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/PUT
Mar 30 13:10:15.088: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 30 13:10:15.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-1412" for this suite. 03/30/23 13:10:15.091
------------------------------
â€¢ [2.058 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:10:13.036
    Mar 30 13:10:13.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename proxy 03/30/23 13:10:13.036
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:10:13.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:10:13.044
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Mar 30 13:10:13.045: INFO: Creating pod...
    Mar 30 13:10:13.049: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1412" to be "running"
    Mar 30 13:10:13.050: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.332067ms
    Mar 30 13:10:15.053: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004250735s
    Mar 30 13:10:15.053: INFO: Pod "agnhost" satisfied condition "running"
    Mar 30 13:10:15.053: INFO: Creating service...
    Mar 30 13:10:15.058: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/DELETE
    Mar 30 13:10:15.060: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 30 13:10:15.060: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/GET
    Mar 30 13:10:15.062: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 30 13:10:15.062: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/HEAD
    Mar 30 13:10:15.064: INFO: http.Client request:HEAD | StatusCode:200
    Mar 30 13:10:15.064: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/OPTIONS
    Mar 30 13:10:15.066: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 30 13:10:15.066: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/PATCH
    Mar 30 13:10:15.068: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 30 13:10:15.068: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/POST
    Mar 30 13:10:15.070: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 30 13:10:15.070: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/pods/agnhost/proxy/some/path/with/PUT
    Mar 30 13:10:15.071: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 30 13:10:15.071: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/DELETE
    Mar 30 13:10:15.074: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 30 13:10:15.074: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/GET
    Mar 30 13:10:15.076: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 30 13:10:15.076: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/HEAD
    Mar 30 13:10:15.078: INFO: http.Client request:HEAD | StatusCode:200
    Mar 30 13:10:15.078: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/OPTIONS
    Mar 30 13:10:15.081: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 30 13:10:15.081: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/PATCH
    Mar 30 13:10:15.083: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 30 13:10:15.083: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/POST
    Mar 30 13:10:15.086: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 30 13:10:15.086: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-1412/services/test-service/proxy/some/path/with/PUT
    Mar 30 13:10:15.088: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:10:15.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-1412" for this suite. 03/30/23 13:10:15.091
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:10:15.094
Mar 30 13:10:15.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename aggregator 03/30/23 13:10:15.095
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:10:15.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:10:15.104
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Mar 30 13:10:15.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 03/30/23 13:10:15.106
Mar 30 13:10:15.445: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 30 13:10:17.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:10:19.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:10:21.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:10:23.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:10:25.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:10:27.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:10:29.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:10:31.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:10:33.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:10:35.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:10:37.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:10:39.583: INFO: Waited 111.521137ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 03/30/23 13:10:39.611
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/30/23 13:10:39.613
STEP: List APIServices 03/30/23 13:10:39.616
Mar 30 13:10:39.619: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Mar 30 13:10:40.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-563" for this suite. 03/30/23 13:10:40.126
------------------------------
â€¢ [SLOW TEST] [25.083 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:10:15.094
    Mar 30 13:10:15.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename aggregator 03/30/23 13:10:15.095
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:10:15.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:10:15.104
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Mar 30 13:10:15.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 03/30/23 13:10:15.106
    Mar 30 13:10:15.445: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Mar 30 13:10:17.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:10:19.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:10:21.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:10:23.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:10:25.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:10:27.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:10:29.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:10:31.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:10:33.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:10:35.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:10:37.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 10, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:10:39.583: INFO: Waited 111.521137ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 03/30/23 13:10:39.611
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/30/23 13:10:39.613
    STEP: List APIServices 03/30/23 13:10:39.616
    Mar 30 13:10:39.619: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:10:40.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-563" for this suite. 03/30/23 13:10:40.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:10:40.177
Mar 30 13:10:40.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename cronjob 03/30/23 13:10:40.178
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:10:40.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:10:40.186
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 03/30/23 13:10:40.188
STEP: Ensuring more than one job is running at a time 03/30/23 13:10:40.191
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/30/23 13:12:00.193
STEP: Removing cronjob 03/30/23 13:12:00.195
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 30 13:12:00.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2402" for this suite. 03/30/23 13:12:00.203
------------------------------
â€¢ [SLOW TEST] [80.029 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:10:40.177
    Mar 30 13:10:40.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename cronjob 03/30/23 13:10:40.178
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:10:40.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:10:40.186
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 03/30/23 13:10:40.188
    STEP: Ensuring more than one job is running at a time 03/30/23 13:10:40.191
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/30/23 13:12:00.193
    STEP: Removing cronjob 03/30/23 13:12:00.195
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:12:00.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2402" for this suite. 03/30/23 13:12:00.203
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:12:00.206
Mar 30 13:12:00.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename replicaset 03/30/23 13:12:00.207
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:00.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:00.217
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Mar 30 13:12:00.224: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 30 13:12:05.228: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/30/23 13:12:05.228
Mar 30 13:12:05.228: INFO: Waiting up to 5m0s for pod "test-rs-p6swr" in namespace "replicaset-4599" to be "running"
Mar 30 13:12:05.229: INFO: Pod "test-rs-p6swr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.399039ms
Mar 30 13:12:07.232: INFO: Pod "test-rs-p6swr": Phase="Running", Reason="", readiness=true. Elapsed: 2.00383914s
Mar 30 13:12:07.232: INFO: Pod "test-rs-p6swr" satisfied condition "running"
STEP: Scaling up "test-rs" replicaset  03/30/23 13:12:07.232
Mar 30 13:12:07.236: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 03/30/23 13:12:07.236
W0330 13:12:07.240711      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 30 13:12:07.241: INFO: observed ReplicaSet test-rs in namespace replicaset-4599 with ReadyReplicas 1, AvailableReplicas 1
Mar 30 13:12:07.246: INFO: observed ReplicaSet test-rs in namespace replicaset-4599 with ReadyReplicas 1, AvailableReplicas 1
Mar 30 13:12:07.251: INFO: observed ReplicaSet test-rs in namespace replicaset-4599 with ReadyReplicas 1, AvailableReplicas 1
Mar 30 13:12:07.253: INFO: observed ReplicaSet test-rs in namespace replicaset-4599 with ReadyReplicas 1, AvailableReplicas 1
Mar 30 13:12:08.207: INFO: observed ReplicaSet test-rs in namespace replicaset-4599 with ReadyReplicas 2, AvailableReplicas 2
Mar 30 13:12:13.414: INFO: observed Replicaset test-rs in namespace replicaset-4599 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 30 13:12:13.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4599" for this suite. 03/30/23 13:12:13.416
------------------------------
â€¢ [SLOW TEST] [13.213 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:12:00.206
    Mar 30 13:12:00.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename replicaset 03/30/23 13:12:00.207
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:00.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:00.217
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Mar 30 13:12:00.224: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 30 13:12:05.228: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/30/23 13:12:05.228
    Mar 30 13:12:05.228: INFO: Waiting up to 5m0s for pod "test-rs-p6swr" in namespace "replicaset-4599" to be "running"
    Mar 30 13:12:05.229: INFO: Pod "test-rs-p6swr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.399039ms
    Mar 30 13:12:07.232: INFO: Pod "test-rs-p6swr": Phase="Running", Reason="", readiness=true. Elapsed: 2.00383914s
    Mar 30 13:12:07.232: INFO: Pod "test-rs-p6swr" satisfied condition "running"
    STEP: Scaling up "test-rs" replicaset  03/30/23 13:12:07.232
    Mar 30 13:12:07.236: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 03/30/23 13:12:07.236
    W0330 13:12:07.240711      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 30 13:12:07.241: INFO: observed ReplicaSet test-rs in namespace replicaset-4599 with ReadyReplicas 1, AvailableReplicas 1
    Mar 30 13:12:07.246: INFO: observed ReplicaSet test-rs in namespace replicaset-4599 with ReadyReplicas 1, AvailableReplicas 1
    Mar 30 13:12:07.251: INFO: observed ReplicaSet test-rs in namespace replicaset-4599 with ReadyReplicas 1, AvailableReplicas 1
    Mar 30 13:12:07.253: INFO: observed ReplicaSet test-rs in namespace replicaset-4599 with ReadyReplicas 1, AvailableReplicas 1
    Mar 30 13:12:08.207: INFO: observed ReplicaSet test-rs in namespace replicaset-4599 with ReadyReplicas 2, AvailableReplicas 2
    Mar 30 13:12:13.414: INFO: observed Replicaset test-rs in namespace replicaset-4599 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:12:13.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4599" for this suite. 03/30/23 13:12:13.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:12:13.42
Mar 30 13:12:13.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-lifecycle-hook 03/30/23 13:12:13.42
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:13.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:13.428
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/30/23 13:12:13.432
Mar 30 13:12:13.435: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-185" to be "running and ready"
Mar 30 13:12:13.437: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.5361ms
Mar 30 13:12:13.437: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:12:15.439: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003405258s
Mar 30 13:12:15.439: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 30 13:12:15.439: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 03/30/23 13:12:15.44
Mar 30 13:12:15.443: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-185" to be "running and ready"
Mar 30 13:12:15.444: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.694807ms
Mar 30 13:12:15.444: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:12:17.447: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004576638s
Mar 30 13:12:17.447: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Mar 30 13:12:17.447: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/30/23 13:12:17.449
STEP: delete the pod with lifecycle hook 03/30/23 13:12:17.459
Mar 30 13:12:17.463: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 30 13:12:17.465: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 30 13:12:19.465: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 30 13:12:19.467: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 30 13:12:19.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-185" for this suite. 03/30/23 13:12:19.469
------------------------------
â€¢ [SLOW TEST] [6.052 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:12:13.42
    Mar 30 13:12:13.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/30/23 13:12:13.42
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:13.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:13.428
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/30/23 13:12:13.432
    Mar 30 13:12:13.435: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-185" to be "running and ready"
    Mar 30 13:12:13.437: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.5361ms
    Mar 30 13:12:13.437: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:12:15.439: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003405258s
    Mar 30 13:12:15.439: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 30 13:12:15.439: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 03/30/23 13:12:15.44
    Mar 30 13:12:15.443: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-185" to be "running and ready"
    Mar 30 13:12:15.444: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.694807ms
    Mar 30 13:12:15.444: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:12:17.447: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004576638s
    Mar 30 13:12:17.447: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Mar 30 13:12:17.447: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/30/23 13:12:17.449
    STEP: delete the pod with lifecycle hook 03/30/23 13:12:17.459
    Mar 30 13:12:17.463: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 30 13:12:17.465: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 30 13:12:19.465: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 30 13:12:19.467: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:12:19.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-185" for this suite. 03/30/23 13:12:19.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:12:19.472
Mar 30 13:12:19.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename deployment 03/30/23 13:12:19.473
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:19.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:19.481
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Mar 30 13:12:19.482: INFO: Creating deployment "webserver-deployment"
Mar 30 13:12:19.484: INFO: Waiting for observed generation 1
Mar 30 13:12:21.488: INFO: Waiting for all required pods to come up
Mar 30 13:12:21.490: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 03/30/23 13:12:21.49
Mar 30 13:12:21.490: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-wvbgx" in namespace "deployment-8922" to be "running"
Mar 30 13:12:21.490: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gv5kd" in namespace "deployment-8922" to be "running"
Mar 30 13:12:21.490: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-lg872" in namespace "deployment-8922" to be "running"
Mar 30 13:12:21.492: INFO: Pod "webserver-deployment-7f5969cbc7-gv5kd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.609434ms
Mar 30 13:12:21.492: INFO: Pod "webserver-deployment-7f5969cbc7-lg872": Phase="Pending", Reason="", readiness=false. Elapsed: 1.581886ms
Mar 30 13:12:21.492: INFO: Pod "webserver-deployment-7f5969cbc7-wvbgx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.713027ms
Mar 30 13:12:23.494: INFO: Pod "webserver-deployment-7f5969cbc7-lg872": Phase="Running", Reason="", readiness=true. Elapsed: 2.003767749s
Mar 30 13:12:23.494: INFO: Pod "webserver-deployment-7f5969cbc7-wvbgx": Phase="Running", Reason="", readiness=true. Elapsed: 2.003845521s
Mar 30 13:12:23.494: INFO: Pod "webserver-deployment-7f5969cbc7-wvbgx" satisfied condition "running"
Mar 30 13:12:23.494: INFO: Pod "webserver-deployment-7f5969cbc7-lg872" satisfied condition "running"
Mar 30 13:12:23.494: INFO: Pod "webserver-deployment-7f5969cbc7-gv5kd": Phase="Running", Reason="", readiness=true. Elapsed: 2.003929881s
Mar 30 13:12:23.494: INFO: Pod "webserver-deployment-7f5969cbc7-gv5kd" satisfied condition "running"
Mar 30 13:12:23.494: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 30 13:12:23.497: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 30 13:12:23.501: INFO: Updating deployment webserver-deployment
Mar 30 13:12:23.501: INFO: Waiting for observed generation 2
Mar 30 13:12:25.505: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 30 13:12:25.507: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 30 13:12:25.508: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 30 13:12:25.512: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 30 13:12:25.512: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 30 13:12:25.513: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 30 13:12:25.516: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 30 13:12:25.516: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 30 13:12:25.520: INFO: Updating deployment webserver-deployment
Mar 30 13:12:25.520: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 30 13:12:25.523: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 30 13:12:25.524: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 30 13:12:25.529: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8922  8efeef08-5bda-44e9-9f1f-fbcd4e650a43 55127 3 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00269b798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-30 13:12:21 +0000 UTC,LastTransitionTime:2023-03-30 13:12:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-03-30 13:12:23 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 30 13:12:25.533: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-8922  837c484e-d662-4700-8beb-be9575877d54 55131 3 2023-03-30 13:12:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8efeef08-5bda-44e9-9f1f-fbcd4e650a43 0xc0038c9b97 0xc0038c9b98}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8efeef08-5bda-44e9-9f1f-fbcd4e650a43\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038c9c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 13:12:25.533: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 30 13:12:25.533: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-8922  69489624-f218-4dd8-be1a-26ebc08caf47 55128 3 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8efeef08-5bda-44e9-9f1f-fbcd4e650a43 0xc0038c9aa7 0xc0038c9aa8}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8efeef08-5bda-44e9-9f1f-fbcd4e650a43\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038c9b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 30 13:12:25.539: INFO: Pod "webserver-deployment-7f5969cbc7-6g8bh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6g8bh webserver-deployment-7f5969cbc7- deployment-8922  1c6e2e30-fcb3-4fbb-aaba-bc72e2f45f07 55006 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc0042400f7 0xc0042400f8}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6779,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6779,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.20,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f9fbb29d5f5aa4bbbea37ba98fe0fb32d46826f7721847d82e398df4e351ef4f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.539: INFO: Pod "webserver-deployment-7f5969cbc7-c2zwf" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c2zwf webserver-deployment-7f5969cbc7- deployment-8922  67feeaf2-c987-423e-a03d-d6a8ac621cc5 55000 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc0042402d0 0xc0042402d1}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.0.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ls6qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ls6qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.29.0.203,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://17e748b040f77a31593a5edc639ccf714dd541c4d230537e14e17b48f6097acc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.0.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.539: INFO: Pod "webserver-deployment-7f5969cbc7-clh6g" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-clh6g webserver-deployment-7f5969cbc7- deployment-8922  65529a08-b7d8-4fdf-ada6-a631773f1f62 54986 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc0042404a0 0xc0042404a1}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.0.205\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68x9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68x9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.29.0.205,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e943b9ecd0f22dc65b19ecacc4724b8332e21a6bd483aec02685e51e10f3b83f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.0.205,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.539: INFO: Pod "webserver-deployment-7f5969cbc7-d8cls" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d8cls webserver-deployment-7f5969cbc7- deployment-8922  0e9ecb6b-f53d-428d-8195-1fd17b51af58 54993 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240670 0xc004240671}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.0.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ll6vv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ll6vv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.29.0.204,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7159a70aaa8036ee5765703490d163159dbd431207acd157f9ef90dce636df95,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.0.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.539: INFO: Pod "webserver-deployment-7f5969cbc7-dl6pt" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dl6pt webserver-deployment-7f5969cbc7- deployment-8922  3a102439-d56b-4a6c-b964-ef07f57c0a29 55144 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240840 0xc004240841}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hjc6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hjc6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.540: INFO: Pod "webserver-deployment-7f5969cbc7-g4svr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-g4svr webserver-deployment-7f5969cbc7- deployment-8922  9a8d785f-f3c0-4d9c-b60e-34a328189b3a 55132 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240977 0xc004240978}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tf9df,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tf9df,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.540: INFO: Pod "webserver-deployment-7f5969cbc7-ggmd2" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ggmd2 webserver-deployment-7f5969cbc7- deployment-8922  722166e7-d5e0-4b4a-acf4-68b3afa216cf 54985 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240ae0 0xc004240ae1}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jv8jl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jv8jl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.21,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5e6b72c535dcaa2ecea373a17d053cf5d51ddcb337a7c866085e11474c8ac6af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.540: INFO: Pod "webserver-deployment-7f5969cbc7-jrppj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jrppj webserver-deployment-7f5969cbc7- deployment-8922  ad98555d-84a7-4cd8-97e4-a6a5427aba95 55147 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240cb0 0xc004240cb1}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62qv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62qv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.540: INFO: Pod "webserver-deployment-7f5969cbc7-kx2gx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kx2gx webserver-deployment-7f5969cbc7- deployment-8922  92e3cec7-308a-454a-a260-94ce376de0b9 54980 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240df7 0xc004240df8}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8fvpl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8fvpl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.22,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3f597ddccd190886bf7e0fe398551de24a161e4abaaa032e641c189d703ad78e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.540: INFO: Pod "webserver-deployment-7f5969cbc7-lg872" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lg872 webserver-deployment-7f5969cbc7- deployment-8922  33f22710-d3b6-4fdc-8a92-dc88f52e2239 55031 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240fd0 0xc004240fd1}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m9l4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m9l4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:10.29.1.78,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://559a5c987158433eb4621322e30afbec4482d6e67cbd3bb6766ace0e96b05837,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.540: INFO: Pod "webserver-deployment-7f5969cbc7-lxv7p" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lxv7p webserver-deployment-7f5969cbc7- deployment-8922  ddfc8040-8515-4770-9b1e-18e9c460c929 55148 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc0042411a0 0xc0042411a1}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hxclg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxclg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.541: INFO: Pod "webserver-deployment-7f5969cbc7-q5d9k" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q5d9k webserver-deployment-7f5969cbc7- deployment-8922  42303a44-c98f-4576-9e0a-504aa53f0178 55146 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc0042412d7 0xc0042412d8}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5666g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5666g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.541: INFO: Pod "webserver-deployment-7f5969cbc7-vmcqd" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vmcqd webserver-deployment-7f5969cbc7- deployment-8922  28119fe9-5c42-483d-963a-68e3bc069758 55143 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004241417 0xc004241418}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-chh9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-chh9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.541: INFO: Pod "webserver-deployment-7f5969cbc7-wvbgx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wvbgx webserver-deployment-7f5969cbc7- deployment-8922  e4bcf507-2665-42dd-8ee9-05e39ffa74dd 55024 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004241570 0xc004241571}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mr9zk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mr9zk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:10.29.1.79,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c02d9f26e6742910a2cfbeaed2a5f6ba33b4956a74282598e40e1f92713e6821,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.541: INFO: Pod "webserver-deployment-7f5969cbc7-xh8hr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xh8hr webserver-deployment-7f5969cbc7- deployment-8922  251a0f1b-1024-43a8-9128-799e078f858f 55145 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004241740 0xc004241741}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wg8rb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wg8rb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.541: INFO: Pod "webserver-deployment-d9f79cb5-4wj5p" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4wj5p webserver-deployment-d9f79cb5- deployment-8922  41785c38-df29-4b42-8cfa-fe2cf0b24dec 55070 0 2023-03-30 13:12:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc00424187f 0xc004241890}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6bmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6bmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:,StartTime:2023-03-30 13:12:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.541: INFO: Pod "webserver-deployment-d9f79cb5-5vzdj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5vzdj webserver-deployment-d9f79cb5- deployment-8922  d1d04c1e-8bfa-4eb2-9c43-5843f9d4eebc 55092 0 2023-03-30 13:12:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc004241a4f 0xc004241a60}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qs88g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qs88g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:,StartTime:2023-03-30 13:12:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.542: INFO: Pod "webserver-deployment-d9f79cb5-9f8xs" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9f8xs webserver-deployment-d9f79cb5- deployment-8922  04aef8e9-76e1-4200-8018-5a48dde0277c 55090 0 2023-03-30 13:12:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc004241c1f 0xc004241c30}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfgd4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfgd4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:,StartTime:2023-03-30 13:12:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.542: INFO: Pod "webserver-deployment-d9f79cb5-dm74p" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dm74p webserver-deployment-d9f79cb5- deployment-8922  37eb1f79-13f9-477b-a29d-3fa71e8ba89f 55142 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc004241def 0xc004241e00}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blb4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blb4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.542: INFO: Pod "webserver-deployment-d9f79cb5-v445r" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v445r webserver-deployment-d9f79cb5- deployment-8922  9d9e3059-e393-437c-a82b-fa82a094de97 55140 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc004241f47 0xc004241f48}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6slhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6slhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.542: INFO: Pod "webserver-deployment-d9f79cb5-vjcbk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vjcbk webserver-deployment-d9f79cb5- deployment-8922  a1d13df4-56a2-4212-bcb4-a69f88a71ad7 55141 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc0040ce09f 0xc0040ce0b0}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qz5xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qz5xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.542: INFO: Pod "webserver-deployment-d9f79cb5-vpt2g" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vpt2g webserver-deployment-d9f79cb5- deployment-8922  7e01a213-ba31-4198-801f-49354063ca74 55072 0 2023-03-30 13:12:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc0040ce1f7 0xc0040ce1f8}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4cwh2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4cwh2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:,StartTime:2023-03-30 13:12:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:12:25.542: INFO: Pod "webserver-deployment-d9f79cb5-zqdc5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zqdc5 webserver-deployment-d9f79cb5- deployment-8922  e59696e6-b9a9-4e67-83ac-1e14a18904db 55065 0 2023-03-30 13:12:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc0040ce3bf 0xc0040ce3d0}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f7ttw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7ttw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:,StartTime:2023-03-30 13:12:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 30 13:12:25.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8922" for this suite. 03/30/23 13:12:25.552
------------------------------
â€¢ [SLOW TEST] [6.085 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:12:19.472
    Mar 30 13:12:19.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename deployment 03/30/23 13:12:19.473
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:19.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:19.481
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Mar 30 13:12:19.482: INFO: Creating deployment "webserver-deployment"
    Mar 30 13:12:19.484: INFO: Waiting for observed generation 1
    Mar 30 13:12:21.488: INFO: Waiting for all required pods to come up
    Mar 30 13:12:21.490: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 03/30/23 13:12:21.49
    Mar 30 13:12:21.490: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-wvbgx" in namespace "deployment-8922" to be "running"
    Mar 30 13:12:21.490: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gv5kd" in namespace "deployment-8922" to be "running"
    Mar 30 13:12:21.490: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-lg872" in namespace "deployment-8922" to be "running"
    Mar 30 13:12:21.492: INFO: Pod "webserver-deployment-7f5969cbc7-gv5kd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.609434ms
    Mar 30 13:12:21.492: INFO: Pod "webserver-deployment-7f5969cbc7-lg872": Phase="Pending", Reason="", readiness=false. Elapsed: 1.581886ms
    Mar 30 13:12:21.492: INFO: Pod "webserver-deployment-7f5969cbc7-wvbgx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.713027ms
    Mar 30 13:12:23.494: INFO: Pod "webserver-deployment-7f5969cbc7-lg872": Phase="Running", Reason="", readiness=true. Elapsed: 2.003767749s
    Mar 30 13:12:23.494: INFO: Pod "webserver-deployment-7f5969cbc7-wvbgx": Phase="Running", Reason="", readiness=true. Elapsed: 2.003845521s
    Mar 30 13:12:23.494: INFO: Pod "webserver-deployment-7f5969cbc7-wvbgx" satisfied condition "running"
    Mar 30 13:12:23.494: INFO: Pod "webserver-deployment-7f5969cbc7-lg872" satisfied condition "running"
    Mar 30 13:12:23.494: INFO: Pod "webserver-deployment-7f5969cbc7-gv5kd": Phase="Running", Reason="", readiness=true. Elapsed: 2.003929881s
    Mar 30 13:12:23.494: INFO: Pod "webserver-deployment-7f5969cbc7-gv5kd" satisfied condition "running"
    Mar 30 13:12:23.494: INFO: Waiting for deployment "webserver-deployment" to complete
    Mar 30 13:12:23.497: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Mar 30 13:12:23.501: INFO: Updating deployment webserver-deployment
    Mar 30 13:12:23.501: INFO: Waiting for observed generation 2
    Mar 30 13:12:25.505: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Mar 30 13:12:25.507: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Mar 30 13:12:25.508: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 30 13:12:25.512: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Mar 30 13:12:25.512: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Mar 30 13:12:25.513: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 30 13:12:25.516: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Mar 30 13:12:25.516: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Mar 30 13:12:25.520: INFO: Updating deployment webserver-deployment
    Mar 30 13:12:25.520: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Mar 30 13:12:25.523: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Mar 30 13:12:25.524: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 30 13:12:25.529: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-8922  8efeef08-5bda-44e9-9f1f-fbcd4e650a43 55127 3 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00269b798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-30 13:12:21 +0000 UTC,LastTransitionTime:2023-03-30 13:12:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-03-30 13:12:23 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Mar 30 13:12:25.533: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-8922  837c484e-d662-4700-8beb-be9575877d54 55131 3 2023-03-30 13:12:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8efeef08-5bda-44e9-9f1f-fbcd4e650a43 0xc0038c9b97 0xc0038c9b98}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8efeef08-5bda-44e9-9f1f-fbcd4e650a43\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038c9c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 13:12:25.533: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Mar 30 13:12:25.533: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-8922  69489624-f218-4dd8-be1a-26ebc08caf47 55128 3 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8efeef08-5bda-44e9-9f1f-fbcd4e650a43 0xc0038c9aa7 0xc0038c9aa8}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8efeef08-5bda-44e9-9f1f-fbcd4e650a43\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038c9b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 13:12:25.539: INFO: Pod "webserver-deployment-7f5969cbc7-6g8bh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6g8bh webserver-deployment-7f5969cbc7- deployment-8922  1c6e2e30-fcb3-4fbb-aaba-bc72e2f45f07 55006 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc0042400f7 0xc0042400f8}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6779,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6779,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.20,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f9fbb29d5f5aa4bbbea37ba98fe0fb32d46826f7721847d82e398df4e351ef4f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.539: INFO: Pod "webserver-deployment-7f5969cbc7-c2zwf" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c2zwf webserver-deployment-7f5969cbc7- deployment-8922  67feeaf2-c987-423e-a03d-d6a8ac621cc5 55000 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc0042402d0 0xc0042402d1}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.0.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ls6qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ls6qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.29.0.203,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://17e748b040f77a31593a5edc639ccf714dd541c4d230537e14e17b48f6097acc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.0.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.539: INFO: Pod "webserver-deployment-7f5969cbc7-clh6g" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-clh6g webserver-deployment-7f5969cbc7- deployment-8922  65529a08-b7d8-4fdf-ada6-a631773f1f62 54986 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc0042404a0 0xc0042404a1}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.0.205\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68x9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68x9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.29.0.205,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e943b9ecd0f22dc65b19ecacc4724b8332e21a6bd483aec02685e51e10f3b83f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.0.205,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.539: INFO: Pod "webserver-deployment-7f5969cbc7-d8cls" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d8cls webserver-deployment-7f5969cbc7- deployment-8922  0e9ecb6b-f53d-428d-8195-1fd17b51af58 54993 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240670 0xc004240671}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.0.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ll6vv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ll6vv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.29.0.204,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7159a70aaa8036ee5765703490d163159dbd431207acd157f9ef90dce636df95,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.0.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.539: INFO: Pod "webserver-deployment-7f5969cbc7-dl6pt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dl6pt webserver-deployment-7f5969cbc7- deployment-8922  3a102439-d56b-4a6c-b964-ef07f57c0a29 55144 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240840 0xc004240841}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hjc6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hjc6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.540: INFO: Pod "webserver-deployment-7f5969cbc7-g4svr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-g4svr webserver-deployment-7f5969cbc7- deployment-8922  9a8d785f-f3c0-4d9c-b60e-34a328189b3a 55132 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240977 0xc004240978}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tf9df,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tf9df,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.540: INFO: Pod "webserver-deployment-7f5969cbc7-ggmd2" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ggmd2 webserver-deployment-7f5969cbc7- deployment-8922  722166e7-d5e0-4b4a-acf4-68b3afa216cf 54985 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240ae0 0xc004240ae1}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jv8jl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jv8jl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.21,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5e6b72c535dcaa2ecea373a17d053cf5d51ddcb337a7c866085e11474c8ac6af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.540: INFO: Pod "webserver-deployment-7f5969cbc7-jrppj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jrppj webserver-deployment-7f5969cbc7- deployment-8922  ad98555d-84a7-4cd8-97e4-a6a5427aba95 55147 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240cb0 0xc004240cb1}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62qv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62qv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.540: INFO: Pod "webserver-deployment-7f5969cbc7-kx2gx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kx2gx webserver-deployment-7f5969cbc7- deployment-8922  92e3cec7-308a-454a-a260-94ce376de0b9 54980 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240df7 0xc004240df8}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8fvpl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8fvpl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.22,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3f597ddccd190886bf7e0fe398551de24a161e4abaaa032e641c189d703ad78e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.540: INFO: Pod "webserver-deployment-7f5969cbc7-lg872" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lg872 webserver-deployment-7f5969cbc7- deployment-8922  33f22710-d3b6-4fdc-8a92-dc88f52e2239 55031 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004240fd0 0xc004240fd1}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m9l4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m9l4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:10.29.1.78,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://559a5c987158433eb4621322e30afbec4482d6e67cbd3bb6766ace0e96b05837,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.540: INFO: Pod "webserver-deployment-7f5969cbc7-lxv7p" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lxv7p webserver-deployment-7f5969cbc7- deployment-8922  ddfc8040-8515-4770-9b1e-18e9c460c929 55148 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc0042411a0 0xc0042411a1}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hxclg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxclg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.541: INFO: Pod "webserver-deployment-7f5969cbc7-q5d9k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q5d9k webserver-deployment-7f5969cbc7- deployment-8922  42303a44-c98f-4576-9e0a-504aa53f0178 55146 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc0042412d7 0xc0042412d8}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5666g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5666g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.541: INFO: Pod "webserver-deployment-7f5969cbc7-vmcqd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vmcqd webserver-deployment-7f5969cbc7- deployment-8922  28119fe9-5c42-483d-963a-68e3bc069758 55143 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004241417 0xc004241418}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-chh9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-chh9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.541: INFO: Pod "webserver-deployment-7f5969cbc7-wvbgx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wvbgx webserver-deployment-7f5969cbc7- deployment-8922  e4bcf507-2665-42dd-8ee9-05e39ffa74dd 55024 0 2023-03-30 13:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004241570 0xc004241571}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mr9zk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mr9zk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:10.29.1.79,StartTime:2023-03-30 13:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c02d9f26e6742910a2cfbeaed2a5f6ba33b4956a74282598e40e1f92713e6821,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.541: INFO: Pod "webserver-deployment-7f5969cbc7-xh8hr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xh8hr webserver-deployment-7f5969cbc7- deployment-8922  251a0f1b-1024-43a8-9128-799e078f858f 55145 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 69489624-f218-4dd8-be1a-26ebc08caf47 0xc004241740 0xc004241741}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69489624-f218-4dd8-be1a-26ebc08caf47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wg8rb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wg8rb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.541: INFO: Pod "webserver-deployment-d9f79cb5-4wj5p" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4wj5p webserver-deployment-d9f79cb5- deployment-8922  41785c38-df29-4b42-8cfa-fe2cf0b24dec 55070 0 2023-03-30 13:12:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc00424187f 0xc004241890}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6bmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6bmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:,StartTime:2023-03-30 13:12:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.541: INFO: Pod "webserver-deployment-d9f79cb5-5vzdj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5vzdj webserver-deployment-d9f79cb5- deployment-8922  d1d04c1e-8bfa-4eb2-9c43-5843f9d4eebc 55092 0 2023-03-30 13:12:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc004241a4f 0xc004241a60}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qs88g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qs88g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:,StartTime:2023-03-30 13:12:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.542: INFO: Pod "webserver-deployment-d9f79cb5-9f8xs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9f8xs webserver-deployment-d9f79cb5- deployment-8922  04aef8e9-76e1-4200-8018-5a48dde0277c 55090 0 2023-03-30 13:12:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc004241c1f 0xc004241c30}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfgd4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfgd4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:,StartTime:2023-03-30 13:12:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.542: INFO: Pod "webserver-deployment-d9f79cb5-dm74p" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dm74p webserver-deployment-d9f79cb5- deployment-8922  37eb1f79-13f9-477b-a29d-3fa71e8ba89f 55142 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc004241def 0xc004241e00}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blb4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blb4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.542: INFO: Pod "webserver-deployment-d9f79cb5-v445r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v445r webserver-deployment-d9f79cb5- deployment-8922  9d9e3059-e393-437c-a82b-fa82a094de97 55140 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc004241f47 0xc004241f48}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6slhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6slhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.542: INFO: Pod "webserver-deployment-d9f79cb5-vjcbk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vjcbk webserver-deployment-d9f79cb5- deployment-8922  a1d13df4-56a2-4212-bcb4-a69f88a71ad7 55141 0 2023-03-30 13:12:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc0040ce09f 0xc0040ce0b0}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qz5xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qz5xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.542: INFO: Pod "webserver-deployment-d9f79cb5-vpt2g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vpt2g webserver-deployment-d9f79cb5- deployment-8922  7e01a213-ba31-4198-801f-49354063ca74 55072 0 2023-03-30 13:12:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc0040ce1f7 0xc0040ce1f8}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4cwh2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4cwh2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:,StartTime:2023-03-30 13:12:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:12:25.542: INFO: Pod "webserver-deployment-d9f79cb5-zqdc5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zqdc5 webserver-deployment-d9f79cb5- deployment-8922  e59696e6-b9a9-4e67-83ac-1e14a18904db 55065 0 2023-03-30 13:12:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 837c484e-d662-4700-8beb-be9575877d54 0xc0040ce3bf 0xc0040ce3d0}] [] [{kube-controller-manager Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837c484e-d662-4700-8beb-be9575877d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:12:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f7ttw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7ttw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:12:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:,StartTime:2023-03-30 13:12:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:12:25.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8922" for this suite. 03/30/23 13:12:25.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:12:25.559
Mar 30 13:12:25.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:12:25.559
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:25.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:25.568
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:12:25.57
Mar 30 13:12:25.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b" in namespace "projected-5970" to be "Succeeded or Failed"
Mar 30 13:12:25.575: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.482956ms
Mar 30 13:12:27.578: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003945974s
Mar 30 13:12:29.578: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004323441s
Mar 30 13:12:31.579: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004820665s
Mar 30 13:12:33.578: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004169306s
Mar 30 13:12:35.577: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.003543208s
STEP: Saw pod success 03/30/23 13:12:35.577
Mar 30 13:12:35.577: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b" satisfied condition "Succeeded or Failed"
Mar 30 13:12:35.579: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b container client-container: <nil>
STEP: delete the pod 03/30/23 13:12:35.589
Mar 30 13:12:35.595: INFO: Waiting for pod downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b to disappear
Mar 30 13:12:35.596: INFO: Pod downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 30 13:12:35.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5970" for this suite. 03/30/23 13:12:35.598
------------------------------
â€¢ [SLOW TEST] [10.042 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:12:25.559
    Mar 30 13:12:25.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:12:25.559
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:25.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:25.568
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:12:25.57
    Mar 30 13:12:25.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b" in namespace "projected-5970" to be "Succeeded or Failed"
    Mar 30 13:12:25.575: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.482956ms
    Mar 30 13:12:27.578: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003945974s
    Mar 30 13:12:29.578: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004323441s
    Mar 30 13:12:31.579: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004820665s
    Mar 30 13:12:33.578: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004169306s
    Mar 30 13:12:35.577: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.003543208s
    STEP: Saw pod success 03/30/23 13:12:35.577
    Mar 30 13:12:35.577: INFO: Pod "downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b" satisfied condition "Succeeded or Failed"
    Mar 30 13:12:35.579: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b container client-container: <nil>
    STEP: delete the pod 03/30/23 13:12:35.589
    Mar 30 13:12:35.595: INFO: Waiting for pod downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b to disappear
    Mar 30 13:12:35.596: INFO: Pod downwardapi-volume-f2da2183-b10f-41a1-a3d6-631175927f5b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:12:35.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5970" for this suite. 03/30/23 13:12:35.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:12:35.601
Mar 30 13:12:35.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pods 03/30/23 13:12:35.602
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:35.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:35.61
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 03/30/23 13:12:35.611
Mar 30 13:12:35.615: INFO: Waiting up to 5m0s for pod "pod-kqsgx" in namespace "pods-3442" to be "running"
Mar 30 13:12:35.616: INFO: Pod "pod-kqsgx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.405389ms
Mar 30 13:12:37.618: INFO: Pod "pod-kqsgx": Phase="Running", Reason="", readiness=true. Elapsed: 2.003544562s
Mar 30 13:12:37.618: INFO: Pod "pod-kqsgx" satisfied condition "running"
STEP: patching /status 03/30/23 13:12:37.618
Mar 30 13:12:37.622: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 30 13:12:37.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3442" for this suite. 03/30/23 13:12:37.625
------------------------------
â€¢ [2.026 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:12:35.601
    Mar 30 13:12:35.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pods 03/30/23 13:12:35.602
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:35.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:35.61
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 03/30/23 13:12:35.611
    Mar 30 13:12:35.615: INFO: Waiting up to 5m0s for pod "pod-kqsgx" in namespace "pods-3442" to be "running"
    Mar 30 13:12:35.616: INFO: Pod "pod-kqsgx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.405389ms
    Mar 30 13:12:37.618: INFO: Pod "pod-kqsgx": Phase="Running", Reason="", readiness=true. Elapsed: 2.003544562s
    Mar 30 13:12:37.618: INFO: Pod "pod-kqsgx" satisfied condition "running"
    STEP: patching /status 03/30/23 13:12:37.618
    Mar 30 13:12:37.622: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:12:37.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3442" for this suite. 03/30/23 13:12:37.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:12:37.628
Mar 30 13:12:37.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:12:37.628
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:37.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:37.636
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-1fcc2d04-4c19-4a14-9939-0ab56265d809 03/30/23 13:12:37.638
STEP: Creating a pod to test consume configMaps 03/30/23 13:12:37.64
Mar 30 13:12:37.643: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7" in namespace "projected-5292" to be "Succeeded or Failed"
Mar 30 13:12:37.644: INFO: Pod "pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.380452ms
Mar 30 13:12:39.646: INFO: Pod "pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003442237s
Mar 30 13:12:41.647: INFO: Pod "pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004450986s
STEP: Saw pod success 03/30/23 13:12:41.647
Mar 30 13:12:41.647: INFO: Pod "pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7" satisfied condition "Succeeded or Failed"
Mar 30 13:12:41.649: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7 container agnhost-container: <nil>
STEP: delete the pod 03/30/23 13:12:41.652
Mar 30 13:12:41.657: INFO: Waiting for pod pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7 to disappear
Mar 30 13:12:41.659: INFO: Pod pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:12:41.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5292" for this suite. 03/30/23 13:12:41.661
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:12:37.628
    Mar 30 13:12:37.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:12:37.628
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:37.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:37.636
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-1fcc2d04-4c19-4a14-9939-0ab56265d809 03/30/23 13:12:37.638
    STEP: Creating a pod to test consume configMaps 03/30/23 13:12:37.64
    Mar 30 13:12:37.643: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7" in namespace "projected-5292" to be "Succeeded or Failed"
    Mar 30 13:12:37.644: INFO: Pod "pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.380452ms
    Mar 30 13:12:39.646: INFO: Pod "pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003442237s
    Mar 30 13:12:41.647: INFO: Pod "pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004450986s
    STEP: Saw pod success 03/30/23 13:12:41.647
    Mar 30 13:12:41.647: INFO: Pod "pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7" satisfied condition "Succeeded or Failed"
    Mar 30 13:12:41.649: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7 container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 13:12:41.652
    Mar 30 13:12:41.657: INFO: Waiting for pod pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7 to disappear
    Mar 30 13:12:41.659: INFO: Pod pod-projected-configmaps-d0ff04a5-76ad-4fc6-be2c-3bf26f40e6b7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:12:41.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5292" for this suite. 03/30/23 13:12:41.661
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:12:41.664
Mar 30 13:12:41.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename cronjob 03/30/23 13:12:41.665
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:41.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:41.672
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 03/30/23 13:12:41.674
STEP: Ensuring a job is scheduled 03/30/23 13:12:41.676
STEP: Ensuring exactly one is scheduled 03/30/23 13:13:01.679
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/30/23 13:13:01.681
STEP: Ensuring no more jobs are scheduled 03/30/23 13:13:01.682
STEP: Removing cronjob 03/30/23 13:18:01.687
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 30 13:18:01.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4418" for this suite. 03/30/23 13:18:01.692
------------------------------
â€¢ [SLOW TEST] [320.031 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:12:41.664
    Mar 30 13:12:41.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename cronjob 03/30/23 13:12:41.665
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:12:41.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:12:41.672
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 03/30/23 13:12:41.674
    STEP: Ensuring a job is scheduled 03/30/23 13:12:41.676
    STEP: Ensuring exactly one is scheduled 03/30/23 13:13:01.679
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/30/23 13:13:01.681
    STEP: Ensuring no more jobs are scheduled 03/30/23 13:13:01.682
    STEP: Removing cronjob 03/30/23 13:18:01.687
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:18:01.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4418" for this suite. 03/30/23 13:18:01.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:18:01.696
Mar 30 13:18:01.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 13:18:01.697
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:01.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:01.707
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 03/30/23 13:18:01.709
Mar 30 13:18:01.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-1679 api-versions'
Mar 30 13:18:01.767: INFO: stderr: ""
Mar 30 13:18:01.767: INFO: stdout: "admissionregistration.k8s.io/v1\nalert.alibabacloud.com/v1beta1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ngateway.networking.k8s.io/v1alpha2\ngateway.networking.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.alibabacloud.com/v1alpha1\nstorage.alibabacloud.com/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 13:18:01.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1679" for this suite. 03/30/23 13:18:01.77
------------------------------
â€¢ [0.076 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:18:01.696
    Mar 30 13:18:01.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 13:18:01.697
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:01.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:01.707
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 03/30/23 13:18:01.709
    Mar 30 13:18:01.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-1679 api-versions'
    Mar 30 13:18:01.767: INFO: stderr: ""
    Mar 30 13:18:01.767: INFO: stdout: "admissionregistration.k8s.io/v1\nalert.alibabacloud.com/v1beta1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ngateway.networking.k8s.io/v1alpha2\ngateway.networking.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.alibabacloud.com/v1alpha1\nstorage.alibabacloud.com/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:18:01.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1679" for this suite. 03/30/23 13:18:01.77
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:18:01.773
Mar 30 13:18:01.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename crd-webhook 03/30/23 13:18:01.774
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:01.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:01.782
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/30/23 13:18:01.784
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/30/23 13:18:02.07
STEP: Deploying the custom resource conversion webhook pod 03/30/23 13:18:02.073
STEP: Wait for the deployment to be ready 03/30/23 13:18:02.078
Mar 30 13:18:02.081: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 03/30/23 13:18:04.087
STEP: Verifying the service has paired with the endpoint 03/30/23 13:18:04.092
Mar 30 13:18:05.092: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Mar 30 13:18:05.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Creating a v1 custom resource 03/30/23 13:18:12.644
STEP: Create a v2 custom resource 03/30/23 13:18:12.651
STEP: List CRs in v1 03/30/23 13:18:12.682
STEP: List CRs in v2 03/30/23 13:18:12.684
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:18:13.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6932" for this suite. 03/30/23 13:18:13.216
------------------------------
â€¢ [SLOW TEST] [11.446 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:18:01.773
    Mar 30 13:18:01.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename crd-webhook 03/30/23 13:18:01.774
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:01.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:01.782
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/30/23 13:18:01.784
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/30/23 13:18:02.07
    STEP: Deploying the custom resource conversion webhook pod 03/30/23 13:18:02.073
    STEP: Wait for the deployment to be ready 03/30/23 13:18:02.078
    Mar 30 13:18:02.081: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 03/30/23 13:18:04.087
    STEP: Verifying the service has paired with the endpoint 03/30/23 13:18:04.092
    Mar 30 13:18:05.092: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Mar 30 13:18:05.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Creating a v1 custom resource 03/30/23 13:18:12.644
    STEP: Create a v2 custom resource 03/30/23 13:18:12.651
    STEP: List CRs in v1 03/30/23 13:18:12.682
    STEP: List CRs in v2 03/30/23 13:18:12.684
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:18:13.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6932" for this suite. 03/30/23 13:18:13.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:18:13.22
Mar 30 13:18:13.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename deployment 03/30/23 13:18:13.22
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:13.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:13.23
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Mar 30 13:18:13.236: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 30 13:18:18.238: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/30/23 13:18:18.238
Mar 30 13:18:18.238: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/30/23 13:18:18.243
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 30 13:18:18.248: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4668  b152f17a-8ac7-4f0b-b713-3c7467c88bb9 57230 1 2023-03-30 13:18:18 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-30 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bb42f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 30 13:18:18.250: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-4668  efa39561-8b74-4040-aee2-f924fee73624 57232 1 2023-03-30 13:18:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b152f17a-8ac7-4f0b-b713-3c7467c88bb9 0xc002bb4777 0xc002bb4778}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b152f17a-8ac7-4f0b-b713-3c7467c88bb9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bb4808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 13:18:18.250: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 30 13:18:18.250: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4668  4251907b-6830-4b17-9709-47f04e2d9aae 57231 1 2023-03-30 13:18:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment b152f17a-8ac7-4f0b-b713-3c7467c88bb9 0xc002bb4647 0xc002bb4648}] [] [{e2e.test Update apps/v1 2023-03-30 13:18:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:18:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-30 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"b152f17a-8ac7-4f0b-b713-3c7467c88bb9\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002bb4708 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 30 13:18:18.252: INFO: Pod "test-cleanup-controller-k8rnq" is available:
&Pod{ObjectMeta:{test-cleanup-controller-k8rnq test-cleanup-controller- deployment-4668  4e7442ae-c158-43bf-9a2e-90c66d0347dd 57211 0 2023-03-30 13:18:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 4251907b-6830-4b17-9709-47f04e2d9aae 0xc002bb4db7 0xc002bb4db8}] [] [{kube-controller-manager Update v1 2023-03-30 13:18:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4251907b-6830-4b17-9709-47f04e2d9aae\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:18:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z7zzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z7zzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:18:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:18:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:18:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:18:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.27,StartTime:2023-03-30 13:18:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:18:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e46b94ff7e34e13d278af6eec97c4215723049295f2cbb8d6aaed313e8949e8c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:18:18.252: INFO: Pod "test-cleanup-deployment-7698ff6f6b-cvhkq" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-cvhkq test-cleanup-deployment-7698ff6f6b- deployment-4668  1ebb2be2-74d8-4b00-9a07-ca9417390cb6 57234 0 2023-03-30 13:18:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b efa39561-8b74-4040-aee2-f924fee73624 0xc002bb4f97 0xc002bb4f98}] [] [{kube-controller-manager Update v1 2023-03-30 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efa39561-8b74-4040-aee2-f924fee73624\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djm69,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djm69,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 30 13:18:18.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4668" for this suite. 03/30/23 13:18:18.255
------------------------------
â€¢ [SLOW TEST] [5.038 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:18:13.22
    Mar 30 13:18:13.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename deployment 03/30/23 13:18:13.22
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:13.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:13.23
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Mar 30 13:18:13.236: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Mar 30 13:18:18.238: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/30/23 13:18:18.238
    Mar 30 13:18:18.238: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/30/23 13:18:18.243
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 30 13:18:18.248: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4668  b152f17a-8ac7-4f0b-b713-3c7467c88bb9 57230 1 2023-03-30 13:18:18 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-30 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bb42f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Mar 30 13:18:18.250: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-4668  efa39561-8b74-4040-aee2-f924fee73624 57232 1 2023-03-30 13:18:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b152f17a-8ac7-4f0b-b713-3c7467c88bb9 0xc002bb4777 0xc002bb4778}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b152f17a-8ac7-4f0b-b713-3c7467c88bb9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bb4808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 13:18:18.250: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Mar 30 13:18:18.250: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4668  4251907b-6830-4b17-9709-47f04e2d9aae 57231 1 2023-03-30 13:18:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment b152f17a-8ac7-4f0b-b713-3c7467c88bb9 0xc002bb4647 0xc002bb4648}] [] [{e2e.test Update apps/v1 2023-03-30 13:18:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:18:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-30 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"b152f17a-8ac7-4f0b-b713-3c7467c88bb9\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002bb4708 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 13:18:18.252: INFO: Pod "test-cleanup-controller-k8rnq" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-k8rnq test-cleanup-controller- deployment-4668  4e7442ae-c158-43bf-9a2e-90c66d0347dd 57211 0 2023-03-30 13:18:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 4251907b-6830-4b17-9709-47f04e2d9aae 0xc002bb4db7 0xc002bb4db8}] [] [{kube-controller-manager Update v1 2023-03-30 13:18:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4251907b-6830-4b17-9709-47f04e2d9aae\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:18:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z7zzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z7zzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:18:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:18:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:18:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:18:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.27,StartTime:2023-03-30 13:18:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:18:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e46b94ff7e34e13d278af6eec97c4215723049295f2cbb8d6aaed313e8949e8c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:18:18.252: INFO: Pod "test-cleanup-deployment-7698ff6f6b-cvhkq" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-cvhkq test-cleanup-deployment-7698ff6f6b- deployment-4668  1ebb2be2-74d8-4b00-9a07-ca9417390cb6 57234 0 2023-03-30 13:18:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b efa39561-8b74-4040-aee2-f924fee73624 0xc002bb4f97 0xc002bb4f98}] [] [{kube-controller-manager Update v1 2023-03-30 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efa39561-8b74-4040-aee2-f924fee73624\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djm69,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djm69,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:18:18.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4668" for this suite. 03/30/23 13:18:18.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:18:18.259
Mar 30 13:18:18.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:18:18.26
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:18.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:18.269
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-1852f534-5dd8-4ab5-85f9-4a9e9c9df0ee 03/30/23 13:18:18.27
STEP: Creating a pod to test consume configMaps 03/30/23 13:18:18.272
Mar 30 13:18:18.276: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1" in namespace "configmap-4780" to be "Succeeded or Failed"
Mar 30 13:18:18.278: INFO: Pod "pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.935929ms
Mar 30 13:18:20.280: INFO: Pod "pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004143062s
Mar 30 13:18:22.280: INFO: Pod "pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004420415s
STEP: Saw pod success 03/30/23 13:18:22.28
Mar 30 13:18:22.280: INFO: Pod "pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1" satisfied condition "Succeeded or Failed"
Mar 30 13:18:22.282: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1 container agnhost-container: <nil>
STEP: delete the pod 03/30/23 13:18:22.292
Mar 30 13:18:22.297: INFO: Waiting for pod pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1 to disappear
Mar 30 13:18:22.298: INFO: Pod pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:18:22.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4780" for this suite. 03/30/23 13:18:22.301
------------------------------
â€¢ [4.044 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:18:18.259
    Mar 30 13:18:18.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:18:18.26
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:18.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:18.269
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-1852f534-5dd8-4ab5-85f9-4a9e9c9df0ee 03/30/23 13:18:18.27
    STEP: Creating a pod to test consume configMaps 03/30/23 13:18:18.272
    Mar 30 13:18:18.276: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1" in namespace "configmap-4780" to be "Succeeded or Failed"
    Mar 30 13:18:18.278: INFO: Pod "pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.935929ms
    Mar 30 13:18:20.280: INFO: Pod "pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004143062s
    Mar 30 13:18:22.280: INFO: Pod "pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004420415s
    STEP: Saw pod success 03/30/23 13:18:22.28
    Mar 30 13:18:22.280: INFO: Pod "pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1" satisfied condition "Succeeded or Failed"
    Mar 30 13:18:22.282: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1 container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 13:18:22.292
    Mar 30 13:18:22.297: INFO: Waiting for pod pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1 to disappear
    Mar 30 13:18:22.298: INFO: Pod pod-configmaps-e7f7cb6d-4faa-431a-97f0-7a2dc064e2a1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:18:22.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4780" for this suite. 03/30/23 13:18:22.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:18:22.304
Mar 30 13:18:22.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename var-expansion 03/30/23 13:18:22.305
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:22.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:22.313
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Mar 30 13:18:22.318: INFO: Waiting up to 2m0s for pod "var-expansion-bfc8e365-4566-4c19-a4dc-9584e272b4b7" in namespace "var-expansion-6463" to be "container 0 failed with reason CreateContainerConfigError"
Mar 30 13:18:22.319: INFO: Pod "var-expansion-bfc8e365-4566-4c19-a4dc-9584e272b4b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.407647ms
Mar 30 13:18:24.322: INFO: Pod "var-expansion-bfc8e365-4566-4c19-a4dc-9584e272b4b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004442888s
Mar 30 13:18:24.322: INFO: Pod "var-expansion-bfc8e365-4566-4c19-a4dc-9584e272b4b7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 30 13:18:24.322: INFO: Deleting pod "var-expansion-bfc8e365-4566-4c19-a4dc-9584e272b4b7" in namespace "var-expansion-6463"
Mar 30 13:18:24.325: INFO: Wait up to 5m0s for pod "var-expansion-bfc8e365-4566-4c19-a4dc-9584e272b4b7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 30 13:18:26.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6463" for this suite. 03/30/23 13:18:26.332
------------------------------
â€¢ [4.030 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:18:22.304
    Mar 30 13:18:22.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename var-expansion 03/30/23 13:18:22.305
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:22.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:22.313
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Mar 30 13:18:22.318: INFO: Waiting up to 2m0s for pod "var-expansion-bfc8e365-4566-4c19-a4dc-9584e272b4b7" in namespace "var-expansion-6463" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 30 13:18:22.319: INFO: Pod "var-expansion-bfc8e365-4566-4c19-a4dc-9584e272b4b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.407647ms
    Mar 30 13:18:24.322: INFO: Pod "var-expansion-bfc8e365-4566-4c19-a4dc-9584e272b4b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004442888s
    Mar 30 13:18:24.322: INFO: Pod "var-expansion-bfc8e365-4566-4c19-a4dc-9584e272b4b7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 30 13:18:24.322: INFO: Deleting pod "var-expansion-bfc8e365-4566-4c19-a4dc-9584e272b4b7" in namespace "var-expansion-6463"
    Mar 30 13:18:24.325: INFO: Wait up to 5m0s for pod "var-expansion-bfc8e365-4566-4c19-a4dc-9584e272b4b7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:18:26.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6463" for this suite. 03/30/23 13:18:26.332
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:18:26.335
Mar 30 13:18:26.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:18:26.335
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:26.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:26.343
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-b028f486-c776-4a38-8825-daaeb090a6e3 03/30/23 13:18:26.345
STEP: Creating a pod to test consume secrets 03/30/23 13:18:26.347
Mar 30 13:18:26.350: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f" in namespace "projected-5979" to be "Succeeded or Failed"
Mar 30 13:18:26.352: INFO: Pod "pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.421352ms
Mar 30 13:18:28.354: INFO: Pod "pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003899806s
Mar 30 13:18:30.355: INFO: Pod "pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004403218s
STEP: Saw pod success 03/30/23 13:18:30.355
Mar 30 13:18:30.355: INFO: Pod "pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f" satisfied condition "Succeeded or Failed"
Mar 30 13:18:30.357: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f container projected-secret-volume-test: <nil>
STEP: delete the pod 03/30/23 13:18:30.36
Mar 30 13:18:30.365: INFO: Waiting for pod pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f to disappear
Mar 30 13:18:30.367: INFO: Pod pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 30 13:18:30.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5979" for this suite. 03/30/23 13:18:30.369
------------------------------
â€¢ [4.037 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:18:26.335
    Mar 30 13:18:26.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:18:26.335
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:26.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:26.343
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-b028f486-c776-4a38-8825-daaeb090a6e3 03/30/23 13:18:26.345
    STEP: Creating a pod to test consume secrets 03/30/23 13:18:26.347
    Mar 30 13:18:26.350: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f" in namespace "projected-5979" to be "Succeeded or Failed"
    Mar 30 13:18:26.352: INFO: Pod "pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.421352ms
    Mar 30 13:18:28.354: INFO: Pod "pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003899806s
    Mar 30 13:18:30.355: INFO: Pod "pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004403218s
    STEP: Saw pod success 03/30/23 13:18:30.355
    Mar 30 13:18:30.355: INFO: Pod "pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f" satisfied condition "Succeeded or Failed"
    Mar 30 13:18:30.357: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 13:18:30.36
    Mar 30 13:18:30.365: INFO: Waiting for pod pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f to disappear
    Mar 30 13:18:30.367: INFO: Pod pod-projected-secrets-ecdc2039-7324-4db0-bc4a-732cebc12f9f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:18:30.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5979" for this suite. 03/30/23 13:18:30.369
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:18:30.372
Mar 30 13:18:30.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 13:18:30.372
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:30.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:30.38
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-4037 03/30/23 13:18:30.382
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4037 to expose endpoints map[] 03/30/23 13:18:30.386
Mar 30 13:18:30.387: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar 30 13:18:31.392: INFO: successfully validated that service multi-endpoint-test in namespace services-4037 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4037 03/30/23 13:18:31.392
Mar 30 13:18:31.396: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4037" to be "running and ready"
Mar 30 13:18:31.398: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.438608ms
Mar 30 13:18:31.398: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:18:33.400: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004068599s
Mar 30 13:18:33.400: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 30 13:18:33.400: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4037 to expose endpoints map[pod1:[100]] 03/30/23 13:18:33.402
Mar 30 13:18:33.407: INFO: successfully validated that service multi-endpoint-test in namespace services-4037 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4037 03/30/23 13:18:33.407
Mar 30 13:18:33.409: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4037" to be "running and ready"
Mar 30 13:18:33.410: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.351816ms
Mar 30 13:18:33.410: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:18:35.412: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003382945s
Mar 30 13:18:35.412: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 30 13:18:35.412: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4037 to expose endpoints map[pod1:[100] pod2:[101]] 03/30/23 13:18:35.414
Mar 30 13:18:35.420: INFO: successfully validated that service multi-endpoint-test in namespace services-4037 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 03/30/23 13:18:35.42
Mar 30 13:18:35.420: INFO: Creating new exec pod
Mar 30 13:18:35.423: INFO: Waiting up to 5m0s for pod "execpod2cqrl" in namespace "services-4037" to be "running"
Mar 30 13:18:35.424: INFO: Pod "execpod2cqrl": Phase="Pending", Reason="", readiness=false. Elapsed: 1.338035ms
Mar 30 13:18:37.426: INFO: Pod "execpod2cqrl": Phase="Running", Reason="", readiness=true. Elapsed: 2.003319679s
Mar 30 13:18:37.426: INFO: Pod "execpod2cqrl" satisfied condition "running"
Mar 30 13:18:38.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-4037 exec execpod2cqrl -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Mar 30 13:18:38.520: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar 30 13:18:38.520: INFO: stdout: ""
Mar 30 13:18:38.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-4037 exec execpod2cqrl -- /bin/sh -x -c nc -v -z -w 2 172.16.9.220 80'
Mar 30 13:18:38.611: INFO: stderr: "+ nc -v -z -w 2 172.16.9.220 80\nConnection to 172.16.9.220 80 port [tcp/http] succeeded!\n"
Mar 30 13:18:38.611: INFO: stdout: ""
Mar 30 13:18:38.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-4037 exec execpod2cqrl -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Mar 30 13:18:38.703: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar 30 13:18:38.703: INFO: stdout: ""
Mar 30 13:18:38.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-4037 exec execpod2cqrl -- /bin/sh -x -c nc -v -z -w 2 172.16.9.220 81'
Mar 30 13:18:38.793: INFO: stderr: "+ nc -v -z -w 2 172.16.9.220 81\nConnection to 172.16.9.220 81 port [tcp/*] succeeded!\n"
Mar 30 13:18:38.793: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-4037 03/30/23 13:18:38.793
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4037 to expose endpoints map[pod2:[101]] 03/30/23 13:18:38.798
Mar 30 13:18:38.804: INFO: successfully validated that service multi-endpoint-test in namespace services-4037 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4037 03/30/23 13:18:38.804
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4037 to expose endpoints map[] 03/30/23 13:18:38.811
Mar 30 13:18:38.817: INFO: successfully validated that service multi-endpoint-test in namespace services-4037 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 13:18:38.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4037" for this suite. 03/30/23 13:18:38.827
------------------------------
â€¢ [SLOW TEST] [8.458 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:18:30.372
    Mar 30 13:18:30.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 13:18:30.372
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:30.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:30.38
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-4037 03/30/23 13:18:30.382
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4037 to expose endpoints map[] 03/30/23 13:18:30.386
    Mar 30 13:18:30.387: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Mar 30 13:18:31.392: INFO: successfully validated that service multi-endpoint-test in namespace services-4037 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4037 03/30/23 13:18:31.392
    Mar 30 13:18:31.396: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4037" to be "running and ready"
    Mar 30 13:18:31.398: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.438608ms
    Mar 30 13:18:31.398: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:18:33.400: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004068599s
    Mar 30 13:18:33.400: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 30 13:18:33.400: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4037 to expose endpoints map[pod1:[100]] 03/30/23 13:18:33.402
    Mar 30 13:18:33.407: INFO: successfully validated that service multi-endpoint-test in namespace services-4037 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-4037 03/30/23 13:18:33.407
    Mar 30 13:18:33.409: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4037" to be "running and ready"
    Mar 30 13:18:33.410: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.351816ms
    Mar 30 13:18:33.410: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:18:35.412: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003382945s
    Mar 30 13:18:35.412: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 30 13:18:35.412: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4037 to expose endpoints map[pod1:[100] pod2:[101]] 03/30/23 13:18:35.414
    Mar 30 13:18:35.420: INFO: successfully validated that service multi-endpoint-test in namespace services-4037 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 03/30/23 13:18:35.42
    Mar 30 13:18:35.420: INFO: Creating new exec pod
    Mar 30 13:18:35.423: INFO: Waiting up to 5m0s for pod "execpod2cqrl" in namespace "services-4037" to be "running"
    Mar 30 13:18:35.424: INFO: Pod "execpod2cqrl": Phase="Pending", Reason="", readiness=false. Elapsed: 1.338035ms
    Mar 30 13:18:37.426: INFO: Pod "execpod2cqrl": Phase="Running", Reason="", readiness=true. Elapsed: 2.003319679s
    Mar 30 13:18:37.426: INFO: Pod "execpod2cqrl" satisfied condition "running"
    Mar 30 13:18:38.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-4037 exec execpod2cqrl -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Mar 30 13:18:38.520: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Mar 30 13:18:38.520: INFO: stdout: ""
    Mar 30 13:18:38.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-4037 exec execpod2cqrl -- /bin/sh -x -c nc -v -z -w 2 172.16.9.220 80'
    Mar 30 13:18:38.611: INFO: stderr: "+ nc -v -z -w 2 172.16.9.220 80\nConnection to 172.16.9.220 80 port [tcp/http] succeeded!\n"
    Mar 30 13:18:38.611: INFO: stdout: ""
    Mar 30 13:18:38.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-4037 exec execpod2cqrl -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Mar 30 13:18:38.703: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Mar 30 13:18:38.703: INFO: stdout: ""
    Mar 30 13:18:38.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-4037 exec execpod2cqrl -- /bin/sh -x -c nc -v -z -w 2 172.16.9.220 81'
    Mar 30 13:18:38.793: INFO: stderr: "+ nc -v -z -w 2 172.16.9.220 81\nConnection to 172.16.9.220 81 port [tcp/*] succeeded!\n"
    Mar 30 13:18:38.793: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-4037 03/30/23 13:18:38.793
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4037 to expose endpoints map[pod2:[101]] 03/30/23 13:18:38.798
    Mar 30 13:18:38.804: INFO: successfully validated that service multi-endpoint-test in namespace services-4037 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-4037 03/30/23 13:18:38.804
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4037 to expose endpoints map[] 03/30/23 13:18:38.811
    Mar 30 13:18:38.817: INFO: successfully validated that service multi-endpoint-test in namespace services-4037 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:18:38.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4037" for this suite. 03/30/23 13:18:38.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:18:38.831
Mar 30 13:18:38.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:18:38.832
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:38.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:38.84
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-abdc133c-8649-4273-89cc-a8e525608d86 03/30/23 13:18:38.844
STEP: Creating configMap with name cm-test-opt-upd-4298507e-43a9-4b00-acd8-010e72d07506 03/30/23 13:18:38.847
STEP: Creating the pod 03/30/23 13:18:38.849
Mar 30 13:18:38.853: INFO: Waiting up to 5m0s for pod "pod-configmaps-526ec73e-87f2-4187-a704-926594dc6386" in namespace "configmap-1430" to be "running and ready"
Mar 30 13:18:38.855: INFO: Pod "pod-configmaps-526ec73e-87f2-4187-a704-926594dc6386": Phase="Pending", Reason="", readiness=false. Elapsed: 1.798472ms
Mar 30 13:18:38.855: INFO: The phase of Pod pod-configmaps-526ec73e-87f2-4187-a704-926594dc6386 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:18:40.857: INFO: Pod "pod-configmaps-526ec73e-87f2-4187-a704-926594dc6386": Phase="Running", Reason="", readiness=true. Elapsed: 2.004454349s
Mar 30 13:18:40.857: INFO: The phase of Pod pod-configmaps-526ec73e-87f2-4187-a704-926594dc6386 is Running (Ready = true)
Mar 30 13:18:40.857: INFO: Pod "pod-configmaps-526ec73e-87f2-4187-a704-926594dc6386" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-abdc133c-8649-4273-89cc-a8e525608d86 03/30/23 13:18:40.876
STEP: Updating configmap cm-test-opt-upd-4298507e-43a9-4b00-acd8-010e72d07506 03/30/23 13:18:40.878
STEP: Creating configMap with name cm-test-opt-create-6067d584-c511-4311-a635-2fe833cfe213 03/30/23 13:18:40.88
STEP: waiting to observe update in volume 03/30/23 13:18:40.883
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:18:42.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1430" for this suite. 03/30/23 13:18:42.897
------------------------------
â€¢ [4.069 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:18:38.831
    Mar 30 13:18:38.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:18:38.832
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:38.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:38.84
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-abdc133c-8649-4273-89cc-a8e525608d86 03/30/23 13:18:38.844
    STEP: Creating configMap with name cm-test-opt-upd-4298507e-43a9-4b00-acd8-010e72d07506 03/30/23 13:18:38.847
    STEP: Creating the pod 03/30/23 13:18:38.849
    Mar 30 13:18:38.853: INFO: Waiting up to 5m0s for pod "pod-configmaps-526ec73e-87f2-4187-a704-926594dc6386" in namespace "configmap-1430" to be "running and ready"
    Mar 30 13:18:38.855: INFO: Pod "pod-configmaps-526ec73e-87f2-4187-a704-926594dc6386": Phase="Pending", Reason="", readiness=false. Elapsed: 1.798472ms
    Mar 30 13:18:38.855: INFO: The phase of Pod pod-configmaps-526ec73e-87f2-4187-a704-926594dc6386 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:18:40.857: INFO: Pod "pod-configmaps-526ec73e-87f2-4187-a704-926594dc6386": Phase="Running", Reason="", readiness=true. Elapsed: 2.004454349s
    Mar 30 13:18:40.857: INFO: The phase of Pod pod-configmaps-526ec73e-87f2-4187-a704-926594dc6386 is Running (Ready = true)
    Mar 30 13:18:40.857: INFO: Pod "pod-configmaps-526ec73e-87f2-4187-a704-926594dc6386" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-abdc133c-8649-4273-89cc-a8e525608d86 03/30/23 13:18:40.876
    STEP: Updating configmap cm-test-opt-upd-4298507e-43a9-4b00-acd8-010e72d07506 03/30/23 13:18:40.878
    STEP: Creating configMap with name cm-test-opt-create-6067d584-c511-4311-a635-2fe833cfe213 03/30/23 13:18:40.88
    STEP: waiting to observe update in volume 03/30/23 13:18:40.883
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:18:42.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1430" for this suite. 03/30/23 13:18:42.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:18:42.9
Mar 30 13:18:42.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 13:18:42.901
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:42.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:42.909
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 13:18:42.915
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:18:43.333
STEP: Deploying the webhook pod 03/30/23 13:18:43.335
STEP: Wait for the deployment to be ready 03/30/23 13:18:43.34
Mar 30 13:18:43.344: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/30/23 13:18:45.349
STEP: Verifying the service has paired with the endpoint 03/30/23 13:18:45.353
Mar 30 13:18:46.354: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 03/30/23 13:18:46.385
STEP: Creating a configMap that does not comply to the validation webhook rules 03/30/23 13:18:46.406
STEP: Deleting the collection of validation webhooks 03/30/23 13:18:46.424
STEP: Creating a configMap that does not comply to the validation webhook rules 03/30/23 13:18:46.442
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:18:46.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-591" for this suite. 03/30/23 13:18:46.462
STEP: Destroying namespace "webhook-591-markers" for this suite. 03/30/23 13:18:46.465
------------------------------
â€¢ [3.568 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:18:42.9
    Mar 30 13:18:42.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 13:18:42.901
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:42.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:42.909
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 13:18:42.915
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:18:43.333
    STEP: Deploying the webhook pod 03/30/23 13:18:43.335
    STEP: Wait for the deployment to be ready 03/30/23 13:18:43.34
    Mar 30 13:18:43.344: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/30/23 13:18:45.349
    STEP: Verifying the service has paired with the endpoint 03/30/23 13:18:45.353
    Mar 30 13:18:46.354: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 03/30/23 13:18:46.385
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/30/23 13:18:46.406
    STEP: Deleting the collection of validation webhooks 03/30/23 13:18:46.424
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/30/23 13:18:46.442
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:18:46.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-591" for this suite. 03/30/23 13:18:46.462
    STEP: Destroying namespace "webhook-591-markers" for this suite. 03/30/23 13:18:46.465
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:18:46.469
Mar 30 13:18:46.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-runtime 03/30/23 13:18:46.469
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:46.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:46.478
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 03/30/23 13:18:46.48
STEP: wait for the container to reach Succeeded 03/30/23 13:18:46.484
STEP: get the container status 03/30/23 13:18:50.495
STEP: the container should be terminated 03/30/23 13:18:50.497
STEP: the termination message should be set 03/30/23 13:18:50.497
Mar 30 13:18:50.497: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 03/30/23 13:18:50.497
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 30 13:18:50.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8871" for this suite. 03/30/23 13:18:50.506
------------------------------
â€¢ [4.039 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:18:46.469
    Mar 30 13:18:46.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-runtime 03/30/23 13:18:46.469
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:46.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:46.478
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 03/30/23 13:18:46.48
    STEP: wait for the container to reach Succeeded 03/30/23 13:18:46.484
    STEP: get the container status 03/30/23 13:18:50.495
    STEP: the container should be terminated 03/30/23 13:18:50.497
    STEP: the termination message should be set 03/30/23 13:18:50.497
    Mar 30 13:18:50.497: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 03/30/23 13:18:50.497
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:18:50.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8871" for this suite. 03/30/23 13:18:50.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:18:50.508
Mar 30 13:18:50.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename custom-resource-definition 03/30/23 13:18:50.509
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:50.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:50.517
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Mar 30 13:18:50.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:18:58.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6151" for this suite. 03/30/23 13:18:58.59
------------------------------
â€¢ [SLOW TEST] [8.085 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:18:50.508
    Mar 30 13:18:50.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename custom-resource-definition 03/30/23 13:18:50.509
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:50.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:50.517
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Mar 30 13:18:50.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:18:58.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6151" for this suite. 03/30/23 13:18:58.59
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:18:58.593
Mar 30 13:18:58.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename resourcequota 03/30/23 13:18:58.594
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:58.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:58.602
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 03/30/23 13:18:58.604
STEP: Creating a ResourceQuota 03/30/23 13:19:03.607
STEP: Ensuring resource quota status is calculated 03/30/23 13:19:03.611
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 30 13:19:05.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9843" for this suite. 03/30/23 13:19:05.616
------------------------------
â€¢ [SLOW TEST] [7.025 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:18:58.593
    Mar 30 13:18:58.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename resourcequota 03/30/23 13:18:58.594
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:18:58.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:18:58.602
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 03/30/23 13:18:58.604
    STEP: Creating a ResourceQuota 03/30/23 13:19:03.607
    STEP: Ensuring resource quota status is calculated 03/30/23 13:19:03.611
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:19:05.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9843" for this suite. 03/30/23 13:19:05.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:19:05.619
Mar 30 13:19:05.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:19:05.62
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:05.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:05.628
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-e2a79def-75e4-42fa-928d-a667186db51f 03/30/23 13:19:05.629
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:19:05.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7270" for this suite. 03/30/23 13:19:05.633
------------------------------
â€¢ [0.017 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:19:05.619
    Mar 30 13:19:05.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:19:05.62
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:05.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:05.628
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-e2a79def-75e4-42fa-928d-a667186db51f 03/30/23 13:19:05.629
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:19:05.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7270" for this suite. 03/30/23 13:19:05.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:19:05.636
Mar 30 13:19:05.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 13:19:05.637
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:05.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:05.644
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Mar 30 13:19:05.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/30/23 13:19:12.244
Mar 30 13:19:12.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-466 --namespace=crd-publish-openapi-466 create -f -'
Mar 30 13:19:12.670: INFO: stderr: ""
Mar 30 13:19:12.670: INFO: stdout: "e2e-test-crd-publish-openapi-7902-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 30 13:19:12.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-466 --namespace=crd-publish-openapi-466 delete e2e-test-crd-publish-openapi-7902-crds test-cr'
Mar 30 13:19:12.740: INFO: stderr: ""
Mar 30 13:19:12.740: INFO: stdout: "e2e-test-crd-publish-openapi-7902-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 30 13:19:12.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-466 --namespace=crd-publish-openapi-466 apply -f -'
Mar 30 13:19:13.130: INFO: stderr: ""
Mar 30 13:19:13.130: INFO: stdout: "e2e-test-crd-publish-openapi-7902-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 30 13:19:13.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-466 --namespace=crd-publish-openapi-466 delete e2e-test-crd-publish-openapi-7902-crds test-cr'
Mar 30 13:19:13.183: INFO: stderr: ""
Mar 30 13:19:13.183: INFO: stdout: "e2e-test-crd-publish-openapi-7902-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/30/23 13:19:13.183
Mar 30 13:19:13.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-466 explain e2e-test-crd-publish-openapi-7902-crds'
Mar 30 13:19:13.560: INFO: stderr: ""
Mar 30 13:19:13.560: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7902-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:19:15.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-466" for this suite. 03/30/23 13:19:15.17
------------------------------
â€¢ [SLOW TEST] [9.537 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:19:05.636
    Mar 30 13:19:05.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 13:19:05.637
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:05.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:05.644
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Mar 30 13:19:05.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/30/23 13:19:12.244
    Mar 30 13:19:12.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-466 --namespace=crd-publish-openapi-466 create -f -'
    Mar 30 13:19:12.670: INFO: stderr: ""
    Mar 30 13:19:12.670: INFO: stdout: "e2e-test-crd-publish-openapi-7902-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 30 13:19:12.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-466 --namespace=crd-publish-openapi-466 delete e2e-test-crd-publish-openapi-7902-crds test-cr'
    Mar 30 13:19:12.740: INFO: stderr: ""
    Mar 30 13:19:12.740: INFO: stdout: "e2e-test-crd-publish-openapi-7902-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Mar 30 13:19:12.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-466 --namespace=crd-publish-openapi-466 apply -f -'
    Mar 30 13:19:13.130: INFO: stderr: ""
    Mar 30 13:19:13.130: INFO: stdout: "e2e-test-crd-publish-openapi-7902-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 30 13:19:13.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-466 --namespace=crd-publish-openapi-466 delete e2e-test-crd-publish-openapi-7902-crds test-cr'
    Mar 30 13:19:13.183: INFO: stderr: ""
    Mar 30 13:19:13.183: INFO: stdout: "e2e-test-crd-publish-openapi-7902-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/30/23 13:19:13.183
    Mar 30 13:19:13.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-466 explain e2e-test-crd-publish-openapi-7902-crds'
    Mar 30 13:19:13.560: INFO: stderr: ""
    Mar 30 13:19:13.560: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7902-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:19:15.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-466" for this suite. 03/30/23 13:19:15.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:19:15.174
Mar 30 13:19:15.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename var-expansion 03/30/23 13:19:15.174
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:15.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:15.182
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 03/30/23 13:19:15.184
Mar 30 13:19:15.188: INFO: Waiting up to 5m0s for pod "var-expansion-92541718-9b1a-4590-adcc-5eacac801877" in namespace "var-expansion-6282" to be "Succeeded or Failed"
Mar 30 13:19:15.190: INFO: Pod "var-expansion-92541718-9b1a-4590-adcc-5eacac801877": Phase="Pending", Reason="", readiness=false. Elapsed: 1.985003ms
Mar 30 13:19:17.193: INFO: Pod "var-expansion-92541718-9b1a-4590-adcc-5eacac801877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004718453s
Mar 30 13:19:19.193: INFO: Pod "var-expansion-92541718-9b1a-4590-adcc-5eacac801877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005011818s
STEP: Saw pod success 03/30/23 13:19:19.193
Mar 30 13:19:19.193: INFO: Pod "var-expansion-92541718-9b1a-4590-adcc-5eacac801877" satisfied condition "Succeeded or Failed"
Mar 30 13:19:19.194: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod var-expansion-92541718-9b1a-4590-adcc-5eacac801877 container dapi-container: <nil>
STEP: delete the pod 03/30/23 13:19:19.202
Mar 30 13:19:19.206: INFO: Waiting for pod var-expansion-92541718-9b1a-4590-adcc-5eacac801877 to disappear
Mar 30 13:19:19.208: INFO: Pod var-expansion-92541718-9b1a-4590-adcc-5eacac801877 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 30 13:19:19.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6282" for this suite. 03/30/23 13:19:19.21
------------------------------
â€¢ [4.039 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:19:15.174
    Mar 30 13:19:15.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename var-expansion 03/30/23 13:19:15.174
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:15.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:15.182
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 03/30/23 13:19:15.184
    Mar 30 13:19:15.188: INFO: Waiting up to 5m0s for pod "var-expansion-92541718-9b1a-4590-adcc-5eacac801877" in namespace "var-expansion-6282" to be "Succeeded or Failed"
    Mar 30 13:19:15.190: INFO: Pod "var-expansion-92541718-9b1a-4590-adcc-5eacac801877": Phase="Pending", Reason="", readiness=false. Elapsed: 1.985003ms
    Mar 30 13:19:17.193: INFO: Pod "var-expansion-92541718-9b1a-4590-adcc-5eacac801877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004718453s
    Mar 30 13:19:19.193: INFO: Pod "var-expansion-92541718-9b1a-4590-adcc-5eacac801877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005011818s
    STEP: Saw pod success 03/30/23 13:19:19.193
    Mar 30 13:19:19.193: INFO: Pod "var-expansion-92541718-9b1a-4590-adcc-5eacac801877" satisfied condition "Succeeded or Failed"
    Mar 30 13:19:19.194: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod var-expansion-92541718-9b1a-4590-adcc-5eacac801877 container dapi-container: <nil>
    STEP: delete the pod 03/30/23 13:19:19.202
    Mar 30 13:19:19.206: INFO: Waiting for pod var-expansion-92541718-9b1a-4590-adcc-5eacac801877 to disappear
    Mar 30 13:19:19.208: INFO: Pod var-expansion-92541718-9b1a-4590-adcc-5eacac801877 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:19:19.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6282" for this suite. 03/30/23 13:19:19.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:19:19.213
Mar 30 13:19:19.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename custom-resource-definition 03/30/23 13:19:19.214
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:19.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:19.222
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Mar 30 13:19:19.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:19:25.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4835" for this suite. 03/30/23 13:19:25.236
------------------------------
â€¢ [SLOW TEST] [6.025 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:19:19.213
    Mar 30 13:19:19.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename custom-resource-definition 03/30/23 13:19:19.214
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:19.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:19.222
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Mar 30 13:19:19.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:19:25.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4835" for this suite. 03/30/23 13:19:25.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:19:25.239
Mar 30 13:19:25.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 13:19:25.24
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:25.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:25.247
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 03/30/23 13:19:25.249
Mar 30 13:19:25.252: INFO: Waiting up to 5m0s for pod "downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8" in namespace "downward-api-8001" to be "Succeeded or Failed"
Mar 30 13:19:25.254: INFO: Pod "downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.400317ms
Mar 30 13:19:27.256: INFO: Pod "downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003549672s
Mar 30 13:19:29.256: INFO: Pod "downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003565546s
STEP: Saw pod success 03/30/23 13:19:29.256
Mar 30 13:19:29.256: INFO: Pod "downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8" satisfied condition "Succeeded or Failed"
Mar 30 13:19:29.258: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8 container dapi-container: <nil>
STEP: delete the pod 03/30/23 13:19:29.261
Mar 30 13:19:29.266: INFO: Waiting for pod downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8 to disappear
Mar 30 13:19:29.267: INFO: Pod downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 30 13:19:29.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8001" for this suite. 03/30/23 13:19:29.269
------------------------------
â€¢ [4.032 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:19:25.239
    Mar 30 13:19:25.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 13:19:25.24
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:25.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:25.247
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 03/30/23 13:19:25.249
    Mar 30 13:19:25.252: INFO: Waiting up to 5m0s for pod "downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8" in namespace "downward-api-8001" to be "Succeeded or Failed"
    Mar 30 13:19:25.254: INFO: Pod "downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.400317ms
    Mar 30 13:19:27.256: INFO: Pod "downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003549672s
    Mar 30 13:19:29.256: INFO: Pod "downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003565546s
    STEP: Saw pod success 03/30/23 13:19:29.256
    Mar 30 13:19:29.256: INFO: Pod "downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8" satisfied condition "Succeeded or Failed"
    Mar 30 13:19:29.258: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8 container dapi-container: <nil>
    STEP: delete the pod 03/30/23 13:19:29.261
    Mar 30 13:19:29.266: INFO: Waiting for pod downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8 to disappear
    Mar 30 13:19:29.267: INFO: Pod downward-api-6171e418-6b73-4eea-add6-f7ceb0985dd8 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:19:29.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8001" for this suite. 03/30/23 13:19:29.269
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:19:29.271
Mar 30 13:19:29.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename var-expansion 03/30/23 13:19:29.272
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:29.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:29.28
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 03/30/23 13:19:29.282
Mar 30 13:19:29.285: INFO: Waiting up to 5m0s for pod "var-expansion-a4951131-b544-4dce-848d-489dcb92b302" in namespace "var-expansion-5434" to be "Succeeded or Failed"
Mar 30 13:19:29.286: INFO: Pod "var-expansion-a4951131-b544-4dce-848d-489dcb92b302": Phase="Pending", Reason="", readiness=false. Elapsed: 1.381328ms
Mar 30 13:19:31.289: INFO: Pod "var-expansion-a4951131-b544-4dce-848d-489dcb92b302": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004090729s
Mar 30 13:19:33.289: INFO: Pod "var-expansion-a4951131-b544-4dce-848d-489dcb92b302": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004351883s
STEP: Saw pod success 03/30/23 13:19:33.289
Mar 30 13:19:33.289: INFO: Pod "var-expansion-a4951131-b544-4dce-848d-489dcb92b302" satisfied condition "Succeeded or Failed"
Mar 30 13:19:33.291: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod var-expansion-a4951131-b544-4dce-848d-489dcb92b302 container dapi-container: <nil>
STEP: delete the pod 03/30/23 13:19:33.294
Mar 30 13:19:33.298: INFO: Waiting for pod var-expansion-a4951131-b544-4dce-848d-489dcb92b302 to disappear
Mar 30 13:19:33.299: INFO: Pod var-expansion-a4951131-b544-4dce-848d-489dcb92b302 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 30 13:19:33.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5434" for this suite. 03/30/23 13:19:33.302
------------------------------
â€¢ [4.032 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:19:29.271
    Mar 30 13:19:29.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename var-expansion 03/30/23 13:19:29.272
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:29.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:29.28
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 03/30/23 13:19:29.282
    Mar 30 13:19:29.285: INFO: Waiting up to 5m0s for pod "var-expansion-a4951131-b544-4dce-848d-489dcb92b302" in namespace "var-expansion-5434" to be "Succeeded or Failed"
    Mar 30 13:19:29.286: INFO: Pod "var-expansion-a4951131-b544-4dce-848d-489dcb92b302": Phase="Pending", Reason="", readiness=false. Elapsed: 1.381328ms
    Mar 30 13:19:31.289: INFO: Pod "var-expansion-a4951131-b544-4dce-848d-489dcb92b302": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004090729s
    Mar 30 13:19:33.289: INFO: Pod "var-expansion-a4951131-b544-4dce-848d-489dcb92b302": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004351883s
    STEP: Saw pod success 03/30/23 13:19:33.289
    Mar 30 13:19:33.289: INFO: Pod "var-expansion-a4951131-b544-4dce-848d-489dcb92b302" satisfied condition "Succeeded or Failed"
    Mar 30 13:19:33.291: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod var-expansion-a4951131-b544-4dce-848d-489dcb92b302 container dapi-container: <nil>
    STEP: delete the pod 03/30/23 13:19:33.294
    Mar 30 13:19:33.298: INFO: Waiting for pod var-expansion-a4951131-b544-4dce-848d-489dcb92b302 to disappear
    Mar 30 13:19:33.299: INFO: Pod var-expansion-a4951131-b544-4dce-848d-489dcb92b302 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:19:33.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5434" for this suite. 03/30/23 13:19:33.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:19:33.304
Mar 30 13:19:33.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 13:19:33.305
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:33.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:33.312
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 03/30/23 13:19:33.314
Mar 30 13:19:33.317: INFO: Waiting up to 5m0s for pod "pod-49b0e7ce-2953-4fc0-80e1-76089f833b89" in namespace "emptydir-1151" to be "Succeeded or Failed"
Mar 30 13:19:33.319: INFO: Pod "pod-49b0e7ce-2953-4fc0-80e1-76089f833b89": Phase="Pending", Reason="", readiness=false. Elapsed: 1.307674ms
Mar 30 13:19:35.321: INFO: Pod "pod-49b0e7ce-2953-4fc0-80e1-76089f833b89": Phase="Running", Reason="", readiness=false. Elapsed: 2.003466096s
Mar 30 13:19:37.321: INFO: Pod "pod-49b0e7ce-2953-4fc0-80e1-76089f833b89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003498181s
STEP: Saw pod success 03/30/23 13:19:37.321
Mar 30 13:19:37.321: INFO: Pod "pod-49b0e7ce-2953-4fc0-80e1-76089f833b89" satisfied condition "Succeeded or Failed"
Mar 30 13:19:37.322: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-49b0e7ce-2953-4fc0-80e1-76089f833b89 container test-container: <nil>
STEP: delete the pod 03/30/23 13:19:37.326
Mar 30 13:19:37.330: INFO: Waiting for pod pod-49b0e7ce-2953-4fc0-80e1-76089f833b89 to disappear
Mar 30 13:19:37.331: INFO: Pod pod-49b0e7ce-2953-4fc0-80e1-76089f833b89 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 13:19:37.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1151" for this suite. 03/30/23 13:19:37.333
------------------------------
â€¢ [4.031 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:19:33.304
    Mar 30 13:19:33.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 13:19:33.305
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:33.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:33.312
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/30/23 13:19:33.314
    Mar 30 13:19:33.317: INFO: Waiting up to 5m0s for pod "pod-49b0e7ce-2953-4fc0-80e1-76089f833b89" in namespace "emptydir-1151" to be "Succeeded or Failed"
    Mar 30 13:19:33.319: INFO: Pod "pod-49b0e7ce-2953-4fc0-80e1-76089f833b89": Phase="Pending", Reason="", readiness=false. Elapsed: 1.307674ms
    Mar 30 13:19:35.321: INFO: Pod "pod-49b0e7ce-2953-4fc0-80e1-76089f833b89": Phase="Running", Reason="", readiness=false. Elapsed: 2.003466096s
    Mar 30 13:19:37.321: INFO: Pod "pod-49b0e7ce-2953-4fc0-80e1-76089f833b89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003498181s
    STEP: Saw pod success 03/30/23 13:19:37.321
    Mar 30 13:19:37.321: INFO: Pod "pod-49b0e7ce-2953-4fc0-80e1-76089f833b89" satisfied condition "Succeeded or Failed"
    Mar 30 13:19:37.322: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-49b0e7ce-2953-4fc0-80e1-76089f833b89 container test-container: <nil>
    STEP: delete the pod 03/30/23 13:19:37.326
    Mar 30 13:19:37.330: INFO: Waiting for pod pod-49b0e7ce-2953-4fc0-80e1-76089f833b89 to disappear
    Mar 30 13:19:37.331: INFO: Pod pod-49b0e7ce-2953-4fc0-80e1-76089f833b89 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:19:37.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1151" for this suite. 03/30/23 13:19:37.333
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:19:37.336
Mar 30 13:19:37.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename sched-pred 03/30/23 13:19:37.337
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:37.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:37.344
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 30 13:19:37.346: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 30 13:19:37.350: INFO: Waiting for terminating namespaces to be deleted...
Mar 30 13:19:37.351: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.3 before test
Mar 30 13:19:37.355: INFO: csi-plugin-2x6v8 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (4 container statuses recorded)
Mar 30 13:19:37.355: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 30 13:19:37.355: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 30 13:19:37.355: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 30 13:19:37.355: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 30 13:19:37.355: INFO: kube-flannel-ds-swjx9 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.355: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 30 13:19:37.355: INFO: kube-proxy-worker-rxqts from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.355: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 30 13:19:37.355: INFO: storage-cnfs-5bbdf677b6-s48rj from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.355: INFO: 	Container storage-cnfs ready: true, restart count 0
Mar 30 13:19:37.355: INFO: storage-snapshot-manager-55c574dc6c-ths98 from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.355: INFO: 	Container storage-snapshot-manager ready: true, restart count 0
Mar 30 13:19:37.355: INFO: sonobuoy-e2e-job-5f38eb9e10894fe7 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:19:37.355: INFO: 	Container e2e ready: true, restart count 0
Mar 30 13:19:37.355: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:19:37.355: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-bn4wq from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:19:37.355: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:19:37.355: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 13:19:37.355: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.4 before test
Mar 30 13:19:37.360: INFO: alicloud-monitor-controller-77f876c7d8-kqc5x from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.360: INFO: 	Container alicloud-monitor-controller ready: true, restart count 0
Mar 30 13:19:37.360: INFO: coredns-5ff46f8d6f-llbsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.360: INFO: 	Container coredns ready: true, restart count 0
Mar 30 13:19:37.360: INFO: csi-plugin-pgrk2 from kube-system started at 2023-03-30 10:07:49 +0000 UTC (4 container statuses recorded)
Mar 30 13:19:37.360: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 30 13:19:37.360: INFO: csi-provisioner-648c597bcb-67xzk from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
Mar 30 13:19:37.360: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-nas-resizer ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-oss-provisioner ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 30 13:19:37.360: INFO: csi-provisioner-648c597bcb-jwm9r from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
Mar 30 13:19:37.360: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-nas-resizer ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-oss-provisioner ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 30 13:19:37.360: INFO: kube-flannel-ds-dftjj from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.360: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 30 13:19:37.360: INFO: kube-proxy-worker-2mrsr from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.360: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 30 13:19:37.360: INFO: metrics-server-5c58794dd-cbrsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.360: INFO: 	Container metrics-server ready: true, restart count 0
Mar 30 13:19:37.360: INFO: storage-operator-5f775996-dphqv from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.360: INFO: 	Container storage-operator ready: true, restart count 0
Mar 30 13:19:37.360: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-prg6s from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:19:37.360: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 13:19:37.360: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.5 before test
Mar 30 13:19:37.364: INFO: coredns-5ff46f8d6f-zz9zg from kube-system started at 2023-03-30 10:07:58 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.364: INFO: 	Container coredns ready: true, restart count 0
Mar 30 13:19:37.364: INFO: csi-plugin-rgcxj from kube-system started at 2023-03-30 10:07:52 +0000 UTC (4 container statuses recorded)
Mar 30 13:19:37.364: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 30 13:19:37.364: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 30 13:19:37.364: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 30 13:19:37.364: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 30 13:19:37.364: INFO: kube-flannel-ds-dspdm from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.364: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 30 13:19:37.364: INFO: kube-proxy-worker-h688q from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.364: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 30 13:19:37.364: INFO: storage-auto-expander-7fd5f8f78-jl4ph from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.364: INFO: 	Container storage-auto-expander ready: true, restart count 0
Mar 30 13:19:37.364: INFO: storage-monitor-8554bcf4c7-c8w4l from kube-system started at 2023-03-30 10:09:36 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.364: INFO: 	Container storage-monitor ready: true, restart count 0
Mar 30 13:19:37.364: INFO: sonobuoy from sonobuoy started at 2023-03-30 13:01:31 +0000 UTC (1 container statuses recorded)
Mar 30 13:19:37.364: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 30 13:19:37.364: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-87ph2 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:19:37.364: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:19:37.364: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node cn-hongkong.192.168.0.3 03/30/23 13:19:37.376
STEP: verifying the node has the label node cn-hongkong.192.168.0.4 03/30/23 13:19:37.383
STEP: verifying the node has the label node cn-hongkong.192.168.0.5 03/30/23 13:19:37.389
Mar 30 13:19:37.398: INFO: Pod alicloud-monitor-controller-77f876c7d8-kqc5x requesting resource cpu=10m on Node cn-hongkong.192.168.0.4
Mar 30 13:19:37.398: INFO: Pod coredns-5ff46f8d6f-llbsl requesting resource cpu=100m on Node cn-hongkong.192.168.0.4
Mar 30 13:19:37.398: INFO: Pod coredns-5ff46f8d6f-zz9zg requesting resource cpu=100m on Node cn-hongkong.192.168.0.5
Mar 30 13:19:37.398: INFO: Pod csi-plugin-2x6v8 requesting resource cpu=130m on Node cn-hongkong.192.168.0.3
Mar 30 13:19:37.398: INFO: Pod csi-plugin-pgrk2 requesting resource cpu=130m on Node cn-hongkong.192.168.0.4
Mar 30 13:19:37.398: INFO: Pod csi-plugin-rgcxj requesting resource cpu=130m on Node cn-hongkong.192.168.0.5
Mar 30 13:19:37.398: INFO: Pod csi-provisioner-648c597bcb-67xzk requesting resource cpu=180m on Node cn-hongkong.192.168.0.4
Mar 30 13:19:37.398: INFO: Pod csi-provisioner-648c597bcb-jwm9r requesting resource cpu=180m on Node cn-hongkong.192.168.0.4
Mar 30 13:19:37.398: INFO: Pod kube-flannel-ds-dftjj requesting resource cpu=100m on Node cn-hongkong.192.168.0.4
Mar 30 13:19:37.398: INFO: Pod kube-flannel-ds-dspdm requesting resource cpu=100m on Node cn-hongkong.192.168.0.5
Mar 30 13:19:37.398: INFO: Pod kube-flannel-ds-swjx9 requesting resource cpu=100m on Node cn-hongkong.192.168.0.3
Mar 30 13:19:37.398: INFO: Pod kube-proxy-worker-2mrsr requesting resource cpu=100m on Node cn-hongkong.192.168.0.4
Mar 30 13:19:37.398: INFO: Pod kube-proxy-worker-h688q requesting resource cpu=100m on Node cn-hongkong.192.168.0.5
Mar 30 13:19:37.398: INFO: Pod kube-proxy-worker-rxqts requesting resource cpu=100m on Node cn-hongkong.192.168.0.3
Mar 30 13:19:37.398: INFO: Pod metrics-server-5c58794dd-cbrsl requesting resource cpu=100m on Node cn-hongkong.192.168.0.4
Mar 30 13:19:37.398: INFO: Pod storage-auto-expander-7fd5f8f78-jl4ph requesting resource cpu=10m on Node cn-hongkong.192.168.0.5
Mar 30 13:19:37.398: INFO: Pod storage-cnfs-5bbdf677b6-s48rj requesting resource cpu=10m on Node cn-hongkong.192.168.0.3
Mar 30 13:19:37.398: INFO: Pod storage-monitor-8554bcf4c7-c8w4l requesting resource cpu=10m on Node cn-hongkong.192.168.0.5
Mar 30 13:19:37.398: INFO: Pod storage-operator-5f775996-dphqv requesting resource cpu=10m on Node cn-hongkong.192.168.0.4
Mar 30 13:19:37.398: INFO: Pod storage-snapshot-manager-55c574dc6c-ths98 requesting resource cpu=100m on Node cn-hongkong.192.168.0.3
Mar 30 13:19:37.398: INFO: Pod sonobuoy requesting resource cpu=0m on Node cn-hongkong.192.168.0.5
Mar 30 13:19:37.398: INFO: Pod sonobuoy-e2e-job-5f38eb9e10894fe7 requesting resource cpu=0m on Node cn-hongkong.192.168.0.3
Mar 30 13:19:37.398: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-87ph2 requesting resource cpu=0m on Node cn-hongkong.192.168.0.5
Mar 30 13:19:37.398: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-bn4wq requesting resource cpu=0m on Node cn-hongkong.192.168.0.3
Mar 30 13:19:37.398: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-prg6s requesting resource cpu=0m on Node cn-hongkong.192.168.0.4
STEP: Starting Pods to consume most of the cluster CPU. 03/30/23 13:19:37.398
Mar 30 13:19:37.398: INFO: Creating a pod which consumes cpu=5152m on Node cn-hongkong.192.168.0.3
Mar 30 13:19:37.402: INFO: Creating a pod which consumes cpu=4823m on Node cn-hongkong.192.168.0.4
Mar 30 13:19:37.404: INFO: Creating a pod which consumes cpu=5145m on Node cn-hongkong.192.168.0.5
Mar 30 13:19:37.407: INFO: Waiting up to 5m0s for pod "filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10" in namespace "sched-pred-8583" to be "running"
Mar 30 13:19:37.411: INFO: Pod "filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568847ms
Mar 30 13:19:39.413: INFO: Pod "filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10": Phase="Running", Reason="", readiness=true. Elapsed: 2.005947186s
Mar 30 13:19:39.413: INFO: Pod "filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10" satisfied condition "running"
Mar 30 13:19:39.413: INFO: Waiting up to 5m0s for pod "filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c" in namespace "sched-pred-8583" to be "running"
Mar 30 13:19:39.414: INFO: Pod "filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c": Phase="Running", Reason="", readiness=true. Elapsed: 1.400318ms
Mar 30 13:19:39.414: INFO: Pod "filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c" satisfied condition "running"
Mar 30 13:19:39.414: INFO: Waiting up to 5m0s for pod "filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495" in namespace "sched-pred-8583" to be "running"
Mar 30 13:19:39.416: INFO: Pod "filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495": Phase="Running", Reason="", readiness=true. Elapsed: 1.476447ms
Mar 30 13:19:39.416: INFO: Pod "filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 03/30/23 13:19:39.416
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c.175134f86339ff21], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8583/filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c to cn-hongkong.192.168.0.4] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c.175134f87cad1d69], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c.175134f87dc91133], Reason = [Created], Message = [Created container filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c.175134f8830081ab], Reason = [Started], Message = [Started container filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10.175134f8630e5612], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8583/filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10 to cn-hongkong.192.168.0.3] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10.175134f87c876cc1], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10.175134f8a99c4baa], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 756.315264ms (756.328389ms including waiting)] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10.175134f8aad031d8], Reason = [Created], Message = [Created container filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10.175134f8b010486a], Reason = [Started], Message = [Started container filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495.175134f863668fd6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8583/filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495 to cn-hongkong.192.168.0.5] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495.175134f87cb7bdb1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495.175134f87e283a61], Reason = [Created], Message = [Created container filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495.175134f883998f03], Reason = [Started], Message = [Started container filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495] 03/30/23 13:19:39.418
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.175134f8db4b99ec], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 03/30/23 13:19:39.423
STEP: removing the label node off the node cn-hongkong.192.168.0.3 03/30/23 13:19:40.423
STEP: verifying the node doesn't have the label node 03/30/23 13:19:40.43
STEP: removing the label node off the node cn-hongkong.192.168.0.4 03/30/23 13:19:40.431
STEP: verifying the node doesn't have the label node 03/30/23 13:19:40.437
STEP: removing the label node off the node cn-hongkong.192.168.0.5 03/30/23 13:19:40.438
STEP: verifying the node doesn't have the label node 03/30/23 13:19:40.444
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:19:40.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8583" for this suite. 03/30/23 13:19:40.449
------------------------------
â€¢ [3.115 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:19:37.336
    Mar 30 13:19:37.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename sched-pred 03/30/23 13:19:37.337
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:37.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:37.344
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 30 13:19:37.346: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 30 13:19:37.350: INFO: Waiting for terminating namespaces to be deleted...
    Mar 30 13:19:37.351: INFO: 
    Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.3 before test
    Mar 30 13:19:37.355: INFO: csi-plugin-2x6v8 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (4 container statuses recorded)
    Mar 30 13:19:37.355: INFO: 	Container csi-plugin ready: true, restart count 0
    Mar 30 13:19:37.355: INFO: 	Container disk-driver-registrar ready: true, restart count 0
    Mar 30 13:19:37.355: INFO: 	Container nas-driver-registrar ready: true, restart count 0
    Mar 30 13:19:37.355: INFO: 	Container oss-driver-registrar ready: true, restart count 0
    Mar 30 13:19:37.355: INFO: kube-flannel-ds-swjx9 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.355: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 30 13:19:37.355: INFO: kube-proxy-worker-rxqts from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.355: INFO: 	Container kube-proxy-worker ready: true, restart count 0
    Mar 30 13:19:37.355: INFO: storage-cnfs-5bbdf677b6-s48rj from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.355: INFO: 	Container storage-cnfs ready: true, restart count 0
    Mar 30 13:19:37.355: INFO: storage-snapshot-manager-55c574dc6c-ths98 from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.355: INFO: 	Container storage-snapshot-manager ready: true, restart count 0
    Mar 30 13:19:37.355: INFO: sonobuoy-e2e-job-5f38eb9e10894fe7 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:19:37.355: INFO: 	Container e2e ready: true, restart count 0
    Mar 30 13:19:37.355: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:19:37.355: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-bn4wq from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:19:37.355: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:19:37.355: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 30 13:19:37.355: INFO: 
    Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.4 before test
    Mar 30 13:19:37.360: INFO: alicloud-monitor-controller-77f876c7d8-kqc5x from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.360: INFO: 	Container alicloud-monitor-controller ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: coredns-5ff46f8d6f-llbsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.360: INFO: 	Container coredns ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: csi-plugin-pgrk2 from kube-system started at 2023-03-30 10:07:49 +0000 UTC (4 container statuses recorded)
    Mar 30 13:19:37.360: INFO: 	Container csi-plugin ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container disk-driver-registrar ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container nas-driver-registrar ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container oss-driver-registrar ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: csi-provisioner-648c597bcb-67xzk from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
    Mar 30 13:19:37.360: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-disk-attacher ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-disk-provisioner ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-disk-resizer ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-nas-provisioner ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-nas-resizer ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-oss-provisioner ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-snapshot-controller ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: csi-provisioner-648c597bcb-jwm9r from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
    Mar 30 13:19:37.360: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-disk-attacher ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-disk-provisioner ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-disk-resizer ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-nas-provisioner ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-nas-resizer ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-oss-provisioner ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container external-snapshot-controller ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: kube-flannel-ds-dftjj from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.360: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: kube-proxy-worker-2mrsr from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.360: INFO: 	Container kube-proxy-worker ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: metrics-server-5c58794dd-cbrsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.360: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: storage-operator-5f775996-dphqv from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.360: INFO: 	Container storage-operator ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-prg6s from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:19:37.360: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 30 13:19:37.360: INFO: 
    Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.5 before test
    Mar 30 13:19:37.364: INFO: coredns-5ff46f8d6f-zz9zg from kube-system started at 2023-03-30 10:07:58 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.364: INFO: 	Container coredns ready: true, restart count 0
    Mar 30 13:19:37.364: INFO: csi-plugin-rgcxj from kube-system started at 2023-03-30 10:07:52 +0000 UTC (4 container statuses recorded)
    Mar 30 13:19:37.364: INFO: 	Container csi-plugin ready: true, restart count 0
    Mar 30 13:19:37.364: INFO: 	Container disk-driver-registrar ready: true, restart count 0
    Mar 30 13:19:37.364: INFO: 	Container nas-driver-registrar ready: true, restart count 0
    Mar 30 13:19:37.364: INFO: 	Container oss-driver-registrar ready: true, restart count 0
    Mar 30 13:19:37.364: INFO: kube-flannel-ds-dspdm from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.364: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 30 13:19:37.364: INFO: kube-proxy-worker-h688q from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.364: INFO: 	Container kube-proxy-worker ready: true, restart count 0
    Mar 30 13:19:37.364: INFO: storage-auto-expander-7fd5f8f78-jl4ph from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.364: INFO: 	Container storage-auto-expander ready: true, restart count 0
    Mar 30 13:19:37.364: INFO: storage-monitor-8554bcf4c7-c8w4l from kube-system started at 2023-03-30 10:09:36 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.364: INFO: 	Container storage-monitor ready: true, restart count 0
    Mar 30 13:19:37.364: INFO: sonobuoy from sonobuoy started at 2023-03-30 13:01:31 +0000 UTC (1 container statuses recorded)
    Mar 30 13:19:37.364: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 30 13:19:37.364: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-87ph2 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:19:37.364: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:19:37.364: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node cn-hongkong.192.168.0.3 03/30/23 13:19:37.376
    STEP: verifying the node has the label node cn-hongkong.192.168.0.4 03/30/23 13:19:37.383
    STEP: verifying the node has the label node cn-hongkong.192.168.0.5 03/30/23 13:19:37.389
    Mar 30 13:19:37.398: INFO: Pod alicloud-monitor-controller-77f876c7d8-kqc5x requesting resource cpu=10m on Node cn-hongkong.192.168.0.4
    Mar 30 13:19:37.398: INFO: Pod coredns-5ff46f8d6f-llbsl requesting resource cpu=100m on Node cn-hongkong.192.168.0.4
    Mar 30 13:19:37.398: INFO: Pod coredns-5ff46f8d6f-zz9zg requesting resource cpu=100m on Node cn-hongkong.192.168.0.5
    Mar 30 13:19:37.398: INFO: Pod csi-plugin-2x6v8 requesting resource cpu=130m on Node cn-hongkong.192.168.0.3
    Mar 30 13:19:37.398: INFO: Pod csi-plugin-pgrk2 requesting resource cpu=130m on Node cn-hongkong.192.168.0.4
    Mar 30 13:19:37.398: INFO: Pod csi-plugin-rgcxj requesting resource cpu=130m on Node cn-hongkong.192.168.0.5
    Mar 30 13:19:37.398: INFO: Pod csi-provisioner-648c597bcb-67xzk requesting resource cpu=180m on Node cn-hongkong.192.168.0.4
    Mar 30 13:19:37.398: INFO: Pod csi-provisioner-648c597bcb-jwm9r requesting resource cpu=180m on Node cn-hongkong.192.168.0.4
    Mar 30 13:19:37.398: INFO: Pod kube-flannel-ds-dftjj requesting resource cpu=100m on Node cn-hongkong.192.168.0.4
    Mar 30 13:19:37.398: INFO: Pod kube-flannel-ds-dspdm requesting resource cpu=100m on Node cn-hongkong.192.168.0.5
    Mar 30 13:19:37.398: INFO: Pod kube-flannel-ds-swjx9 requesting resource cpu=100m on Node cn-hongkong.192.168.0.3
    Mar 30 13:19:37.398: INFO: Pod kube-proxy-worker-2mrsr requesting resource cpu=100m on Node cn-hongkong.192.168.0.4
    Mar 30 13:19:37.398: INFO: Pod kube-proxy-worker-h688q requesting resource cpu=100m on Node cn-hongkong.192.168.0.5
    Mar 30 13:19:37.398: INFO: Pod kube-proxy-worker-rxqts requesting resource cpu=100m on Node cn-hongkong.192.168.0.3
    Mar 30 13:19:37.398: INFO: Pod metrics-server-5c58794dd-cbrsl requesting resource cpu=100m on Node cn-hongkong.192.168.0.4
    Mar 30 13:19:37.398: INFO: Pod storage-auto-expander-7fd5f8f78-jl4ph requesting resource cpu=10m on Node cn-hongkong.192.168.0.5
    Mar 30 13:19:37.398: INFO: Pod storage-cnfs-5bbdf677b6-s48rj requesting resource cpu=10m on Node cn-hongkong.192.168.0.3
    Mar 30 13:19:37.398: INFO: Pod storage-monitor-8554bcf4c7-c8w4l requesting resource cpu=10m on Node cn-hongkong.192.168.0.5
    Mar 30 13:19:37.398: INFO: Pod storage-operator-5f775996-dphqv requesting resource cpu=10m on Node cn-hongkong.192.168.0.4
    Mar 30 13:19:37.398: INFO: Pod storage-snapshot-manager-55c574dc6c-ths98 requesting resource cpu=100m on Node cn-hongkong.192.168.0.3
    Mar 30 13:19:37.398: INFO: Pod sonobuoy requesting resource cpu=0m on Node cn-hongkong.192.168.0.5
    Mar 30 13:19:37.398: INFO: Pod sonobuoy-e2e-job-5f38eb9e10894fe7 requesting resource cpu=0m on Node cn-hongkong.192.168.0.3
    Mar 30 13:19:37.398: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-87ph2 requesting resource cpu=0m on Node cn-hongkong.192.168.0.5
    Mar 30 13:19:37.398: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-bn4wq requesting resource cpu=0m on Node cn-hongkong.192.168.0.3
    Mar 30 13:19:37.398: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-prg6s requesting resource cpu=0m on Node cn-hongkong.192.168.0.4
    STEP: Starting Pods to consume most of the cluster CPU. 03/30/23 13:19:37.398
    Mar 30 13:19:37.398: INFO: Creating a pod which consumes cpu=5152m on Node cn-hongkong.192.168.0.3
    Mar 30 13:19:37.402: INFO: Creating a pod which consumes cpu=4823m on Node cn-hongkong.192.168.0.4
    Mar 30 13:19:37.404: INFO: Creating a pod which consumes cpu=5145m on Node cn-hongkong.192.168.0.5
    Mar 30 13:19:37.407: INFO: Waiting up to 5m0s for pod "filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10" in namespace "sched-pred-8583" to be "running"
    Mar 30 13:19:37.411: INFO: Pod "filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568847ms
    Mar 30 13:19:39.413: INFO: Pod "filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10": Phase="Running", Reason="", readiness=true. Elapsed: 2.005947186s
    Mar 30 13:19:39.413: INFO: Pod "filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10" satisfied condition "running"
    Mar 30 13:19:39.413: INFO: Waiting up to 5m0s for pod "filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c" in namespace "sched-pred-8583" to be "running"
    Mar 30 13:19:39.414: INFO: Pod "filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c": Phase="Running", Reason="", readiness=true. Elapsed: 1.400318ms
    Mar 30 13:19:39.414: INFO: Pod "filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c" satisfied condition "running"
    Mar 30 13:19:39.414: INFO: Waiting up to 5m0s for pod "filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495" in namespace "sched-pred-8583" to be "running"
    Mar 30 13:19:39.416: INFO: Pod "filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495": Phase="Running", Reason="", readiness=true. Elapsed: 1.476447ms
    Mar 30 13:19:39.416: INFO: Pod "filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 03/30/23 13:19:39.416
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c.175134f86339ff21], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8583/filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c to cn-hongkong.192.168.0.4] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c.175134f87cad1d69], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c.175134f87dc91133], Reason = [Created], Message = [Created container filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c.175134f8830081ab], Reason = [Started], Message = [Started container filler-pod-44ec021c-8f37-4025-9140-13b553f6d72c] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10.175134f8630e5612], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8583/filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10 to cn-hongkong.192.168.0.3] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10.175134f87c876cc1], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10.175134f8a99c4baa], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 756.315264ms (756.328389ms including waiting)] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10.175134f8aad031d8], Reason = [Created], Message = [Created container filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10.175134f8b010486a], Reason = [Started], Message = [Started container filler-pod-ca5e59e9-b5ed-4643-b933-0eae10d9ce10] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495.175134f863668fd6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8583/filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495 to cn-hongkong.192.168.0.5] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495.175134f87cb7bdb1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495.175134f87e283a61], Reason = [Created], Message = [Created container filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495.175134f883998f03], Reason = [Started], Message = [Started container filler-pod-ee23ec32-9ef3-40ca-9831-647e9f96d495] 03/30/23 13:19:39.418
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.175134f8db4b99ec], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 03/30/23 13:19:39.423
    STEP: removing the label node off the node cn-hongkong.192.168.0.3 03/30/23 13:19:40.423
    STEP: verifying the node doesn't have the label node 03/30/23 13:19:40.43
    STEP: removing the label node off the node cn-hongkong.192.168.0.4 03/30/23 13:19:40.431
    STEP: verifying the node doesn't have the label node 03/30/23 13:19:40.437
    STEP: removing the label node off the node cn-hongkong.192.168.0.5 03/30/23 13:19:40.438
    STEP: verifying the node doesn't have the label node 03/30/23 13:19:40.444
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:19:40.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8583" for this suite. 03/30/23 13:19:40.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:19:40.451
Mar 30 13:19:40.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-lifecycle-hook 03/30/23 13:19:40.453
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:40.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:40.461
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/30/23 13:19:40.466
Mar 30 13:19:40.470: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9184" to be "running and ready"
Mar 30 13:19:40.471: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.658972ms
Mar 30 13:19:40.471: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:19:42.473: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003747666s
Mar 30 13:19:42.473: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 30 13:19:42.473: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 03/30/23 13:19:42.475
Mar 30 13:19:42.477: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9184" to be "running and ready"
Mar 30 13:19:42.479: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.396852ms
Mar 30 13:19:42.479: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:19:44.481: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00368057s
Mar 30 13:19:44.481: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Mar 30 13:19:44.481: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/30/23 13:19:44.482
STEP: delete the pod with lifecycle hook 03/30/23 13:19:44.488
Mar 30 13:19:44.491: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 30 13:19:44.492: INFO: Pod pod-with-poststart-http-hook still exists
Mar 30 13:19:46.493: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 30 13:19:46.495: INFO: Pod pod-with-poststart-http-hook still exists
Mar 30 13:19:48.493: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 30 13:19:48.495: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 30 13:19:48.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9184" for this suite. 03/30/23 13:19:48.497
------------------------------
â€¢ [SLOW TEST] [8.049 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:19:40.451
    Mar 30 13:19:40.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/30/23 13:19:40.453
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:40.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:40.461
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/30/23 13:19:40.466
    Mar 30 13:19:40.470: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9184" to be "running and ready"
    Mar 30 13:19:40.471: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.658972ms
    Mar 30 13:19:40.471: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:19:42.473: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003747666s
    Mar 30 13:19:42.473: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 30 13:19:42.473: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 03/30/23 13:19:42.475
    Mar 30 13:19:42.477: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9184" to be "running and ready"
    Mar 30 13:19:42.479: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.396852ms
    Mar 30 13:19:42.479: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:19:44.481: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00368057s
    Mar 30 13:19:44.481: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Mar 30 13:19:44.481: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/30/23 13:19:44.482
    STEP: delete the pod with lifecycle hook 03/30/23 13:19:44.488
    Mar 30 13:19:44.491: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 30 13:19:44.492: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 30 13:19:46.493: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 30 13:19:46.495: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 30 13:19:48.493: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 30 13:19:48.495: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:19:48.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9184" for this suite. 03/30/23 13:19:48.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:19:48.501
Mar 30 13:19:48.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 13:19:48.502
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:48.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:48.51
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/30/23 13:19:48.511
Mar 30 13:19:48.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/30/23 13:19:59.342
Mar 30 13:19:59.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:20:05.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:20:17.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7581" for this suite. 03/30/23 13:20:17.08
------------------------------
â€¢ [SLOW TEST] [28.582 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:19:48.501
    Mar 30 13:19:48.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 13:19:48.502
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:19:48.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:19:48.51
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/30/23 13:19:48.511
    Mar 30 13:19:48.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/30/23 13:19:59.342
    Mar 30 13:19:59.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:20:05.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:20:17.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7581" for this suite. 03/30/23 13:20:17.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:20:17.084
Mar 30 13:20:17.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:20:17.084
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:20:17.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:20:17.093
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-fff349de-88f3-46fa-83e0-51ca804ba18f 03/30/23 13:20:17.094
STEP: Creating a pod to test consume secrets 03/30/23 13:20:17.097
Mar 30 13:20:17.100: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313" in namespace "projected-3650" to be "Succeeded or Failed"
Mar 30 13:20:17.102: INFO: Pod "pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313": Phase="Pending", Reason="", readiness=false. Elapsed: 1.628021ms
Mar 30 13:20:19.104: INFO: Pod "pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003721157s
Mar 30 13:20:21.104: INFO: Pod "pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003979282s
STEP: Saw pod success 03/30/23 13:20:21.104
Mar 30 13:20:21.104: INFO: Pod "pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313" satisfied condition "Succeeded or Failed"
Mar 30 13:20:21.106: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/30/23 13:20:21.109
Mar 30 13:20:21.113: INFO: Waiting for pod pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313 to disappear
Mar 30 13:20:21.114: INFO: Pod pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 30 13:20:21.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3650" for this suite. 03/30/23 13:20:21.117
------------------------------
â€¢ [4.035 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:20:17.084
    Mar 30 13:20:17.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:20:17.084
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:20:17.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:20:17.093
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-fff349de-88f3-46fa-83e0-51ca804ba18f 03/30/23 13:20:17.094
    STEP: Creating a pod to test consume secrets 03/30/23 13:20:17.097
    Mar 30 13:20:17.100: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313" in namespace "projected-3650" to be "Succeeded or Failed"
    Mar 30 13:20:17.102: INFO: Pod "pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313": Phase="Pending", Reason="", readiness=false. Elapsed: 1.628021ms
    Mar 30 13:20:19.104: INFO: Pod "pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003721157s
    Mar 30 13:20:21.104: INFO: Pod "pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003979282s
    STEP: Saw pod success 03/30/23 13:20:21.104
    Mar 30 13:20:21.104: INFO: Pod "pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313" satisfied condition "Succeeded or Failed"
    Mar 30 13:20:21.106: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 13:20:21.109
    Mar 30 13:20:21.113: INFO: Waiting for pod pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313 to disappear
    Mar 30 13:20:21.114: INFO: Pod pod-projected-secrets-ea4af78b-4da4-463a-a2b1-6cde37cfe313 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:20:21.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3650" for this suite. 03/30/23 13:20:21.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:20:21.12
Mar 30 13:20:21.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename svcaccounts 03/30/23 13:20:21.12
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:20:21.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:20:21.128
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  03/30/23 13:20:21.13
Mar 30 13:20:21.133: INFO: Waiting up to 5m0s for pod "test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5" in namespace "svcaccounts-3032" to be "Succeeded or Failed"
Mar 30 13:20:21.135: INFO: Pod "test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.821639ms
Mar 30 13:20:23.138: INFO: Pod "test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5": Phase="Running", Reason="", readiness=false. Elapsed: 2.004539214s
Mar 30 13:20:25.138: INFO: Pod "test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004633296s
STEP: Saw pod success 03/30/23 13:20:25.138
Mar 30 13:20:25.138: INFO: Pod "test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5" satisfied condition "Succeeded or Failed"
Mar 30 13:20:25.139: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5 container agnhost-container: <nil>
STEP: delete the pod 03/30/23 13:20:25.143
Mar 30 13:20:25.148: INFO: Waiting for pod test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5 to disappear
Mar 30 13:20:25.149: INFO: Pod test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 30 13:20:25.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3032" for this suite. 03/30/23 13:20:25.151
------------------------------
â€¢ [4.033 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:20:21.12
    Mar 30 13:20:21.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename svcaccounts 03/30/23 13:20:21.12
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:20:21.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:20:21.128
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  03/30/23 13:20:21.13
    Mar 30 13:20:21.133: INFO: Waiting up to 5m0s for pod "test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5" in namespace "svcaccounts-3032" to be "Succeeded or Failed"
    Mar 30 13:20:21.135: INFO: Pod "test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.821639ms
    Mar 30 13:20:23.138: INFO: Pod "test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5": Phase="Running", Reason="", readiness=false. Elapsed: 2.004539214s
    Mar 30 13:20:25.138: INFO: Pod "test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004633296s
    STEP: Saw pod success 03/30/23 13:20:25.138
    Mar 30 13:20:25.138: INFO: Pod "test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5" satisfied condition "Succeeded or Failed"
    Mar 30 13:20:25.139: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5 container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 13:20:25.143
    Mar 30 13:20:25.148: INFO: Waiting for pod test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5 to disappear
    Mar 30 13:20:25.149: INFO: Pod test-pod-8ebe4774-c5bc-4a8a-aff6-3c0ca0ca95b5 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:20:25.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3032" for this suite. 03/30/23 13:20:25.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:20:25.154
Mar 30 13:20:25.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename resourcequota 03/30/23 13:20:25.154
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:20:25.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:20:25.162
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 03/30/23 13:20:42.166
STEP: Creating a ResourceQuota 03/30/23 13:20:47.169
STEP: Ensuring resource quota status is calculated 03/30/23 13:20:47.171
STEP: Creating a ConfigMap 03/30/23 13:20:49.174
STEP: Ensuring resource quota status captures configMap creation 03/30/23 13:20:49.18
STEP: Deleting a ConfigMap 03/30/23 13:20:51.182
STEP: Ensuring resource quota status released usage 03/30/23 13:20:51.185
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 30 13:20:53.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-816" for this suite. 03/30/23 13:20:53.189
------------------------------
â€¢ [SLOW TEST] [28.039 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:20:25.154
    Mar 30 13:20:25.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename resourcequota 03/30/23 13:20:25.154
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:20:25.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:20:25.162
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 03/30/23 13:20:42.166
    STEP: Creating a ResourceQuota 03/30/23 13:20:47.169
    STEP: Ensuring resource quota status is calculated 03/30/23 13:20:47.171
    STEP: Creating a ConfigMap 03/30/23 13:20:49.174
    STEP: Ensuring resource quota status captures configMap creation 03/30/23 13:20:49.18
    STEP: Deleting a ConfigMap 03/30/23 13:20:51.182
    STEP: Ensuring resource quota status released usage 03/30/23 13:20:51.185
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:20:53.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-816" for this suite. 03/30/23 13:20:53.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:20:53.193
Mar 30 13:20:53.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-runtime 03/30/23 13:20:53.194
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:20:53.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:20:53.202
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 03/30/23 13:20:53.203
STEP: wait for the container to reach Succeeded 03/30/23 13:20:53.207
STEP: get the container status 03/30/23 13:20:56.215
STEP: the container should be terminated 03/30/23 13:20:56.216
STEP: the termination message should be set 03/30/23 13:20:56.216
Mar 30 13:20:56.216: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/30/23 13:20:56.216
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 30 13:20:56.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6441" for this suite. 03/30/23 13:20:56.224
------------------------------
â€¢ [3.033 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:20:53.193
    Mar 30 13:20:53.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-runtime 03/30/23 13:20:53.194
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:20:53.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:20:53.202
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 03/30/23 13:20:53.203
    STEP: wait for the container to reach Succeeded 03/30/23 13:20:53.207
    STEP: get the container status 03/30/23 13:20:56.215
    STEP: the container should be terminated 03/30/23 13:20:56.216
    STEP: the termination message should be set 03/30/23 13:20:56.216
    Mar 30 13:20:56.216: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/30/23 13:20:56.216
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:20:56.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6441" for this suite. 03/30/23 13:20:56.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:20:56.227
Mar 30 13:20:56.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 13:20:56.228
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:20:56.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:20:56.236
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:20:56.238
Mar 30 13:20:56.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13" in namespace "downward-api-620" to be "Succeeded or Failed"
Mar 30 13:20:56.243: INFO: Pod "downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13": Phase="Pending", Reason="", readiness=false. Elapsed: 1.447163ms
Mar 30 13:20:58.245: INFO: Pod "downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003732309s
Mar 30 13:21:00.245: INFO: Pod "downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003570474s
STEP: Saw pod success 03/30/23 13:21:00.245
Mar 30 13:21:00.245: INFO: Pod "downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13" satisfied condition "Succeeded or Failed"
Mar 30 13:21:00.246: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13 container client-container: <nil>
STEP: delete the pod 03/30/23 13:21:00.254
Mar 30 13:21:00.258: INFO: Waiting for pod downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13 to disappear
Mar 30 13:21:00.260: INFO: Pod downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 30 13:21:00.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-620" for this suite. 03/30/23 13:21:00.262
------------------------------
â€¢ [4.037 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:20:56.227
    Mar 30 13:20:56.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 13:20:56.228
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:20:56.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:20:56.236
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:20:56.238
    Mar 30 13:20:56.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13" in namespace "downward-api-620" to be "Succeeded or Failed"
    Mar 30 13:20:56.243: INFO: Pod "downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13": Phase="Pending", Reason="", readiness=false. Elapsed: 1.447163ms
    Mar 30 13:20:58.245: INFO: Pod "downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003732309s
    Mar 30 13:21:00.245: INFO: Pod "downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003570474s
    STEP: Saw pod success 03/30/23 13:21:00.245
    Mar 30 13:21:00.245: INFO: Pod "downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13" satisfied condition "Succeeded or Failed"
    Mar 30 13:21:00.246: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13 container client-container: <nil>
    STEP: delete the pod 03/30/23 13:21:00.254
    Mar 30 13:21:00.258: INFO: Waiting for pod downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13 to disappear
    Mar 30 13:21:00.260: INFO: Pod downwardapi-volume-34e7a14e-f2ce-4831-9231-99da41c10a13 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:21:00.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-620" for this suite. 03/30/23 13:21:00.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:21:00.265
Mar 30 13:21:00.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:21:00.265
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:00.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:00.273
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:21:00.275
Mar 30 13:21:00.278: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828" in namespace "projected-5307" to be "Succeeded or Failed"
Mar 30 13:21:00.280: INFO: Pod "downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828": Phase="Pending", Reason="", readiness=false. Elapsed: 1.409764ms
Mar 30 13:21:02.282: INFO: Pod "downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00343557s
Mar 30 13:21:04.282: INFO: Pod "downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003288519s
STEP: Saw pod success 03/30/23 13:21:04.282
Mar 30 13:21:04.282: INFO: Pod "downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828" satisfied condition "Succeeded or Failed"
Mar 30 13:21:04.283: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828 container client-container: <nil>
STEP: delete the pod 03/30/23 13:21:04.286
Mar 30 13:21:04.290: INFO: Waiting for pod downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828 to disappear
Mar 30 13:21:04.292: INFO: Pod downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 30 13:21:04.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5307" for this suite. 03/30/23 13:21:04.294
------------------------------
â€¢ [4.032 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:21:00.265
    Mar 30 13:21:00.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:21:00.265
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:00.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:00.273
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:21:00.275
    Mar 30 13:21:00.278: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828" in namespace "projected-5307" to be "Succeeded or Failed"
    Mar 30 13:21:00.280: INFO: Pod "downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828": Phase="Pending", Reason="", readiness=false. Elapsed: 1.409764ms
    Mar 30 13:21:02.282: INFO: Pod "downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00343557s
    Mar 30 13:21:04.282: INFO: Pod "downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003288519s
    STEP: Saw pod success 03/30/23 13:21:04.282
    Mar 30 13:21:04.282: INFO: Pod "downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828" satisfied condition "Succeeded or Failed"
    Mar 30 13:21:04.283: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828 container client-container: <nil>
    STEP: delete the pod 03/30/23 13:21:04.286
    Mar 30 13:21:04.290: INFO: Waiting for pod downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828 to disappear
    Mar 30 13:21:04.292: INFO: Pod downwardapi-volume-72e18432-708c-4197-8acf-aca49e840828 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:21:04.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5307" for this suite. 03/30/23 13:21:04.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:21:04.298
Mar 30 13:21:04.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename daemonsets 03/30/23 13:21:04.298
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:04.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:04.306
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 03/30/23 13:21:04.316
STEP: Check that daemon pods launch on every node of the cluster. 03/30/23 13:21:04.318
Mar 30 13:21:04.320: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:04.320: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:04.320: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:04.322: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 13:21:04.322: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
Mar 30 13:21:05.324: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:05.324: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:05.324: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:05.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 30 13:21:05.326: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 03/30/23 13:21:05.327
Mar 30 13:21:05.334: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:05.334: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:05.334: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:05.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 30 13:21:05.335: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
Mar 30 13:21:06.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:06.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:06.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:06.341: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 30 13:21:06.341: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
Mar 30 13:21:07.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:07.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:07.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:07.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 30 13:21:07.340: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
Mar 30 13:21:08.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:08.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:08.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:08.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 30 13:21:08.340: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
Mar 30 13:21:09.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:09.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:09.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:21:09.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 30 13:21:09.340: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/30/23 13:21:09.341
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-295, will wait for the garbage collector to delete the pods 03/30/23 13:21:09.341
Mar 30 13:21:09.396: INFO: Deleting DaemonSet.extensions daemon-set took: 2.474577ms
Mar 30 13:21:09.496: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.479843ms
Mar 30 13:21:12.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 13:21:12.098: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 30 13:21:12.101: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"58940"},"items":null}

Mar 30 13:21:12.102: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"58940"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:21:12.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-295" for this suite. 03/30/23 13:21:12.111
------------------------------
â€¢ [SLOW TEST] [7.816 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:21:04.298
    Mar 30 13:21:04.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename daemonsets 03/30/23 13:21:04.298
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:04.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:04.306
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 03/30/23 13:21:04.316
    STEP: Check that daemon pods launch on every node of the cluster. 03/30/23 13:21:04.318
    Mar 30 13:21:04.320: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:04.320: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:04.320: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:04.322: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 13:21:04.322: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
    Mar 30 13:21:05.324: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:05.324: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:05.324: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:05.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 30 13:21:05.326: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 03/30/23 13:21:05.327
    Mar 30 13:21:05.334: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:05.334: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:05.334: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:05.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 30 13:21:05.335: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
    Mar 30 13:21:06.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:06.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:06.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:06.341: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 30 13:21:06.341: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
    Mar 30 13:21:07.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:07.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:07.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:07.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 30 13:21:07.340: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
    Mar 30 13:21:08.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:08.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:08.339: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:08.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 30 13:21:08.340: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
    Mar 30 13:21:09.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:09.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:09.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:21:09.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 30 13:21:09.340: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/30/23 13:21:09.341
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-295, will wait for the garbage collector to delete the pods 03/30/23 13:21:09.341
    Mar 30 13:21:09.396: INFO: Deleting DaemonSet.extensions daemon-set took: 2.474577ms
    Mar 30 13:21:09.496: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.479843ms
    Mar 30 13:21:12.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 13:21:12.098: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 30 13:21:12.101: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"58940"},"items":null}

    Mar 30 13:21:12.102: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"58940"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:21:12.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-295" for this suite. 03/30/23 13:21:12.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:21:12.114
Mar 30 13:21:12.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename custom-resource-definition 03/30/23 13:21:12.115
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:12.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:12.122
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Mar 30 13:21:12.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:21:17.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1970" for this suite. 03/30/23 13:21:17.644
------------------------------
â€¢ [SLOW TEST] [5.533 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:21:12.114
    Mar 30 13:21:12.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename custom-resource-definition 03/30/23 13:21:12.115
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:12.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:12.122
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Mar 30 13:21:12.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:21:17.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1970" for this suite. 03/30/23 13:21:17.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:21:17.647
Mar 30 13:21:17.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename runtimeclass 03/30/23 13:21:17.648
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:17.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:17.655
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 03/30/23 13:21:17.657
STEP: getting /apis/node.k8s.io 03/30/23 13:21:17.658
STEP: getting /apis/node.k8s.io/v1 03/30/23 13:21:17.659
STEP: creating 03/30/23 13:21:17.659
STEP: watching 03/30/23 13:21:17.665
Mar 30 13:21:17.666: INFO: starting watch
STEP: getting 03/30/23 13:21:17.668
STEP: listing 03/30/23 13:21:17.669
STEP: patching 03/30/23 13:21:17.671
STEP: updating 03/30/23 13:21:17.672
Mar 30 13:21:17.674: INFO: waiting for watch events with expected annotations
STEP: deleting 03/30/23 13:21:17.674
STEP: deleting a collection 03/30/23 13:21:17.679
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 30 13:21:17.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9034" for this suite. 03/30/23 13:21:17.686
------------------------------
â€¢ [0.041 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:21:17.647
    Mar 30 13:21:17.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename runtimeclass 03/30/23 13:21:17.648
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:17.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:17.655
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 03/30/23 13:21:17.657
    STEP: getting /apis/node.k8s.io 03/30/23 13:21:17.658
    STEP: getting /apis/node.k8s.io/v1 03/30/23 13:21:17.659
    STEP: creating 03/30/23 13:21:17.659
    STEP: watching 03/30/23 13:21:17.665
    Mar 30 13:21:17.666: INFO: starting watch
    STEP: getting 03/30/23 13:21:17.668
    STEP: listing 03/30/23 13:21:17.669
    STEP: patching 03/30/23 13:21:17.671
    STEP: updating 03/30/23 13:21:17.672
    Mar 30 13:21:17.674: INFO: waiting for watch events with expected annotations
    STEP: deleting 03/30/23 13:21:17.674
    STEP: deleting a collection 03/30/23 13:21:17.679
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:21:17.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9034" for this suite. 03/30/23 13:21:17.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:21:17.689
Mar 30 13:21:17.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename namespaces 03/30/23 13:21:17.69
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:17.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:17.696
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 03/30/23 13:21:17.698
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:17.703
STEP: Creating a pod in the namespace 03/30/23 13:21:17.705
STEP: Waiting for the pod to have running status 03/30/23 13:21:17.708
Mar 30 13:21:17.708: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1166" to be "running"
Mar 30 13:21:17.709: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.338974ms
Mar 30 13:21:19.712: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004049064s
Mar 30 13:21:19.712: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 03/30/23 13:21:19.712
STEP: Waiting for the namespace to be removed. 03/30/23 13:21:19.714
STEP: Recreating the namespace 03/30/23 13:21:30.717
STEP: Verifying there are no pods in the namespace 03/30/23 13:21:30.723
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:21:30.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3711" for this suite. 03/30/23 13:21:30.727
STEP: Destroying namespace "nsdeletetest-1166" for this suite. 03/30/23 13:21:30.729
Mar 30 13:21:30.730: INFO: Namespace nsdeletetest-1166 was already deleted
STEP: Destroying namespace "nsdeletetest-2444" for this suite. 03/30/23 13:21:30.73
------------------------------
â€¢ [SLOW TEST] [13.044 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:21:17.689
    Mar 30 13:21:17.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename namespaces 03/30/23 13:21:17.69
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:17.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:17.696
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 03/30/23 13:21:17.698
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:17.703
    STEP: Creating a pod in the namespace 03/30/23 13:21:17.705
    STEP: Waiting for the pod to have running status 03/30/23 13:21:17.708
    Mar 30 13:21:17.708: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1166" to be "running"
    Mar 30 13:21:17.709: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.338974ms
    Mar 30 13:21:19.712: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004049064s
    Mar 30 13:21:19.712: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 03/30/23 13:21:19.712
    STEP: Waiting for the namespace to be removed. 03/30/23 13:21:19.714
    STEP: Recreating the namespace 03/30/23 13:21:30.717
    STEP: Verifying there are no pods in the namespace 03/30/23 13:21:30.723
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:21:30.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3711" for this suite. 03/30/23 13:21:30.727
    STEP: Destroying namespace "nsdeletetest-1166" for this suite. 03/30/23 13:21:30.729
    Mar 30 13:21:30.730: INFO: Namespace nsdeletetest-1166 was already deleted
    STEP: Destroying namespace "nsdeletetest-2444" for this suite. 03/30/23 13:21:30.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:21:30.733
Mar 30 13:21:30.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 13:21:30.734
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:30.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:30.741
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Mar 30 13:21:30.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/30/23 13:21:37.354
Mar 30 13:21:37.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 create -f -'
Mar 30 13:21:37.789: INFO: stderr: ""
Mar 30 13:21:37.789: INFO: stdout: "e2e-test-crd-publish-openapi-9288-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 30 13:21:37.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 delete e2e-test-crd-publish-openapi-9288-crds test-foo'
Mar 30 13:21:37.844: INFO: stderr: ""
Mar 30 13:21:37.844: INFO: stdout: "e2e-test-crd-publish-openapi-9288-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 30 13:21:37.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 apply -f -'
Mar 30 13:21:38.226: INFO: stderr: ""
Mar 30 13:21:38.226: INFO: stdout: "e2e-test-crd-publish-openapi-9288-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 30 13:21:38.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 delete e2e-test-crd-publish-openapi-9288-crds test-foo'
Mar 30 13:21:38.279: INFO: stderr: ""
Mar 30 13:21:38.279: INFO: stdout: "e2e-test-crd-publish-openapi-9288-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/30/23 13:21:38.279
Mar 30 13:21:38.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 create -f -'
Mar 30 13:21:38.643: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/30/23 13:21:38.643
Mar 30 13:21:38.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 create -f -'
Mar 30 13:21:38.782: INFO: rc: 1
Mar 30 13:21:38.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 apply -f -'
Mar 30 13:21:38.932: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/30/23 13:21:38.932
Mar 30 13:21:38.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 create -f -'
Mar 30 13:21:39.079: INFO: rc: 1
Mar 30 13:21:39.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 apply -f -'
Mar 30 13:21:39.223: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 03/30/23 13:21:39.223
Mar 30 13:21:39.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 explain e2e-test-crd-publish-openapi-9288-crds'
Mar 30 13:21:39.365: INFO: stderr: ""
Mar 30 13:21:39.365: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9288-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 03/30/23 13:21:39.365
Mar 30 13:21:39.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 explain e2e-test-crd-publish-openapi-9288-crds.metadata'
Mar 30 13:21:39.509: INFO: stderr: ""
Mar 30 13:21:39.509: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9288-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 30 13:21:39.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 explain e2e-test-crd-publish-openapi-9288-crds.spec'
Mar 30 13:21:39.649: INFO: stderr: ""
Mar 30 13:21:39.649: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9288-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 30 13:21:39.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 explain e2e-test-crd-publish-openapi-9288-crds.spec.bars'
Mar 30 13:21:39.791: INFO: stderr: ""
Mar 30 13:21:39.791: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9288-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/30/23 13:21:39.791
Mar 30 13:21:39.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 explain e2e-test-crd-publish-openapi-9288-crds.spec.bars2'
Mar 30 13:21:39.935: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:21:41.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7090" for this suite. 03/30/23 13:21:41.548
------------------------------
â€¢ [SLOW TEST] [10.818 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:21:30.733
    Mar 30 13:21:30.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 13:21:30.734
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:30.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:30.741
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Mar 30 13:21:30.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/30/23 13:21:37.354
    Mar 30 13:21:37.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 create -f -'
    Mar 30 13:21:37.789: INFO: stderr: ""
    Mar 30 13:21:37.789: INFO: stdout: "e2e-test-crd-publish-openapi-9288-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 30 13:21:37.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 delete e2e-test-crd-publish-openapi-9288-crds test-foo'
    Mar 30 13:21:37.844: INFO: stderr: ""
    Mar 30 13:21:37.844: INFO: stdout: "e2e-test-crd-publish-openapi-9288-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Mar 30 13:21:37.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 apply -f -'
    Mar 30 13:21:38.226: INFO: stderr: ""
    Mar 30 13:21:38.226: INFO: stdout: "e2e-test-crd-publish-openapi-9288-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 30 13:21:38.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 delete e2e-test-crd-publish-openapi-9288-crds test-foo'
    Mar 30 13:21:38.279: INFO: stderr: ""
    Mar 30 13:21:38.279: INFO: stdout: "e2e-test-crd-publish-openapi-9288-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/30/23 13:21:38.279
    Mar 30 13:21:38.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 create -f -'
    Mar 30 13:21:38.643: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/30/23 13:21:38.643
    Mar 30 13:21:38.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 create -f -'
    Mar 30 13:21:38.782: INFO: rc: 1
    Mar 30 13:21:38.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 apply -f -'
    Mar 30 13:21:38.932: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/30/23 13:21:38.932
    Mar 30 13:21:38.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 create -f -'
    Mar 30 13:21:39.079: INFO: rc: 1
    Mar 30 13:21:39.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 --namespace=crd-publish-openapi-7090 apply -f -'
    Mar 30 13:21:39.223: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 03/30/23 13:21:39.223
    Mar 30 13:21:39.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 explain e2e-test-crd-publish-openapi-9288-crds'
    Mar 30 13:21:39.365: INFO: stderr: ""
    Mar 30 13:21:39.365: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9288-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 03/30/23 13:21:39.365
    Mar 30 13:21:39.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 explain e2e-test-crd-publish-openapi-9288-crds.metadata'
    Mar 30 13:21:39.509: INFO: stderr: ""
    Mar 30 13:21:39.509: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9288-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Mar 30 13:21:39.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 explain e2e-test-crd-publish-openapi-9288-crds.spec'
    Mar 30 13:21:39.649: INFO: stderr: ""
    Mar 30 13:21:39.649: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9288-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Mar 30 13:21:39.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 explain e2e-test-crd-publish-openapi-9288-crds.spec.bars'
    Mar 30 13:21:39.791: INFO: stderr: ""
    Mar 30 13:21:39.791: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9288-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/30/23 13:21:39.791
    Mar 30 13:21:39.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-7090 explain e2e-test-crd-publish-openapi-9288-crds.spec.bars2'
    Mar 30 13:21:39.935: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:21:41.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7090" for this suite. 03/30/23 13:21:41.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:21:41.553
Mar 30 13:21:41.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename replication-controller 03/30/23 13:21:41.554
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:41.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:41.563
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-mxpx9" 03/30/23 13:21:41.564
Mar 30 13:21:41.566: INFO: Get Replication Controller "e2e-rc-mxpx9" to confirm replicas
Mar 30 13:21:42.569: INFO: Get Replication Controller "e2e-rc-mxpx9" to confirm replicas
Mar 30 13:21:42.570: INFO: Found 1 replicas for "e2e-rc-mxpx9" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-mxpx9" 03/30/23 13:21:42.57
STEP: Updating a scale subresource 03/30/23 13:21:42.572
STEP: Verifying replicas where modified for replication controller "e2e-rc-mxpx9" 03/30/23 13:21:42.575
Mar 30 13:21:42.575: INFO: Get Replication Controller "e2e-rc-mxpx9" to confirm replicas
Mar 30 13:21:43.577: INFO: Get Replication Controller "e2e-rc-mxpx9" to confirm replicas
Mar 30 13:21:43.579: INFO: Found 2 replicas for "e2e-rc-mxpx9" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 30 13:21:43.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3778" for this suite. 03/30/23 13:21:43.581
------------------------------
â€¢ [2.031 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:21:41.553
    Mar 30 13:21:41.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename replication-controller 03/30/23 13:21:41.554
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:41.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:41.563
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-mxpx9" 03/30/23 13:21:41.564
    Mar 30 13:21:41.566: INFO: Get Replication Controller "e2e-rc-mxpx9" to confirm replicas
    Mar 30 13:21:42.569: INFO: Get Replication Controller "e2e-rc-mxpx9" to confirm replicas
    Mar 30 13:21:42.570: INFO: Found 1 replicas for "e2e-rc-mxpx9" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-mxpx9" 03/30/23 13:21:42.57
    STEP: Updating a scale subresource 03/30/23 13:21:42.572
    STEP: Verifying replicas where modified for replication controller "e2e-rc-mxpx9" 03/30/23 13:21:42.575
    Mar 30 13:21:42.575: INFO: Get Replication Controller "e2e-rc-mxpx9" to confirm replicas
    Mar 30 13:21:43.577: INFO: Get Replication Controller "e2e-rc-mxpx9" to confirm replicas
    Mar 30 13:21:43.579: INFO: Found 2 replicas for "e2e-rc-mxpx9" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:21:43.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3778" for this suite. 03/30/23 13:21:43.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:21:43.585
Mar 30 13:21:43.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename ingress 03/30/23 13:21:43.585
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:43.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:43.593
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 03/30/23 13:21:43.595
STEP: getting /apis/networking.k8s.io 03/30/23 13:21:43.596
STEP: getting /apis/networking.k8s.iov1 03/30/23 13:21:43.597
STEP: creating 03/30/23 13:21:43.598
STEP: getting 03/30/23 13:21:43.604
STEP: listing 03/30/23 13:21:43.605
STEP: watching 03/30/23 13:21:43.606
Mar 30 13:21:43.606: INFO: starting watch
STEP: cluster-wide listing 03/30/23 13:21:43.607
STEP: cluster-wide watching 03/30/23 13:21:43.608
Mar 30 13:21:43.609: INFO: starting watch
STEP: patching 03/30/23 13:21:43.609
STEP: updating 03/30/23 13:21:43.611
Mar 30 13:21:43.615: INFO: waiting for watch events with expected annotations
Mar 30 13:21:43.615: INFO: saw patched and updated annotations
STEP: patching /status 03/30/23 13:21:43.615
STEP: updating /status 03/30/23 13:21:43.617
STEP: get /status 03/30/23 13:21:43.621
STEP: deleting 03/30/23 13:21:43.622
STEP: deleting a collection 03/30/23 13:21:43.627
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Mar 30 13:21:43.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-4290" for this suite. 03/30/23 13:21:43.634
------------------------------
â€¢ [0.052 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:21:43.585
    Mar 30 13:21:43.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename ingress 03/30/23 13:21:43.585
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:43.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:43.593
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 03/30/23 13:21:43.595
    STEP: getting /apis/networking.k8s.io 03/30/23 13:21:43.596
    STEP: getting /apis/networking.k8s.iov1 03/30/23 13:21:43.597
    STEP: creating 03/30/23 13:21:43.598
    STEP: getting 03/30/23 13:21:43.604
    STEP: listing 03/30/23 13:21:43.605
    STEP: watching 03/30/23 13:21:43.606
    Mar 30 13:21:43.606: INFO: starting watch
    STEP: cluster-wide listing 03/30/23 13:21:43.607
    STEP: cluster-wide watching 03/30/23 13:21:43.608
    Mar 30 13:21:43.609: INFO: starting watch
    STEP: patching 03/30/23 13:21:43.609
    STEP: updating 03/30/23 13:21:43.611
    Mar 30 13:21:43.615: INFO: waiting for watch events with expected annotations
    Mar 30 13:21:43.615: INFO: saw patched and updated annotations
    STEP: patching /status 03/30/23 13:21:43.615
    STEP: updating /status 03/30/23 13:21:43.617
    STEP: get /status 03/30/23 13:21:43.621
    STEP: deleting 03/30/23 13:21:43.622
    STEP: deleting a collection 03/30/23 13:21:43.627
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:21:43.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-4290" for this suite. 03/30/23 13:21:43.634
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:21:43.637
Mar 30 13:21:43.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 13:21:43.637
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:43.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:43.645
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 13:21:43.651
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:21:44.03
STEP: Deploying the webhook pod 03/30/23 13:21:44.032
STEP: Wait for the deployment to be ready 03/30/23 13:21:44.037
Mar 30 13:21:44.040: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/30/23 13:21:46.045
STEP: Verifying the service has paired with the endpoint 03/30/23 13:21:46.049
Mar 30 13:21:47.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Mar 30 13:21:47.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3637-crds.webhook.example.com via the AdmissionRegistration API 03/30/23 13:21:52.557
STEP: Creating a custom resource that should be mutated by the webhook 03/30/23 13:21:52.566
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:21:55.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5080" for this suite. 03/30/23 13:21:55.116
STEP: Destroying namespace "webhook-5080-markers" for this suite. 03/30/23 13:21:55.119
------------------------------
â€¢ [SLOW TEST] [11.485 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:21:43.637
    Mar 30 13:21:43.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 13:21:43.637
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:43.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:43.645
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 13:21:43.651
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:21:44.03
    STEP: Deploying the webhook pod 03/30/23 13:21:44.032
    STEP: Wait for the deployment to be ready 03/30/23 13:21:44.037
    Mar 30 13:21:44.040: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/30/23 13:21:46.045
    STEP: Verifying the service has paired with the endpoint 03/30/23 13:21:46.049
    Mar 30 13:21:47.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Mar 30 13:21:47.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3637-crds.webhook.example.com via the AdmissionRegistration API 03/30/23 13:21:52.557
    STEP: Creating a custom resource that should be mutated by the webhook 03/30/23 13:21:52.566
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:21:55.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5080" for this suite. 03/30/23 13:21:55.116
    STEP: Destroying namespace "webhook-5080-markers" for this suite. 03/30/23 13:21:55.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:21:55.123
Mar 30 13:21:55.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename resourcequota 03/30/23 13:21:55.124
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:55.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:55.132
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 03/30/23 13:21:55.134
STEP: Creating a ResourceQuota 03/30/23 13:22:00.136
STEP: Ensuring resource quota status is calculated 03/30/23 13:22:00.138
STEP: Creating a ReplicationController 03/30/23 13:22:02.14
STEP: Ensuring resource quota status captures replication controller creation 03/30/23 13:22:02.146
STEP: Deleting a ReplicationController 03/30/23 13:22:04.148
STEP: Ensuring resource quota status released usage 03/30/23 13:22:04.15
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 30 13:22:06.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7332" for this suite. 03/30/23 13:22:06.155
------------------------------
â€¢ [SLOW TEST] [11.035 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:21:55.123
    Mar 30 13:21:55.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename resourcequota 03/30/23 13:21:55.124
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:21:55.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:21:55.132
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 03/30/23 13:21:55.134
    STEP: Creating a ResourceQuota 03/30/23 13:22:00.136
    STEP: Ensuring resource quota status is calculated 03/30/23 13:22:00.138
    STEP: Creating a ReplicationController 03/30/23 13:22:02.14
    STEP: Ensuring resource quota status captures replication controller creation 03/30/23 13:22:02.146
    STEP: Deleting a ReplicationController 03/30/23 13:22:04.148
    STEP: Ensuring resource quota status released usage 03/30/23 13:22:04.15
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:22:06.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7332" for this suite. 03/30/23 13:22:06.155
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:22:06.158
Mar 30 13:22:06.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:22:06.159
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:06.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:06.167
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:22:06.169
Mar 30 13:22:06.173: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2" in namespace "projected-6200" to be "Succeeded or Failed"
Mar 30 13:22:06.174: INFO: Pod "downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.422124ms
Mar 30 13:22:08.177: INFO: Pod "downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004314985s
Mar 30 13:22:10.178: INFO: Pod "downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005192976s
STEP: Saw pod success 03/30/23 13:22:10.178
Mar 30 13:22:10.178: INFO: Pod "downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2" satisfied condition "Succeeded or Failed"
Mar 30 13:22:10.180: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2 container client-container: <nil>
STEP: delete the pod 03/30/23 13:22:10.188
Mar 30 13:22:10.193: INFO: Waiting for pod downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2 to disappear
Mar 30 13:22:10.194: INFO: Pod downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 30 13:22:10.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6200" for this suite. 03/30/23 13:22:10.196
------------------------------
â€¢ [4.041 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:22:06.158
    Mar 30 13:22:06.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:22:06.159
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:06.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:06.167
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:22:06.169
    Mar 30 13:22:06.173: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2" in namespace "projected-6200" to be "Succeeded or Failed"
    Mar 30 13:22:06.174: INFO: Pod "downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.422124ms
    Mar 30 13:22:08.177: INFO: Pod "downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004314985s
    Mar 30 13:22:10.178: INFO: Pod "downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005192976s
    STEP: Saw pod success 03/30/23 13:22:10.178
    Mar 30 13:22:10.178: INFO: Pod "downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2" satisfied condition "Succeeded or Failed"
    Mar 30 13:22:10.180: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2 container client-container: <nil>
    STEP: delete the pod 03/30/23 13:22:10.188
    Mar 30 13:22:10.193: INFO: Waiting for pod downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2 to disappear
    Mar 30 13:22:10.194: INFO: Pod downwardapi-volume-3c1780c5-2f24-4fd0-a373-868783f063f2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:22:10.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6200" for this suite. 03/30/23 13:22:10.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:22:10.199
Mar 30 13:22:10.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename limitrange 03/30/23 13:22:10.2
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:10.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:10.208
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 03/30/23 13:22:10.209
STEP: Setting up watch 03/30/23 13:22:10.209
STEP: Submitting a LimitRange 03/30/23 13:22:10.311
STEP: Verifying LimitRange creation was observed 03/30/23 13:22:10.314
STEP: Fetching the LimitRange to ensure it has proper values 03/30/23 13:22:10.314
Mar 30 13:22:10.316: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 30 13:22:10.316: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 03/30/23 13:22:10.316
STEP: Ensuring Pod has resource requirements applied from LimitRange 03/30/23 13:22:10.32
Mar 30 13:22:10.322: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 30 13:22:10.322: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 03/30/23 13:22:10.322
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/30/23 13:22:10.324
Mar 30 13:22:10.326: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 30 13:22:10.326: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 03/30/23 13:22:10.326
STEP: Failing to create a Pod with more than max resources 03/30/23 13:22:10.327
STEP: Updating a LimitRange 03/30/23 13:22:10.328
STEP: Verifying LimitRange updating is effective 03/30/23 13:22:10.33
STEP: Creating a Pod with less than former min resources 03/30/23 13:22:12.333
STEP: Failing to create a Pod with more than max resources 03/30/23 13:22:12.336
STEP: Deleting a LimitRange 03/30/23 13:22:12.337
STEP: Verifying the LimitRange was deleted 03/30/23 13:22:12.34
Mar 30 13:22:17.343: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 03/30/23 13:22:17.343
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Mar 30 13:22:17.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-7616" for this suite. 03/30/23 13:22:17.35
------------------------------
â€¢ [SLOW TEST] [7.154 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:22:10.199
    Mar 30 13:22:10.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename limitrange 03/30/23 13:22:10.2
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:10.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:10.208
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 03/30/23 13:22:10.209
    STEP: Setting up watch 03/30/23 13:22:10.209
    STEP: Submitting a LimitRange 03/30/23 13:22:10.311
    STEP: Verifying LimitRange creation was observed 03/30/23 13:22:10.314
    STEP: Fetching the LimitRange to ensure it has proper values 03/30/23 13:22:10.314
    Mar 30 13:22:10.316: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 30 13:22:10.316: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 03/30/23 13:22:10.316
    STEP: Ensuring Pod has resource requirements applied from LimitRange 03/30/23 13:22:10.32
    Mar 30 13:22:10.322: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 30 13:22:10.322: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 03/30/23 13:22:10.322
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/30/23 13:22:10.324
    Mar 30 13:22:10.326: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Mar 30 13:22:10.326: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 03/30/23 13:22:10.326
    STEP: Failing to create a Pod with more than max resources 03/30/23 13:22:10.327
    STEP: Updating a LimitRange 03/30/23 13:22:10.328
    STEP: Verifying LimitRange updating is effective 03/30/23 13:22:10.33
    STEP: Creating a Pod with less than former min resources 03/30/23 13:22:12.333
    STEP: Failing to create a Pod with more than max resources 03/30/23 13:22:12.336
    STEP: Deleting a LimitRange 03/30/23 13:22:12.337
    STEP: Verifying the LimitRange was deleted 03/30/23 13:22:12.34
    Mar 30 13:22:17.343: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 03/30/23 13:22:17.343
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:22:17.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-7616" for this suite. 03/30/23 13:22:17.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:22:17.354
Mar 30 13:22:17.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename watch 03/30/23 13:22:17.354
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:17.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:17.362
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 03/30/23 13:22:17.364
STEP: starting a background goroutine to produce watch events 03/30/23 13:22:17.365
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/30/23 13:22:17.365
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 30 13:22:20.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5167" for this suite. 03/30/23 13:22:20.207
------------------------------
â€¢ [2.904 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:22:17.354
    Mar 30 13:22:17.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename watch 03/30/23 13:22:17.354
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:17.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:17.362
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 03/30/23 13:22:17.364
    STEP: starting a background goroutine to produce watch events 03/30/23 13:22:17.365
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/30/23 13:22:17.365
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:22:20.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5167" for this suite. 03/30/23 13:22:20.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:22:20.258
Mar 30 13:22:20.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename resourcequota 03/30/23 13:22:20.259
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:20.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:20.267
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 03/30/23 13:22:20.269
STEP: Creating a ResourceQuota 03/30/23 13:22:25.271
STEP: Ensuring resource quota status is calculated 03/30/23 13:22:25.273
STEP: Creating a Service 03/30/23 13:22:27.276
STEP: Creating a NodePort Service 03/30/23 13:22:27.285
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/30/23 13:22:27.295
STEP: Ensuring resource quota status captures service creation 03/30/23 13:22:27.304
STEP: Deleting Services 03/30/23 13:22:29.306
STEP: Ensuring resource quota status released usage 03/30/23 13:22:29.32
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 30 13:22:31.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2724" for this suite. 03/30/23 13:22:31.325
------------------------------
â€¢ [SLOW TEST] [11.070 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:22:20.258
    Mar 30 13:22:20.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename resourcequota 03/30/23 13:22:20.259
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:20.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:20.267
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 03/30/23 13:22:20.269
    STEP: Creating a ResourceQuota 03/30/23 13:22:25.271
    STEP: Ensuring resource quota status is calculated 03/30/23 13:22:25.273
    STEP: Creating a Service 03/30/23 13:22:27.276
    STEP: Creating a NodePort Service 03/30/23 13:22:27.285
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/30/23 13:22:27.295
    STEP: Ensuring resource quota status captures service creation 03/30/23 13:22:27.304
    STEP: Deleting Services 03/30/23 13:22:29.306
    STEP: Ensuring resource quota status released usage 03/30/23 13:22:29.32
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:22:31.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2724" for this suite. 03/30/23 13:22:31.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:22:31.329
Mar 30 13:22:31.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:22:31.329
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:31.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:31.338
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:22:31.339
Mar 30 13:22:31.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e" in namespace "projected-617" to be "Succeeded or Failed"
Mar 30 13:22:31.344: INFO: Pod "downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.319313ms
Mar 30 13:22:33.347: INFO: Pod "downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003884282s
Mar 30 13:22:35.347: INFO: Pod "downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003951417s
STEP: Saw pod success 03/30/23 13:22:35.347
Mar 30 13:22:35.347: INFO: Pod "downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e" satisfied condition "Succeeded or Failed"
Mar 30 13:22:35.349: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e container client-container: <nil>
STEP: delete the pod 03/30/23 13:22:35.352
Mar 30 13:22:35.357: INFO: Waiting for pod downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e to disappear
Mar 30 13:22:35.358: INFO: Pod downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 30 13:22:35.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-617" for this suite. 03/30/23 13:22:35.36
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:22:31.329
    Mar 30 13:22:31.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:22:31.329
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:31.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:31.338
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:22:31.339
    Mar 30 13:22:31.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e" in namespace "projected-617" to be "Succeeded or Failed"
    Mar 30 13:22:31.344: INFO: Pod "downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.319313ms
    Mar 30 13:22:33.347: INFO: Pod "downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003884282s
    Mar 30 13:22:35.347: INFO: Pod "downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003951417s
    STEP: Saw pod success 03/30/23 13:22:35.347
    Mar 30 13:22:35.347: INFO: Pod "downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e" satisfied condition "Succeeded or Failed"
    Mar 30 13:22:35.349: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e container client-container: <nil>
    STEP: delete the pod 03/30/23 13:22:35.352
    Mar 30 13:22:35.357: INFO: Waiting for pod downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e to disappear
    Mar 30 13:22:35.358: INFO: Pod downwardapi-volume-0c501e71-5da0-4dce-ad1e-09b6441aad9e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:22:35.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-617" for this suite. 03/30/23 13:22:35.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:22:35.363
Mar 30 13:22:35.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename svcaccounts 03/30/23 13:22:35.363
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:35.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:35.371
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Mar 30 13:22:35.378: INFO: Waiting up to 5m0s for pod "pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175" in namespace "svcaccounts-5294" to be "running"
Mar 30 13:22:35.379: INFO: Pod "pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335157ms
Mar 30 13:22:37.382: INFO: Pod "pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175": Phase="Running", Reason="", readiness=true. Elapsed: 2.004075185s
Mar 30 13:22:37.382: INFO: Pod "pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175" satisfied condition "running"
STEP: reading a file in the container 03/30/23 13:22:37.382
Mar 30 13:22:37.382: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5294 pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 03/30/23 13:22:37.476
Mar 30 13:22:37.476: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5294 pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 03/30/23 13:22:37.564
Mar 30 13:22:37.564: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5294 pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar 30 13:22:37.657: INFO: Got root ca configmap in namespace "svcaccounts-5294"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 30 13:22:37.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5294" for this suite. 03/30/23 13:22:37.661
------------------------------
â€¢ [2.301 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:22:35.363
    Mar 30 13:22:35.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename svcaccounts 03/30/23 13:22:35.363
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:35.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:35.371
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Mar 30 13:22:35.378: INFO: Waiting up to 5m0s for pod "pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175" in namespace "svcaccounts-5294" to be "running"
    Mar 30 13:22:35.379: INFO: Pod "pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335157ms
    Mar 30 13:22:37.382: INFO: Pod "pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175": Phase="Running", Reason="", readiness=true. Elapsed: 2.004075185s
    Mar 30 13:22:37.382: INFO: Pod "pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175" satisfied condition "running"
    STEP: reading a file in the container 03/30/23 13:22:37.382
    Mar 30 13:22:37.382: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5294 pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 03/30/23 13:22:37.476
    Mar 30 13:22:37.476: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5294 pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 03/30/23 13:22:37.564
    Mar 30 13:22:37.564: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5294 pod-service-account-b3cc3f4c-4171-45a0-b39a-560c00446175 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Mar 30 13:22:37.657: INFO: Got root ca configmap in namespace "svcaccounts-5294"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:22:37.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5294" for this suite. 03/30/23 13:22:37.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:22:37.664
Mar 30 13:22:37.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename dns 03/30/23 13:22:37.665
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:37.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:37.672
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 03/30/23 13:22:37.674
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-705.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-705.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 03/30/23 13:22:37.676
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-705.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-705.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 03/30/23 13:22:37.676
STEP: creating a pod to probe DNS 03/30/23 13:22:37.676
STEP: submitting the pod to kubernetes 03/30/23 13:22:37.676
Mar 30 13:22:37.680: INFO: Waiting up to 15m0s for pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3" in namespace "dns-705" to be "running"
Mar 30 13:22:37.681: INFO: Pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.491272ms
Mar 30 13:22:39.684: INFO: Pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004044859s
Mar 30 13:22:41.684: INFO: Pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00432103s
Mar 30 13:22:43.684: INFO: Pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003948236s
Mar 30 13:22:45.684: INFO: Pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3": Phase="Running", Reason="", readiness=true. Elapsed: 8.004371365s
Mar 30 13:22:45.684: INFO: Pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3" satisfied condition "running"
STEP: retrieving the pod 03/30/23 13:22:45.684
STEP: looking for the results for each expected name from probers 03/30/23 13:22:45.686
Mar 30 13:22:45.693: INFO: DNS probes using dns-705/dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3 succeeded

STEP: deleting the pod 03/30/23 13:22:45.693
STEP: deleting the test headless service 03/30/23 13:22:45.699
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 30 13:22:45.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-705" for this suite. 03/30/23 13:22:45.709
------------------------------
â€¢ [SLOW TEST] [8.047 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:22:37.664
    Mar 30 13:22:37.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename dns 03/30/23 13:22:37.665
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:37.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:37.672
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 03/30/23 13:22:37.674
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-705.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-705.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     03/30/23 13:22:37.676
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-705.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-705.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     03/30/23 13:22:37.676
    STEP: creating a pod to probe DNS 03/30/23 13:22:37.676
    STEP: submitting the pod to kubernetes 03/30/23 13:22:37.676
    Mar 30 13:22:37.680: INFO: Waiting up to 15m0s for pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3" in namespace "dns-705" to be "running"
    Mar 30 13:22:37.681: INFO: Pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.491272ms
    Mar 30 13:22:39.684: INFO: Pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004044859s
    Mar 30 13:22:41.684: INFO: Pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00432103s
    Mar 30 13:22:43.684: INFO: Pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003948236s
    Mar 30 13:22:45.684: INFO: Pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3": Phase="Running", Reason="", readiness=true. Elapsed: 8.004371365s
    Mar 30 13:22:45.684: INFO: Pod "dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3" satisfied condition "running"
    STEP: retrieving the pod 03/30/23 13:22:45.684
    STEP: looking for the results for each expected name from probers 03/30/23 13:22:45.686
    Mar 30 13:22:45.693: INFO: DNS probes using dns-705/dns-test-771b0a10-6aba-4419-be82-478fdb36bfc3 succeeded

    STEP: deleting the pod 03/30/23 13:22:45.693
    STEP: deleting the test headless service 03/30/23 13:22:45.699
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:22:45.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-705" for this suite. 03/30/23 13:22:45.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:22:45.712
Mar 30 13:22:45.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 13:22:45.713
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:45.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:45.72
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-8475 03/30/23 13:22:45.722
STEP: creating replication controller nodeport-test in namespace services-8475 03/30/23 13:22:45.727
I0330 13:22:45.730802      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8475, replica count: 2
I0330 13:22:48.782558      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 13:22:48.782: INFO: Creating new exec pod
Mar 30 13:22:48.786: INFO: Waiting up to 5m0s for pod "execpodw2nk9" in namespace "services-8475" to be "running"
Mar 30 13:22:48.789: INFO: Pod "execpodw2nk9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.173247ms
Mar 30 13:22:50.791: INFO: Pod "execpodw2nk9": Phase="Running", Reason="", readiness=true. Elapsed: 2.004397939s
Mar 30 13:22:50.791: INFO: Pod "execpodw2nk9" satisfied condition "running"
Mar 30 13:22:51.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-8475 exec execpodw2nk9 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Mar 30 13:22:51.883: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 30 13:22:51.883: INFO: stdout: ""
Mar 30 13:22:51.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-8475 exec execpodw2nk9 -- /bin/sh -x -c nc -v -z -w 2 172.16.108.46 80'
Mar 30 13:22:51.971: INFO: stderr: "+ nc -v -z -w 2 172.16.108.46 80\nConnection to 172.16.108.46 80 port [tcp/http] succeeded!\n"
Mar 30 13:22:51.971: INFO: stdout: ""
Mar 30 13:22:51.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-8475 exec execpodw2nk9 -- /bin/sh -x -c nc -v -z -w 2 192.168.0.4 32659'
Mar 30 13:22:52.065: INFO: stderr: "+ nc -v -z -w 2 192.168.0.4 32659\nConnection to 192.168.0.4 32659 port [tcp/*] succeeded!\n"
Mar 30 13:22:52.065: INFO: stdout: ""
Mar 30 13:22:52.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-8475 exec execpodw2nk9 -- /bin/sh -x -c nc -v -z -w 2 192.168.0.3 32659'
Mar 30 13:22:52.159: INFO: stderr: "+ nc -v -z -w 2 192.168.0.3 32659\nConnection to 192.168.0.3 32659 port [tcp/*] succeeded!\n"
Mar 30 13:22:52.159: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 13:22:52.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8475" for this suite. 03/30/23 13:22:52.162
------------------------------
â€¢ [SLOW TEST] [6.452 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:22:45.712
    Mar 30 13:22:45.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 13:22:45.713
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:45.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:45.72
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-8475 03/30/23 13:22:45.722
    STEP: creating replication controller nodeport-test in namespace services-8475 03/30/23 13:22:45.727
    I0330 13:22:45.730802      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8475, replica count: 2
    I0330 13:22:48.782558      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 30 13:22:48.782: INFO: Creating new exec pod
    Mar 30 13:22:48.786: INFO: Waiting up to 5m0s for pod "execpodw2nk9" in namespace "services-8475" to be "running"
    Mar 30 13:22:48.789: INFO: Pod "execpodw2nk9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.173247ms
    Mar 30 13:22:50.791: INFO: Pod "execpodw2nk9": Phase="Running", Reason="", readiness=true. Elapsed: 2.004397939s
    Mar 30 13:22:50.791: INFO: Pod "execpodw2nk9" satisfied condition "running"
    Mar 30 13:22:51.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-8475 exec execpodw2nk9 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Mar 30 13:22:51.883: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar 30 13:22:51.883: INFO: stdout: ""
    Mar 30 13:22:51.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-8475 exec execpodw2nk9 -- /bin/sh -x -c nc -v -z -w 2 172.16.108.46 80'
    Mar 30 13:22:51.971: INFO: stderr: "+ nc -v -z -w 2 172.16.108.46 80\nConnection to 172.16.108.46 80 port [tcp/http] succeeded!\n"
    Mar 30 13:22:51.971: INFO: stdout: ""
    Mar 30 13:22:51.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-8475 exec execpodw2nk9 -- /bin/sh -x -c nc -v -z -w 2 192.168.0.4 32659'
    Mar 30 13:22:52.065: INFO: stderr: "+ nc -v -z -w 2 192.168.0.4 32659\nConnection to 192.168.0.4 32659 port [tcp/*] succeeded!\n"
    Mar 30 13:22:52.065: INFO: stdout: ""
    Mar 30 13:22:52.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-8475 exec execpodw2nk9 -- /bin/sh -x -c nc -v -z -w 2 192.168.0.3 32659'
    Mar 30 13:22:52.159: INFO: stderr: "+ nc -v -z -w 2 192.168.0.3 32659\nConnection to 192.168.0.3 32659 port [tcp/*] succeeded!\n"
    Mar 30 13:22:52.159: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:22:52.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8475" for this suite. 03/30/23 13:22:52.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:22:52.165
Mar 30 13:22:52.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 13:22:52.165
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:52.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:52.174
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4601 03/30/23 13:22:52.175
STEP: changing the ExternalName service to type=ClusterIP 03/30/23 13:22:52.177
STEP: creating replication controller externalname-service in namespace services-4601 03/30/23 13:22:52.183
I0330 13:22:52.186680      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4601, replica count: 2
I0330 13:22:55.238549      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 13:22:55.238: INFO: Creating new exec pod
Mar 30 13:22:55.242: INFO: Waiting up to 5m0s for pod "execpod8qs72" in namespace "services-4601" to be "running"
Mar 30 13:22:55.243: INFO: Pod "execpod8qs72": Phase="Pending", Reason="", readiness=false. Elapsed: 1.541949ms
Mar 30 13:22:57.246: INFO: Pod "execpod8qs72": Phase="Running", Reason="", readiness=true. Elapsed: 2.003577227s
Mar 30 13:22:57.246: INFO: Pod "execpod8qs72" satisfied condition "running"
Mar 30 13:22:58.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-4601 exec execpod8qs72 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Mar 30 13:22:58.342: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 30 13:22:58.342: INFO: stdout: ""
Mar 30 13:22:58.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-4601 exec execpod8qs72 -- /bin/sh -x -c nc -v -z -w 2 172.16.142.196 80'
Mar 30 13:22:58.436: INFO: stderr: "+ nc -v -z -w 2 172.16.142.196 80\nConnection to 172.16.142.196 80 port [tcp/http] succeeded!\n"
Mar 30 13:22:58.436: INFO: stdout: ""
Mar 30 13:22:58.436: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 13:22:58.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4601" for this suite. 03/30/23 13:22:58.447
------------------------------
â€¢ [SLOW TEST] [6.285 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:22:52.165
    Mar 30 13:22:52.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 13:22:52.165
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:52.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:52.174
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4601 03/30/23 13:22:52.175
    STEP: changing the ExternalName service to type=ClusterIP 03/30/23 13:22:52.177
    STEP: creating replication controller externalname-service in namespace services-4601 03/30/23 13:22:52.183
    I0330 13:22:52.186680      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4601, replica count: 2
    I0330 13:22:55.238549      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 30 13:22:55.238: INFO: Creating new exec pod
    Mar 30 13:22:55.242: INFO: Waiting up to 5m0s for pod "execpod8qs72" in namespace "services-4601" to be "running"
    Mar 30 13:22:55.243: INFO: Pod "execpod8qs72": Phase="Pending", Reason="", readiness=false. Elapsed: 1.541949ms
    Mar 30 13:22:57.246: INFO: Pod "execpod8qs72": Phase="Running", Reason="", readiness=true. Elapsed: 2.003577227s
    Mar 30 13:22:57.246: INFO: Pod "execpod8qs72" satisfied condition "running"
    Mar 30 13:22:58.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-4601 exec execpod8qs72 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Mar 30 13:22:58.342: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 30 13:22:58.342: INFO: stdout: ""
    Mar 30 13:22:58.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-4601 exec execpod8qs72 -- /bin/sh -x -c nc -v -z -w 2 172.16.142.196 80'
    Mar 30 13:22:58.436: INFO: stderr: "+ nc -v -z -w 2 172.16.142.196 80\nConnection to 172.16.142.196 80 port [tcp/http] succeeded!\n"
    Mar 30 13:22:58.436: INFO: stdout: ""
    Mar 30 13:22:58.436: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:22:58.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4601" for this suite. 03/30/23 13:22:58.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:22:58.451
Mar 30 13:22:58.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:22:58.452
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:58.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:58.461
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-9941/configmap-test-c8b721b7-c4f3-482d-9347-325fc9d4a770 03/30/23 13:22:58.463
STEP: Creating a pod to test consume configMaps 03/30/23 13:22:58.466
Mar 30 13:22:58.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134" in namespace "configmap-9941" to be "Succeeded or Failed"
Mar 30 13:22:58.471: INFO: Pod "pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134": Phase="Pending", Reason="", readiness=false. Elapsed: 1.587924ms
Mar 30 13:23:00.474: INFO: Pod "pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004491647s
Mar 30 13:23:02.474: INFO: Pod "pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004991081s
STEP: Saw pod success 03/30/23 13:23:02.474
Mar 30 13:23:02.475: INFO: Pod "pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134" satisfied condition "Succeeded or Failed"
Mar 30 13:23:02.476: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134 container env-test: <nil>
STEP: delete the pod 03/30/23 13:23:02.479
Mar 30 13:23:02.484: INFO: Waiting for pod pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134 to disappear
Mar 30 13:23:02.485: INFO: Pod pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:23:02.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9941" for this suite. 03/30/23 13:23:02.488
------------------------------
â€¢ [4.039 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:22:58.451
    Mar 30 13:22:58.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:22:58.452
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:22:58.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:22:58.461
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-9941/configmap-test-c8b721b7-c4f3-482d-9347-325fc9d4a770 03/30/23 13:22:58.463
    STEP: Creating a pod to test consume configMaps 03/30/23 13:22:58.466
    Mar 30 13:22:58.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134" in namespace "configmap-9941" to be "Succeeded or Failed"
    Mar 30 13:22:58.471: INFO: Pod "pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134": Phase="Pending", Reason="", readiness=false. Elapsed: 1.587924ms
    Mar 30 13:23:00.474: INFO: Pod "pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004491647s
    Mar 30 13:23:02.474: INFO: Pod "pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004991081s
    STEP: Saw pod success 03/30/23 13:23:02.474
    Mar 30 13:23:02.475: INFO: Pod "pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134" satisfied condition "Succeeded or Failed"
    Mar 30 13:23:02.476: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134 container env-test: <nil>
    STEP: delete the pod 03/30/23 13:23:02.479
    Mar 30 13:23:02.484: INFO: Waiting for pod pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134 to disappear
    Mar 30 13:23:02.485: INFO: Pod pod-configmaps-b423b855-515c-4cc7-a9a0-4f12dfd79134 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:23:02.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9941" for this suite. 03/30/23 13:23:02.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:23:02.491
Mar 30 13:23:02.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename gc 03/30/23 13:23:02.492
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:23:02.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:23:02.499
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 03/30/23 13:23:02.503
STEP: delete the rc 03/30/23 13:23:07.508
STEP: wait for the rc to be deleted 03/30/23 13:23:07.51
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/30/23 13:23:12.513
STEP: Gathering metrics 03/30/23 13:23:42.519
Mar 30 13:23:42.534: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cn-hongkong.192.168.0.252" in namespace "kube-system" to be "running and ready"
Mar 30 13:23:42.535: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252": Phase="Running", Reason="", readiness=true. Elapsed: 1.380926ms
Mar 30 13:23:42.535: INFO: The phase of Pod kube-controller-manager-cn-hongkong.192.168.0.252 is Running (Ready = true)
Mar 30 13:23:42.535: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252" satisfied condition "running and ready"
Mar 30 13:23:42.572: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 30 13:23:42.572: INFO: Deleting pod "simpletest.rc-22qlt" in namespace "gc-7381"
Mar 30 13:23:42.577: INFO: Deleting pod "simpletest.rc-24h9m" in namespace "gc-7381"
Mar 30 13:23:42.583: INFO: Deleting pod "simpletest.rc-2lcsn" in namespace "gc-7381"
Mar 30 13:23:42.587: INFO: Deleting pod "simpletest.rc-2mcmr" in namespace "gc-7381"
Mar 30 13:23:42.591: INFO: Deleting pod "simpletest.rc-4dxbt" in namespace "gc-7381"
Mar 30 13:23:42.595: INFO: Deleting pod "simpletest.rc-4ktft" in namespace "gc-7381"
Mar 30 13:23:42.599: INFO: Deleting pod "simpletest.rc-4mnxg" in namespace "gc-7381"
Mar 30 13:23:42.604: INFO: Deleting pod "simpletest.rc-4tkcm" in namespace "gc-7381"
Mar 30 13:23:42.608: INFO: Deleting pod "simpletest.rc-59f8m" in namespace "gc-7381"
Mar 30 13:23:42.612: INFO: Deleting pod "simpletest.rc-5qtmh" in namespace "gc-7381"
Mar 30 13:23:42.616: INFO: Deleting pod "simpletest.rc-5ww87" in namespace "gc-7381"
Mar 30 13:23:42.620: INFO: Deleting pod "simpletest.rc-68bgl" in namespace "gc-7381"
Mar 30 13:23:42.624: INFO: Deleting pod "simpletest.rc-6bb7g" in namespace "gc-7381"
Mar 30 13:23:42.629: INFO: Deleting pod "simpletest.rc-6gcjr" in namespace "gc-7381"
Mar 30 13:23:42.633: INFO: Deleting pod "simpletest.rc-6gkbq" in namespace "gc-7381"
Mar 30 13:23:42.640: INFO: Deleting pod "simpletest.rc-6pnxw" in namespace "gc-7381"
Mar 30 13:23:42.644: INFO: Deleting pod "simpletest.rc-7brpl" in namespace "gc-7381"
Mar 30 13:23:42.649: INFO: Deleting pod "simpletest.rc-7h6rt" in namespace "gc-7381"
Mar 30 13:23:42.654: INFO: Deleting pod "simpletest.rc-7spbn" in namespace "gc-7381"
Mar 30 13:23:42.659: INFO: Deleting pod "simpletest.rc-7w545" in namespace "gc-7381"
Mar 30 13:23:42.666: INFO: Deleting pod "simpletest.rc-8bkm4" in namespace "gc-7381"
Mar 30 13:23:42.670: INFO: Deleting pod "simpletest.rc-8cv2k" in namespace "gc-7381"
Mar 30 13:23:42.676: INFO: Deleting pod "simpletest.rc-8f87f" in namespace "gc-7381"
Mar 30 13:23:42.681: INFO: Deleting pod "simpletest.rc-8fkz9" in namespace "gc-7381"
Mar 30 13:23:42.685: INFO: Deleting pod "simpletest.rc-8pwvh" in namespace "gc-7381"
Mar 30 13:23:42.690: INFO: Deleting pod "simpletest.rc-8vvpg" in namespace "gc-7381"
Mar 30 13:23:42.695: INFO: Deleting pod "simpletest.rc-9tf5n" in namespace "gc-7381"
Mar 30 13:23:42.700: INFO: Deleting pod "simpletest.rc-b5jdt" in namespace "gc-7381"
Mar 30 13:23:42.705: INFO: Deleting pod "simpletest.rc-bf4t2" in namespace "gc-7381"
Mar 30 13:23:42.710: INFO: Deleting pod "simpletest.rc-bmhll" in namespace "gc-7381"
Mar 30 13:23:42.714: INFO: Deleting pod "simpletest.rc-bmqvg" in namespace "gc-7381"
Mar 30 13:23:42.719: INFO: Deleting pod "simpletest.rc-bn8rk" in namespace "gc-7381"
Mar 30 13:23:42.724: INFO: Deleting pod "simpletest.rc-bwtsj" in namespace "gc-7381"
Mar 30 13:23:42.731: INFO: Deleting pod "simpletest.rc-bzv4z" in namespace "gc-7381"
Mar 30 13:23:42.735: INFO: Deleting pod "simpletest.rc-cbl76" in namespace "gc-7381"
Mar 30 13:23:42.740: INFO: Deleting pod "simpletest.rc-cdrfw" in namespace "gc-7381"
Mar 30 13:23:42.745: INFO: Deleting pod "simpletest.rc-ctzzw" in namespace "gc-7381"
Mar 30 13:23:42.749: INFO: Deleting pod "simpletest.rc-czxrg" in namespace "gc-7381"
Mar 30 13:23:42.757: INFO: Deleting pod "simpletest.rc-dtppj" in namespace "gc-7381"
Mar 30 13:23:42.761: INFO: Deleting pod "simpletest.rc-f5jtj" in namespace "gc-7381"
Mar 30 13:23:42.765: INFO: Deleting pod "simpletest.rc-f6xdc" in namespace "gc-7381"
Mar 30 13:23:42.770: INFO: Deleting pod "simpletest.rc-fzxtt" in namespace "gc-7381"
Mar 30 13:23:42.773: INFO: Deleting pod "simpletest.rc-gq7s7" in namespace "gc-7381"
Mar 30 13:23:42.778: INFO: Deleting pod "simpletest.rc-gsv77" in namespace "gc-7381"
Mar 30 13:23:42.783: INFO: Deleting pod "simpletest.rc-hgpvn" in namespace "gc-7381"
Mar 30 13:23:42.787: INFO: Deleting pod "simpletest.rc-hk68v" in namespace "gc-7381"
Mar 30 13:23:42.791: INFO: Deleting pod "simpletest.rc-hq5xd" in namespace "gc-7381"
Mar 30 13:23:42.796: INFO: Deleting pod "simpletest.rc-jqqkp" in namespace "gc-7381"
Mar 30 13:23:42.802: INFO: Deleting pod "simpletest.rc-k5vtc" in namespace "gc-7381"
Mar 30 13:23:42.806: INFO: Deleting pod "simpletest.rc-kcwsl" in namespace "gc-7381"
Mar 30 13:23:42.809: INFO: Deleting pod "simpletest.rc-kffbm" in namespace "gc-7381"
Mar 30 13:23:42.815: INFO: Deleting pod "simpletest.rc-kflqh" in namespace "gc-7381"
Mar 30 13:23:42.819: INFO: Deleting pod "simpletest.rc-kr6t5" in namespace "gc-7381"
Mar 30 13:23:42.867: INFO: Deleting pod "simpletest.rc-kwchq" in namespace "gc-7381"
Mar 30 13:23:42.917: INFO: Deleting pod "simpletest.rc-l7wrr" in namespace "gc-7381"
Mar 30 13:23:42.968: INFO: Deleting pod "simpletest.rc-mc4dl" in namespace "gc-7381"
Mar 30 13:23:43.019: INFO: Deleting pod "simpletest.rc-mk5sn" in namespace "gc-7381"
Mar 30 13:23:43.067: INFO: Deleting pod "simpletest.rc-mlqhb" in namespace "gc-7381"
Mar 30 13:23:43.119: INFO: Deleting pod "simpletest.rc-mwhth" in namespace "gc-7381"
Mar 30 13:23:43.168: INFO: Deleting pod "simpletest.rc-nvgcz" in namespace "gc-7381"
Mar 30 13:23:43.218: INFO: Deleting pod "simpletest.rc-p8gj4" in namespace "gc-7381"
Mar 30 13:23:43.268: INFO: Deleting pod "simpletest.rc-pmtqm" in namespace "gc-7381"
Mar 30 13:23:43.319: INFO: Deleting pod "simpletest.rc-pzs9n" in namespace "gc-7381"
Mar 30 13:23:43.369: INFO: Deleting pod "simpletest.rc-r4ld7" in namespace "gc-7381"
Mar 30 13:23:43.418: INFO: Deleting pod "simpletest.rc-r67mb" in namespace "gc-7381"
Mar 30 13:23:43.468: INFO: Deleting pod "simpletest.rc-rjmjm" in namespace "gc-7381"
Mar 30 13:23:43.517: INFO: Deleting pod "simpletest.rc-rlqvw" in namespace "gc-7381"
Mar 30 13:23:43.567: INFO: Deleting pod "simpletest.rc-rrk9d" in namespace "gc-7381"
Mar 30 13:23:43.619: INFO: Deleting pod "simpletest.rc-rzhnz" in namespace "gc-7381"
Mar 30 13:23:43.669: INFO: Deleting pod "simpletest.rc-s2vkm" in namespace "gc-7381"
Mar 30 13:23:43.719: INFO: Deleting pod "simpletest.rc-sb2sc" in namespace "gc-7381"
Mar 30 13:23:43.768: INFO: Deleting pod "simpletest.rc-sf82d" in namespace "gc-7381"
Mar 30 13:23:43.819: INFO: Deleting pod "simpletest.rc-sqxpp" in namespace "gc-7381"
Mar 30 13:23:43.868: INFO: Deleting pod "simpletest.rc-stkc8" in namespace "gc-7381"
Mar 30 13:23:43.918: INFO: Deleting pod "simpletest.rc-szfdt" in namespace "gc-7381"
Mar 30 13:23:43.967: INFO: Deleting pod "simpletest.rc-tbdvh" in namespace "gc-7381"
Mar 30 13:23:44.018: INFO: Deleting pod "simpletest.rc-tfppf" in namespace "gc-7381"
Mar 30 13:23:44.068: INFO: Deleting pod "simpletest.rc-tqz5n" in namespace "gc-7381"
Mar 30 13:23:44.118: INFO: Deleting pod "simpletest.rc-v6w6w" in namespace "gc-7381"
Mar 30 13:23:44.168: INFO: Deleting pod "simpletest.rc-vdlcf" in namespace "gc-7381"
Mar 30 13:23:44.217: INFO: Deleting pod "simpletest.rc-vj8r9" in namespace "gc-7381"
Mar 30 13:23:44.267: INFO: Deleting pod "simpletest.rc-vpjps" in namespace "gc-7381"
Mar 30 13:23:44.317: INFO: Deleting pod "simpletest.rc-vtm6h" in namespace "gc-7381"
Mar 30 13:23:44.369: INFO: Deleting pod "simpletest.rc-vtmhl" in namespace "gc-7381"
Mar 30 13:23:44.419: INFO: Deleting pod "simpletest.rc-vx79l" in namespace "gc-7381"
Mar 30 13:23:44.469: INFO: Deleting pod "simpletest.rc-wkd69" in namespace "gc-7381"
Mar 30 13:23:44.518: INFO: Deleting pod "simpletest.rc-wm5km" in namespace "gc-7381"
Mar 30 13:23:44.570: INFO: Deleting pod "simpletest.rc-x7v4q" in namespace "gc-7381"
Mar 30 13:23:44.618: INFO: Deleting pod "simpletest.rc-xmz27" in namespace "gc-7381"
Mar 30 13:23:44.668: INFO: Deleting pod "simpletest.rc-xsd9x" in namespace "gc-7381"
Mar 30 13:23:44.718: INFO: Deleting pod "simpletest.rc-xtmjh" in namespace "gc-7381"
Mar 30 13:23:44.767: INFO: Deleting pod "simpletest.rc-z4dwp" in namespace "gc-7381"
Mar 30 13:23:44.817: INFO: Deleting pod "simpletest.rc-zcv8g" in namespace "gc-7381"
Mar 30 13:23:44.868: INFO: Deleting pod "simpletest.rc-zggtg" in namespace "gc-7381"
Mar 30 13:23:44.918: INFO: Deleting pod "simpletest.rc-zkdvq" in namespace "gc-7381"
Mar 30 13:23:44.967: INFO: Deleting pod "simpletest.rc-zkzf7" in namespace "gc-7381"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 30 13:23:45.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7381" for this suite. 03/30/23 13:23:45.067
------------------------------
â€¢ [SLOW TEST] [42.626 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:23:02.491
    Mar 30 13:23:02.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename gc 03/30/23 13:23:02.492
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:23:02.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:23:02.499
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 03/30/23 13:23:02.503
    STEP: delete the rc 03/30/23 13:23:07.508
    STEP: wait for the rc to be deleted 03/30/23 13:23:07.51
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/30/23 13:23:12.513
    STEP: Gathering metrics 03/30/23 13:23:42.519
    Mar 30 13:23:42.534: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cn-hongkong.192.168.0.252" in namespace "kube-system" to be "running and ready"
    Mar 30 13:23:42.535: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252": Phase="Running", Reason="", readiness=true. Elapsed: 1.380926ms
    Mar 30 13:23:42.535: INFO: The phase of Pod kube-controller-manager-cn-hongkong.192.168.0.252 is Running (Ready = true)
    Mar 30 13:23:42.535: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252" satisfied condition "running and ready"
    Mar 30 13:23:42.572: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 30 13:23:42.572: INFO: Deleting pod "simpletest.rc-22qlt" in namespace "gc-7381"
    Mar 30 13:23:42.577: INFO: Deleting pod "simpletest.rc-24h9m" in namespace "gc-7381"
    Mar 30 13:23:42.583: INFO: Deleting pod "simpletest.rc-2lcsn" in namespace "gc-7381"
    Mar 30 13:23:42.587: INFO: Deleting pod "simpletest.rc-2mcmr" in namespace "gc-7381"
    Mar 30 13:23:42.591: INFO: Deleting pod "simpletest.rc-4dxbt" in namespace "gc-7381"
    Mar 30 13:23:42.595: INFO: Deleting pod "simpletest.rc-4ktft" in namespace "gc-7381"
    Mar 30 13:23:42.599: INFO: Deleting pod "simpletest.rc-4mnxg" in namespace "gc-7381"
    Mar 30 13:23:42.604: INFO: Deleting pod "simpletest.rc-4tkcm" in namespace "gc-7381"
    Mar 30 13:23:42.608: INFO: Deleting pod "simpletest.rc-59f8m" in namespace "gc-7381"
    Mar 30 13:23:42.612: INFO: Deleting pod "simpletest.rc-5qtmh" in namespace "gc-7381"
    Mar 30 13:23:42.616: INFO: Deleting pod "simpletest.rc-5ww87" in namespace "gc-7381"
    Mar 30 13:23:42.620: INFO: Deleting pod "simpletest.rc-68bgl" in namespace "gc-7381"
    Mar 30 13:23:42.624: INFO: Deleting pod "simpletest.rc-6bb7g" in namespace "gc-7381"
    Mar 30 13:23:42.629: INFO: Deleting pod "simpletest.rc-6gcjr" in namespace "gc-7381"
    Mar 30 13:23:42.633: INFO: Deleting pod "simpletest.rc-6gkbq" in namespace "gc-7381"
    Mar 30 13:23:42.640: INFO: Deleting pod "simpletest.rc-6pnxw" in namespace "gc-7381"
    Mar 30 13:23:42.644: INFO: Deleting pod "simpletest.rc-7brpl" in namespace "gc-7381"
    Mar 30 13:23:42.649: INFO: Deleting pod "simpletest.rc-7h6rt" in namespace "gc-7381"
    Mar 30 13:23:42.654: INFO: Deleting pod "simpletest.rc-7spbn" in namespace "gc-7381"
    Mar 30 13:23:42.659: INFO: Deleting pod "simpletest.rc-7w545" in namespace "gc-7381"
    Mar 30 13:23:42.666: INFO: Deleting pod "simpletest.rc-8bkm4" in namespace "gc-7381"
    Mar 30 13:23:42.670: INFO: Deleting pod "simpletest.rc-8cv2k" in namespace "gc-7381"
    Mar 30 13:23:42.676: INFO: Deleting pod "simpletest.rc-8f87f" in namespace "gc-7381"
    Mar 30 13:23:42.681: INFO: Deleting pod "simpletest.rc-8fkz9" in namespace "gc-7381"
    Mar 30 13:23:42.685: INFO: Deleting pod "simpletest.rc-8pwvh" in namespace "gc-7381"
    Mar 30 13:23:42.690: INFO: Deleting pod "simpletest.rc-8vvpg" in namespace "gc-7381"
    Mar 30 13:23:42.695: INFO: Deleting pod "simpletest.rc-9tf5n" in namespace "gc-7381"
    Mar 30 13:23:42.700: INFO: Deleting pod "simpletest.rc-b5jdt" in namespace "gc-7381"
    Mar 30 13:23:42.705: INFO: Deleting pod "simpletest.rc-bf4t2" in namespace "gc-7381"
    Mar 30 13:23:42.710: INFO: Deleting pod "simpletest.rc-bmhll" in namespace "gc-7381"
    Mar 30 13:23:42.714: INFO: Deleting pod "simpletest.rc-bmqvg" in namespace "gc-7381"
    Mar 30 13:23:42.719: INFO: Deleting pod "simpletest.rc-bn8rk" in namespace "gc-7381"
    Mar 30 13:23:42.724: INFO: Deleting pod "simpletest.rc-bwtsj" in namespace "gc-7381"
    Mar 30 13:23:42.731: INFO: Deleting pod "simpletest.rc-bzv4z" in namespace "gc-7381"
    Mar 30 13:23:42.735: INFO: Deleting pod "simpletest.rc-cbl76" in namespace "gc-7381"
    Mar 30 13:23:42.740: INFO: Deleting pod "simpletest.rc-cdrfw" in namespace "gc-7381"
    Mar 30 13:23:42.745: INFO: Deleting pod "simpletest.rc-ctzzw" in namespace "gc-7381"
    Mar 30 13:23:42.749: INFO: Deleting pod "simpletest.rc-czxrg" in namespace "gc-7381"
    Mar 30 13:23:42.757: INFO: Deleting pod "simpletest.rc-dtppj" in namespace "gc-7381"
    Mar 30 13:23:42.761: INFO: Deleting pod "simpletest.rc-f5jtj" in namespace "gc-7381"
    Mar 30 13:23:42.765: INFO: Deleting pod "simpletest.rc-f6xdc" in namespace "gc-7381"
    Mar 30 13:23:42.770: INFO: Deleting pod "simpletest.rc-fzxtt" in namespace "gc-7381"
    Mar 30 13:23:42.773: INFO: Deleting pod "simpletest.rc-gq7s7" in namespace "gc-7381"
    Mar 30 13:23:42.778: INFO: Deleting pod "simpletest.rc-gsv77" in namespace "gc-7381"
    Mar 30 13:23:42.783: INFO: Deleting pod "simpletest.rc-hgpvn" in namespace "gc-7381"
    Mar 30 13:23:42.787: INFO: Deleting pod "simpletest.rc-hk68v" in namespace "gc-7381"
    Mar 30 13:23:42.791: INFO: Deleting pod "simpletest.rc-hq5xd" in namespace "gc-7381"
    Mar 30 13:23:42.796: INFO: Deleting pod "simpletest.rc-jqqkp" in namespace "gc-7381"
    Mar 30 13:23:42.802: INFO: Deleting pod "simpletest.rc-k5vtc" in namespace "gc-7381"
    Mar 30 13:23:42.806: INFO: Deleting pod "simpletest.rc-kcwsl" in namespace "gc-7381"
    Mar 30 13:23:42.809: INFO: Deleting pod "simpletest.rc-kffbm" in namespace "gc-7381"
    Mar 30 13:23:42.815: INFO: Deleting pod "simpletest.rc-kflqh" in namespace "gc-7381"
    Mar 30 13:23:42.819: INFO: Deleting pod "simpletest.rc-kr6t5" in namespace "gc-7381"
    Mar 30 13:23:42.867: INFO: Deleting pod "simpletest.rc-kwchq" in namespace "gc-7381"
    Mar 30 13:23:42.917: INFO: Deleting pod "simpletest.rc-l7wrr" in namespace "gc-7381"
    Mar 30 13:23:42.968: INFO: Deleting pod "simpletest.rc-mc4dl" in namespace "gc-7381"
    Mar 30 13:23:43.019: INFO: Deleting pod "simpletest.rc-mk5sn" in namespace "gc-7381"
    Mar 30 13:23:43.067: INFO: Deleting pod "simpletest.rc-mlqhb" in namespace "gc-7381"
    Mar 30 13:23:43.119: INFO: Deleting pod "simpletest.rc-mwhth" in namespace "gc-7381"
    Mar 30 13:23:43.168: INFO: Deleting pod "simpletest.rc-nvgcz" in namespace "gc-7381"
    Mar 30 13:23:43.218: INFO: Deleting pod "simpletest.rc-p8gj4" in namespace "gc-7381"
    Mar 30 13:23:43.268: INFO: Deleting pod "simpletest.rc-pmtqm" in namespace "gc-7381"
    Mar 30 13:23:43.319: INFO: Deleting pod "simpletest.rc-pzs9n" in namespace "gc-7381"
    Mar 30 13:23:43.369: INFO: Deleting pod "simpletest.rc-r4ld7" in namespace "gc-7381"
    Mar 30 13:23:43.418: INFO: Deleting pod "simpletest.rc-r67mb" in namespace "gc-7381"
    Mar 30 13:23:43.468: INFO: Deleting pod "simpletest.rc-rjmjm" in namespace "gc-7381"
    Mar 30 13:23:43.517: INFO: Deleting pod "simpletest.rc-rlqvw" in namespace "gc-7381"
    Mar 30 13:23:43.567: INFO: Deleting pod "simpletest.rc-rrk9d" in namespace "gc-7381"
    Mar 30 13:23:43.619: INFO: Deleting pod "simpletest.rc-rzhnz" in namespace "gc-7381"
    Mar 30 13:23:43.669: INFO: Deleting pod "simpletest.rc-s2vkm" in namespace "gc-7381"
    Mar 30 13:23:43.719: INFO: Deleting pod "simpletest.rc-sb2sc" in namespace "gc-7381"
    Mar 30 13:23:43.768: INFO: Deleting pod "simpletest.rc-sf82d" in namespace "gc-7381"
    Mar 30 13:23:43.819: INFO: Deleting pod "simpletest.rc-sqxpp" in namespace "gc-7381"
    Mar 30 13:23:43.868: INFO: Deleting pod "simpletest.rc-stkc8" in namespace "gc-7381"
    Mar 30 13:23:43.918: INFO: Deleting pod "simpletest.rc-szfdt" in namespace "gc-7381"
    Mar 30 13:23:43.967: INFO: Deleting pod "simpletest.rc-tbdvh" in namespace "gc-7381"
    Mar 30 13:23:44.018: INFO: Deleting pod "simpletest.rc-tfppf" in namespace "gc-7381"
    Mar 30 13:23:44.068: INFO: Deleting pod "simpletest.rc-tqz5n" in namespace "gc-7381"
    Mar 30 13:23:44.118: INFO: Deleting pod "simpletest.rc-v6w6w" in namespace "gc-7381"
    Mar 30 13:23:44.168: INFO: Deleting pod "simpletest.rc-vdlcf" in namespace "gc-7381"
    Mar 30 13:23:44.217: INFO: Deleting pod "simpletest.rc-vj8r9" in namespace "gc-7381"
    Mar 30 13:23:44.267: INFO: Deleting pod "simpletest.rc-vpjps" in namespace "gc-7381"
    Mar 30 13:23:44.317: INFO: Deleting pod "simpletest.rc-vtm6h" in namespace "gc-7381"
    Mar 30 13:23:44.369: INFO: Deleting pod "simpletest.rc-vtmhl" in namespace "gc-7381"
    Mar 30 13:23:44.419: INFO: Deleting pod "simpletest.rc-vx79l" in namespace "gc-7381"
    Mar 30 13:23:44.469: INFO: Deleting pod "simpletest.rc-wkd69" in namespace "gc-7381"
    Mar 30 13:23:44.518: INFO: Deleting pod "simpletest.rc-wm5km" in namespace "gc-7381"
    Mar 30 13:23:44.570: INFO: Deleting pod "simpletest.rc-x7v4q" in namespace "gc-7381"
    Mar 30 13:23:44.618: INFO: Deleting pod "simpletest.rc-xmz27" in namespace "gc-7381"
    Mar 30 13:23:44.668: INFO: Deleting pod "simpletest.rc-xsd9x" in namespace "gc-7381"
    Mar 30 13:23:44.718: INFO: Deleting pod "simpletest.rc-xtmjh" in namespace "gc-7381"
    Mar 30 13:23:44.767: INFO: Deleting pod "simpletest.rc-z4dwp" in namespace "gc-7381"
    Mar 30 13:23:44.817: INFO: Deleting pod "simpletest.rc-zcv8g" in namespace "gc-7381"
    Mar 30 13:23:44.868: INFO: Deleting pod "simpletest.rc-zggtg" in namespace "gc-7381"
    Mar 30 13:23:44.918: INFO: Deleting pod "simpletest.rc-zkdvq" in namespace "gc-7381"
    Mar 30 13:23:44.967: INFO: Deleting pod "simpletest.rc-zkzf7" in namespace "gc-7381"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:23:45.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7381" for this suite. 03/30/23 13:23:45.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:23:45.119
Mar 30 13:23:45.119: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 13:23:45.119
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:23:45.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:23:45.128
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 03/30/23 13:23:45.129
Mar 30 13:23:45.133: INFO: Waiting up to 5m0s for pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036" in namespace "downward-api-1411" to be "running and ready"
Mar 30 13:23:45.134: INFO: Pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643001ms
Mar 30 13:23:45.134: INFO: The phase of Pod labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:23:47.137: INFO: Pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004228445s
Mar 30 13:23:47.137: INFO: The phase of Pod labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:23:49.137: INFO: Pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004144236s
Mar 30 13:23:49.137: INFO: The phase of Pod labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:23:51.137: INFO: Pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004689086s
Mar 30 13:23:51.137: INFO: The phase of Pod labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:23:53.137: INFO: Pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036": Phase="Running", Reason="", readiness=true. Elapsed: 8.004598364s
Mar 30 13:23:53.137: INFO: The phase of Pod labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036 is Running (Ready = true)
Mar 30 13:23:53.137: INFO: Pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036" satisfied condition "running and ready"
Mar 30 13:23:53.650: INFO: Successfully updated pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 30 13:23:55.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1411" for this suite. 03/30/23 13:23:55.66
------------------------------
â€¢ [SLOW TEST] [10.544 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:23:45.119
    Mar 30 13:23:45.119: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 13:23:45.119
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:23:45.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:23:45.128
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 03/30/23 13:23:45.129
    Mar 30 13:23:45.133: INFO: Waiting up to 5m0s for pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036" in namespace "downward-api-1411" to be "running and ready"
    Mar 30 13:23:45.134: INFO: Pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643001ms
    Mar 30 13:23:45.134: INFO: The phase of Pod labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:23:47.137: INFO: Pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004228445s
    Mar 30 13:23:47.137: INFO: The phase of Pod labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:23:49.137: INFO: Pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004144236s
    Mar 30 13:23:49.137: INFO: The phase of Pod labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:23:51.137: INFO: Pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004689086s
    Mar 30 13:23:51.137: INFO: The phase of Pod labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:23:53.137: INFO: Pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036": Phase="Running", Reason="", readiness=true. Elapsed: 8.004598364s
    Mar 30 13:23:53.137: INFO: The phase of Pod labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036 is Running (Ready = true)
    Mar 30 13:23:53.137: INFO: Pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036" satisfied condition "running and ready"
    Mar 30 13:23:53.650: INFO: Successfully updated pod "labelsupdate1aa566f1-ede5-43e1-90a9-01e365735036"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:23:55.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1411" for this suite. 03/30/23 13:23:55.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:23:55.664
Mar 30 13:23:55.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:23:55.665
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:23:55.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:23:55.673
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-54651543-10db-44d5-b3a9-d00e5fc61280 03/30/23 13:23:55.674
STEP: Creating a pod to test consume secrets 03/30/23 13:23:55.676
Mar 30 13:23:55.680: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b" in namespace "projected-5179" to be "Succeeded or Failed"
Mar 30 13:23:55.681: INFO: Pod "pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.416102ms
Mar 30 13:23:57.684: INFO: Pod "pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004261345s
Mar 30 13:23:59.684: INFO: Pod "pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004461086s
STEP: Saw pod success 03/30/23 13:23:59.684
Mar 30 13:23:59.684: INFO: Pod "pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b" satisfied condition "Succeeded or Failed"
Mar 30 13:23:59.686: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b container projected-secret-volume-test: <nil>
STEP: delete the pod 03/30/23 13:23:59.689
Mar 30 13:23:59.694: INFO: Waiting for pod pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b to disappear
Mar 30 13:23:59.695: INFO: Pod pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 30 13:23:59.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5179" for this suite. 03/30/23 13:23:59.697
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:23:55.664
    Mar 30 13:23:55.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:23:55.665
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:23:55.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:23:55.673
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-54651543-10db-44d5-b3a9-d00e5fc61280 03/30/23 13:23:55.674
    STEP: Creating a pod to test consume secrets 03/30/23 13:23:55.676
    Mar 30 13:23:55.680: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b" in namespace "projected-5179" to be "Succeeded or Failed"
    Mar 30 13:23:55.681: INFO: Pod "pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.416102ms
    Mar 30 13:23:57.684: INFO: Pod "pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004261345s
    Mar 30 13:23:59.684: INFO: Pod "pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004461086s
    STEP: Saw pod success 03/30/23 13:23:59.684
    Mar 30 13:23:59.684: INFO: Pod "pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b" satisfied condition "Succeeded or Failed"
    Mar 30 13:23:59.686: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 13:23:59.689
    Mar 30 13:23:59.694: INFO: Waiting for pod pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b to disappear
    Mar 30 13:23:59.695: INFO: Pod pod-projected-secrets-18ff522c-9692-46a4-8ea1-0b9f35fdad7b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:23:59.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5179" for this suite. 03/30/23 13:23:59.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:23:59.7
Mar 30 13:23:59.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename svcaccounts 03/30/23 13:23:59.701
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:23:59.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:23:59.708
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 03/30/23 13:23:59.71
STEP: watching for the ServiceAccount to be added 03/30/23 13:23:59.713
STEP: patching the ServiceAccount 03/30/23 13:23:59.714
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/30/23 13:23:59.716
STEP: deleting the ServiceAccount 03/30/23 13:23:59.718
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 30 13:23:59.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1677" for this suite. 03/30/23 13:23:59.724
------------------------------
â€¢ [0.026 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:23:59.7
    Mar 30 13:23:59.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename svcaccounts 03/30/23 13:23:59.701
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:23:59.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:23:59.708
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 03/30/23 13:23:59.71
    STEP: watching for the ServiceAccount to be added 03/30/23 13:23:59.713
    STEP: patching the ServiceAccount 03/30/23 13:23:59.714
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/30/23 13:23:59.716
    STEP: deleting the ServiceAccount 03/30/23 13:23:59.718
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:23:59.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1677" for this suite. 03/30/23 13:23:59.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:23:59.728
Mar 30 13:23:59.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename endpointslice 03/30/23 13:23:59.729
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:23:59.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:23:59.736
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:03.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2056" for this suite. 03/30/23 13:24:03.764
------------------------------
â€¢ [4.038 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:23:59.728
    Mar 30 13:23:59.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename endpointslice 03/30/23 13:23:59.729
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:23:59.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:23:59.736
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:03.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2056" for this suite. 03/30/23 13:24:03.764
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:03.767
Mar 30 13:24:03.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename replicaset 03/30/23 13:24:03.767
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:03.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:03.776
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/30/23 13:24:03.777
Mar 30 13:24:03.781: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9577" to be "running and ready"
Mar 30 13:24:03.782: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.329988ms
Mar 30 13:24:03.782: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:24:05.785: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.004098803s
Mar 30 13:24:05.785: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Mar 30 13:24:05.785: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 03/30/23 13:24:05.787
STEP: Then the orphan pod is adopted 03/30/23 13:24:05.789
STEP: When the matched label of one of its pods change 03/30/23 13:24:06.793
Mar 30 13:24:06.795: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 03/30/23 13:24:06.8
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:07.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9577" for this suite. 03/30/23 13:24:07.807
------------------------------
â€¢ [4.043 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:03.767
    Mar 30 13:24:03.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename replicaset 03/30/23 13:24:03.767
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:03.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:03.776
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/30/23 13:24:03.777
    Mar 30 13:24:03.781: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9577" to be "running and ready"
    Mar 30 13:24:03.782: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.329988ms
    Mar 30 13:24:03.782: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:24:05.785: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.004098803s
    Mar 30 13:24:05.785: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Mar 30 13:24:05.785: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 03/30/23 13:24:05.787
    STEP: Then the orphan pod is adopted 03/30/23 13:24:05.789
    STEP: When the matched label of one of its pods change 03/30/23 13:24:06.793
    Mar 30 13:24:06.795: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/30/23 13:24:06.8
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:07.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9577" for this suite. 03/30/23 13:24:07.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:07.81
Mar 30 13:24:07.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 13:24:07.811
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:07.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:07.819
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-fb08f5f0-c933-4a9a-851c-aa16f0d53c86 03/30/23 13:24:07.821
STEP: Creating a pod to test consume secrets 03/30/23 13:24:07.823
Mar 30 13:24:07.826: INFO: Waiting up to 5m0s for pod "pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4" in namespace "secrets-6693" to be "Succeeded or Failed"
Mar 30 13:24:07.827: INFO: Pod "pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.443997ms
Mar 30 13:24:09.829: INFO: Pod "pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003422436s
Mar 30 13:24:11.829: INFO: Pod "pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003743352s
STEP: Saw pod success 03/30/23 13:24:11.83
Mar 30 13:24:11.830: INFO: Pod "pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4" satisfied condition "Succeeded or Failed"
Mar 30 13:24:11.831: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4 container secret-volume-test: <nil>
STEP: delete the pod 03/30/23 13:24:11.834
Mar 30 13:24:11.839: INFO: Waiting for pod pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4 to disappear
Mar 30 13:24:11.841: INFO: Pod pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:11.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6693" for this suite. 03/30/23 13:24:11.843
------------------------------
â€¢ [4.035 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:07.81
    Mar 30 13:24:07.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 13:24:07.811
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:07.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:07.819
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-fb08f5f0-c933-4a9a-851c-aa16f0d53c86 03/30/23 13:24:07.821
    STEP: Creating a pod to test consume secrets 03/30/23 13:24:07.823
    Mar 30 13:24:07.826: INFO: Waiting up to 5m0s for pod "pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4" in namespace "secrets-6693" to be "Succeeded or Failed"
    Mar 30 13:24:07.827: INFO: Pod "pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.443997ms
    Mar 30 13:24:09.829: INFO: Pod "pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003422436s
    Mar 30 13:24:11.829: INFO: Pod "pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003743352s
    STEP: Saw pod success 03/30/23 13:24:11.83
    Mar 30 13:24:11.830: INFO: Pod "pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4" satisfied condition "Succeeded or Failed"
    Mar 30 13:24:11.831: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4 container secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 13:24:11.834
    Mar 30 13:24:11.839: INFO: Waiting for pod pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4 to disappear
    Mar 30 13:24:11.841: INFO: Pod pod-secrets-e174be0a-b16b-4049-8d3f-849c9dff98b4 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:11.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6693" for this suite. 03/30/23 13:24:11.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:11.846
Mar 30 13:24:11.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename ingressclass 03/30/23 13:24:11.847
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:11.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:11.854
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 03/30/23 13:24:11.856
STEP: getting /apis/networking.k8s.io 03/30/23 13:24:11.857
STEP: getting /apis/networking.k8s.iov1 03/30/23 13:24:11.858
STEP: creating 03/30/23 13:24:11.859
STEP: getting 03/30/23 13:24:11.864
STEP: listing 03/30/23 13:24:11.865
STEP: watching 03/30/23 13:24:11.867
Mar 30 13:24:11.867: INFO: starting watch
STEP: patching 03/30/23 13:24:11.867
STEP: updating 03/30/23 13:24:11.869
Mar 30 13:24:11.871: INFO: waiting for watch events with expected annotations
Mar 30 13:24:11.871: INFO: saw patched and updated annotations
STEP: deleting 03/30/23 13:24:11.871
STEP: deleting a collection 03/30/23 13:24:11.876
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:11.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-2957" for this suite. 03/30/23 13:24:11.884
------------------------------
â€¢ [0.040 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:11.846
    Mar 30 13:24:11.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename ingressclass 03/30/23 13:24:11.847
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:11.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:11.854
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 03/30/23 13:24:11.856
    STEP: getting /apis/networking.k8s.io 03/30/23 13:24:11.857
    STEP: getting /apis/networking.k8s.iov1 03/30/23 13:24:11.858
    STEP: creating 03/30/23 13:24:11.859
    STEP: getting 03/30/23 13:24:11.864
    STEP: listing 03/30/23 13:24:11.865
    STEP: watching 03/30/23 13:24:11.867
    Mar 30 13:24:11.867: INFO: starting watch
    STEP: patching 03/30/23 13:24:11.867
    STEP: updating 03/30/23 13:24:11.869
    Mar 30 13:24:11.871: INFO: waiting for watch events with expected annotations
    Mar 30 13:24:11.871: INFO: saw patched and updated annotations
    STEP: deleting 03/30/23 13:24:11.871
    STEP: deleting a collection 03/30/23 13:24:11.876
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:11.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-2957" for this suite. 03/30/23 13:24:11.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:11.887
Mar 30 13:24:11.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 13:24:11.887
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:11.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:11.895
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 13:24:11.903
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:24:12.453
STEP: Deploying the webhook pod 03/30/23 13:24:12.456
STEP: Wait for the deployment to be ready 03/30/23 13:24:12.462
Mar 30 13:24:12.465: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/30/23 13:24:14.472
STEP: Verifying the service has paired with the endpoint 03/30/23 13:24:14.477
Mar 30 13:24:15.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/30/23 13:24:15.479
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/30/23 13:24:15.487
STEP: Creating a dummy validating-webhook-configuration object 03/30/23 13:24:15.495
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/30/23 13:24:15.499
STEP: Creating a dummy mutating-webhook-configuration object 03/30/23 13:24:15.501
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/30/23 13:24:15.505
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:15.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7545" for this suite. 03/30/23 13:24:15.528
STEP: Destroying namespace "webhook-7545-markers" for this suite. 03/30/23 13:24:15.531
------------------------------
â€¢ [3.647 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:11.887
    Mar 30 13:24:11.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 13:24:11.887
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:11.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:11.895
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 13:24:11.903
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:24:12.453
    STEP: Deploying the webhook pod 03/30/23 13:24:12.456
    STEP: Wait for the deployment to be ready 03/30/23 13:24:12.462
    Mar 30 13:24:12.465: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/30/23 13:24:14.472
    STEP: Verifying the service has paired with the endpoint 03/30/23 13:24:14.477
    Mar 30 13:24:15.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/30/23 13:24:15.479
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/30/23 13:24:15.487
    STEP: Creating a dummy validating-webhook-configuration object 03/30/23 13:24:15.495
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/30/23 13:24:15.499
    STEP: Creating a dummy mutating-webhook-configuration object 03/30/23 13:24:15.501
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/30/23 13:24:15.505
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:15.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7545" for this suite. 03/30/23 13:24:15.528
    STEP: Destroying namespace "webhook-7545-markers" for this suite. 03/30/23 13:24:15.531
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:15.534
Mar 30 13:24:15.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename daemonsets 03/30/23 13:24:15.535
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:15.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:15.544
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 03/30/23 13:24:15.558
STEP: Check that daemon pods launch on every node of the cluster. 03/30/23 13:24:15.561
Mar 30 13:24:15.563: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:15.563: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:15.563: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:15.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 13:24:15.565: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
Mar 30 13:24:16.568: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:16.568: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:16.568: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:16.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 30 13:24:16.569: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
Mar 30 13:24:17.568: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:17.569: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:17.569: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:17.570: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 30 13:24:17.570: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 03/30/23 13:24:17.572
Mar 30 13:24:17.574: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 03/30/23 13:24:17.574
Mar 30 13:24:17.578: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 03/30/23 13:24:17.578
Mar 30 13:24:17.579: INFO: Observed &DaemonSet event: ADDED
Mar 30 13:24:17.580: INFO: Observed &DaemonSet event: MODIFIED
Mar 30 13:24:17.580: INFO: Observed &DaemonSet event: MODIFIED
Mar 30 13:24:17.580: INFO: Observed &DaemonSet event: MODIFIED
Mar 30 13:24:17.580: INFO: Observed &DaemonSet event: MODIFIED
Mar 30 13:24:17.580: INFO: Observed &DaemonSet event: MODIFIED
Mar 30 13:24:17.580: INFO: Found daemon set daemon-set in namespace daemonsets-7030 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 30 13:24:17.580: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 03/30/23 13:24:17.58
STEP: watching for the daemon set status to be patched 03/30/23 13:24:17.583
Mar 30 13:24:17.584: INFO: Observed &DaemonSet event: ADDED
Mar 30 13:24:17.584: INFO: Observed &DaemonSet event: MODIFIED
Mar 30 13:24:17.584: INFO: Observed &DaemonSet event: MODIFIED
Mar 30 13:24:17.584: INFO: Observed &DaemonSet event: MODIFIED
Mar 30 13:24:17.584: INFO: Observed &DaemonSet event: MODIFIED
Mar 30 13:24:17.585: INFO: Observed &DaemonSet event: MODIFIED
Mar 30 13:24:17.585: INFO: Observed daemon set daemon-set in namespace daemonsets-7030 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 30 13:24:17.585: INFO: Observed &DaemonSet event: MODIFIED
Mar 30 13:24:17.585: INFO: Found daemon set daemon-set in namespace daemonsets-7030 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar 30 13:24:17.585: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/30/23 13:24:17.586
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7030, will wait for the garbage collector to delete the pods 03/30/23 13:24:17.586
Mar 30 13:24:17.641: INFO: Deleting DaemonSet.extensions daemon-set took: 2.706231ms
Mar 30 13:24:17.741: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.115091ms
Mar 30 13:24:19.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 13:24:19.643: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 30 13:24:19.645: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"62643"},"items":null}

Mar 30 13:24:19.646: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"62643"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:19.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7030" for this suite. 03/30/23 13:24:19.655
------------------------------
â€¢ [4.123 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:15.534
    Mar 30 13:24:15.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename daemonsets 03/30/23 13:24:15.535
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:15.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:15.544
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 03/30/23 13:24:15.558
    STEP: Check that daemon pods launch on every node of the cluster. 03/30/23 13:24:15.561
    Mar 30 13:24:15.563: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:15.563: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:15.563: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:15.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 13:24:15.565: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
    Mar 30 13:24:16.568: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:16.568: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:16.568: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:16.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 30 13:24:16.569: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
    Mar 30 13:24:17.568: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:17.569: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:17.569: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:17.570: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 30 13:24:17.570: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 03/30/23 13:24:17.572
    Mar 30 13:24:17.574: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 03/30/23 13:24:17.574
    Mar 30 13:24:17.578: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 03/30/23 13:24:17.578
    Mar 30 13:24:17.579: INFO: Observed &DaemonSet event: ADDED
    Mar 30 13:24:17.580: INFO: Observed &DaemonSet event: MODIFIED
    Mar 30 13:24:17.580: INFO: Observed &DaemonSet event: MODIFIED
    Mar 30 13:24:17.580: INFO: Observed &DaemonSet event: MODIFIED
    Mar 30 13:24:17.580: INFO: Observed &DaemonSet event: MODIFIED
    Mar 30 13:24:17.580: INFO: Observed &DaemonSet event: MODIFIED
    Mar 30 13:24:17.580: INFO: Found daemon set daemon-set in namespace daemonsets-7030 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 30 13:24:17.580: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 03/30/23 13:24:17.58
    STEP: watching for the daemon set status to be patched 03/30/23 13:24:17.583
    Mar 30 13:24:17.584: INFO: Observed &DaemonSet event: ADDED
    Mar 30 13:24:17.584: INFO: Observed &DaemonSet event: MODIFIED
    Mar 30 13:24:17.584: INFO: Observed &DaemonSet event: MODIFIED
    Mar 30 13:24:17.584: INFO: Observed &DaemonSet event: MODIFIED
    Mar 30 13:24:17.584: INFO: Observed &DaemonSet event: MODIFIED
    Mar 30 13:24:17.585: INFO: Observed &DaemonSet event: MODIFIED
    Mar 30 13:24:17.585: INFO: Observed daemon set daemon-set in namespace daemonsets-7030 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 30 13:24:17.585: INFO: Observed &DaemonSet event: MODIFIED
    Mar 30 13:24:17.585: INFO: Found daemon set daemon-set in namespace daemonsets-7030 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Mar 30 13:24:17.585: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/30/23 13:24:17.586
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7030, will wait for the garbage collector to delete the pods 03/30/23 13:24:17.586
    Mar 30 13:24:17.641: INFO: Deleting DaemonSet.extensions daemon-set took: 2.706231ms
    Mar 30 13:24:17.741: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.115091ms
    Mar 30 13:24:19.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 13:24:19.643: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 30 13:24:19.645: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"62643"},"items":null}

    Mar 30 13:24:19.646: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"62643"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:19.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7030" for this suite. 03/30/23 13:24:19.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:19.658
Mar 30 13:24:19.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename init-container 03/30/23 13:24:19.659
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:19.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:19.667
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 03/30/23 13:24:19.668
Mar 30 13:24:19.668: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:22.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7931" for this suite. 03/30/23 13:24:22.524
------------------------------
â€¢ [2.868 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:19.658
    Mar 30 13:24:19.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename init-container 03/30/23 13:24:19.659
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:19.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:19.667
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 03/30/23 13:24:19.668
    Mar 30 13:24:19.668: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:22.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7931" for this suite. 03/30/23 13:24:22.524
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:22.527
Mar 30 13:24:22.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 13:24:22.527
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:22.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:22.535
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-1860 03/30/23 13:24:22.536
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1860 to expose endpoints map[] 03/30/23 13:24:22.54
Mar 30 13:24:22.543: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Mar 30 13:24:23.547: INFO: successfully validated that service endpoint-test2 in namespace services-1860 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1860 03/30/23 13:24:23.547
Mar 30 13:24:23.551: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1860" to be "running and ready"
Mar 30 13:24:23.553: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457781ms
Mar 30 13:24:23.553: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:24:25.555: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003913587s
Mar 30 13:24:25.555: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 30 13:24:25.555: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1860 to expose endpoints map[pod1:[80]] 03/30/23 13:24:25.557
Mar 30 13:24:25.561: INFO: successfully validated that service endpoint-test2 in namespace services-1860 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 03/30/23 13:24:25.561
Mar 30 13:24:25.561: INFO: Creating new exec pod
Mar 30 13:24:25.564: INFO: Waiting up to 5m0s for pod "execpodp74f5" in namespace "services-1860" to be "running"
Mar 30 13:24:25.565: INFO: Pod "execpodp74f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.390167ms
Mar 30 13:24:27.567: INFO: Pod "execpodp74f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.003748974s
Mar 30 13:24:27.567: INFO: Pod "execpodp74f5" satisfied condition "running"
Mar 30 13:24:28.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1860 exec execpodp74f5 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 30 13:24:28.664: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 30 13:24:28.664: INFO: stdout: ""
Mar 30 13:24:28.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1860 exec execpodp74f5 -- /bin/sh -x -c nc -v -z -w 2 172.16.52.108 80'
Mar 30 13:24:28.755: INFO: stderr: "+ nc -v -z -w 2 172.16.52.108 80\nConnection to 172.16.52.108 80 port [tcp/http] succeeded!\n"
Mar 30 13:24:28.755: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-1860 03/30/23 13:24:28.755
Mar 30 13:24:28.758: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1860" to be "running and ready"
Mar 30 13:24:28.759: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.435794ms
Mar 30 13:24:28.759: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:24:30.762: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004073734s
Mar 30 13:24:30.762: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 30 13:24:30.762: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1860 to expose endpoints map[pod1:[80] pod2:[80]] 03/30/23 13:24:30.763
Mar 30 13:24:30.769: INFO: successfully validated that service endpoint-test2 in namespace services-1860 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 03/30/23 13:24:30.769
Mar 30 13:24:31.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1860 exec execpodp74f5 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 30 13:24:31.862: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 30 13:24:31.862: INFO: stdout: ""
Mar 30 13:24:31.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1860 exec execpodp74f5 -- /bin/sh -x -c nc -v -z -w 2 172.16.52.108 80'
Mar 30 13:24:31.950: INFO: stderr: "+ nc -v -z -w 2 172.16.52.108 80\nConnection to 172.16.52.108 80 port [tcp/http] succeeded!\n"
Mar 30 13:24:31.950: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-1860 03/30/23 13:24:31.95
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1860 to expose endpoints map[pod2:[80]] 03/30/23 13:24:31.955
Mar 30 13:24:31.961: INFO: successfully validated that service endpoint-test2 in namespace services-1860 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 03/30/23 13:24:31.961
Mar 30 13:24:32.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1860 exec execpodp74f5 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 30 13:24:33.056: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 30 13:24:33.056: INFO: stdout: ""
Mar 30 13:24:33.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1860 exec execpodp74f5 -- /bin/sh -x -c nc -v -z -w 2 172.16.52.108 80'
Mar 30 13:24:33.148: INFO: stderr: "+ nc -v -z -w 2 172.16.52.108 80\nConnection to 172.16.52.108 80 port [tcp/http] succeeded!\n"
Mar 30 13:24:33.148: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-1860 03/30/23 13:24:33.148
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1860 to expose endpoints map[] 03/30/23 13:24:33.154
Mar 30 13:24:33.158: INFO: successfully validated that service endpoint-test2 in namespace services-1860 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:33.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1860" for this suite. 03/30/23 13:24:33.168
------------------------------
â€¢ [SLOW TEST] [10.644 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:22.527
    Mar 30 13:24:22.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 13:24:22.527
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:22.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:22.535
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-1860 03/30/23 13:24:22.536
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1860 to expose endpoints map[] 03/30/23 13:24:22.54
    Mar 30 13:24:22.543: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Mar 30 13:24:23.547: INFO: successfully validated that service endpoint-test2 in namespace services-1860 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1860 03/30/23 13:24:23.547
    Mar 30 13:24:23.551: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1860" to be "running and ready"
    Mar 30 13:24:23.553: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457781ms
    Mar 30 13:24:23.553: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:24:25.555: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003913587s
    Mar 30 13:24:25.555: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 30 13:24:25.555: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1860 to expose endpoints map[pod1:[80]] 03/30/23 13:24:25.557
    Mar 30 13:24:25.561: INFO: successfully validated that service endpoint-test2 in namespace services-1860 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 03/30/23 13:24:25.561
    Mar 30 13:24:25.561: INFO: Creating new exec pod
    Mar 30 13:24:25.564: INFO: Waiting up to 5m0s for pod "execpodp74f5" in namespace "services-1860" to be "running"
    Mar 30 13:24:25.565: INFO: Pod "execpodp74f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.390167ms
    Mar 30 13:24:27.567: INFO: Pod "execpodp74f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.003748974s
    Mar 30 13:24:27.567: INFO: Pod "execpodp74f5" satisfied condition "running"
    Mar 30 13:24:28.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1860 exec execpodp74f5 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 30 13:24:28.664: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 30 13:24:28.664: INFO: stdout: ""
    Mar 30 13:24:28.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1860 exec execpodp74f5 -- /bin/sh -x -c nc -v -z -w 2 172.16.52.108 80'
    Mar 30 13:24:28.755: INFO: stderr: "+ nc -v -z -w 2 172.16.52.108 80\nConnection to 172.16.52.108 80 port [tcp/http] succeeded!\n"
    Mar 30 13:24:28.755: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-1860 03/30/23 13:24:28.755
    Mar 30 13:24:28.758: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1860" to be "running and ready"
    Mar 30 13:24:28.759: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.435794ms
    Mar 30 13:24:28.759: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:24:30.762: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004073734s
    Mar 30 13:24:30.762: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 30 13:24:30.762: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1860 to expose endpoints map[pod1:[80] pod2:[80]] 03/30/23 13:24:30.763
    Mar 30 13:24:30.769: INFO: successfully validated that service endpoint-test2 in namespace services-1860 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 03/30/23 13:24:30.769
    Mar 30 13:24:31.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1860 exec execpodp74f5 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 30 13:24:31.862: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 30 13:24:31.862: INFO: stdout: ""
    Mar 30 13:24:31.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1860 exec execpodp74f5 -- /bin/sh -x -c nc -v -z -w 2 172.16.52.108 80'
    Mar 30 13:24:31.950: INFO: stderr: "+ nc -v -z -w 2 172.16.52.108 80\nConnection to 172.16.52.108 80 port [tcp/http] succeeded!\n"
    Mar 30 13:24:31.950: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-1860 03/30/23 13:24:31.95
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1860 to expose endpoints map[pod2:[80]] 03/30/23 13:24:31.955
    Mar 30 13:24:31.961: INFO: successfully validated that service endpoint-test2 in namespace services-1860 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 03/30/23 13:24:31.961
    Mar 30 13:24:32.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1860 exec execpodp74f5 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 30 13:24:33.056: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 30 13:24:33.056: INFO: stdout: ""
    Mar 30 13:24:33.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1860 exec execpodp74f5 -- /bin/sh -x -c nc -v -z -w 2 172.16.52.108 80'
    Mar 30 13:24:33.148: INFO: stderr: "+ nc -v -z -w 2 172.16.52.108 80\nConnection to 172.16.52.108 80 port [tcp/http] succeeded!\n"
    Mar 30 13:24:33.148: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-1860 03/30/23 13:24:33.148
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1860 to expose endpoints map[] 03/30/23 13:24:33.154
    Mar 30 13:24:33.158: INFO: successfully validated that service endpoint-test2 in namespace services-1860 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:33.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1860" for this suite. 03/30/23 13:24:33.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:33.171
Mar 30 13:24:33.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:24:33.172
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:33.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:33.18
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-8837b213-faa8-44f2-9e23-ef36f1009117 03/30/23 13:24:33.182
STEP: Creating a pod to test consume configMaps 03/30/23 13:24:33.184
Mar 30 13:24:33.188: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732" in namespace "projected-7453" to be "Succeeded or Failed"
Mar 30 13:24:33.190: INFO: Pod "pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732": Phase="Pending", Reason="", readiness=false. Elapsed: 1.579925ms
Mar 30 13:24:35.192: INFO: Pod "pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00384397s
Mar 30 13:24:37.193: INFO: Pod "pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004305669s
STEP: Saw pod success 03/30/23 13:24:37.193
Mar 30 13:24:37.193: INFO: Pod "pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732" satisfied condition "Succeeded or Failed"
Mar 30 13:24:37.195: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732 container agnhost-container: <nil>
STEP: delete the pod 03/30/23 13:24:37.205
Mar 30 13:24:37.210: INFO: Waiting for pod pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732 to disappear
Mar 30 13:24:37.211: INFO: Pod pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:37.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7453" for this suite. 03/30/23 13:24:37.213
------------------------------
â€¢ [4.045 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:33.171
    Mar 30 13:24:33.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:24:33.172
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:33.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:33.18
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-8837b213-faa8-44f2-9e23-ef36f1009117 03/30/23 13:24:33.182
    STEP: Creating a pod to test consume configMaps 03/30/23 13:24:33.184
    Mar 30 13:24:33.188: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732" in namespace "projected-7453" to be "Succeeded or Failed"
    Mar 30 13:24:33.190: INFO: Pod "pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732": Phase="Pending", Reason="", readiness=false. Elapsed: 1.579925ms
    Mar 30 13:24:35.192: INFO: Pod "pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00384397s
    Mar 30 13:24:37.193: INFO: Pod "pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004305669s
    STEP: Saw pod success 03/30/23 13:24:37.193
    Mar 30 13:24:37.193: INFO: Pod "pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732" satisfied condition "Succeeded or Failed"
    Mar 30 13:24:37.195: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732 container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 13:24:37.205
    Mar 30 13:24:37.210: INFO: Waiting for pod pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732 to disappear
    Mar 30 13:24:37.211: INFO: Pod pod-projected-configmaps-aa37f6be-f30d-4431-904c-063b59523732 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:37.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7453" for this suite. 03/30/23 13:24:37.213
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:37.216
Mar 30 13:24:37.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 13:24:37.217
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:37.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:37.224
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 03/30/23 13:24:37.226
Mar 30 13:24:37.229: INFO: Waiting up to 5m0s for pod "pod-746f952e-94a9-4e2f-aaf3-f426e70a0897" in namespace "emptydir-1129" to be "Succeeded or Failed"
Mar 30 13:24:37.231: INFO: Pod "pod-746f952e-94a9-4e2f-aaf3-f426e70a0897": Phase="Pending", Reason="", readiness=false. Elapsed: 1.36006ms
Mar 30 13:24:39.233: INFO: Pod "pod-746f952e-94a9-4e2f-aaf3-f426e70a0897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00363837s
Mar 30 13:24:41.234: INFO: Pod "pod-746f952e-94a9-4e2f-aaf3-f426e70a0897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004579682s
STEP: Saw pod success 03/30/23 13:24:41.234
Mar 30 13:24:41.234: INFO: Pod "pod-746f952e-94a9-4e2f-aaf3-f426e70a0897" satisfied condition "Succeeded or Failed"
Mar 30 13:24:41.236: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-746f952e-94a9-4e2f-aaf3-f426e70a0897 container test-container: <nil>
STEP: delete the pod 03/30/23 13:24:41.239
Mar 30 13:24:41.244: INFO: Waiting for pod pod-746f952e-94a9-4e2f-aaf3-f426e70a0897 to disappear
Mar 30 13:24:41.245: INFO: Pod pod-746f952e-94a9-4e2f-aaf3-f426e70a0897 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:41.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1129" for this suite. 03/30/23 13:24:41.248
------------------------------
â€¢ [4.034 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:37.216
    Mar 30 13:24:37.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 13:24:37.217
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:37.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:37.224
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/30/23 13:24:37.226
    Mar 30 13:24:37.229: INFO: Waiting up to 5m0s for pod "pod-746f952e-94a9-4e2f-aaf3-f426e70a0897" in namespace "emptydir-1129" to be "Succeeded or Failed"
    Mar 30 13:24:37.231: INFO: Pod "pod-746f952e-94a9-4e2f-aaf3-f426e70a0897": Phase="Pending", Reason="", readiness=false. Elapsed: 1.36006ms
    Mar 30 13:24:39.233: INFO: Pod "pod-746f952e-94a9-4e2f-aaf3-f426e70a0897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00363837s
    Mar 30 13:24:41.234: INFO: Pod "pod-746f952e-94a9-4e2f-aaf3-f426e70a0897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004579682s
    STEP: Saw pod success 03/30/23 13:24:41.234
    Mar 30 13:24:41.234: INFO: Pod "pod-746f952e-94a9-4e2f-aaf3-f426e70a0897" satisfied condition "Succeeded or Failed"
    Mar 30 13:24:41.236: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-746f952e-94a9-4e2f-aaf3-f426e70a0897 container test-container: <nil>
    STEP: delete the pod 03/30/23 13:24:41.239
    Mar 30 13:24:41.244: INFO: Waiting for pod pod-746f952e-94a9-4e2f-aaf3-f426e70a0897 to disappear
    Mar 30 13:24:41.245: INFO: Pod pod-746f952e-94a9-4e2f-aaf3-f426e70a0897 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:41.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1129" for this suite. 03/30/23 13:24:41.248
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:41.25
Mar 30 13:24:41.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename podtemplate 03/30/23 13:24:41.251
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:41.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:41.258
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 03/30/23 13:24:41.26
Mar 30 13:24:41.262: INFO: created test-podtemplate-1
Mar 30 13:24:41.264: INFO: created test-podtemplate-2
Mar 30 13:24:41.266: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 03/30/23 13:24:41.266
STEP: delete collection of pod templates 03/30/23 13:24:41.268
Mar 30 13:24:41.268: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 03/30/23 13:24:41.274
Mar 30 13:24:41.274: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:41.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-507" for this suite. 03/30/23 13:24:41.277
------------------------------
â€¢ [0.029 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:41.25
    Mar 30 13:24:41.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename podtemplate 03/30/23 13:24:41.251
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:41.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:41.258
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 03/30/23 13:24:41.26
    Mar 30 13:24:41.262: INFO: created test-podtemplate-1
    Mar 30 13:24:41.264: INFO: created test-podtemplate-2
    Mar 30 13:24:41.266: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 03/30/23 13:24:41.266
    STEP: delete collection of pod templates 03/30/23 13:24:41.268
    Mar 30 13:24:41.268: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 03/30/23 13:24:41.274
    Mar 30 13:24:41.274: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:41.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-507" for this suite. 03/30/23 13:24:41.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:41.28
Mar 30 13:24:41.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 13:24:41.281
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:41.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:41.288
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 03/30/23 13:24:41.29
Mar 30 13:24:41.293: INFO: Waiting up to 5m0s for pod "pod-58b305fb-11b7-498f-a20a-90807d7ae788" in namespace "emptydir-1277" to be "Succeeded or Failed"
Mar 30 13:24:41.295: INFO: Pod "pod-58b305fb-11b7-498f-a20a-90807d7ae788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089181ms
Mar 30 13:24:43.298: INFO: Pod "pod-58b305fb-11b7-498f-a20a-90807d7ae788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005125125s
Mar 30 13:24:45.298: INFO: Pod "pod-58b305fb-11b7-498f-a20a-90807d7ae788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004856473s
STEP: Saw pod success 03/30/23 13:24:45.298
Mar 30 13:24:45.298: INFO: Pod "pod-58b305fb-11b7-498f-a20a-90807d7ae788" satisfied condition "Succeeded or Failed"
Mar 30 13:24:45.299: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-58b305fb-11b7-498f-a20a-90807d7ae788 container test-container: <nil>
STEP: delete the pod 03/30/23 13:24:45.303
Mar 30 13:24:45.309: INFO: Waiting for pod pod-58b305fb-11b7-498f-a20a-90807d7ae788 to disappear
Mar 30 13:24:45.311: INFO: Pod pod-58b305fb-11b7-498f-a20a-90807d7ae788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:45.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1277" for this suite. 03/30/23 13:24:45.313
------------------------------
â€¢ [4.035 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:41.28
    Mar 30 13:24:41.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 13:24:41.281
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:41.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:41.288
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/30/23 13:24:41.29
    Mar 30 13:24:41.293: INFO: Waiting up to 5m0s for pod "pod-58b305fb-11b7-498f-a20a-90807d7ae788" in namespace "emptydir-1277" to be "Succeeded or Failed"
    Mar 30 13:24:41.295: INFO: Pod "pod-58b305fb-11b7-498f-a20a-90807d7ae788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089181ms
    Mar 30 13:24:43.298: INFO: Pod "pod-58b305fb-11b7-498f-a20a-90807d7ae788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005125125s
    Mar 30 13:24:45.298: INFO: Pod "pod-58b305fb-11b7-498f-a20a-90807d7ae788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004856473s
    STEP: Saw pod success 03/30/23 13:24:45.298
    Mar 30 13:24:45.298: INFO: Pod "pod-58b305fb-11b7-498f-a20a-90807d7ae788" satisfied condition "Succeeded or Failed"
    Mar 30 13:24:45.299: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-58b305fb-11b7-498f-a20a-90807d7ae788 container test-container: <nil>
    STEP: delete the pod 03/30/23 13:24:45.303
    Mar 30 13:24:45.309: INFO: Waiting for pod pod-58b305fb-11b7-498f-a20a-90807d7ae788 to disappear
    Mar 30 13:24:45.311: INFO: Pod pod-58b305fb-11b7-498f-a20a-90807d7ae788 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:45.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1277" for this suite. 03/30/23 13:24:45.313
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:45.316
Mar 30 13:24:45.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename daemonsets 03/30/23 13:24:45.316
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:45.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:45.324
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Mar 30 13:24:45.334: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 03/30/23 13:24:45.336
Mar 30 13:24:45.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:45.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:45.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:45.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 13:24:45.340: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
Mar 30 13:24:46.343: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:46.343: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:46.343: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:46.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 13:24:46.345: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
Mar 30 13:24:47.343: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:47.343: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:47.343: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:47.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 30 13:24:47.345: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 03/30/23 13:24:47.351
STEP: Check that daemon pods images are updated. 03/30/23 13:24:47.359
Mar 30 13:24:47.360: INFO: Wrong image for pod: daemon-set-8925n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 30 13:24:47.360: INFO: Wrong image for pod: daemon-set-dhxf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 30 13:24:47.360: INFO: Wrong image for pod: daemon-set-vvv8t. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 30 13:24:47.362: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:47.362: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:47.362: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:48.365: INFO: Wrong image for pod: daemon-set-dhxf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 30 13:24:48.365: INFO: Wrong image for pod: daemon-set-vvv8t. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 30 13:24:48.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:48.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:48.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:49.365: INFO: Wrong image for pod: daemon-set-dhxf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 30 13:24:49.365: INFO: Wrong image for pod: daemon-set-vvv8t. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 30 13:24:49.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:49.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:49.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:50.365: INFO: Pod daemon-set-bqbvg is not available
Mar 30 13:24:50.365: INFO: Wrong image for pod: daemon-set-dhxf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 30 13:24:50.365: INFO: Wrong image for pod: daemon-set-vvv8t. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 30 13:24:50.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:50.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:50.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:51.365: INFO: Wrong image for pod: daemon-set-dhxf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 30 13:24:51.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:51.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:51.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:52.365: INFO: Wrong image for pod: daemon-set-dhxf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 30 13:24:52.365: INFO: Pod daemon-set-p5zb9 is not available
Mar 30 13:24:52.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:52.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:52.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:53.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:53.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:53.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:54.365: INFO: Pod daemon-set-4x2f2 is not available
Mar 30 13:24:54.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:54.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:54.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 03/30/23 13:24:54.368
Mar 30 13:24:54.370: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:54.370: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:54.370: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:54.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 30 13:24:54.372: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
Mar 30 13:24:55.375: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:55.375: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:55.375: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:24:55.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 30 13:24:55.377: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/30/23 13:24:55.384
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5315, will wait for the garbage collector to delete the pods 03/30/23 13:24:55.384
Mar 30 13:24:55.438: INFO: Deleting DaemonSet.extensions daemon-set took: 2.531042ms
Mar 30 13:24:55.539: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.101542ms
Mar 30 13:24:57.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 13:24:57.641: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 30 13:24:57.643: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"63172"},"items":null}

Mar 30 13:24:57.644: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"63172"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:24:57.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5315" for this suite. 03/30/23 13:24:57.652
------------------------------
â€¢ [SLOW TEST] [12.339 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:45.316
    Mar 30 13:24:45.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename daemonsets 03/30/23 13:24:45.316
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:45.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:45.324
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Mar 30 13:24:45.334: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 03/30/23 13:24:45.336
    Mar 30 13:24:45.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:45.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:45.338: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:45.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 13:24:45.340: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
    Mar 30 13:24:46.343: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:46.343: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:46.343: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:46.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 13:24:46.345: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
    Mar 30 13:24:47.343: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:47.343: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:47.343: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:47.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 30 13:24:47.345: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 03/30/23 13:24:47.351
    STEP: Check that daemon pods images are updated. 03/30/23 13:24:47.359
    Mar 30 13:24:47.360: INFO: Wrong image for pod: daemon-set-8925n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 30 13:24:47.360: INFO: Wrong image for pod: daemon-set-dhxf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 30 13:24:47.360: INFO: Wrong image for pod: daemon-set-vvv8t. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 30 13:24:47.362: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:47.362: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:47.362: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:48.365: INFO: Wrong image for pod: daemon-set-dhxf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 30 13:24:48.365: INFO: Wrong image for pod: daemon-set-vvv8t. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 30 13:24:48.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:48.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:48.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:49.365: INFO: Wrong image for pod: daemon-set-dhxf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 30 13:24:49.365: INFO: Wrong image for pod: daemon-set-vvv8t. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 30 13:24:49.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:49.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:49.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:50.365: INFO: Pod daemon-set-bqbvg is not available
    Mar 30 13:24:50.365: INFO: Wrong image for pod: daemon-set-dhxf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 30 13:24:50.365: INFO: Wrong image for pod: daemon-set-vvv8t. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 30 13:24:50.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:50.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:50.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:51.365: INFO: Wrong image for pod: daemon-set-dhxf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 30 13:24:51.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:51.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:51.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:52.365: INFO: Wrong image for pod: daemon-set-dhxf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 30 13:24:52.365: INFO: Pod daemon-set-p5zb9 is not available
    Mar 30 13:24:52.367: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:52.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:52.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:53.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:53.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:53.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:54.365: INFO: Pod daemon-set-4x2f2 is not available
    Mar 30 13:24:54.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:54.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:54.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 03/30/23 13:24:54.368
    Mar 30 13:24:54.370: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:54.370: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:54.370: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:54.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 30 13:24:54.372: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
    Mar 30 13:24:55.375: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:55.375: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:55.375: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:24:55.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 30 13:24:55.377: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/30/23 13:24:55.384
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5315, will wait for the garbage collector to delete the pods 03/30/23 13:24:55.384
    Mar 30 13:24:55.438: INFO: Deleting DaemonSet.extensions daemon-set took: 2.531042ms
    Mar 30 13:24:55.539: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.101542ms
    Mar 30 13:24:57.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 13:24:57.641: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 30 13:24:57.643: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"63172"},"items":null}

    Mar 30 13:24:57.644: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"63172"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:24:57.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5315" for this suite. 03/30/23 13:24:57.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:24:57.655
Mar 30 13:24:57.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 13:24:57.655
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:57.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:57.663
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-749 03/30/23 13:24:57.665
STEP: changing the ExternalName service to type=NodePort 03/30/23 13:24:57.667
STEP: creating replication controller externalname-service in namespace services-749 03/30/23 13:24:57.675
I0330 13:24:57.678602      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-749, replica count: 2
I0330 13:25:00.730552      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 13:25:00.730: INFO: Creating new exec pod
Mar 30 13:25:00.734: INFO: Waiting up to 5m0s for pod "execpodlz85x" in namespace "services-749" to be "running"
Mar 30 13:25:00.736: INFO: Pod "execpodlz85x": Phase="Pending", Reason="", readiness=false. Elapsed: 1.484265ms
Mar 30 13:25:02.737: INFO: Pod "execpodlz85x": Phase="Running", Reason="", readiness=true. Elapsed: 2.003303045s
Mar 30 13:25:02.737: INFO: Pod "execpodlz85x" satisfied condition "running"
Mar 30 13:25:03.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-749 exec execpodlz85x -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Mar 30 13:25:03.834: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 30 13:25:03.834: INFO: stdout: ""
Mar 30 13:25:03.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-749 exec execpodlz85x -- /bin/sh -x -c nc -v -z -w 2 172.16.73.164 80'
Mar 30 13:25:03.929: INFO: stderr: "+ nc -v -z -w 2 172.16.73.164 80\nConnection to 172.16.73.164 80 port [tcp/http] succeeded!\n"
Mar 30 13:25:03.929: INFO: stdout: ""
Mar 30 13:25:03.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-749 exec execpodlz85x -- /bin/sh -x -c nc -v -z -w 2 192.168.0.5 30422'
Mar 30 13:25:04.027: INFO: stderr: "+ nc -v -z -w 2 192.168.0.5 30422\nConnection to 192.168.0.5 30422 port [tcp/*] succeeded!\n"
Mar 30 13:25:04.027: INFO: stdout: ""
Mar 30 13:25:04.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-749 exec execpodlz85x -- /bin/sh -x -c nc -v -z -w 2 192.168.0.4 30422'
Mar 30 13:25:04.119: INFO: stderr: "+ nc -v -z -w 2 192.168.0.4 30422\nConnection to 192.168.0.4 30422 port [tcp/*] succeeded!\n"
Mar 30 13:25:04.119: INFO: stdout: ""
Mar 30 13:25:04.119: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 13:25:04.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-749" for this suite. 03/30/23 13:25:04.133
------------------------------
â€¢ [SLOW TEST] [6.481 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:24:57.655
    Mar 30 13:24:57.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 13:24:57.655
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:24:57.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:24:57.663
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-749 03/30/23 13:24:57.665
    STEP: changing the ExternalName service to type=NodePort 03/30/23 13:24:57.667
    STEP: creating replication controller externalname-service in namespace services-749 03/30/23 13:24:57.675
    I0330 13:24:57.678602      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-749, replica count: 2
    I0330 13:25:00.730552      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 30 13:25:00.730: INFO: Creating new exec pod
    Mar 30 13:25:00.734: INFO: Waiting up to 5m0s for pod "execpodlz85x" in namespace "services-749" to be "running"
    Mar 30 13:25:00.736: INFO: Pod "execpodlz85x": Phase="Pending", Reason="", readiness=false. Elapsed: 1.484265ms
    Mar 30 13:25:02.737: INFO: Pod "execpodlz85x": Phase="Running", Reason="", readiness=true. Elapsed: 2.003303045s
    Mar 30 13:25:02.737: INFO: Pod "execpodlz85x" satisfied condition "running"
    Mar 30 13:25:03.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-749 exec execpodlz85x -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Mar 30 13:25:03.834: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 30 13:25:03.834: INFO: stdout: ""
    Mar 30 13:25:03.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-749 exec execpodlz85x -- /bin/sh -x -c nc -v -z -w 2 172.16.73.164 80'
    Mar 30 13:25:03.929: INFO: stderr: "+ nc -v -z -w 2 172.16.73.164 80\nConnection to 172.16.73.164 80 port [tcp/http] succeeded!\n"
    Mar 30 13:25:03.929: INFO: stdout: ""
    Mar 30 13:25:03.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-749 exec execpodlz85x -- /bin/sh -x -c nc -v -z -w 2 192.168.0.5 30422'
    Mar 30 13:25:04.027: INFO: stderr: "+ nc -v -z -w 2 192.168.0.5 30422\nConnection to 192.168.0.5 30422 port [tcp/*] succeeded!\n"
    Mar 30 13:25:04.027: INFO: stdout: ""
    Mar 30 13:25:04.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-749 exec execpodlz85x -- /bin/sh -x -c nc -v -z -w 2 192.168.0.4 30422'
    Mar 30 13:25:04.119: INFO: stderr: "+ nc -v -z -w 2 192.168.0.4 30422\nConnection to 192.168.0.4 30422 port [tcp/*] succeeded!\n"
    Mar 30 13:25:04.119: INFO: stdout: ""
    Mar 30 13:25:04.119: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:25:04.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-749" for this suite. 03/30/23 13:25:04.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:25:04.136
Mar 30 13:25:04.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 13:25:04.137
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:25:04.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:25:04.146
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-502 03/30/23 13:25:04.148
STEP: creating service affinity-clusterip-transition in namespace services-502 03/30/23 13:25:04.148
STEP: creating replication controller affinity-clusterip-transition in namespace services-502 03/30/23 13:25:04.152
I0330 13:25:04.155919      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-502, replica count: 3
I0330 13:25:07.208066      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 13:25:07.211: INFO: Creating new exec pod
Mar 30 13:25:07.214: INFO: Waiting up to 5m0s for pod "execpod-affinityfgtml" in namespace "services-502" to be "running"
Mar 30 13:25:07.217: INFO: Pod "execpod-affinityfgtml": Phase="Pending", Reason="", readiness=false. Elapsed: 2.252884ms
Mar 30 13:25:09.220: INFO: Pod "execpod-affinityfgtml": Phase="Running", Reason="", readiness=true. Elapsed: 2.005241358s
Mar 30 13:25:09.220: INFO: Pod "execpod-affinityfgtml" satisfied condition "running"
Mar 30 13:25:10.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-502 exec execpod-affinityfgtml -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Mar 30 13:25:10.320: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 30 13:25:10.320: INFO: stdout: ""
Mar 30 13:25:10.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-502 exec execpod-affinityfgtml -- /bin/sh -x -c nc -v -z -w 2 172.16.136.136 80'
Mar 30 13:25:10.415: INFO: stderr: "+ nc -v -z -w 2 172.16.136.136 80\nConnection to 172.16.136.136 80 port [tcp/http] succeeded!\n"
Mar 30 13:25:10.415: INFO: stdout: ""
Mar 30 13:25:10.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-502 exec execpod-affinityfgtml -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.136.136:80/ ; done'
Mar 30 13:25:10.570: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n"
Mar 30 13:25:10.570: INFO: stdout: "\naffinity-clusterip-transition-ztszj\naffinity-clusterip-transition-9d7t6\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-ztszj\naffinity-clusterip-transition-9d7t6\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-ztszj\naffinity-clusterip-transition-9d7t6\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-ztszj\naffinity-clusterip-transition-9d7t6\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-ztszj\naffinity-clusterip-transition-9d7t6\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-ztszj"
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-ztszj
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-9d7t6
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-ztszj
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-9d7t6
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-ztszj
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-9d7t6
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-ztszj
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-9d7t6
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-ztszj
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-9d7t6
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-ztszj
Mar 30 13:25:10.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-502 exec execpod-affinityfgtml -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.136.136:80/ ; done'
Mar 30 13:25:10.726: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n"
Mar 30 13:25:10.726: INFO: stdout: "\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh"
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
Mar 30 13:25:10.726: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-502, will wait for the garbage collector to delete the pods 03/30/23 13:25:10.73
Mar 30 13:25:10.785: INFO: Deleting ReplicationController affinity-clusterip-transition took: 2.7334ms
Mar 30 13:25:10.886: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.954664ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 13:25:12.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-502" for this suite. 03/30/23 13:25:12.797
------------------------------
â€¢ [SLOW TEST] [8.663 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:25:04.136
    Mar 30 13:25:04.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 13:25:04.137
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:25:04.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:25:04.146
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-502 03/30/23 13:25:04.148
    STEP: creating service affinity-clusterip-transition in namespace services-502 03/30/23 13:25:04.148
    STEP: creating replication controller affinity-clusterip-transition in namespace services-502 03/30/23 13:25:04.152
    I0330 13:25:04.155919      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-502, replica count: 3
    I0330 13:25:07.208066      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 30 13:25:07.211: INFO: Creating new exec pod
    Mar 30 13:25:07.214: INFO: Waiting up to 5m0s for pod "execpod-affinityfgtml" in namespace "services-502" to be "running"
    Mar 30 13:25:07.217: INFO: Pod "execpod-affinityfgtml": Phase="Pending", Reason="", readiness=false. Elapsed: 2.252884ms
    Mar 30 13:25:09.220: INFO: Pod "execpod-affinityfgtml": Phase="Running", Reason="", readiness=true. Elapsed: 2.005241358s
    Mar 30 13:25:09.220: INFO: Pod "execpod-affinityfgtml" satisfied condition "running"
    Mar 30 13:25:10.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-502 exec execpod-affinityfgtml -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Mar 30 13:25:10.320: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Mar 30 13:25:10.320: INFO: stdout: ""
    Mar 30 13:25:10.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-502 exec execpod-affinityfgtml -- /bin/sh -x -c nc -v -z -w 2 172.16.136.136 80'
    Mar 30 13:25:10.415: INFO: stderr: "+ nc -v -z -w 2 172.16.136.136 80\nConnection to 172.16.136.136 80 port [tcp/http] succeeded!\n"
    Mar 30 13:25:10.415: INFO: stdout: ""
    Mar 30 13:25:10.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-502 exec execpod-affinityfgtml -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.136.136:80/ ; done'
    Mar 30 13:25:10.570: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n"
    Mar 30 13:25:10.570: INFO: stdout: "\naffinity-clusterip-transition-ztszj\naffinity-clusterip-transition-9d7t6\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-ztszj\naffinity-clusterip-transition-9d7t6\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-ztszj\naffinity-clusterip-transition-9d7t6\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-ztszj\naffinity-clusterip-transition-9d7t6\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-ztszj\naffinity-clusterip-transition-9d7t6\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-ztszj"
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-ztszj
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-9d7t6
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-ztszj
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-9d7t6
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-ztszj
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-9d7t6
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-ztszj
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-9d7t6
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-ztszj
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-9d7t6
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.570: INFO: Received response from host: affinity-clusterip-transition-ztszj
    Mar 30 13:25:10.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-502 exec execpod-affinityfgtml -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.136.136:80/ ; done'
    Mar 30 13:25:10.726: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.136.136:80/\n"
    Mar 30 13:25:10.726: INFO: stdout: "\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh\naffinity-clusterip-transition-txmvh"
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Received response from host: affinity-clusterip-transition-txmvh
    Mar 30 13:25:10.726: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-502, will wait for the garbage collector to delete the pods 03/30/23 13:25:10.73
    Mar 30 13:25:10.785: INFO: Deleting ReplicationController affinity-clusterip-transition took: 2.7334ms
    Mar 30 13:25:10.886: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.954664ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:25:12.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-502" for this suite. 03/30/23 13:25:12.797
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:25:12.8
Mar 30 13:25:12.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 13:25:12.801
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:25:12.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:25:12.809
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 03/30/23 13:25:12.811
Mar 30 13:25:12.816: INFO: Waiting up to 5m0s for pod "pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa" in namespace "emptydir-7449" to be "Succeeded or Failed"
Mar 30 13:25:12.818: INFO: Pod "pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.60469ms
Mar 30 13:25:14.821: INFO: Pod "pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004307887s
Mar 30 13:25:16.820: INFO: Pod "pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004048769s
STEP: Saw pod success 03/30/23 13:25:16.82
Mar 30 13:25:16.821: INFO: Pod "pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa" satisfied condition "Succeeded or Failed"
Mar 30 13:25:16.822: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa container test-container: <nil>
STEP: delete the pod 03/30/23 13:25:16.825
Mar 30 13:25:16.831: INFO: Waiting for pod pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa to disappear
Mar 30 13:25:16.832: INFO: Pod pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 13:25:16.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7449" for this suite. 03/30/23 13:25:16.834
------------------------------
â€¢ [4.037 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:25:12.8
    Mar 30 13:25:12.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 13:25:12.801
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:25:12.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:25:12.809
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/30/23 13:25:12.811
    Mar 30 13:25:12.816: INFO: Waiting up to 5m0s for pod "pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa" in namespace "emptydir-7449" to be "Succeeded or Failed"
    Mar 30 13:25:12.818: INFO: Pod "pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.60469ms
    Mar 30 13:25:14.821: INFO: Pod "pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004307887s
    Mar 30 13:25:16.820: INFO: Pod "pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004048769s
    STEP: Saw pod success 03/30/23 13:25:16.82
    Mar 30 13:25:16.821: INFO: Pod "pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa" satisfied condition "Succeeded or Failed"
    Mar 30 13:25:16.822: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa container test-container: <nil>
    STEP: delete the pod 03/30/23 13:25:16.825
    Mar 30 13:25:16.831: INFO: Waiting for pod pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa to disappear
    Mar 30 13:25:16.832: INFO: Pod pod-51e2c74e-f61a-4efc-bca2-516b8c2337aa no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:25:16.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7449" for this suite. 03/30/23 13:25:16.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:25:16.838
Mar 30 13:25:16.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename crd-watch 03/30/23 13:25:16.838
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:25:16.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:25:16.846
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Mar 30 13:25:16.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Creating first CR  03/30/23 13:25:24.376
Mar 30 13:25:24.378: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-30T13:25:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-30T13:25:24Z]] name:name1 resourceVersion:63583 uid:9b333bc9-c9a6-40ab-92b4-f48a0a5ff4ae] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 03/30/23 13:25:34.38
Mar 30 13:25:34.383: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-30T13:25:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-30T13:25:34Z]] name:name2 resourceVersion:63630 uid:024626c9-77fd-45a8-b82e-e1af1e29ed17] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 03/30/23 13:25:44.384
Mar 30 13:25:44.387: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-30T13:25:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-30T13:25:44Z]] name:name1 resourceVersion:63677 uid:9b333bc9-c9a6-40ab-92b4-f48a0a5ff4ae] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 03/30/23 13:25:54.388
Mar 30 13:25:54.392: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-30T13:25:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-30T13:25:54Z]] name:name2 resourceVersion:63726 uid:024626c9-77fd-45a8-b82e-e1af1e29ed17] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 03/30/23 13:26:04.394
Mar 30 13:26:04.397: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-30T13:25:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-30T13:25:44Z]] name:name1 resourceVersion:63773 uid:9b333bc9-c9a6-40ab-92b4-f48a0a5ff4ae] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 03/30/23 13:26:14.399
Mar 30 13:26:14.403: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-30T13:25:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-30T13:25:54Z]] name:name2 resourceVersion:63820 uid:024626c9-77fd-45a8-b82e-e1af1e29ed17] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:26:24.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-1639" for this suite. 03/30/23 13:26:24.912
------------------------------
â€¢ [SLOW TEST] [68.077 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:25:16.838
    Mar 30 13:25:16.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename crd-watch 03/30/23 13:25:16.838
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:25:16.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:25:16.846
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Mar 30 13:25:16.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Creating first CR  03/30/23 13:25:24.376
    Mar 30 13:25:24.378: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-30T13:25:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-30T13:25:24Z]] name:name1 resourceVersion:63583 uid:9b333bc9-c9a6-40ab-92b4-f48a0a5ff4ae] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 03/30/23 13:25:34.38
    Mar 30 13:25:34.383: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-30T13:25:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-30T13:25:34Z]] name:name2 resourceVersion:63630 uid:024626c9-77fd-45a8-b82e-e1af1e29ed17] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 03/30/23 13:25:44.384
    Mar 30 13:25:44.387: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-30T13:25:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-30T13:25:44Z]] name:name1 resourceVersion:63677 uid:9b333bc9-c9a6-40ab-92b4-f48a0a5ff4ae] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 03/30/23 13:25:54.388
    Mar 30 13:25:54.392: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-30T13:25:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-30T13:25:54Z]] name:name2 resourceVersion:63726 uid:024626c9-77fd-45a8-b82e-e1af1e29ed17] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 03/30/23 13:26:04.394
    Mar 30 13:26:04.397: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-30T13:25:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-30T13:25:44Z]] name:name1 resourceVersion:63773 uid:9b333bc9-c9a6-40ab-92b4-f48a0a5ff4ae] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 03/30/23 13:26:14.399
    Mar 30 13:26:14.403: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-30T13:25:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-30T13:25:54Z]] name:name2 resourceVersion:63820 uid:024626c9-77fd-45a8-b82e-e1af1e29ed17] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:26:24.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-1639" for this suite. 03/30/23 13:26:24.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:26:24.915
Mar 30 13:26:24.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename namespaces 03/30/23 13:26:24.916
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:26:24.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:26:24.925
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-1687" 03/30/23 13:26:24.927
Mar 30 13:26:24.930: INFO: Namespace "namespaces-1687" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"2220ea2c-1613-43b9-bb5d-81b7f052acb2", "kubernetes.io/metadata.name":"namespaces-1687", "namespaces-1687":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:26:24.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1687" for this suite. 03/30/23 13:26:24.932
------------------------------
â€¢ [0.019 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:26:24.915
    Mar 30 13:26:24.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename namespaces 03/30/23 13:26:24.916
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:26:24.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:26:24.925
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-1687" 03/30/23 13:26:24.927
    Mar 30 13:26:24.930: INFO: Namespace "namespaces-1687" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"2220ea2c-1613-43b9-bb5d-81b7f052acb2", "kubernetes.io/metadata.name":"namespaces-1687", "namespaces-1687":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:26:24.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1687" for this suite. 03/30/23 13:26:24.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:26:24.936
Mar 30 13:26:24.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 13:26:24.936
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:26:24.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:26:24.943
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-bd34c4f7-57cc-4448-8830-55fa413a8545 03/30/23 13:26:24.945
STEP: Creating a pod to test consume secrets 03/30/23 13:26:24.947
Mar 30 13:26:24.950: INFO: Waiting up to 5m0s for pod "pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8" in namespace "secrets-1925" to be "Succeeded or Failed"
Mar 30 13:26:24.952: INFO: Pod "pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.851378ms
Mar 30 13:26:26.954: INFO: Pod "pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004238187s
Mar 30 13:26:28.955: INFO: Pod "pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004879044s
STEP: Saw pod success 03/30/23 13:26:28.955
Mar 30 13:26:28.955: INFO: Pod "pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8" satisfied condition "Succeeded or Failed"
Mar 30 13:26:28.957: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8 container secret-volume-test: <nil>
STEP: delete the pod 03/30/23 13:26:28.96
Mar 30 13:26:28.965: INFO: Waiting for pod pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8 to disappear
Mar 30 13:26:28.967: INFO: Pod pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 13:26:28.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1925" for this suite. 03/30/23 13:26:28.969
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:26:24.936
    Mar 30 13:26:24.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 13:26:24.936
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:26:24.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:26:24.943
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-bd34c4f7-57cc-4448-8830-55fa413a8545 03/30/23 13:26:24.945
    STEP: Creating a pod to test consume secrets 03/30/23 13:26:24.947
    Mar 30 13:26:24.950: INFO: Waiting up to 5m0s for pod "pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8" in namespace "secrets-1925" to be "Succeeded or Failed"
    Mar 30 13:26:24.952: INFO: Pod "pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.851378ms
    Mar 30 13:26:26.954: INFO: Pod "pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004238187s
    Mar 30 13:26:28.955: INFO: Pod "pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004879044s
    STEP: Saw pod success 03/30/23 13:26:28.955
    Mar 30 13:26:28.955: INFO: Pod "pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8" satisfied condition "Succeeded or Failed"
    Mar 30 13:26:28.957: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8 container secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 13:26:28.96
    Mar 30 13:26:28.965: INFO: Waiting for pod pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8 to disappear
    Mar 30 13:26:28.967: INFO: Pod pod-secrets-6c46f86f-04b7-4e1e-85fe-7033f3d573a8 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:26:28.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1925" for this suite. 03/30/23 13:26:28.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:26:28.972
Mar 30 13:26:28.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename runtimeclass 03/30/23 13:26:28.973
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:26:28.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:26:28.981
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Mar 30 13:26:28.987: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5461 to be scheduled
Mar 30 13:26:28.989: INFO: 1 pods are not scheduled: [runtimeclass-5461/test-runtimeclass-runtimeclass-5461-preconfigured-handler-8hfhb(eb7fcd37-4c33-4fae-8304-6f0b361c344b)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 30 13:26:30.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5461" for this suite. 03/30/23 13:26:30.996
------------------------------
â€¢ [2.027 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:26:28.972
    Mar 30 13:26:28.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename runtimeclass 03/30/23 13:26:28.973
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:26:28.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:26:28.981
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Mar 30 13:26:28.987: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5461 to be scheduled
    Mar 30 13:26:28.989: INFO: 1 pods are not scheduled: [runtimeclass-5461/test-runtimeclass-runtimeclass-5461-preconfigured-handler-8hfhb(eb7fcd37-4c33-4fae-8304-6f0b361c344b)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:26:30.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5461" for this suite. 03/30/23 13:26:30.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:26:31
Mar 30 13:26:31.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 13:26:31.001
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:26:31.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:26:31.008
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 03/30/23 13:26:31.01
Mar 30 13:26:31.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 create -f -'
Mar 30 13:26:31.568: INFO: stderr: ""
Mar 30 13:26:31.568: INFO: stdout: "pod/pause created\n"
Mar 30 13:26:31.568: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 30 13:26:31.568: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4954" to be "running and ready"
Mar 30 13:26:31.570: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.638186ms
Mar 30 13:26:31.570: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'cn-hongkong.192.168.0.5' to be 'Running' but was 'Pending'
Mar 30 13:26:33.573: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004861387s
Mar 30 13:26:33.573: INFO: Pod "pause" satisfied condition "running and ready"
Mar 30 13:26:33.573: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 03/30/23 13:26:33.573
Mar 30 13:26:33.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 label pods pause testing-label=testing-label-value'
Mar 30 13:26:33.629: INFO: stderr: ""
Mar 30 13:26:33.629: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 03/30/23 13:26:33.629
Mar 30 13:26:33.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 get pod pause -L testing-label'
Mar 30 13:26:33.681: INFO: stderr: ""
Mar 30 13:26:33.681: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 03/30/23 13:26:33.681
Mar 30 13:26:33.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 label pods pause testing-label-'
Mar 30 13:26:33.738: INFO: stderr: ""
Mar 30 13:26:33.738: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 03/30/23 13:26:33.738
Mar 30 13:26:33.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 get pod pause -L testing-label'
Mar 30 13:26:33.790: INFO: stderr: ""
Mar 30 13:26:33.790: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 03/30/23 13:26:33.79
Mar 30 13:26:33.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 delete --grace-period=0 --force -f -'
Mar 30 13:26:33.844: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 13:26:33.844: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 30 13:26:33.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 get rc,svc -l name=pause --no-headers'
Mar 30 13:26:33.897: INFO: stderr: "No resources found in kubectl-4954 namespace.\n"
Mar 30 13:26:33.897: INFO: stdout: ""
Mar 30 13:26:33.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 30 13:26:33.946: INFO: stderr: ""
Mar 30 13:26:33.946: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 13:26:33.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4954" for this suite. 03/30/23 13:26:33.949
------------------------------
â€¢ [2.952 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:26:31
    Mar 30 13:26:31.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 13:26:31.001
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:26:31.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:26:31.008
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 03/30/23 13:26:31.01
    Mar 30 13:26:31.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 create -f -'
    Mar 30 13:26:31.568: INFO: stderr: ""
    Mar 30 13:26:31.568: INFO: stdout: "pod/pause created\n"
    Mar 30 13:26:31.568: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Mar 30 13:26:31.568: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4954" to be "running and ready"
    Mar 30 13:26:31.570: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.638186ms
    Mar 30 13:26:31.570: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'cn-hongkong.192.168.0.5' to be 'Running' but was 'Pending'
    Mar 30 13:26:33.573: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004861387s
    Mar 30 13:26:33.573: INFO: Pod "pause" satisfied condition "running and ready"
    Mar 30 13:26:33.573: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 03/30/23 13:26:33.573
    Mar 30 13:26:33.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 label pods pause testing-label=testing-label-value'
    Mar 30 13:26:33.629: INFO: stderr: ""
    Mar 30 13:26:33.629: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 03/30/23 13:26:33.629
    Mar 30 13:26:33.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 get pod pause -L testing-label'
    Mar 30 13:26:33.681: INFO: stderr: ""
    Mar 30 13:26:33.681: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 03/30/23 13:26:33.681
    Mar 30 13:26:33.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 label pods pause testing-label-'
    Mar 30 13:26:33.738: INFO: stderr: ""
    Mar 30 13:26:33.738: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 03/30/23 13:26:33.738
    Mar 30 13:26:33.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 get pod pause -L testing-label'
    Mar 30 13:26:33.790: INFO: stderr: ""
    Mar 30 13:26:33.790: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 03/30/23 13:26:33.79
    Mar 30 13:26:33.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 delete --grace-period=0 --force -f -'
    Mar 30 13:26:33.844: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 30 13:26:33.844: INFO: stdout: "pod \"pause\" force deleted\n"
    Mar 30 13:26:33.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 get rc,svc -l name=pause --no-headers'
    Mar 30 13:26:33.897: INFO: stderr: "No resources found in kubectl-4954 namespace.\n"
    Mar 30 13:26:33.897: INFO: stdout: ""
    Mar 30 13:26:33.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4954 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 30 13:26:33.946: INFO: stderr: ""
    Mar 30 13:26:33.946: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:26:33.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4954" for this suite. 03/30/23 13:26:33.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:26:33.952
Mar 30 13:26:33.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename podtemplate 03/30/23 13:26:33.953
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:26:33.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:26:33.961
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 30 13:26:33.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-3190" for this suite. 03/30/23 13:26:33.977
------------------------------
â€¢ [0.027 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:26:33.952
    Mar 30 13:26:33.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename podtemplate 03/30/23 13:26:33.953
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:26:33.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:26:33.961
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:26:33.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-3190" for this suite. 03/30/23 13:26:33.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:26:33.982
Mar 30 13:26:33.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:26:33.982
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:26:33.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:26:33.99
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-e5b71a6e-1d89-4aea-9036-85237845e2c4 03/30/23 13:26:33.994
STEP: Creating the pod 03/30/23 13:26:33.996
Mar 30 13:26:33.999: INFO: Waiting up to 5m0s for pod "pod-configmaps-6b51eb02-eba4-4a26-98c7-9b3681773f4c" in namespace "configmap-4759" to be "running and ready"
Mar 30 13:26:34.001: INFO: Pod "pod-configmaps-6b51eb02-eba4-4a26-98c7-9b3681773f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335911ms
Mar 30 13:26:34.001: INFO: The phase of Pod pod-configmaps-6b51eb02-eba4-4a26-98c7-9b3681773f4c is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:26:36.003: INFO: Pod "pod-configmaps-6b51eb02-eba4-4a26-98c7-9b3681773f4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.00376826s
Mar 30 13:26:36.003: INFO: The phase of Pod pod-configmaps-6b51eb02-eba4-4a26-98c7-9b3681773f4c is Running (Ready = true)
Mar 30 13:26:36.003: INFO: Pod "pod-configmaps-6b51eb02-eba4-4a26-98c7-9b3681773f4c" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-e5b71a6e-1d89-4aea-9036-85237845e2c4 03/30/23 13:26:36.008
STEP: waiting to observe update in volume 03/30/23 13:26:36.01
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:27:48.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4759" for this suite. 03/30/23 13:27:48.177
------------------------------
â€¢ [SLOW TEST] [74.198 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:26:33.982
    Mar 30 13:26:33.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:26:33.982
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:26:33.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:26:33.99
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-e5b71a6e-1d89-4aea-9036-85237845e2c4 03/30/23 13:26:33.994
    STEP: Creating the pod 03/30/23 13:26:33.996
    Mar 30 13:26:33.999: INFO: Waiting up to 5m0s for pod "pod-configmaps-6b51eb02-eba4-4a26-98c7-9b3681773f4c" in namespace "configmap-4759" to be "running and ready"
    Mar 30 13:26:34.001: INFO: Pod "pod-configmaps-6b51eb02-eba4-4a26-98c7-9b3681773f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335911ms
    Mar 30 13:26:34.001: INFO: The phase of Pod pod-configmaps-6b51eb02-eba4-4a26-98c7-9b3681773f4c is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:26:36.003: INFO: Pod "pod-configmaps-6b51eb02-eba4-4a26-98c7-9b3681773f4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.00376826s
    Mar 30 13:26:36.003: INFO: The phase of Pod pod-configmaps-6b51eb02-eba4-4a26-98c7-9b3681773f4c is Running (Ready = true)
    Mar 30 13:26:36.003: INFO: Pod "pod-configmaps-6b51eb02-eba4-4a26-98c7-9b3681773f4c" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-e5b71a6e-1d89-4aea-9036-85237845e2c4 03/30/23 13:26:36.008
    STEP: waiting to observe update in volume 03/30/23 13:26:36.01
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:27:48.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4759" for this suite. 03/30/23 13:27:48.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:27:48.18
Mar 30 13:27:48.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename hostport 03/30/23 13:27:48.181
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:27:48.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:27:48.189
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/30/23 13:27:48.193
Mar 30 13:27:48.196: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5376" to be "running and ready"
Mar 30 13:27:48.198: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.490647ms
Mar 30 13:27:48.198: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:27:50.201: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004695904s
Mar 30 13:27:50.201: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 30 13:27:50.201: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.0.3 on the node which pod1 resides and expect scheduled 03/30/23 13:27:50.201
Mar 30 13:27:50.204: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5376" to be "running and ready"
Mar 30 13:27:50.205: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.501012ms
Mar 30 13:27:50.205: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:27:52.208: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004280955s
Mar 30 13:27:52.208: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 30 13:27:52.208: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.0.3 but use UDP protocol on the node which pod2 resides 03/30/23 13:27:52.208
Mar 30 13:27:52.211: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5376" to be "running and ready"
Mar 30 13:27:52.212: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.549308ms
Mar 30 13:27:52.212: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:27:54.215: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004389684s
Mar 30 13:27:54.215: INFO: The phase of Pod pod3 is Running (Ready = true)
Mar 30 13:27:54.215: INFO: Pod "pod3" satisfied condition "running and ready"
Mar 30 13:27:54.218: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5376" to be "running and ready"
Mar 30 13:27:54.219: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.584853ms
Mar 30 13:27:54.219: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:27:56.222: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004669933s
Mar 30 13:27:56.222: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Mar 30 13:27:56.222: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/30/23 13:27:56.224
Mar 30 13:27:56.224: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.0.3 http://127.0.0.1:54323/hostname] Namespace:hostport-5376 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:27:56.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:27:56.224: INFO: ExecWithOptions: Clientset creation
Mar 30 13:27:56.224: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/hostport-5376/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.0.3+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.3, port: 54323 03/30/23 13:27:56.269
Mar 30 13:27:56.269: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.0.3:54323/hostname] Namespace:hostport-5376 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:27:56.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:27:56.269: INFO: ExecWithOptions: Clientset creation
Mar 30 13:27:56.270: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/hostport-5376/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.0.3%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.3, port: 54323 UDP 03/30/23 13:27:56.311
Mar 30 13:27:56.311: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.0.3 54323] Namespace:hostport-5376 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:27:56.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:27:56.311: INFO: ExecWithOptions: Clientset creation
Mar 30 13:27:56.311: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/hostport-5376/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.0.3+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Mar 30 13:28:01.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-5376" for this suite. 03/30/23 13:28:01.359
------------------------------
â€¢ [SLOW TEST] [13.182 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:27:48.18
    Mar 30 13:27:48.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename hostport 03/30/23 13:27:48.181
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:27:48.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:27:48.189
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/30/23 13:27:48.193
    Mar 30 13:27:48.196: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5376" to be "running and ready"
    Mar 30 13:27:48.198: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.490647ms
    Mar 30 13:27:48.198: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:27:50.201: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004695904s
    Mar 30 13:27:50.201: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 30 13:27:50.201: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.0.3 on the node which pod1 resides and expect scheduled 03/30/23 13:27:50.201
    Mar 30 13:27:50.204: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5376" to be "running and ready"
    Mar 30 13:27:50.205: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.501012ms
    Mar 30 13:27:50.205: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:27:52.208: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004280955s
    Mar 30 13:27:52.208: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 30 13:27:52.208: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.0.3 but use UDP protocol on the node which pod2 resides 03/30/23 13:27:52.208
    Mar 30 13:27:52.211: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5376" to be "running and ready"
    Mar 30 13:27:52.212: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.549308ms
    Mar 30 13:27:52.212: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:27:54.215: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004389684s
    Mar 30 13:27:54.215: INFO: The phase of Pod pod3 is Running (Ready = true)
    Mar 30 13:27:54.215: INFO: Pod "pod3" satisfied condition "running and ready"
    Mar 30 13:27:54.218: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5376" to be "running and ready"
    Mar 30 13:27:54.219: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.584853ms
    Mar 30 13:27:54.219: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:27:56.222: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004669933s
    Mar 30 13:27:56.222: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Mar 30 13:27:56.222: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/30/23 13:27:56.224
    Mar 30 13:27:56.224: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.0.3 http://127.0.0.1:54323/hostname] Namespace:hostport-5376 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:27:56.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:27:56.224: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:27:56.224: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/hostport-5376/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.0.3+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.3, port: 54323 03/30/23 13:27:56.269
    Mar 30 13:27:56.269: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.0.3:54323/hostname] Namespace:hostport-5376 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:27:56.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:27:56.269: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:27:56.270: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/hostport-5376/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.0.3%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.3, port: 54323 UDP 03/30/23 13:27:56.311
    Mar 30 13:27:56.311: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.0.3 54323] Namespace:hostport-5376 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:27:56.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:27:56.311: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:27:56.311: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/hostport-5376/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.0.3+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:28:01.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-5376" for this suite. 03/30/23 13:28:01.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:28:01.363
Mar 30 13:28:01.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename job 03/30/23 13:28:01.363
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:01.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:01.371
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 03/30/23 13:28:01.373
STEP: Ensuring active pods == parallelism 03/30/23 13:28:01.376
STEP: Orphaning one of the Job's Pods 03/30/23 13:28:03.379
Mar 30 13:28:03.887: INFO: Successfully updated pod "adopt-release-c4525"
STEP: Checking that the Job readopts the Pod 03/30/23 13:28:03.887
Mar 30 13:28:03.887: INFO: Waiting up to 15m0s for pod "adopt-release-c4525" in namespace "job-1462" to be "adopted"
Mar 30 13:28:03.888: INFO: Pod "adopt-release-c4525": Phase="Running", Reason="", readiness=true. Elapsed: 1.678727ms
Mar 30 13:28:05.891: INFO: Pod "adopt-release-c4525": Phase="Running", Reason="", readiness=true. Elapsed: 2.004079409s
Mar 30 13:28:05.891: INFO: Pod "adopt-release-c4525" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 03/30/23 13:28:05.891
Mar 30 13:28:06.397: INFO: Successfully updated pod "adopt-release-c4525"
STEP: Checking that the Job releases the Pod 03/30/23 13:28:06.397
Mar 30 13:28:06.397: INFO: Waiting up to 15m0s for pod "adopt-release-c4525" in namespace "job-1462" to be "released"
Mar 30 13:28:06.398: INFO: Pod "adopt-release-c4525": Phase="Running", Reason="", readiness=true. Elapsed: 1.517643ms
Mar 30 13:28:08.400: INFO: Pod "adopt-release-c4525": Phase="Running", Reason="", readiness=true. Elapsed: 2.003541065s
Mar 30 13:28:08.400: INFO: Pod "adopt-release-c4525" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 30 13:28:08.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1462" for this suite. 03/30/23 13:28:08.403
------------------------------
â€¢ [SLOW TEST] [7.043 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:28:01.363
    Mar 30 13:28:01.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename job 03/30/23 13:28:01.363
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:01.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:01.371
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 03/30/23 13:28:01.373
    STEP: Ensuring active pods == parallelism 03/30/23 13:28:01.376
    STEP: Orphaning one of the Job's Pods 03/30/23 13:28:03.379
    Mar 30 13:28:03.887: INFO: Successfully updated pod "adopt-release-c4525"
    STEP: Checking that the Job readopts the Pod 03/30/23 13:28:03.887
    Mar 30 13:28:03.887: INFO: Waiting up to 15m0s for pod "adopt-release-c4525" in namespace "job-1462" to be "adopted"
    Mar 30 13:28:03.888: INFO: Pod "adopt-release-c4525": Phase="Running", Reason="", readiness=true. Elapsed: 1.678727ms
    Mar 30 13:28:05.891: INFO: Pod "adopt-release-c4525": Phase="Running", Reason="", readiness=true. Elapsed: 2.004079409s
    Mar 30 13:28:05.891: INFO: Pod "adopt-release-c4525" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 03/30/23 13:28:05.891
    Mar 30 13:28:06.397: INFO: Successfully updated pod "adopt-release-c4525"
    STEP: Checking that the Job releases the Pod 03/30/23 13:28:06.397
    Mar 30 13:28:06.397: INFO: Waiting up to 15m0s for pod "adopt-release-c4525" in namespace "job-1462" to be "released"
    Mar 30 13:28:06.398: INFO: Pod "adopt-release-c4525": Phase="Running", Reason="", readiness=true. Elapsed: 1.517643ms
    Mar 30 13:28:08.400: INFO: Pod "adopt-release-c4525": Phase="Running", Reason="", readiness=true. Elapsed: 2.003541065s
    Mar 30 13:28:08.400: INFO: Pod "adopt-release-c4525" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:28:08.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1462" for this suite. 03/30/23 13:28:08.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:28:08.407
Mar 30 13:28:08.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename proxy 03/30/23 13:28:08.407
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:08.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:08.415
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 03/30/23 13:28:08.421
STEP: creating replication controller proxy-service-rsjfz in namespace proxy-2486 03/30/23 13:28:08.421
I0330 13:28:08.426478      23 runners.go:193] Created replication controller with name: proxy-service-rsjfz, namespace: proxy-2486, replica count: 1
I0330 13:28:09.477632      23 runners.go:193] proxy-service-rsjfz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 13:28:10.478056      23 runners.go:193] proxy-service-rsjfz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0330 13:28:11.478842      23 runners.go:193] proxy-service-rsjfz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 13:28:11.480: INFO: setup took 3.062904897s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/30/23 13:28:11.48
Mar 30 13:28:11.483: INFO: (0) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.834542ms)
Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 5.08769ms)
Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 5.164199ms)
Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 5.208649ms)
Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 5.252184ms)
Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 5.322218ms)
Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 5.22524ms)
Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 5.220467ms)
Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 5.258486ms)
Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 5.229474ms)
Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 5.229549ms)
Mar 30 13:28:11.487: INFO: (0) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 6.58924ms)
Mar 30 13:28:11.487: INFO: (0) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 6.656656ms)
Mar 30 13:28:11.487: INFO: (0) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 6.6746ms)
Mar 30 13:28:11.487: INFO: (0) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 6.771662ms)
Mar 30 13:28:11.487: INFO: (0) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 6.745461ms)
Mar 30 13:28:11.489: INFO: (1) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.767536ms)
Mar 30 13:28:11.489: INFO: (1) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.858285ms)
Mar 30 13:28:11.489: INFO: (1) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.169294ms)
Mar 30 13:28:11.489: INFO: (1) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.290699ms)
Mar 30 13:28:11.489: INFO: (1) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.4019ms)
Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.366257ms)
Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.579201ms)
Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.571684ms)
Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.683934ms)
Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.7179ms)
Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.600049ms)
Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.752386ms)
Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.852742ms)
Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.01114ms)
Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.268221ms)
Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.202675ms)
Mar 30 13:28:11.492: INFO: (2) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.766047ms)
Mar 30 13:28:11.492: INFO: (2) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.843127ms)
Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.105623ms)
Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.05917ms)
Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.167485ms)
Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.140514ms)
Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.344371ms)
Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.567827ms)
Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.608753ms)
Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.768724ms)
Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.775261ms)
Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.777725ms)
Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.932633ms)
Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.909558ms)
Mar 30 13:28:11.494: INFO: (2) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.119696ms)
Mar 30 13:28:11.494: INFO: (2) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 3.446199ms)
Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.75733ms)
Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.774368ms)
Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.948714ms)
Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.993477ms)
Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.072111ms)
Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.307186ms)
Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.411478ms)
Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.473891ms)
Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.512623ms)
Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.906317ms)
Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 3.116058ms)
Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 3.154116ms)
Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.279601ms)
Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 3.303977ms)
Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 3.284734ms)
Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.343058ms)
Mar 30 13:28:11.499: INFO: (4) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.787055ms)
Mar 30 13:28:11.499: INFO: (4) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.768247ms)
Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.088908ms)
Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.047262ms)
Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.099149ms)
Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.104002ms)
Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.321433ms)
Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.268417ms)
Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.313419ms)
Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.598595ms)
Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.816681ms)
Mar 30 13:28:11.501: INFO: (4) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.979849ms)
Mar 30 13:28:11.501: INFO: (4) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.165244ms)
Mar 30 13:28:11.501: INFO: (4) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.199436ms)
Mar 30 13:28:11.501: INFO: (4) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 3.21738ms)
Mar 30 13:28:11.501: INFO: (4) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 3.280312ms)
Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 1.721064ms)
Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.724092ms)
Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.860538ms)
Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.962367ms)
Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.090284ms)
Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.210371ms)
Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.228819ms)
Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.518447ms)
Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.635208ms)
Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.687069ms)
Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.831238ms)
Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.821535ms)
Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.873421ms)
Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.810807ms)
Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.057236ms)
Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.055334ms)
Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.735307ms)
Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.798904ms)
Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 1.888026ms)
Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.015662ms)
Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.117438ms)
Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.242208ms)
Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.26908ms)
Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.464828ms)
Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.461006ms)
Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.475229ms)
Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.646402ms)
Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.832974ms)
Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.805083ms)
Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.852525ms)
Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.875916ms)
Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.135373ms)
Mar 30 13:28:11.509: INFO: (7) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.901366ms)
Mar 30 13:28:11.509: INFO: (7) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.061551ms)
Mar 30 13:28:11.509: INFO: (7) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.076927ms)
Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.151854ms)
Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.168124ms)
Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.134559ms)
Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.472784ms)
Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.794276ms)
Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.777212ms)
Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.779081ms)
Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.931064ms)
Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.934443ms)
Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 3.062061ms)
Mar 30 13:28:11.511: INFO: (7) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 3.310826ms)
Mar 30 13:28:11.511: INFO: (7) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.366987ms)
Mar 30 13:28:11.511: INFO: (7) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.674519ms)
Mar 30 13:28:11.513: INFO: (8) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.752568ms)
Mar 30 13:28:11.513: INFO: (8) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.071621ms)
Mar 30 13:28:11.513: INFO: (8) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.067459ms)
Mar 30 13:28:11.513: INFO: (8) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.068052ms)
Mar 30 13:28:11.513: INFO: (8) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.267887ms)
Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.336674ms)
Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.410456ms)
Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.464075ms)
Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.438693ms)
Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.475764ms)
Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.765769ms)
Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.741112ms)
Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.994618ms)
Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.109665ms)
Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.140706ms)
Mar 30 13:28:11.515: INFO: (8) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.537224ms)
Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.002864ms)
Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 1.983886ms)
Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.077575ms)
Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.168171ms)
Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.251903ms)
Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.305928ms)
Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.341091ms)
Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.414532ms)
Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.460428ms)
Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.512198ms)
Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.590376ms)
Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.633521ms)
Mar 30 13:28:11.518: INFO: (9) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.859664ms)
Mar 30 13:28:11.518: INFO: (9) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.923678ms)
Mar 30 13:28:11.518: INFO: (9) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.990483ms)
Mar 30 13:28:11.518: INFO: (9) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.078217ms)
Mar 30 13:28:11.519: INFO: (10) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.565131ms)
Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 1.759453ms)
Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.823012ms)
Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 1.91812ms)
Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.954749ms)
Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.978902ms)
Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.12968ms)
Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.36164ms)
Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.535881ms)
Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.497493ms)
Mar 30 13:28:11.521: INFO: (10) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.568596ms)
Mar 30 13:28:11.521: INFO: (10) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.586419ms)
Mar 30 13:28:11.521: INFO: (10) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.858843ms)
Mar 30 13:28:11.521: INFO: (10) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.897327ms)
Mar 30 13:28:11.521: INFO: (10) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.140827ms)
Mar 30 13:28:11.521: INFO: (10) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.268336ms)
Mar 30 13:28:11.523: INFO: (11) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.701943ms)
Mar 30 13:28:11.523: INFO: (11) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.733469ms)
Mar 30 13:28:11.523: INFO: (11) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.93832ms)
Mar 30 13:28:11.523: INFO: (11) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 1.9612ms)
Mar 30 13:28:11.523: INFO: (11) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.989833ms)
Mar 30 13:28:11.523: INFO: (11) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.132308ms)
Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.249104ms)
Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.41142ms)
Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.4824ms)
Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.693167ms)
Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.675108ms)
Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.767627ms)
Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.717041ms)
Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.759504ms)
Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.927988ms)
Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 3.221056ms)
Mar 30 13:28:11.526: INFO: (12) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.765859ms)
Mar 30 13:28:11.526: INFO: (12) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 1.904512ms)
Mar 30 13:28:11.526: INFO: (12) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.874752ms)
Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.093177ms)
Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.158096ms)
Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.147266ms)
Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.177882ms)
Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.457906ms)
Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.601849ms)
Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.585908ms)
Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.56024ms)
Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.576971ms)
Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.615419ms)
Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.708303ms)
Mar 30 13:28:11.528: INFO: (12) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.074742ms)
Mar 30 13:28:11.528: INFO: (12) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.275906ms)
Mar 30 13:28:11.529: INFO: (13) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 1.569567ms)
Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.728516ms)
Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.859793ms)
Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.156686ms)
Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.153408ms)
Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.199683ms)
Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.187049ms)
Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.328744ms)
Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.887605ms)
Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.841234ms)
Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.869228ms)
Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.896239ms)
Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.977517ms)
Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.911444ms)
Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 3.092298ms)
Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.149692ms)
Mar 30 13:28:11.533: INFO: (14) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.746559ms)
Mar 30 13:28:11.533: INFO: (14) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 1.760727ms)
Mar 30 13:28:11.533: INFO: (14) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.039764ms)
Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.432811ms)
Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.441584ms)
Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.432516ms)
Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.41761ms)
Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.596506ms)
Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.64214ms)
Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.697011ms)
Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.697684ms)
Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.967957ms)
Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.049148ms)
Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.229747ms)
Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 3.360743ms)
Mar 30 13:28:11.535: INFO: (14) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.567656ms)
Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.17262ms)
Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.115876ms)
Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.085672ms)
Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.116569ms)
Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.227924ms)
Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.295254ms)
Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.340531ms)
Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.554338ms)
Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.585341ms)
Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.564705ms)
Mar 30 13:28:11.538: INFO: (15) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.733547ms)
Mar 30 13:28:11.538: INFO: (15) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.758053ms)
Mar 30 13:28:11.538: INFO: (15) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.825518ms)
Mar 30 13:28:11.538: INFO: (15) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.833566ms)
Mar 30 13:28:11.538: INFO: (15) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.003575ms)
Mar 30 13:28:11.538: INFO: (15) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.522415ms)
Mar 30 13:28:11.540: INFO: (16) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 1.754869ms)
Mar 30 13:28:11.540: INFO: (16) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 1.955771ms)
Mar 30 13:28:11.540: INFO: (16) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.071931ms)
Mar 30 13:28:11.540: INFO: (16) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.103811ms)
Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.199396ms)
Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.273949ms)
Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.23596ms)
Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.334033ms)
Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.516398ms)
Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.490643ms)
Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.497612ms)
Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.764407ms)
Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.929579ms)
Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.871491ms)
Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.011891ms)
Mar 30 13:28:11.542: INFO: (16) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.219339ms)
Mar 30 13:28:11.543: INFO: (17) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 1.73406ms)
Mar 30 13:28:11.543: INFO: (17) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.76133ms)
Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.898474ms)
Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 1.941087ms)
Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.947341ms)
Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.161817ms)
Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.247919ms)
Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.2301ms)
Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.429246ms)
Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.545269ms)
Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.672966ms)
Mar 30 13:28:11.545: INFO: (17) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.842168ms)
Mar 30 13:28:11.545: INFO: (17) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.893572ms)
Mar 30 13:28:11.545: INFO: (17) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.949853ms)
Mar 30 13:28:11.545: INFO: (17) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.920016ms)
Mar 30 13:28:11.545: INFO: (17) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.137495ms)
Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.657804ms)
Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 1.66557ms)
Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.807751ms)
Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.000691ms)
Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.969394ms)
Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.153476ms)
Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.391008ms)
Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.575061ms)
Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.725736ms)
Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.799256ms)
Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.959153ms)
Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.931198ms)
Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.953523ms)
Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 3.064037ms)
Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.951687ms)
Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.352174ms)
Mar 30 13:28:11.550: INFO: (19) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.773395ms)
Mar 30 13:28:11.550: INFO: (19) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 1.846534ms)
Mar 30 13:28:11.550: INFO: (19) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 1.920292ms)
Mar 30 13:28:11.550: INFO: (19) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.108773ms)
Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.144817ms)
Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.26158ms)
Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.250379ms)
Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.206006ms)
Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.200465ms)
Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.646279ms)
Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.590025ms)
Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.578834ms)
Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.691659ms)
Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.984691ms)
Mar 30 13:28:11.552: INFO: (19) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.085992ms)
Mar 30 13:28:11.552: INFO: (19) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.161479ms)
STEP: deleting ReplicationController proxy-service-rsjfz in namespace proxy-2486, will wait for the garbage collector to delete the pods 03/30/23 13:28:11.552
Mar 30 13:28:11.607: INFO: Deleting ReplicationController proxy-service-rsjfz took: 2.693464ms
Mar 30 13:28:11.708: INFO: Terminating ReplicationController proxy-service-rsjfz pods took: 100.992425ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 30 13:28:14.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2486" for this suite. 03/30/23 13:28:14.011
------------------------------
â€¢ [SLOW TEST] [5.608 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:28:08.407
    Mar 30 13:28:08.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename proxy 03/30/23 13:28:08.407
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:08.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:08.415
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 03/30/23 13:28:08.421
    STEP: creating replication controller proxy-service-rsjfz in namespace proxy-2486 03/30/23 13:28:08.421
    I0330 13:28:08.426478      23 runners.go:193] Created replication controller with name: proxy-service-rsjfz, namespace: proxy-2486, replica count: 1
    I0330 13:28:09.477632      23 runners.go:193] proxy-service-rsjfz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0330 13:28:10.478056      23 runners.go:193] proxy-service-rsjfz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0330 13:28:11.478842      23 runners.go:193] proxy-service-rsjfz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 30 13:28:11.480: INFO: setup took 3.062904897s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/30/23 13:28:11.48
    Mar 30 13:28:11.483: INFO: (0) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.834542ms)
    Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 5.08769ms)
    Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 5.164199ms)
    Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 5.208649ms)
    Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 5.252184ms)
    Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 5.322218ms)
    Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 5.22524ms)
    Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 5.220467ms)
    Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 5.258486ms)
    Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 5.229474ms)
    Mar 30 13:28:11.485: INFO: (0) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 5.229549ms)
    Mar 30 13:28:11.487: INFO: (0) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 6.58924ms)
    Mar 30 13:28:11.487: INFO: (0) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 6.656656ms)
    Mar 30 13:28:11.487: INFO: (0) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 6.6746ms)
    Mar 30 13:28:11.487: INFO: (0) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 6.771662ms)
    Mar 30 13:28:11.487: INFO: (0) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 6.745461ms)
    Mar 30 13:28:11.489: INFO: (1) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.767536ms)
    Mar 30 13:28:11.489: INFO: (1) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.858285ms)
    Mar 30 13:28:11.489: INFO: (1) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.169294ms)
    Mar 30 13:28:11.489: INFO: (1) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.290699ms)
    Mar 30 13:28:11.489: INFO: (1) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.4019ms)
    Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.366257ms)
    Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.579201ms)
    Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.571684ms)
    Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.683934ms)
    Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.7179ms)
    Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.600049ms)
    Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.752386ms)
    Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.852742ms)
    Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.01114ms)
    Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.268221ms)
    Mar 30 13:28:11.490: INFO: (1) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.202675ms)
    Mar 30 13:28:11.492: INFO: (2) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.766047ms)
    Mar 30 13:28:11.492: INFO: (2) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.843127ms)
    Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.105623ms)
    Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.05917ms)
    Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.167485ms)
    Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.140514ms)
    Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.344371ms)
    Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.567827ms)
    Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.608753ms)
    Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.768724ms)
    Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.775261ms)
    Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.777725ms)
    Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.932633ms)
    Mar 30 13:28:11.493: INFO: (2) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.909558ms)
    Mar 30 13:28:11.494: INFO: (2) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.119696ms)
    Mar 30 13:28:11.494: INFO: (2) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 3.446199ms)
    Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.75733ms)
    Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.774368ms)
    Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.948714ms)
    Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.993477ms)
    Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.072111ms)
    Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.307186ms)
    Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.411478ms)
    Mar 30 13:28:11.496: INFO: (3) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.473891ms)
    Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.512623ms)
    Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.906317ms)
    Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 3.116058ms)
    Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 3.154116ms)
    Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.279601ms)
    Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 3.303977ms)
    Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 3.284734ms)
    Mar 30 13:28:11.497: INFO: (3) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.343058ms)
    Mar 30 13:28:11.499: INFO: (4) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.787055ms)
    Mar 30 13:28:11.499: INFO: (4) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.768247ms)
    Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.088908ms)
    Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.047262ms)
    Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.099149ms)
    Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.104002ms)
    Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.321433ms)
    Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.268417ms)
    Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.313419ms)
    Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.598595ms)
    Mar 30 13:28:11.500: INFO: (4) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.816681ms)
    Mar 30 13:28:11.501: INFO: (4) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.979849ms)
    Mar 30 13:28:11.501: INFO: (4) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.165244ms)
    Mar 30 13:28:11.501: INFO: (4) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.199436ms)
    Mar 30 13:28:11.501: INFO: (4) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 3.21738ms)
    Mar 30 13:28:11.501: INFO: (4) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 3.280312ms)
    Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 1.721064ms)
    Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.724092ms)
    Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.860538ms)
    Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.962367ms)
    Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.090284ms)
    Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.210371ms)
    Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.228819ms)
    Mar 30 13:28:11.503: INFO: (5) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.518447ms)
    Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.635208ms)
    Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.687069ms)
    Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.831238ms)
    Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.821535ms)
    Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.873421ms)
    Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.810807ms)
    Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.057236ms)
    Mar 30 13:28:11.504: INFO: (5) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.055334ms)
    Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.735307ms)
    Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.798904ms)
    Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 1.888026ms)
    Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.015662ms)
    Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.117438ms)
    Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.242208ms)
    Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.26908ms)
    Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.464828ms)
    Mar 30 13:28:11.506: INFO: (6) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.461006ms)
    Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.475229ms)
    Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.646402ms)
    Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.832974ms)
    Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.805083ms)
    Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.852525ms)
    Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.875916ms)
    Mar 30 13:28:11.507: INFO: (6) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.135373ms)
    Mar 30 13:28:11.509: INFO: (7) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.901366ms)
    Mar 30 13:28:11.509: INFO: (7) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.061551ms)
    Mar 30 13:28:11.509: INFO: (7) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.076927ms)
    Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.151854ms)
    Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.168124ms)
    Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.134559ms)
    Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.472784ms)
    Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.794276ms)
    Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.777212ms)
    Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.779081ms)
    Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.931064ms)
    Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.934443ms)
    Mar 30 13:28:11.510: INFO: (7) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 3.062061ms)
    Mar 30 13:28:11.511: INFO: (7) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 3.310826ms)
    Mar 30 13:28:11.511: INFO: (7) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.366987ms)
    Mar 30 13:28:11.511: INFO: (7) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.674519ms)
    Mar 30 13:28:11.513: INFO: (8) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.752568ms)
    Mar 30 13:28:11.513: INFO: (8) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.071621ms)
    Mar 30 13:28:11.513: INFO: (8) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.067459ms)
    Mar 30 13:28:11.513: INFO: (8) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.068052ms)
    Mar 30 13:28:11.513: INFO: (8) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.267887ms)
    Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.336674ms)
    Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.410456ms)
    Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.464075ms)
    Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.438693ms)
    Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.475764ms)
    Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.765769ms)
    Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.741112ms)
    Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.994618ms)
    Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.109665ms)
    Mar 30 13:28:11.514: INFO: (8) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.140706ms)
    Mar 30 13:28:11.515: INFO: (8) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.537224ms)
    Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.002864ms)
    Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 1.983886ms)
    Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.077575ms)
    Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.168171ms)
    Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.251903ms)
    Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.305928ms)
    Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.341091ms)
    Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.414532ms)
    Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.460428ms)
    Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.512198ms)
    Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.590376ms)
    Mar 30 13:28:11.517: INFO: (9) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.633521ms)
    Mar 30 13:28:11.518: INFO: (9) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.859664ms)
    Mar 30 13:28:11.518: INFO: (9) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.923678ms)
    Mar 30 13:28:11.518: INFO: (9) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.990483ms)
    Mar 30 13:28:11.518: INFO: (9) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.078217ms)
    Mar 30 13:28:11.519: INFO: (10) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.565131ms)
    Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 1.759453ms)
    Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.823012ms)
    Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 1.91812ms)
    Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.954749ms)
    Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.978902ms)
    Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.12968ms)
    Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.36164ms)
    Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.535881ms)
    Mar 30 13:28:11.520: INFO: (10) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.497493ms)
    Mar 30 13:28:11.521: INFO: (10) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.568596ms)
    Mar 30 13:28:11.521: INFO: (10) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.586419ms)
    Mar 30 13:28:11.521: INFO: (10) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.858843ms)
    Mar 30 13:28:11.521: INFO: (10) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.897327ms)
    Mar 30 13:28:11.521: INFO: (10) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.140827ms)
    Mar 30 13:28:11.521: INFO: (10) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.268336ms)
    Mar 30 13:28:11.523: INFO: (11) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.701943ms)
    Mar 30 13:28:11.523: INFO: (11) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.733469ms)
    Mar 30 13:28:11.523: INFO: (11) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.93832ms)
    Mar 30 13:28:11.523: INFO: (11) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 1.9612ms)
    Mar 30 13:28:11.523: INFO: (11) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.989833ms)
    Mar 30 13:28:11.523: INFO: (11) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.132308ms)
    Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.249104ms)
    Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.41142ms)
    Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.4824ms)
    Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.693167ms)
    Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.675108ms)
    Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.767627ms)
    Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.717041ms)
    Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.759504ms)
    Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.927988ms)
    Mar 30 13:28:11.524: INFO: (11) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 3.221056ms)
    Mar 30 13:28:11.526: INFO: (12) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.765859ms)
    Mar 30 13:28:11.526: INFO: (12) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 1.904512ms)
    Mar 30 13:28:11.526: INFO: (12) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.874752ms)
    Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.093177ms)
    Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.158096ms)
    Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.147266ms)
    Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.177882ms)
    Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.457906ms)
    Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.601849ms)
    Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.585908ms)
    Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.56024ms)
    Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.576971ms)
    Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.615419ms)
    Mar 30 13:28:11.527: INFO: (12) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.708303ms)
    Mar 30 13:28:11.528: INFO: (12) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.074742ms)
    Mar 30 13:28:11.528: INFO: (12) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.275906ms)
    Mar 30 13:28:11.529: INFO: (13) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 1.569567ms)
    Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.728516ms)
    Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.859793ms)
    Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.156686ms)
    Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.153408ms)
    Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.199683ms)
    Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.187049ms)
    Mar 30 13:28:11.530: INFO: (13) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.328744ms)
    Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.887605ms)
    Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.841234ms)
    Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.869228ms)
    Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.896239ms)
    Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.977517ms)
    Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.911444ms)
    Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 3.092298ms)
    Mar 30 13:28:11.531: INFO: (13) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.149692ms)
    Mar 30 13:28:11.533: INFO: (14) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.746559ms)
    Mar 30 13:28:11.533: INFO: (14) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 1.760727ms)
    Mar 30 13:28:11.533: INFO: (14) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.039764ms)
    Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.432811ms)
    Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.441584ms)
    Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.432516ms)
    Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.41761ms)
    Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.596506ms)
    Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.64214ms)
    Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.697011ms)
    Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.697684ms)
    Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.967957ms)
    Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.049148ms)
    Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.229747ms)
    Mar 30 13:28:11.534: INFO: (14) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 3.360743ms)
    Mar 30 13:28:11.535: INFO: (14) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.567656ms)
    Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.17262ms)
    Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.115876ms)
    Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.085672ms)
    Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 2.116569ms)
    Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.227924ms)
    Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.295254ms)
    Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.340531ms)
    Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.554338ms)
    Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.585341ms)
    Mar 30 13:28:11.537: INFO: (15) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.564705ms)
    Mar 30 13:28:11.538: INFO: (15) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.733547ms)
    Mar 30 13:28:11.538: INFO: (15) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.758053ms)
    Mar 30 13:28:11.538: INFO: (15) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.825518ms)
    Mar 30 13:28:11.538: INFO: (15) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.833566ms)
    Mar 30 13:28:11.538: INFO: (15) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.003575ms)
    Mar 30 13:28:11.538: INFO: (15) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.522415ms)
    Mar 30 13:28:11.540: INFO: (16) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 1.754869ms)
    Mar 30 13:28:11.540: INFO: (16) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 1.955771ms)
    Mar 30 13:28:11.540: INFO: (16) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.071931ms)
    Mar 30 13:28:11.540: INFO: (16) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.103811ms)
    Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.199396ms)
    Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.273949ms)
    Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.23596ms)
    Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 2.334033ms)
    Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.516398ms)
    Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.490643ms)
    Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.497612ms)
    Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.764407ms)
    Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.929579ms)
    Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.871491ms)
    Mar 30 13:28:11.541: INFO: (16) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.011891ms)
    Mar 30 13:28:11.542: INFO: (16) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.219339ms)
    Mar 30 13:28:11.543: INFO: (17) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 1.73406ms)
    Mar 30 13:28:11.543: INFO: (17) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.76133ms)
    Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 1.898474ms)
    Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 1.941087ms)
    Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.947341ms)
    Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.161817ms)
    Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.247919ms)
    Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.2301ms)
    Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.429246ms)
    Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.545269ms)
    Mar 30 13:28:11.544: INFO: (17) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.672966ms)
    Mar 30 13:28:11.545: INFO: (17) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.842168ms)
    Mar 30 13:28:11.545: INFO: (17) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.893572ms)
    Mar 30 13:28:11.545: INFO: (17) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.949853ms)
    Mar 30 13:28:11.545: INFO: (17) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.920016ms)
    Mar 30 13:28:11.545: INFO: (17) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 3.137495ms)
    Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 1.657804ms)
    Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 1.66557ms)
    Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 1.807751ms)
    Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 2.000691ms)
    Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.969394ms)
    Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.153476ms)
    Mar 30 13:28:11.547: INFO: (18) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 2.391008ms)
    Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.575061ms)
    Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.725736ms)
    Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.799256ms)
    Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.959153ms)
    Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.931198ms)
    Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 2.953523ms)
    Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 3.064037ms)
    Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.951687ms)
    Mar 30 13:28:11.548: INFO: (18) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 3.352174ms)
    Mar 30 13:28:11.550: INFO: (19) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:462/proxy/: tls qux (200; 1.773395ms)
    Mar 30 13:28:11.550: INFO: (19) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr/proxy/rewriteme">test</a> (200; 1.846534ms)
    Mar 30 13:28:11.550: INFO: (19) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:443/proxy/tlsrewritem... (200; 1.920292ms)
    Mar 30 13:28:11.550: INFO: (19) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">test<... (200; 2.108773ms)
    Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.144817ms)
    Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname2/proxy/: tls qux (200; 2.26158ms)
    Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:160/proxy/: foo (200; 2.250379ms)
    Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:1080/proxy/rewriteme">... (200; 2.206006ms)
    Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/pods/http:proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.200465ms)
    Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/pods/https:proxy-service-rsjfz-hknbr:460/proxy/: tls baz (200; 2.646279ms)
    Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname1/proxy/: foo (200; 2.590025ms)
    Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/pods/proxy-service-rsjfz-hknbr:162/proxy/: bar (200; 2.578834ms)
    Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/services/proxy-service-rsjfz:portname2/proxy/: bar (200; 2.691659ms)
    Mar 30 13:28:11.551: INFO: (19) /api/v1/namespaces/proxy-2486/services/https:proxy-service-rsjfz:tlsportname1/proxy/: tls baz (200; 2.984691ms)
    Mar 30 13:28:11.552: INFO: (19) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname2/proxy/: bar (200; 3.085992ms)
    Mar 30 13:28:11.552: INFO: (19) /api/v1/namespaces/proxy-2486/services/http:proxy-service-rsjfz:portname1/proxy/: foo (200; 3.161479ms)
    STEP: deleting ReplicationController proxy-service-rsjfz in namespace proxy-2486, will wait for the garbage collector to delete the pods 03/30/23 13:28:11.552
    Mar 30 13:28:11.607: INFO: Deleting ReplicationController proxy-service-rsjfz took: 2.693464ms
    Mar 30 13:28:11.708: INFO: Terminating ReplicationController proxy-service-rsjfz pods took: 100.992425ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:28:14.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2486" for this suite. 03/30/23 13:28:14.011
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:28:14.015
Mar 30 13:28:14.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename disruption 03/30/23 13:28:14.015
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:14.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:14.025
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 03/30/23 13:28:14.029
STEP: Waiting for all pods to be running 03/30/23 13:28:16.042
Mar 30 13:28:16.044: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 30 13:28:18.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2867" for this suite. 03/30/23 13:28:18.05
------------------------------
â€¢ [4.038 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:28:14.015
    Mar 30 13:28:14.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename disruption 03/30/23 13:28:14.015
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:14.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:14.025
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 03/30/23 13:28:14.029
    STEP: Waiting for all pods to be running 03/30/23 13:28:16.042
    Mar 30 13:28:16.044: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:28:18.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2867" for this suite. 03/30/23 13:28:18.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:28:18.054
Mar 30 13:28:18.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 13:28:18.054
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:18.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:18.062
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Mar 30 13:28:18.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4359 version'
Mar 30 13:28:18.111: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar 30 13:28:18.111: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26+\", GitVersion:\"v1.26.3-aliyun.1\", GitCommit:\"87950e33acbeebf5d25933ecfacc7a40d193f0fe\", GitTreeState:\"clean\", BuildDate:\"2023-03-29T02:35:30Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 13:28:18.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4359" for this suite. 03/30/23 13:28:18.113
------------------------------
â€¢ [0.062 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:28:18.054
    Mar 30 13:28:18.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 13:28:18.054
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:18.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:18.062
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Mar 30 13:28:18.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4359 version'
    Mar 30 13:28:18.111: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Mar 30 13:28:18.111: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26+\", GitVersion:\"v1.26.3-aliyun.1\", GitCommit:\"87950e33acbeebf5d25933ecfacc7a40d193f0fe\", GitTreeState:\"clean\", BuildDate:\"2023-03-29T02:35:30Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:28:18.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4359" for this suite. 03/30/23 13:28:18.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:28:18.116
Mar 30 13:28:18.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:28:18.117
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:18.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:18.125
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-1545cc91-1877-421a-95b4-a9ddfc744c4c 03/30/23 13:28:18.128
STEP: Creating the pod 03/30/23 13:28:18.13
Mar 30 13:28:18.134: INFO: Waiting up to 5m0s for pod "pod-configmaps-251e348c-fb47-4b5f-8f7f-954c09d68fe3" in namespace "configmap-198" to be "running"
Mar 30 13:28:18.135: INFO: Pod "pod-configmaps-251e348c-fb47-4b5f-8f7f-954c09d68fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.478658ms
Mar 30 13:28:20.138: INFO: Pod "pod-configmaps-251e348c-fb47-4b5f-8f7f-954c09d68fe3": Phase="Running", Reason="", readiness=false. Elapsed: 2.004331631s
Mar 30 13:28:20.138: INFO: Pod "pod-configmaps-251e348c-fb47-4b5f-8f7f-954c09d68fe3" satisfied condition "running"
STEP: Waiting for pod with text data 03/30/23 13:28:20.138
STEP: Waiting for pod with binary data 03/30/23 13:28:20.146
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:28:20.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-198" for this suite. 03/30/23 13:28:20.152
------------------------------
â€¢ [2.038 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:28:18.116
    Mar 30 13:28:18.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:28:18.117
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:18.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:18.125
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-1545cc91-1877-421a-95b4-a9ddfc744c4c 03/30/23 13:28:18.128
    STEP: Creating the pod 03/30/23 13:28:18.13
    Mar 30 13:28:18.134: INFO: Waiting up to 5m0s for pod "pod-configmaps-251e348c-fb47-4b5f-8f7f-954c09d68fe3" in namespace "configmap-198" to be "running"
    Mar 30 13:28:18.135: INFO: Pod "pod-configmaps-251e348c-fb47-4b5f-8f7f-954c09d68fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.478658ms
    Mar 30 13:28:20.138: INFO: Pod "pod-configmaps-251e348c-fb47-4b5f-8f7f-954c09d68fe3": Phase="Running", Reason="", readiness=false. Elapsed: 2.004331631s
    Mar 30 13:28:20.138: INFO: Pod "pod-configmaps-251e348c-fb47-4b5f-8f7f-954c09d68fe3" satisfied condition "running"
    STEP: Waiting for pod with text data 03/30/23 13:28:20.138
    STEP: Waiting for pod with binary data 03/30/23 13:28:20.146
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:28:20.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-198" for this suite. 03/30/23 13:28:20.152
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:28:20.155
Mar 30 13:28:20.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 13:28:20.155
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:20.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:20.163
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:28:20.165
Mar 30 13:28:20.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b" in namespace "downward-api-341" to be "Succeeded or Failed"
Mar 30 13:28:20.170: INFO: Pod "downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.362975ms
Mar 30 13:28:22.172: INFO: Pod "downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003435039s
Mar 30 13:28:24.172: INFO: Pod "downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003469073s
STEP: Saw pod success 03/30/23 13:28:24.172
Mar 30 13:28:24.172: INFO: Pod "downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b" satisfied condition "Succeeded or Failed"
Mar 30 13:28:24.174: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b container client-container: <nil>
STEP: delete the pod 03/30/23 13:28:24.177
Mar 30 13:28:24.182: INFO: Waiting for pod downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b to disappear
Mar 30 13:28:24.201: INFO: Pod downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 30 13:28:24.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-341" for this suite. 03/30/23 13:28:24.203
------------------------------
â€¢ [4.051 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:28:20.155
    Mar 30 13:28:20.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 13:28:20.155
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:20.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:20.163
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:28:20.165
    Mar 30 13:28:20.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b" in namespace "downward-api-341" to be "Succeeded or Failed"
    Mar 30 13:28:20.170: INFO: Pod "downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.362975ms
    Mar 30 13:28:22.172: INFO: Pod "downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003435039s
    Mar 30 13:28:24.172: INFO: Pod "downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003469073s
    STEP: Saw pod success 03/30/23 13:28:24.172
    Mar 30 13:28:24.172: INFO: Pod "downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b" satisfied condition "Succeeded or Failed"
    Mar 30 13:28:24.174: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b container client-container: <nil>
    STEP: delete the pod 03/30/23 13:28:24.177
    Mar 30 13:28:24.182: INFO: Waiting for pod downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b to disappear
    Mar 30 13:28:24.201: INFO: Pod downwardapi-volume-c071a264-5f07-428c-9d41-e3ee75f54b3b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:28:24.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-341" for this suite. 03/30/23 13:28:24.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:28:24.206
Mar 30 13:28:24.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubelet-test 03/30/23 13:28:24.207
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:24.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:24.215
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Mar 30 13:28:24.220: INFO: Waiting up to 5m0s for pod "busybox-scheduling-0121b275-d402-49c3-9429-2b75ba3fdf14" in namespace "kubelet-test-7110" to be "running and ready"
Mar 30 13:28:24.221: INFO: Pod "busybox-scheduling-0121b275-d402-49c3-9429-2b75ba3fdf14": Phase="Pending", Reason="", readiness=false. Elapsed: 1.427513ms
Mar 30 13:28:24.221: INFO: The phase of Pod busybox-scheduling-0121b275-d402-49c3-9429-2b75ba3fdf14 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:28:26.224: INFO: Pod "busybox-scheduling-0121b275-d402-49c3-9429-2b75ba3fdf14": Phase="Running", Reason="", readiness=true. Elapsed: 2.004314465s
Mar 30 13:28:26.224: INFO: The phase of Pod busybox-scheduling-0121b275-d402-49c3-9429-2b75ba3fdf14 is Running (Ready = true)
Mar 30 13:28:26.224: INFO: Pod "busybox-scheduling-0121b275-d402-49c3-9429-2b75ba3fdf14" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 30 13:28:26.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7110" for this suite. 03/30/23 13:28:26.231
------------------------------
â€¢ [2.028 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:28:24.206
    Mar 30 13:28:24.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubelet-test 03/30/23 13:28:24.207
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:24.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:24.215
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Mar 30 13:28:24.220: INFO: Waiting up to 5m0s for pod "busybox-scheduling-0121b275-d402-49c3-9429-2b75ba3fdf14" in namespace "kubelet-test-7110" to be "running and ready"
    Mar 30 13:28:24.221: INFO: Pod "busybox-scheduling-0121b275-d402-49c3-9429-2b75ba3fdf14": Phase="Pending", Reason="", readiness=false. Elapsed: 1.427513ms
    Mar 30 13:28:24.221: INFO: The phase of Pod busybox-scheduling-0121b275-d402-49c3-9429-2b75ba3fdf14 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:28:26.224: INFO: Pod "busybox-scheduling-0121b275-d402-49c3-9429-2b75ba3fdf14": Phase="Running", Reason="", readiness=true. Elapsed: 2.004314465s
    Mar 30 13:28:26.224: INFO: The phase of Pod busybox-scheduling-0121b275-d402-49c3-9429-2b75ba3fdf14 is Running (Ready = true)
    Mar 30 13:28:26.224: INFO: Pod "busybox-scheduling-0121b275-d402-49c3-9429-2b75ba3fdf14" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:28:26.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7110" for this suite. 03/30/23 13:28:26.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:28:26.234
Mar 30 13:28:26.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename statefulset 03/30/23 13:28:26.235
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:26.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:26.243
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2704 03/30/23 13:28:26.245
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 03/30/23 13:28:26.247
STEP: Creating stateful set ss in namespace statefulset-2704 03/30/23 13:28:26.248
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2704 03/30/23 13:28:26.251
Mar 30 13:28:26.252: INFO: Found 0 stateful pods, waiting for 1
Mar 30 13:28:36.255: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/30/23 13:28:36.255
Mar 30 13:28:36.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 13:28:36.353: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 13:28:36.353: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 13:28:36.353: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 13:28:36.355: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 30 13:28:46.361: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 13:28:46.361: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 13:28:46.368: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999987s
Mar 30 13:28:47.371: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997685688s
Mar 30 13:28:48.374: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994552809s
Mar 30 13:28:49.377: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991549493s
Mar 30 13:28:50.379: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989553343s
Mar 30 13:28:51.381: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.987608107s
Mar 30 13:28:52.383: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.985611378s
Mar 30 13:28:53.386: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.982621305s
Mar 30 13:28:54.389: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.979545868s
Mar 30 13:28:55.391: INFO: Verifying statefulset ss doesn't scale past 1 for another 977.492636ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2704 03/30/23 13:28:56.392
Mar 30 13:28:56.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 13:28:56.483: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 13:28:56.483: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 13:28:56.483: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 13:28:56.485: INFO: Found 1 stateful pods, waiting for 3
Mar 30 13:29:06.491: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 13:29:06.491: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 13:29:06.491: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 03/30/23 13:29:06.491
STEP: Scale down will halt with unhealthy stateful pod 03/30/23 13:29:06.491
Mar 30 13:29:06.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 13:29:06.591: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 13:29:06.591: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 13:29:06.591: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 13:29:06.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 13:29:06.683: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 13:29:06.683: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 13:29:06.683: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 13:29:06.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 13:29:06.778: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 13:29:06.778: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 13:29:06.778: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 13:29:06.778: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 13:29:06.779: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 30 13:29:16.783: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 13:29:16.783: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 13:29:16.783: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 13:29:16.789: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999857s
Mar 30 13:29:17.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997657518s
Mar 30 13:29:18.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994657141s
Mar 30 13:29:19.798: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991423247s
Mar 30 13:29:20.801: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988651928s
Mar 30 13:29:21.804: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98616904s
Mar 30 13:29:22.807: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982652773s
Mar 30 13:29:23.810: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97965488s
Mar 30 13:29:24.815: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.976655624s
Mar 30 13:29:25.817: INFO: Verifying statefulset ss doesn't scale past 3 for another 972.651419ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2704 03/30/23 13:29:26.818
Mar 30 13:29:26.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 13:29:26.917: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 13:29:26.917: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 13:29:26.917: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 13:29:26.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 13:29:27.010: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 13:29:27.010: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 13:29:27.010: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 13:29:27.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 13:29:27.100: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 13:29:27.100: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 13:29:27.100: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 13:29:27.100: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 03/30/23 13:29:37.108
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 30 13:29:37.108: INFO: Deleting all statefulset in ns statefulset-2704
Mar 30 13:29:37.110: INFO: Scaling statefulset ss to 0
Mar 30 13:29:37.115: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 13:29:37.116: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 30 13:29:37.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2704" for this suite. 03/30/23 13:29:37.123
------------------------------
â€¢ [SLOW TEST] [70.891 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:28:26.234
    Mar 30 13:28:26.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename statefulset 03/30/23 13:28:26.235
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:28:26.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:28:26.243
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2704 03/30/23 13:28:26.245
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 03/30/23 13:28:26.247
    STEP: Creating stateful set ss in namespace statefulset-2704 03/30/23 13:28:26.248
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2704 03/30/23 13:28:26.251
    Mar 30 13:28:26.252: INFO: Found 0 stateful pods, waiting for 1
    Mar 30 13:28:36.255: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/30/23 13:28:36.255
    Mar 30 13:28:36.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 30 13:28:36.353: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 30 13:28:36.353: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 30 13:28:36.353: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 30 13:28:36.355: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 30 13:28:46.361: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 30 13:28:46.361: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 30 13:28:46.368: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999987s
    Mar 30 13:28:47.371: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997685688s
    Mar 30 13:28:48.374: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994552809s
    Mar 30 13:28:49.377: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991549493s
    Mar 30 13:28:50.379: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989553343s
    Mar 30 13:28:51.381: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.987608107s
    Mar 30 13:28:52.383: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.985611378s
    Mar 30 13:28:53.386: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.982621305s
    Mar 30 13:28:54.389: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.979545868s
    Mar 30 13:28:55.391: INFO: Verifying statefulset ss doesn't scale past 1 for another 977.492636ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2704 03/30/23 13:28:56.392
    Mar 30 13:28:56.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 30 13:28:56.483: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 30 13:28:56.483: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 30 13:28:56.483: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 30 13:28:56.485: INFO: Found 1 stateful pods, waiting for 3
    Mar 30 13:29:06.491: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 30 13:29:06.491: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 30 13:29:06.491: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 03/30/23 13:29:06.491
    STEP: Scale down will halt with unhealthy stateful pod 03/30/23 13:29:06.491
    Mar 30 13:29:06.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 30 13:29:06.591: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 30 13:29:06.591: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 30 13:29:06.591: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 30 13:29:06.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 30 13:29:06.683: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 30 13:29:06.683: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 30 13:29:06.683: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 30 13:29:06.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 30 13:29:06.778: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 30 13:29:06.778: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 30 13:29:06.778: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 30 13:29:06.778: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 30 13:29:06.779: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Mar 30 13:29:16.783: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 30 13:29:16.783: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 30 13:29:16.783: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 30 13:29:16.789: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999857s
    Mar 30 13:29:17.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997657518s
    Mar 30 13:29:18.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994657141s
    Mar 30 13:29:19.798: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991423247s
    Mar 30 13:29:20.801: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988651928s
    Mar 30 13:29:21.804: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98616904s
    Mar 30 13:29:22.807: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982652773s
    Mar 30 13:29:23.810: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97965488s
    Mar 30 13:29:24.815: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.976655624s
    Mar 30 13:29:25.817: INFO: Verifying statefulset ss doesn't scale past 3 for another 972.651419ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2704 03/30/23 13:29:26.818
    Mar 30 13:29:26.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 30 13:29:26.917: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 30 13:29:26.917: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 30 13:29:26.917: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 30 13:29:26.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 30 13:29:27.010: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 30 13:29:27.010: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 30 13:29:27.010: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 30 13:29:27.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-2704 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 30 13:29:27.100: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 30 13:29:27.100: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 30 13:29:27.100: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 30 13:29:27.100: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 03/30/23 13:29:37.108
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 30 13:29:37.108: INFO: Deleting all statefulset in ns statefulset-2704
    Mar 30 13:29:37.110: INFO: Scaling statefulset ss to 0
    Mar 30 13:29:37.115: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 30 13:29:37.116: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:29:37.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2704" for this suite. 03/30/23 13:29:37.123
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:29:37.126
Mar 30 13:29:37.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename replication-controller 03/30/23 13:29:37.126
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:29:37.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:29:37.134
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 03/30/23 13:29:37.136
STEP: When the matched label of one of its pods change 03/30/23 13:29:37.138
Mar 30 13:29:37.140: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 30 13:29:42.142: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 03/30/23 13:29:42.148
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 30 13:29:43.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1955" for this suite. 03/30/23 13:29:43.153
------------------------------
â€¢ [SLOW TEST] [6.030 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:29:37.126
    Mar 30 13:29:37.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename replication-controller 03/30/23 13:29:37.126
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:29:37.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:29:37.134
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 03/30/23 13:29:37.136
    STEP: When the matched label of one of its pods change 03/30/23 13:29:37.138
    Mar 30 13:29:37.140: INFO: Pod name pod-release: Found 0 pods out of 1
    Mar 30 13:29:42.142: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/30/23 13:29:42.148
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:29:43.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1955" for this suite. 03/30/23 13:29:43.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:29:43.156
Mar 30 13:29:43.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename sched-preemption 03/30/23 13:29:43.156
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:29:43.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:29:43.164
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 30 13:29:43.171: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 30 13:30:43.196: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:30:43.197
Mar 30 13:30:43.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename sched-preemption-path 03/30/23 13:30:43.198
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:30:43.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:30:43.206
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Mar 30 13:30:43.214: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Mar 30 13:30:43.215: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Mar 30 13:30:43.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:30:43.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7442" for this suite. 03/30/23 13:30:43.25
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-0" for this suite. 03/30/23 13:30:43.253
------------------------------
â€¢ [SLOW TEST] [60.099 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:29:43.156
    Mar 30 13:29:43.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename sched-preemption 03/30/23 13:29:43.156
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:29:43.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:29:43.164
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 30 13:29:43.171: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 30 13:30:43.196: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:30:43.197
    Mar 30 13:30:43.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename sched-preemption-path 03/30/23 13:30:43.198
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:30:43.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:30:43.206
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Mar 30 13:30:43.214: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Mar 30 13:30:43.215: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:30:43.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:30:43.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7442" for this suite. 03/30/23 13:30:43.25
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-0" for this suite. 03/30/23 13:30:43.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:30:43.256
Mar 30 13:30:43.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename namespaces 03/30/23 13:30:43.256
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:30:43.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:30:43.264
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 03/30/23 13:30:43.266
STEP: patching the Namespace 03/30/23 13:30:43.271
STEP: get the Namespace and ensuring it has the label 03/30/23 13:30:43.273
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:30:43.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3288" for this suite. 03/30/23 13:30:43.277
STEP: Destroying namespace "nspatchtest-4e1b4d98-481a-4086-9505-bb24382a35a7-7859" for this suite. 03/30/23 13:30:43.279
------------------------------
â€¢ [0.026 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:30:43.256
    Mar 30 13:30:43.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename namespaces 03/30/23 13:30:43.256
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:30:43.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:30:43.264
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 03/30/23 13:30:43.266
    STEP: patching the Namespace 03/30/23 13:30:43.271
    STEP: get the Namespace and ensuring it has the label 03/30/23 13:30:43.273
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:30:43.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3288" for this suite. 03/30/23 13:30:43.277
    STEP: Destroying namespace "nspatchtest-4e1b4d98-481a-4086-9505-bb24382a35a7-7859" for this suite. 03/30/23 13:30:43.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:30:43.282
Mar 30 13:30:43.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 13:30:43.282
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:30:43.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:30:43.289
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8975 03/30/23 13:30:43.291
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/30/23 13:30:43.295
STEP: creating service externalsvc in namespace services-8975 03/30/23 13:30:43.295
STEP: creating replication controller externalsvc in namespace services-8975 03/30/23 13:30:43.3
I0330 13:30:43.303807      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8975, replica count: 2
I0330 13:30:46.355548      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 03/30/23 13:30:46.357
Mar 30 13:30:46.363: INFO: Creating new exec pod
Mar 30 13:30:46.367: INFO: Waiting up to 5m0s for pod "execpodh4hfn" in namespace "services-8975" to be "running"
Mar 30 13:30:46.369: INFO: Pod "execpodh4hfn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.692652ms
Mar 30 13:30:48.374: INFO: Pod "execpodh4hfn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006633564s
Mar 30 13:30:48.374: INFO: Pod "execpodh4hfn" satisfied condition "running"
Mar 30 13:30:48.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-8975 exec execpodh4hfn -- /bin/sh -x -c nslookup clusterip-service.services-8975.svc.cluster.local'
Mar 30 13:30:48.485: INFO: stderr: "+ nslookup clusterip-service.services-8975.svc.cluster.local\n"
Mar 30 13:30:48.485: INFO: stdout: "Server:\t\t172.16.0.10\nAddress:\t172.16.0.10#53\n\nclusterip-service.services-8975.svc.cluster.local\tcanonical name = externalsvc.services-8975.svc.cluster.local.\nName:\texternalsvc.services-8975.svc.cluster.local\nAddress: 172.16.22.141\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8975, will wait for the garbage collector to delete the pods 03/30/23 13:30:48.485
Mar 30 13:30:48.540: INFO: Deleting ReplicationController externalsvc took: 2.533205ms
Mar 30 13:30:48.641: INFO: Terminating ReplicationController externalsvc pods took: 100.501974ms
Mar 30 13:30:50.248: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 13:30:50.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8975" for this suite. 03/30/23 13:30:50.255
------------------------------
â€¢ [SLOW TEST] [6.976 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:30:43.282
    Mar 30 13:30:43.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 13:30:43.282
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:30:43.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:30:43.289
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8975 03/30/23 13:30:43.291
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/30/23 13:30:43.295
    STEP: creating service externalsvc in namespace services-8975 03/30/23 13:30:43.295
    STEP: creating replication controller externalsvc in namespace services-8975 03/30/23 13:30:43.3
    I0330 13:30:43.303807      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8975, replica count: 2
    I0330 13:30:46.355548      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 03/30/23 13:30:46.357
    Mar 30 13:30:46.363: INFO: Creating new exec pod
    Mar 30 13:30:46.367: INFO: Waiting up to 5m0s for pod "execpodh4hfn" in namespace "services-8975" to be "running"
    Mar 30 13:30:46.369: INFO: Pod "execpodh4hfn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.692652ms
    Mar 30 13:30:48.374: INFO: Pod "execpodh4hfn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006633564s
    Mar 30 13:30:48.374: INFO: Pod "execpodh4hfn" satisfied condition "running"
    Mar 30 13:30:48.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-8975 exec execpodh4hfn -- /bin/sh -x -c nslookup clusterip-service.services-8975.svc.cluster.local'
    Mar 30 13:30:48.485: INFO: stderr: "+ nslookup clusterip-service.services-8975.svc.cluster.local\n"
    Mar 30 13:30:48.485: INFO: stdout: "Server:\t\t172.16.0.10\nAddress:\t172.16.0.10#53\n\nclusterip-service.services-8975.svc.cluster.local\tcanonical name = externalsvc.services-8975.svc.cluster.local.\nName:\texternalsvc.services-8975.svc.cluster.local\nAddress: 172.16.22.141\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8975, will wait for the garbage collector to delete the pods 03/30/23 13:30:48.485
    Mar 30 13:30:48.540: INFO: Deleting ReplicationController externalsvc took: 2.533205ms
    Mar 30 13:30:48.641: INFO: Terminating ReplicationController externalsvc pods took: 100.501974ms
    Mar 30 13:30:50.248: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:30:50.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8975" for this suite. 03/30/23 13:30:50.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:30:50.259
Mar 30 13:30:50.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:30:50.26
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:30:50.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:30:50.268
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-1483/configmap-test-8994928f-bef4-409d-8238-5e33a6c3dff6 03/30/23 13:30:50.27
STEP: Creating a pod to test consume configMaps 03/30/23 13:30:50.272
Mar 30 13:30:50.276: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa" in namespace "configmap-1483" to be "Succeeded or Failed"
Mar 30 13:30:50.277: INFO: Pod "pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463407ms
Mar 30 13:30:52.280: INFO: Pod "pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00425881s
Mar 30 13:30:54.280: INFO: Pod "pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004275315s
STEP: Saw pod success 03/30/23 13:30:54.28
Mar 30 13:30:54.280: INFO: Pod "pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa" satisfied condition "Succeeded or Failed"
Mar 30 13:30:54.282: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa container env-test: <nil>
STEP: delete the pod 03/30/23 13:30:54.292
Mar 30 13:30:54.296: INFO: Waiting for pod pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa to disappear
Mar 30 13:30:54.298: INFO: Pod pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:30:54.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1483" for this suite. 03/30/23 13:30:54.3
------------------------------
â€¢ [4.044 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:30:50.259
    Mar 30 13:30:50.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:30:50.26
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:30:50.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:30:50.268
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-1483/configmap-test-8994928f-bef4-409d-8238-5e33a6c3dff6 03/30/23 13:30:50.27
    STEP: Creating a pod to test consume configMaps 03/30/23 13:30:50.272
    Mar 30 13:30:50.276: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa" in namespace "configmap-1483" to be "Succeeded or Failed"
    Mar 30 13:30:50.277: INFO: Pod "pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463407ms
    Mar 30 13:30:52.280: INFO: Pod "pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00425881s
    Mar 30 13:30:54.280: INFO: Pod "pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004275315s
    STEP: Saw pod success 03/30/23 13:30:54.28
    Mar 30 13:30:54.280: INFO: Pod "pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa" satisfied condition "Succeeded or Failed"
    Mar 30 13:30:54.282: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa container env-test: <nil>
    STEP: delete the pod 03/30/23 13:30:54.292
    Mar 30 13:30:54.296: INFO: Waiting for pod pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa to disappear
    Mar 30 13:30:54.298: INFO: Pod pod-configmaps-ba10b730-72ec-4c9a-bf2d-f0c71fd38faa no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:30:54.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1483" for this suite. 03/30/23 13:30:54.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:30:54.303
Mar 30 13:30:54.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename daemonsets 03/30/23 13:30:54.304
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:30:54.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:30:54.312
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Mar 30 13:30:54.324: INFO: Create a RollingUpdate DaemonSet
Mar 30 13:30:54.326: INFO: Check that daemon pods launch on every node of the cluster
Mar 30 13:30:54.328: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:30:54.328: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:30:54.328: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:30:54.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 13:30:54.329: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
Mar 30 13:30:55.332: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:30:55.332: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:30:55.332: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:30:55.334: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 30 13:30:55.334: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Mar 30 13:30:55.334: INFO: Update the DaemonSet to trigger a rollout
Mar 30 13:30:55.338: INFO: Updating DaemonSet daemon-set
Mar 30 13:30:58.349: INFO: Roll back the DaemonSet before rollout is complete
Mar 30 13:30:58.354: INFO: Updating DaemonSet daemon-set
Mar 30 13:30:58.354: INFO: Make sure DaemonSet rollback is complete
Mar 30 13:30:58.355: INFO: Wrong image for pod: daemon-set-c7hxm. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Mar 30 13:30:58.355: INFO: Pod daemon-set-c7hxm is not available
Mar 30 13:30:58.360: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:30:58.360: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:30:58.360: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:30:59.365: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:30:59.365: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:30:59.365: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:31:00.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:31:00.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:31:00.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:31:01.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:31:01.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:31:01.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:31:02.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:31:02.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:31:02.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:31:03.363: INFO: Pod daemon-set-cvlcr is not available
Mar 30 13:31:03.365: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:31:03.365: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:31:03.365: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/30/23 13:31:03.368
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1502, will wait for the garbage collector to delete the pods 03/30/23 13:31:03.368
Mar 30 13:31:03.423: INFO: Deleting DaemonSet.extensions daemon-set took: 2.559516ms
Mar 30 13:31:03.523: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.405574ms
Mar 30 13:31:05.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 13:31:05.225: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 30 13:31:05.227: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"66190"},"items":null}

Mar 30 13:31:05.228: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"66190"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:31:05.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1502" for this suite. 03/30/23 13:31:05.237
------------------------------
â€¢ [SLOW TEST] [10.936 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:30:54.303
    Mar 30 13:30:54.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename daemonsets 03/30/23 13:30:54.304
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:30:54.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:30:54.312
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Mar 30 13:30:54.324: INFO: Create a RollingUpdate DaemonSet
    Mar 30 13:30:54.326: INFO: Check that daemon pods launch on every node of the cluster
    Mar 30 13:30:54.328: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:30:54.328: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:30:54.328: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:30:54.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 13:30:54.329: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
    Mar 30 13:30:55.332: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:30:55.332: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:30:55.332: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:30:55.334: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 30 13:30:55.334: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Mar 30 13:30:55.334: INFO: Update the DaemonSet to trigger a rollout
    Mar 30 13:30:55.338: INFO: Updating DaemonSet daemon-set
    Mar 30 13:30:58.349: INFO: Roll back the DaemonSet before rollout is complete
    Mar 30 13:30:58.354: INFO: Updating DaemonSet daemon-set
    Mar 30 13:30:58.354: INFO: Make sure DaemonSet rollback is complete
    Mar 30 13:30:58.355: INFO: Wrong image for pod: daemon-set-c7hxm. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Mar 30 13:30:58.355: INFO: Pod daemon-set-c7hxm is not available
    Mar 30 13:30:58.360: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:30:58.360: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:30:58.360: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:30:59.365: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:30:59.365: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:30:59.365: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:31:00.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:31:00.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:31:00.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:31:01.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:31:01.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:31:01.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:31:02.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:31:02.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:31:02.366: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:31:03.363: INFO: Pod daemon-set-cvlcr is not available
    Mar 30 13:31:03.365: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:31:03.365: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:31:03.365: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/30/23 13:31:03.368
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1502, will wait for the garbage collector to delete the pods 03/30/23 13:31:03.368
    Mar 30 13:31:03.423: INFO: Deleting DaemonSet.extensions daemon-set took: 2.559516ms
    Mar 30 13:31:03.523: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.405574ms
    Mar 30 13:31:05.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 13:31:05.225: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 30 13:31:05.227: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"66190"},"items":null}

    Mar 30 13:31:05.228: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"66190"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:31:05.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1502" for this suite. 03/30/23 13:31:05.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:31:05.24
Mar 30 13:31:05.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename replication-controller 03/30/23 13:31:05.241
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:05.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:05.249
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7 03/30/23 13:31:05.251
Mar 30 13:31:05.256: INFO: Pod name my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7: Found 0 pods out of 1
Mar 30 13:31:10.261: INFO: Pod name my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7: Found 1 pods out of 1
Mar 30 13:31:10.261: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7" are running
Mar 30 13:31:10.261: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7-mx2vd" in namespace "replication-controller-3012" to be "running"
Mar 30 13:31:10.263: INFO: Pod "my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7-mx2vd": Phase="Running", Reason="", readiness=true. Elapsed: 1.797692ms
Mar 30 13:31:10.263: INFO: Pod "my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7-mx2vd" satisfied condition "running"
Mar 30 13:31:10.263: INFO: Pod "my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7-mx2vd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 13:31:05 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 13:31:06 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 13:31:06 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 13:31:05 +0000 UTC Reason: Message:}])
Mar 30 13:31:10.263: INFO: Trying to dial the pod
Mar 30 13:31:15.269: INFO: Controller my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7: Got expected result from replica 1 [my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7-mx2vd]: "my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7-mx2vd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 30 13:31:15.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3012" for this suite. 03/30/23 13:31:15.271
------------------------------
â€¢ [SLOW TEST] [10.033 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:31:05.24
    Mar 30 13:31:05.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename replication-controller 03/30/23 13:31:05.241
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:05.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:05.249
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7 03/30/23 13:31:05.251
    Mar 30 13:31:05.256: INFO: Pod name my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7: Found 0 pods out of 1
    Mar 30 13:31:10.261: INFO: Pod name my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7: Found 1 pods out of 1
    Mar 30 13:31:10.261: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7" are running
    Mar 30 13:31:10.261: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7-mx2vd" in namespace "replication-controller-3012" to be "running"
    Mar 30 13:31:10.263: INFO: Pod "my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7-mx2vd": Phase="Running", Reason="", readiness=true. Elapsed: 1.797692ms
    Mar 30 13:31:10.263: INFO: Pod "my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7-mx2vd" satisfied condition "running"
    Mar 30 13:31:10.263: INFO: Pod "my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7-mx2vd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 13:31:05 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 13:31:06 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 13:31:06 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 13:31:05 +0000 UTC Reason: Message:}])
    Mar 30 13:31:10.263: INFO: Trying to dial the pod
    Mar 30 13:31:15.269: INFO: Controller my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7: Got expected result from replica 1 [my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7-mx2vd]: "my-hostname-basic-9b692e9e-957c-4902-89ba-49bcd10cb1b7-mx2vd", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:31:15.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3012" for this suite. 03/30/23 13:31:15.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:31:15.274
Mar 30 13:31:15.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename resourcequota 03/30/23 13:31:15.275
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:15.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:15.283
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 03/30/23 13:31:15.284
STEP: Creating a ResourceQuota 03/30/23 13:31:20.287
STEP: Ensuring resource quota status is calculated 03/30/23 13:31:20.289
STEP: Creating a Pod that fits quota 03/30/23 13:31:22.291
STEP: Ensuring ResourceQuota status captures the pod usage 03/30/23 13:31:22.298
STEP: Not allowing a pod to be created that exceeds remaining quota 03/30/23 13:31:24.3
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/30/23 13:31:24.302
STEP: Ensuring a pod cannot update its resource requirements 03/30/23 13:31:24.303
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/30/23 13:31:24.305
STEP: Deleting the pod 03/30/23 13:31:26.307
STEP: Ensuring resource quota status released the pod usage 03/30/23 13:31:26.313
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 30 13:31:28.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5498" for this suite. 03/30/23 13:31:28.318
------------------------------
â€¢ [SLOW TEST] [13.047 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:31:15.274
    Mar 30 13:31:15.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename resourcequota 03/30/23 13:31:15.275
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:15.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:15.283
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 03/30/23 13:31:15.284
    STEP: Creating a ResourceQuota 03/30/23 13:31:20.287
    STEP: Ensuring resource quota status is calculated 03/30/23 13:31:20.289
    STEP: Creating a Pod that fits quota 03/30/23 13:31:22.291
    STEP: Ensuring ResourceQuota status captures the pod usage 03/30/23 13:31:22.298
    STEP: Not allowing a pod to be created that exceeds remaining quota 03/30/23 13:31:24.3
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/30/23 13:31:24.302
    STEP: Ensuring a pod cannot update its resource requirements 03/30/23 13:31:24.303
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/30/23 13:31:24.305
    STEP: Deleting the pod 03/30/23 13:31:26.307
    STEP: Ensuring resource quota status released the pod usage 03/30/23 13:31:26.313
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:31:28.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5498" for this suite. 03/30/23 13:31:28.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:31:28.322
Mar 30 13:31:28.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 13:31:28.323
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:28.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:28.331
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 03/30/23 13:31:28.332
Mar 30 13:31:28.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-2357 cluster-info'
Mar 30 13:31:28.382: INFO: stderr: ""
Mar 30 13:31:28.382: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.16.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 13:31:28.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2357" for this suite. 03/30/23 13:31:28.385
------------------------------
â€¢ [0.065 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:31:28.322
    Mar 30 13:31:28.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 13:31:28.323
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:28.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:28.331
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 03/30/23 13:31:28.332
    Mar 30 13:31:28.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-2357 cluster-info'
    Mar 30 13:31:28.382: INFO: stderr: ""
    Mar 30 13:31:28.382: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.16.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:31:28.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2357" for this suite. 03/30/23 13:31:28.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:31:28.388
Mar 30 13:31:28.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 13:31:28.389
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:28.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:28.396
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 03/30/23 13:31:28.398
Mar 30 13:31:28.402: INFO: Waiting up to 5m0s for pod "downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b" in namespace "downward-api-1219" to be "Succeeded or Failed"
Mar 30 13:31:28.403: INFO: Pod "downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463716ms
Mar 30 13:31:30.406: INFO: Pod "downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003910445s
Mar 30 13:31:32.406: INFO: Pod "downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004111426s
STEP: Saw pod success 03/30/23 13:31:32.406
Mar 30 13:31:32.406: INFO: Pod "downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b" satisfied condition "Succeeded or Failed"
Mar 30 13:31:32.408: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b container dapi-container: <nil>
STEP: delete the pod 03/30/23 13:31:32.418
Mar 30 13:31:32.424: INFO: Waiting for pod downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b to disappear
Mar 30 13:31:32.425: INFO: Pod downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 30 13:31:32.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1219" for this suite. 03/30/23 13:31:32.427
------------------------------
â€¢ [4.042 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:31:28.388
    Mar 30 13:31:28.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 13:31:28.389
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:28.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:28.396
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 03/30/23 13:31:28.398
    Mar 30 13:31:28.402: INFO: Waiting up to 5m0s for pod "downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b" in namespace "downward-api-1219" to be "Succeeded or Failed"
    Mar 30 13:31:28.403: INFO: Pod "downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463716ms
    Mar 30 13:31:30.406: INFO: Pod "downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003910445s
    Mar 30 13:31:32.406: INFO: Pod "downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004111426s
    STEP: Saw pod success 03/30/23 13:31:32.406
    Mar 30 13:31:32.406: INFO: Pod "downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b" satisfied condition "Succeeded or Failed"
    Mar 30 13:31:32.408: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b container dapi-container: <nil>
    STEP: delete the pod 03/30/23 13:31:32.418
    Mar 30 13:31:32.424: INFO: Waiting for pod downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b to disappear
    Mar 30 13:31:32.425: INFO: Pod downward-api-3d3be47c-d51a-425a-85af-0f1edfe5057b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:31:32.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1219" for this suite. 03/30/23 13:31:32.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:31:32.431
Mar 30 13:31:32.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 13:31:32.431
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:32.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:32.439
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 03/30/23 13:31:32.442
STEP: waiting for available Endpoint 03/30/23 13:31:32.444
STEP: listing all Endpoints 03/30/23 13:31:32.444
STEP: updating the Endpoint 03/30/23 13:31:32.446
STEP: fetching the Endpoint 03/30/23 13:31:32.449
STEP: patching the Endpoint 03/30/23 13:31:32.45
STEP: fetching the Endpoint 03/30/23 13:31:32.454
STEP: deleting the Endpoint by Collection 03/30/23 13:31:32.455
STEP: waiting for Endpoint deletion 03/30/23 13:31:32.458
STEP: fetching the Endpoint 03/30/23 13:31:32.459
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 13:31:32.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9924" for this suite. 03/30/23 13:31:32.462
------------------------------
â€¢ [0.034 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:31:32.431
    Mar 30 13:31:32.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 13:31:32.431
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:32.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:32.439
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 03/30/23 13:31:32.442
    STEP: waiting for available Endpoint 03/30/23 13:31:32.444
    STEP: listing all Endpoints 03/30/23 13:31:32.444
    STEP: updating the Endpoint 03/30/23 13:31:32.446
    STEP: fetching the Endpoint 03/30/23 13:31:32.449
    STEP: patching the Endpoint 03/30/23 13:31:32.45
    STEP: fetching the Endpoint 03/30/23 13:31:32.454
    STEP: deleting the Endpoint by Collection 03/30/23 13:31:32.455
    STEP: waiting for Endpoint deletion 03/30/23 13:31:32.458
    STEP: fetching the Endpoint 03/30/23 13:31:32.459
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:31:32.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9924" for this suite. 03/30/23 13:31:32.462
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:31:32.465
Mar 30 13:31:32.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename gc 03/30/23 13:31:32.465
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:32.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:32.473
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 03/30/23 13:31:32.476
STEP: create the rc2 03/30/23 13:31:32.478
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/30/23 13:31:37.484
STEP: delete the rc simpletest-rc-to-be-deleted 03/30/23 13:31:37.625
STEP: wait for the rc to be deleted 03/30/23 13:31:37.628
Mar 30 13:31:42.634: INFO: 65 pods remaining
Mar 30 13:31:42.634: INFO: 65 pods has nil DeletionTimestamp
Mar 30 13:31:42.634: INFO: 
STEP: Gathering metrics 03/30/23 13:31:47.633
Mar 30 13:31:47.648: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cn-hongkong.192.168.0.252" in namespace "kube-system" to be "running and ready"
Mar 30 13:31:47.649: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252": Phase="Running", Reason="", readiness=true. Elapsed: 1.335094ms
Mar 30 13:31:47.649: INFO: The phase of Pod kube-controller-manager-cn-hongkong.192.168.0.252 is Running (Ready = true)
Mar 30 13:31:47.649: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252" satisfied condition "running and ready"
Mar 30 13:31:47.685: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 30 13:31:47.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-2c8v9" in namespace "gc-1414"
Mar 30 13:31:47.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jkm6" in namespace "gc-1414"
Mar 30 13:31:47.694: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vlkg" in namespace "gc-1414"
Mar 30 13:31:47.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-4274m" in namespace "gc-1414"
Mar 30 13:31:47.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hpxk" in namespace "gc-1414"
Mar 30 13:31:47.705: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qkl7" in namespace "gc-1414"
Mar 30 13:31:47.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tthq" in namespace "gc-1414"
Mar 30 13:31:47.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-684v5" in namespace "gc-1414"
Mar 30 13:31:47.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-695qf" in namespace "gc-1414"
Mar 30 13:31:47.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dh95" in namespace "gc-1414"
Mar 30 13:31:47.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mngz" in namespace "gc-1414"
Mar 30 13:31:47.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sdjr" in namespace "gc-1414"
Mar 30 13:31:47.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-7h6dd" in namespace "gc-1414"
Mar 30 13:31:47.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hw5n" in namespace "gc-1414"
Mar 30 13:31:47.740: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qmd7" in namespace "gc-1414"
Mar 30 13:31:47.745: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sdj5" in namespace "gc-1414"
Mar 30 13:31:47.749: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vw42" in namespace "gc-1414"
Mar 30 13:31:47.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-98ggj" in namespace "gc-1414"
Mar 30 13:31:47.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cp6q" in namespace "gc-1414"
Mar 30 13:31:47.761: INFO: Deleting pod "simpletest-rc-to-be-deleted-9g6wk" in namespace "gc-1414"
Mar 30 13:31:47.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wpxp" in namespace "gc-1414"
Mar 30 13:31:47.769: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9j5q" in namespace "gc-1414"
Mar 30 13:31:47.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-bb7sb" in namespace "gc-1414"
Mar 30 13:31:47.779: INFO: Deleting pod "simpletest-rc-to-be-deleted-blfz5" in namespace "gc-1414"
Mar 30 13:31:47.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnqm8" in namespace "gc-1414"
Mar 30 13:31:47.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv2st" in namespace "gc-1414"
Mar 30 13:31:47.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-cc9fq" in namespace "gc-1414"
Mar 30 13:31:47.795: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfnlp" in namespace "gc-1414"
Mar 30 13:31:47.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-chrd6" in namespace "gc-1414"
Mar 30 13:31:47.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-clfbs" in namespace "gc-1414"
Mar 30 13:31:47.806: INFO: Deleting pod "simpletest-rc-to-be-deleted-cpchp" in namespace "gc-1414"
Mar 30 13:31:47.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqmgq" in namespace "gc-1414"
Mar 30 13:31:47.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-cr59w" in namespace "gc-1414"
Mar 30 13:31:47.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-css8j" in namespace "gc-1414"
Mar 30 13:31:47.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxwpr" in namespace "gc-1414"
Mar 30 13:31:47.832: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9hbb" in namespace "gc-1414"
Mar 30 13:31:47.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkkdb" in namespace "gc-1414"
Mar 30 13:31:47.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlcfg" in namespace "gc-1414"
Mar 30 13:31:47.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-ds968" in namespace "gc-1414"
Mar 30 13:31:47.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxtps" in namespace "gc-1414"
Mar 30 13:31:47.855: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgqnb" in namespace "gc-1414"
Mar 30 13:31:47.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmcdr" in namespace "gc-1414"
Mar 30 13:31:47.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw68h" in namespace "gc-1414"
Mar 30 13:31:47.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwp2f" in namespace "gc-1414"
Mar 30 13:31:47.871: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2ffx" in namespace "gc-1414"
Mar 30 13:31:47.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdp6v" in namespace "gc-1414"
Mar 30 13:31:47.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-gz96k" in namespace "gc-1414"
Mar 30 13:31:47.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjwdq" in namespace "gc-1414"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 30 13:31:47.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1414" for this suite. 03/30/23 13:31:47.892
------------------------------
â€¢ [SLOW TEST] [15.429 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:31:32.465
    Mar 30 13:31:32.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename gc 03/30/23 13:31:32.465
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:32.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:32.473
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 03/30/23 13:31:32.476
    STEP: create the rc2 03/30/23 13:31:32.478
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/30/23 13:31:37.484
    STEP: delete the rc simpletest-rc-to-be-deleted 03/30/23 13:31:37.625
    STEP: wait for the rc to be deleted 03/30/23 13:31:37.628
    Mar 30 13:31:42.634: INFO: 65 pods remaining
    Mar 30 13:31:42.634: INFO: 65 pods has nil DeletionTimestamp
    Mar 30 13:31:42.634: INFO: 
    STEP: Gathering metrics 03/30/23 13:31:47.633
    Mar 30 13:31:47.648: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cn-hongkong.192.168.0.252" in namespace "kube-system" to be "running and ready"
    Mar 30 13:31:47.649: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252": Phase="Running", Reason="", readiness=true. Elapsed: 1.335094ms
    Mar 30 13:31:47.649: INFO: The phase of Pod kube-controller-manager-cn-hongkong.192.168.0.252 is Running (Ready = true)
    Mar 30 13:31:47.649: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252" satisfied condition "running and ready"
    Mar 30 13:31:47.685: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 30 13:31:47.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-2c8v9" in namespace "gc-1414"
    Mar 30 13:31:47.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jkm6" in namespace "gc-1414"
    Mar 30 13:31:47.694: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vlkg" in namespace "gc-1414"
    Mar 30 13:31:47.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-4274m" in namespace "gc-1414"
    Mar 30 13:31:47.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hpxk" in namespace "gc-1414"
    Mar 30 13:31:47.705: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qkl7" in namespace "gc-1414"
    Mar 30 13:31:47.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tthq" in namespace "gc-1414"
    Mar 30 13:31:47.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-684v5" in namespace "gc-1414"
    Mar 30 13:31:47.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-695qf" in namespace "gc-1414"
    Mar 30 13:31:47.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dh95" in namespace "gc-1414"
    Mar 30 13:31:47.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mngz" in namespace "gc-1414"
    Mar 30 13:31:47.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sdjr" in namespace "gc-1414"
    Mar 30 13:31:47.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-7h6dd" in namespace "gc-1414"
    Mar 30 13:31:47.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hw5n" in namespace "gc-1414"
    Mar 30 13:31:47.740: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qmd7" in namespace "gc-1414"
    Mar 30 13:31:47.745: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sdj5" in namespace "gc-1414"
    Mar 30 13:31:47.749: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vw42" in namespace "gc-1414"
    Mar 30 13:31:47.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-98ggj" in namespace "gc-1414"
    Mar 30 13:31:47.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cp6q" in namespace "gc-1414"
    Mar 30 13:31:47.761: INFO: Deleting pod "simpletest-rc-to-be-deleted-9g6wk" in namespace "gc-1414"
    Mar 30 13:31:47.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wpxp" in namespace "gc-1414"
    Mar 30 13:31:47.769: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9j5q" in namespace "gc-1414"
    Mar 30 13:31:47.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-bb7sb" in namespace "gc-1414"
    Mar 30 13:31:47.779: INFO: Deleting pod "simpletest-rc-to-be-deleted-blfz5" in namespace "gc-1414"
    Mar 30 13:31:47.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnqm8" in namespace "gc-1414"
    Mar 30 13:31:47.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv2st" in namespace "gc-1414"
    Mar 30 13:31:47.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-cc9fq" in namespace "gc-1414"
    Mar 30 13:31:47.795: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfnlp" in namespace "gc-1414"
    Mar 30 13:31:47.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-chrd6" in namespace "gc-1414"
    Mar 30 13:31:47.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-clfbs" in namespace "gc-1414"
    Mar 30 13:31:47.806: INFO: Deleting pod "simpletest-rc-to-be-deleted-cpchp" in namespace "gc-1414"
    Mar 30 13:31:47.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqmgq" in namespace "gc-1414"
    Mar 30 13:31:47.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-cr59w" in namespace "gc-1414"
    Mar 30 13:31:47.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-css8j" in namespace "gc-1414"
    Mar 30 13:31:47.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxwpr" in namespace "gc-1414"
    Mar 30 13:31:47.832: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9hbb" in namespace "gc-1414"
    Mar 30 13:31:47.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkkdb" in namespace "gc-1414"
    Mar 30 13:31:47.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlcfg" in namespace "gc-1414"
    Mar 30 13:31:47.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-ds968" in namespace "gc-1414"
    Mar 30 13:31:47.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxtps" in namespace "gc-1414"
    Mar 30 13:31:47.855: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgqnb" in namespace "gc-1414"
    Mar 30 13:31:47.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmcdr" in namespace "gc-1414"
    Mar 30 13:31:47.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw68h" in namespace "gc-1414"
    Mar 30 13:31:47.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwp2f" in namespace "gc-1414"
    Mar 30 13:31:47.871: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2ffx" in namespace "gc-1414"
    Mar 30 13:31:47.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdp6v" in namespace "gc-1414"
    Mar 30 13:31:47.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-gz96k" in namespace "gc-1414"
    Mar 30 13:31:47.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjwdq" in namespace "gc-1414"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:31:47.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1414" for this suite. 03/30/23 13:31:47.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:31:47.895
Mar 30 13:31:47.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:31:47.896
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:47.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:47.904
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:31:47.907
Mar 30 13:31:47.911: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204" in namespace "projected-1756" to be "Succeeded or Failed"
Mar 30 13:31:47.912: INFO: Pod "downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204": Phase="Pending", Reason="", readiness=false. Elapsed: 1.914073ms
Mar 30 13:31:49.915: INFO: Pod "downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004079344s
Mar 30 13:31:51.915: INFO: Pod "downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004614118s
Mar 30 13:31:53.915: INFO: Pod "downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004139633s
STEP: Saw pod success 03/30/23 13:31:53.915
Mar 30 13:31:53.915: INFO: Pod "downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204" satisfied condition "Succeeded or Failed"
Mar 30 13:31:53.916: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204 container client-container: <nil>
STEP: delete the pod 03/30/23 13:31:53.919
Mar 30 13:31:53.924: INFO: Waiting for pod downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204 to disappear
Mar 30 13:31:53.926: INFO: Pod downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 30 13:31:53.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1756" for this suite. 03/30/23 13:31:53.928
------------------------------
â€¢ [SLOW TEST] [6.035 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:31:47.895
    Mar 30 13:31:47.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:31:47.896
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:47.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:47.904
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:31:47.907
    Mar 30 13:31:47.911: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204" in namespace "projected-1756" to be "Succeeded or Failed"
    Mar 30 13:31:47.912: INFO: Pod "downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204": Phase="Pending", Reason="", readiness=false. Elapsed: 1.914073ms
    Mar 30 13:31:49.915: INFO: Pod "downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004079344s
    Mar 30 13:31:51.915: INFO: Pod "downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004614118s
    Mar 30 13:31:53.915: INFO: Pod "downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004139633s
    STEP: Saw pod success 03/30/23 13:31:53.915
    Mar 30 13:31:53.915: INFO: Pod "downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204" satisfied condition "Succeeded or Failed"
    Mar 30 13:31:53.916: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204 container client-container: <nil>
    STEP: delete the pod 03/30/23 13:31:53.919
    Mar 30 13:31:53.924: INFO: Waiting for pod downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204 to disappear
    Mar 30 13:31:53.926: INFO: Pod downwardapi-volume-14300080-71d1-4ed1-929c-4caec93fc204 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:31:53.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1756" for this suite. 03/30/23 13:31:53.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:31:53.93
Mar 30 13:31:53.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 13:31:53.931
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:53.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:53.939
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 03/30/23 13:31:53.94
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 13:31:53.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1147" for this suite. 03/30/23 13:31:53.947
------------------------------
â€¢ [0.031 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:31:53.93
    Mar 30 13:31:53.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 13:31:53.931
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:53.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:53.939
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 03/30/23 13:31:53.94
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:31:53.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1147" for this suite. 03/30/23 13:31:53.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:31:53.962
Mar 30 13:31:53.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubelet-test 03/30/23 13:31:53.963
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:53.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:53.971
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Mar 30 13:31:53.976: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsbc422540-18f6-4342-80e6-d1fbfa817dda" in namespace "kubelet-test-4648" to be "running and ready"
Mar 30 13:31:53.977: INFO: Pod "busybox-readonly-fsbc422540-18f6-4342-80e6-d1fbfa817dda": Phase="Pending", Reason="", readiness=false. Elapsed: 1.398974ms
Mar 30 13:31:53.977: INFO: The phase of Pod busybox-readonly-fsbc422540-18f6-4342-80e6-d1fbfa817dda is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:31:55.980: INFO: Pod "busybox-readonly-fsbc422540-18f6-4342-80e6-d1fbfa817dda": Phase="Running", Reason="", readiness=true. Elapsed: 2.004027103s
Mar 30 13:31:55.980: INFO: The phase of Pod busybox-readonly-fsbc422540-18f6-4342-80e6-d1fbfa817dda is Running (Ready = true)
Mar 30 13:31:55.980: INFO: Pod "busybox-readonly-fsbc422540-18f6-4342-80e6-d1fbfa817dda" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 30 13:31:55.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4648" for this suite. 03/30/23 13:31:55.987
------------------------------
â€¢ [2.027 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:31:53.962
    Mar 30 13:31:53.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubelet-test 03/30/23 13:31:53.963
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:53.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:53.971
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Mar 30 13:31:53.976: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsbc422540-18f6-4342-80e6-d1fbfa817dda" in namespace "kubelet-test-4648" to be "running and ready"
    Mar 30 13:31:53.977: INFO: Pod "busybox-readonly-fsbc422540-18f6-4342-80e6-d1fbfa817dda": Phase="Pending", Reason="", readiness=false. Elapsed: 1.398974ms
    Mar 30 13:31:53.977: INFO: The phase of Pod busybox-readonly-fsbc422540-18f6-4342-80e6-d1fbfa817dda is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:31:55.980: INFO: Pod "busybox-readonly-fsbc422540-18f6-4342-80e6-d1fbfa817dda": Phase="Running", Reason="", readiness=true. Elapsed: 2.004027103s
    Mar 30 13:31:55.980: INFO: The phase of Pod busybox-readonly-fsbc422540-18f6-4342-80e6-d1fbfa817dda is Running (Ready = true)
    Mar 30 13:31:55.980: INFO: Pod "busybox-readonly-fsbc422540-18f6-4342-80e6-d1fbfa817dda" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:31:55.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4648" for this suite. 03/30/23 13:31:55.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:31:55.989
Mar 30 13:31:55.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 13:31:55.99
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:55.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:55.998
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 03/30/23 13:31:56
Mar 30 13:31:56.000: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 30 13:31:56.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 create -f -'
Mar 30 13:31:56.537: INFO: stderr: ""
Mar 30 13:31:56.537: INFO: stdout: "service/agnhost-replica created\n"
Mar 30 13:31:56.537: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 30 13:31:56.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 create -f -'
Mar 30 13:31:56.695: INFO: stderr: ""
Mar 30 13:31:56.695: INFO: stdout: "service/agnhost-primary created\n"
Mar 30 13:31:56.695: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 30 13:31:56.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 create -f -'
Mar 30 13:31:57.251: INFO: stderr: ""
Mar 30 13:31:57.251: INFO: stdout: "service/frontend created\n"
Mar 30 13:31:57.251: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 30 13:31:57.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 create -f -'
Mar 30 13:31:57.404: INFO: stderr: ""
Mar 30 13:31:57.404: INFO: stdout: "deployment.apps/frontend created\n"
Mar 30 13:31:57.404: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 30 13:31:57.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 create -f -'
Mar 30 13:31:57.559: INFO: stderr: ""
Mar 30 13:31:57.559: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 30 13:31:57.559: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 30 13:31:57.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 create -f -'
Mar 30 13:31:57.704: INFO: stderr: ""
Mar 30 13:31:57.704: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 03/30/23 13:31:57.704
Mar 30 13:31:57.704: INFO: Waiting for all frontend pods to be Running.
Mar 30 13:32:02.755: INFO: Waiting for frontend to serve content.
Mar 30 13:32:02.760: INFO: Trying to add a new entry to the guestbook.
Mar 30 13:32:02.765: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 03/30/23 13:32:02.77
Mar 30 13:32:02.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 delete --grace-period=0 --force -f -'
Mar 30 13:32:02.829: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 13:32:02.829: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 03/30/23 13:32:02.829
Mar 30 13:32:02.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 delete --grace-period=0 --force -f -'
Mar 30 13:32:02.889: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 13:32:02.889: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/30/23 13:32:02.889
Mar 30 13:32:02.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 delete --grace-period=0 --force -f -'
Mar 30 13:32:02.951: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 13:32:02.951: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/30/23 13:32:02.951
Mar 30 13:32:02.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 delete --grace-period=0 --force -f -'
Mar 30 13:32:03.004: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 13:32:03.004: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/30/23 13:32:03.004
Mar 30 13:32:03.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 delete --grace-period=0 --force -f -'
Mar 30 13:32:03.064: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 13:32:03.064: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/30/23 13:32:03.064
Mar 30 13:32:03.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 delete --grace-period=0 --force -f -'
Mar 30 13:32:03.115: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 13:32:03.115: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 13:32:03.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9899" for this suite. 03/30/23 13:32:03.118
------------------------------
â€¢ [SLOW TEST] [7.132 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:31:55.989
    Mar 30 13:31:55.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 13:31:55.99
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:31:55.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:31:55.998
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 03/30/23 13:31:56
    Mar 30 13:31:56.000: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Mar 30 13:31:56.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 create -f -'
    Mar 30 13:31:56.537: INFO: stderr: ""
    Mar 30 13:31:56.537: INFO: stdout: "service/agnhost-replica created\n"
    Mar 30 13:31:56.537: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Mar 30 13:31:56.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 create -f -'
    Mar 30 13:31:56.695: INFO: stderr: ""
    Mar 30 13:31:56.695: INFO: stdout: "service/agnhost-primary created\n"
    Mar 30 13:31:56.695: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Mar 30 13:31:56.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 create -f -'
    Mar 30 13:31:57.251: INFO: stderr: ""
    Mar 30 13:31:57.251: INFO: stdout: "service/frontend created\n"
    Mar 30 13:31:57.251: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Mar 30 13:31:57.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 create -f -'
    Mar 30 13:31:57.404: INFO: stderr: ""
    Mar 30 13:31:57.404: INFO: stdout: "deployment.apps/frontend created\n"
    Mar 30 13:31:57.404: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 30 13:31:57.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 create -f -'
    Mar 30 13:31:57.559: INFO: stderr: ""
    Mar 30 13:31:57.559: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Mar 30 13:31:57.559: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 30 13:31:57.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 create -f -'
    Mar 30 13:31:57.704: INFO: stderr: ""
    Mar 30 13:31:57.704: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 03/30/23 13:31:57.704
    Mar 30 13:31:57.704: INFO: Waiting for all frontend pods to be Running.
    Mar 30 13:32:02.755: INFO: Waiting for frontend to serve content.
    Mar 30 13:32:02.760: INFO: Trying to add a new entry to the guestbook.
    Mar 30 13:32:02.765: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 03/30/23 13:32:02.77
    Mar 30 13:32:02.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 delete --grace-period=0 --force -f -'
    Mar 30 13:32:02.829: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 30 13:32:02.829: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 03/30/23 13:32:02.829
    Mar 30 13:32:02.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 delete --grace-period=0 --force -f -'
    Mar 30 13:32:02.889: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 30 13:32:02.889: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/30/23 13:32:02.889
    Mar 30 13:32:02.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 delete --grace-period=0 --force -f -'
    Mar 30 13:32:02.951: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 30 13:32:02.951: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/30/23 13:32:02.951
    Mar 30 13:32:02.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 delete --grace-period=0 --force -f -'
    Mar 30 13:32:03.004: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 30 13:32:03.004: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/30/23 13:32:03.004
    Mar 30 13:32:03.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 delete --grace-period=0 --force -f -'
    Mar 30 13:32:03.064: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 30 13:32:03.064: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/30/23 13:32:03.064
    Mar 30 13:32:03.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9899 delete --grace-period=0 --force -f -'
    Mar 30 13:32:03.115: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 30 13:32:03.115: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:32:03.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9899" for this suite. 03/30/23 13:32:03.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:32:03.122
Mar 30 13:32:03.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 13:32:03.123
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:03.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:03.13
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/30/23 13:32:03.133
Mar 30 13:32:03.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3868 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 30 13:32:03.191: INFO: stderr: ""
Mar 30 13:32:03.191: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 03/30/23 13:32:03.191
Mar 30 13:32:03.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3868 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Mar 30 13:32:03.348: INFO: stderr: ""
Mar 30 13:32:03.348: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/30/23 13:32:03.348
Mar 30 13:32:03.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3868 delete pods e2e-test-httpd-pod'
Mar 30 13:32:05.446: INFO: stderr: ""
Mar 30 13:32:05.446: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 13:32:05.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3868" for this suite. 03/30/23 13:32:05.449
------------------------------
â€¢ [2.330 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:32:03.122
    Mar 30 13:32:03.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 13:32:03.123
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:03.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:03.13
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/30/23 13:32:03.133
    Mar 30 13:32:03.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3868 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 30 13:32:03.191: INFO: stderr: ""
    Mar 30 13:32:03.191: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 03/30/23 13:32:03.191
    Mar 30 13:32:03.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3868 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Mar 30 13:32:03.348: INFO: stderr: ""
    Mar 30 13:32:03.348: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/30/23 13:32:03.348
    Mar 30 13:32:03.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3868 delete pods e2e-test-httpd-pod'
    Mar 30 13:32:05.446: INFO: stderr: ""
    Mar 30 13:32:05.446: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:32:05.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3868" for this suite. 03/30/23 13:32:05.449
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:32:05.452
Mar 30 13:32:05.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename disruption 03/30/23 13:32:05.452
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:05.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:05.46
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 03/30/23 13:32:05.462
STEP: Waiting for the pdb to be processed 03/30/23 13:32:05.463
STEP: First trying to evict a pod which shouldn't be evictable 03/30/23 13:32:07.47
STEP: Waiting for all pods to be running 03/30/23 13:32:07.47
Mar 30 13:32:07.471: INFO: pods: 0 < 3
STEP: locating a running pod 03/30/23 13:32:09.474
STEP: Updating the pdb to allow a pod to be evicted 03/30/23 13:32:09.478
STEP: Waiting for the pdb to be processed 03/30/23 13:32:09.482
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/30/23 13:32:11.486
STEP: Waiting for all pods to be running 03/30/23 13:32:11.486
STEP: Waiting for the pdb to observed all healthy pods 03/30/23 13:32:11.488
STEP: Patching the pdb to disallow a pod to be evicted 03/30/23 13:32:11.498
STEP: Waiting for the pdb to be processed 03/30/23 13:32:11.503
STEP: Waiting for all pods to be running 03/30/23 13:32:13.507
STEP: locating a running pod 03/30/23 13:32:13.509
STEP: Deleting the pdb to allow a pod to be evicted 03/30/23 13:32:13.514
STEP: Waiting for the pdb to be deleted 03/30/23 13:32:13.516
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/30/23 13:32:13.517
STEP: Waiting for all pods to be running 03/30/23 13:32:13.517
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 30 13:32:13.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6108" for this suite. 03/30/23 13:32:13.527
------------------------------
â€¢ [SLOW TEST] [8.080 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:32:05.452
    Mar 30 13:32:05.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename disruption 03/30/23 13:32:05.452
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:05.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:05.46
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 03/30/23 13:32:05.462
    STEP: Waiting for the pdb to be processed 03/30/23 13:32:05.463
    STEP: First trying to evict a pod which shouldn't be evictable 03/30/23 13:32:07.47
    STEP: Waiting for all pods to be running 03/30/23 13:32:07.47
    Mar 30 13:32:07.471: INFO: pods: 0 < 3
    STEP: locating a running pod 03/30/23 13:32:09.474
    STEP: Updating the pdb to allow a pod to be evicted 03/30/23 13:32:09.478
    STEP: Waiting for the pdb to be processed 03/30/23 13:32:09.482
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/30/23 13:32:11.486
    STEP: Waiting for all pods to be running 03/30/23 13:32:11.486
    STEP: Waiting for the pdb to observed all healthy pods 03/30/23 13:32:11.488
    STEP: Patching the pdb to disallow a pod to be evicted 03/30/23 13:32:11.498
    STEP: Waiting for the pdb to be processed 03/30/23 13:32:11.503
    STEP: Waiting for all pods to be running 03/30/23 13:32:13.507
    STEP: locating a running pod 03/30/23 13:32:13.509
    STEP: Deleting the pdb to allow a pod to be evicted 03/30/23 13:32:13.514
    STEP: Waiting for the pdb to be deleted 03/30/23 13:32:13.516
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/30/23 13:32:13.517
    STEP: Waiting for all pods to be running 03/30/23 13:32:13.517
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:32:13.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6108" for this suite. 03/30/23 13:32:13.527
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:32:13.532
Mar 30 13:32:13.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:32:13.533
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:13.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:13.541
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-262c839e-1fa3-4be2-83ae-986117465d56 03/30/23 13:32:13.542
STEP: Creating a pod to test consume configMaps 03/30/23 13:32:13.544
Mar 30 13:32:13.548: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e" in namespace "projected-2730" to be "Succeeded or Failed"
Mar 30 13:32:13.550: INFO: Pod "pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.539767ms
Mar 30 13:32:15.553: INFO: Pod "pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004601339s
Mar 30 13:32:17.552: INFO: Pod "pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003977707s
STEP: Saw pod success 03/30/23 13:32:17.552
Mar 30 13:32:17.552: INFO: Pod "pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e" satisfied condition "Succeeded or Failed"
Mar 30 13:32:17.554: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e container agnhost-container: <nil>
STEP: delete the pod 03/30/23 13:32:17.557
Mar 30 13:32:17.563: INFO: Waiting for pod pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e to disappear
Mar 30 13:32:17.564: INFO: Pod pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:32:17.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2730" for this suite. 03/30/23 13:32:17.566
------------------------------
â€¢ [4.037 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:32:13.532
    Mar 30 13:32:13.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:32:13.533
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:13.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:13.541
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-262c839e-1fa3-4be2-83ae-986117465d56 03/30/23 13:32:13.542
    STEP: Creating a pod to test consume configMaps 03/30/23 13:32:13.544
    Mar 30 13:32:13.548: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e" in namespace "projected-2730" to be "Succeeded or Failed"
    Mar 30 13:32:13.550: INFO: Pod "pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.539767ms
    Mar 30 13:32:15.553: INFO: Pod "pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004601339s
    Mar 30 13:32:17.552: INFO: Pod "pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003977707s
    STEP: Saw pod success 03/30/23 13:32:17.552
    Mar 30 13:32:17.552: INFO: Pod "pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e" satisfied condition "Succeeded or Failed"
    Mar 30 13:32:17.554: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 13:32:17.557
    Mar 30 13:32:17.563: INFO: Waiting for pod pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e to disappear
    Mar 30 13:32:17.564: INFO: Pod pod-projected-configmaps-3a433bd1-c224-4209-aef5-58e7327e6a0e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:32:17.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2730" for this suite. 03/30/23 13:32:17.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:32:17.57
Mar 30 13:32:17.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename replication-controller 03/30/23 13:32:17.571
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:17.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:17.579
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 03/30/23 13:32:17.582
STEP: waiting for RC to be added 03/30/23 13:32:17.584
STEP: waiting for available Replicas 03/30/23 13:32:17.584
STEP: patching ReplicationController 03/30/23 13:32:18.4
STEP: waiting for RC to be modified 03/30/23 13:32:18.403
STEP: patching ReplicationController status 03/30/23 13:32:18.403
STEP: waiting for RC to be modified 03/30/23 13:32:18.406
STEP: waiting for available Replicas 03/30/23 13:32:18.406
STEP: fetching ReplicationController status 03/30/23 13:32:18.409
STEP: patching ReplicationController scale 03/30/23 13:32:18.41
STEP: waiting for RC to be modified 03/30/23 13:32:18.413
STEP: waiting for ReplicationController's scale to be the max amount 03/30/23 13:32:18.413
STEP: fetching ReplicationController; ensuring that it's patched 03/30/23 13:32:19.467
STEP: updating ReplicationController status 03/30/23 13:32:19.469
STEP: waiting for RC to be modified 03/30/23 13:32:19.472
STEP: listing all ReplicationControllers 03/30/23 13:32:19.472
STEP: checking that ReplicationController has expected values 03/30/23 13:32:19.473
STEP: deleting ReplicationControllers by collection 03/30/23 13:32:19.473
STEP: waiting for ReplicationController to have a DELETED watchEvent 03/30/23 13:32:19.477
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 30 13:32:19.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7713" for this suite. 03/30/23 13:32:19.502
------------------------------
â€¢ [1.935 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:32:17.57
    Mar 30 13:32:17.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename replication-controller 03/30/23 13:32:17.571
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:17.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:17.579
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 03/30/23 13:32:17.582
    STEP: waiting for RC to be added 03/30/23 13:32:17.584
    STEP: waiting for available Replicas 03/30/23 13:32:17.584
    STEP: patching ReplicationController 03/30/23 13:32:18.4
    STEP: waiting for RC to be modified 03/30/23 13:32:18.403
    STEP: patching ReplicationController status 03/30/23 13:32:18.403
    STEP: waiting for RC to be modified 03/30/23 13:32:18.406
    STEP: waiting for available Replicas 03/30/23 13:32:18.406
    STEP: fetching ReplicationController status 03/30/23 13:32:18.409
    STEP: patching ReplicationController scale 03/30/23 13:32:18.41
    STEP: waiting for RC to be modified 03/30/23 13:32:18.413
    STEP: waiting for ReplicationController's scale to be the max amount 03/30/23 13:32:18.413
    STEP: fetching ReplicationController; ensuring that it's patched 03/30/23 13:32:19.467
    STEP: updating ReplicationController status 03/30/23 13:32:19.469
    STEP: waiting for RC to be modified 03/30/23 13:32:19.472
    STEP: listing all ReplicationControllers 03/30/23 13:32:19.472
    STEP: checking that ReplicationController has expected values 03/30/23 13:32:19.473
    STEP: deleting ReplicationControllers by collection 03/30/23 13:32:19.473
    STEP: waiting for ReplicationController to have a DELETED watchEvent 03/30/23 13:32:19.477
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:32:19.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7713" for this suite. 03/30/23 13:32:19.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:32:19.505
Mar 30 13:32:19.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename subpath 03/30/23 13:32:19.506
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:19.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:19.513
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/30/23 13:32:19.514
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-rrfc 03/30/23 13:32:19.519
STEP: Creating a pod to test atomic-volume-subpath 03/30/23 13:32:19.519
Mar 30 13:32:19.523: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rrfc" in namespace "subpath-7023" to be "Succeeded or Failed"
Mar 30 13:32:19.525: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.977299ms
Mar 30 13:32:21.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003995765s
Mar 30 13:32:23.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 4.00385972s
Mar 30 13:32:25.528: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 6.00478648s
Mar 30 13:32:27.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 8.0042354s
Mar 30 13:32:29.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 10.004217701s
Mar 30 13:32:31.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 12.00411931s
Mar 30 13:32:33.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 14.004048211s
Mar 30 13:32:35.528: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 16.004957336s
Mar 30 13:32:37.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 18.004114187s
Mar 30 13:32:39.528: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 20.00517512s
Mar 30 13:32:41.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=false. Elapsed: 22.004300152s
Mar 30 13:32:43.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003841814s
STEP: Saw pod success 03/30/23 13:32:43.527
Mar 30 13:32:43.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc" satisfied condition "Succeeded or Failed"
Mar 30 13:32:43.528: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-subpath-test-downwardapi-rrfc container test-container-subpath-downwardapi-rrfc: <nil>
STEP: delete the pod 03/30/23 13:32:43.532
Mar 30 13:32:43.536: INFO: Waiting for pod pod-subpath-test-downwardapi-rrfc to disappear
Mar 30 13:32:43.537: INFO: Pod pod-subpath-test-downwardapi-rrfc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rrfc 03/30/23 13:32:43.537
Mar 30 13:32:43.537: INFO: Deleting pod "pod-subpath-test-downwardapi-rrfc" in namespace "subpath-7023"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 30 13:32:43.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7023" for this suite. 03/30/23 13:32:43.54
------------------------------
â€¢ [SLOW TEST] [24.038 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:32:19.505
    Mar 30 13:32:19.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename subpath 03/30/23 13:32:19.506
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:19.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:19.513
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/30/23 13:32:19.514
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-rrfc 03/30/23 13:32:19.519
    STEP: Creating a pod to test atomic-volume-subpath 03/30/23 13:32:19.519
    Mar 30 13:32:19.523: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rrfc" in namespace "subpath-7023" to be "Succeeded or Failed"
    Mar 30 13:32:19.525: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.977299ms
    Mar 30 13:32:21.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003995765s
    Mar 30 13:32:23.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 4.00385972s
    Mar 30 13:32:25.528: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 6.00478648s
    Mar 30 13:32:27.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 8.0042354s
    Mar 30 13:32:29.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 10.004217701s
    Mar 30 13:32:31.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 12.00411931s
    Mar 30 13:32:33.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 14.004048211s
    Mar 30 13:32:35.528: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 16.004957336s
    Mar 30 13:32:37.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 18.004114187s
    Mar 30 13:32:39.528: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=true. Elapsed: 20.00517512s
    Mar 30 13:32:41.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Running", Reason="", readiness=false. Elapsed: 22.004300152s
    Mar 30 13:32:43.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003841814s
    STEP: Saw pod success 03/30/23 13:32:43.527
    Mar 30 13:32:43.527: INFO: Pod "pod-subpath-test-downwardapi-rrfc" satisfied condition "Succeeded or Failed"
    Mar 30 13:32:43.528: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-subpath-test-downwardapi-rrfc container test-container-subpath-downwardapi-rrfc: <nil>
    STEP: delete the pod 03/30/23 13:32:43.532
    Mar 30 13:32:43.536: INFO: Waiting for pod pod-subpath-test-downwardapi-rrfc to disappear
    Mar 30 13:32:43.537: INFO: Pod pod-subpath-test-downwardapi-rrfc no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-rrfc 03/30/23 13:32:43.537
    Mar 30 13:32:43.537: INFO: Deleting pod "pod-subpath-test-downwardapi-rrfc" in namespace "subpath-7023"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:32:43.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7023" for this suite. 03/30/23 13:32:43.54
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:32:43.543
Mar 30 13:32:43.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 13:32:43.544
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:43.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:43.551
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:32:43.553
Mar 30 13:32:43.556: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf" in namespace "downward-api-9429" to be "Succeeded or Failed"
Mar 30 13:32:43.557: INFO: Pod "downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.240604ms
Mar 30 13:32:45.560: INFO: Pod "downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003646649s
Mar 30 13:32:47.560: INFO: Pod "downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003669263s
STEP: Saw pod success 03/30/23 13:32:47.56
Mar 30 13:32:47.560: INFO: Pod "downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf" satisfied condition "Succeeded or Failed"
Mar 30 13:32:47.562: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf container client-container: <nil>
STEP: delete the pod 03/30/23 13:32:47.565
Mar 30 13:32:47.570: INFO: Waiting for pod downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf to disappear
Mar 30 13:32:47.571: INFO: Pod downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 30 13:32:47.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9429" for this suite. 03/30/23 13:32:47.573
------------------------------
â€¢ [4.033 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:32:43.543
    Mar 30 13:32:43.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 13:32:43.544
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:43.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:43.551
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:32:43.553
    Mar 30 13:32:43.556: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf" in namespace "downward-api-9429" to be "Succeeded or Failed"
    Mar 30 13:32:43.557: INFO: Pod "downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.240604ms
    Mar 30 13:32:45.560: INFO: Pod "downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003646649s
    Mar 30 13:32:47.560: INFO: Pod "downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003669263s
    STEP: Saw pod success 03/30/23 13:32:47.56
    Mar 30 13:32:47.560: INFO: Pod "downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf" satisfied condition "Succeeded or Failed"
    Mar 30 13:32:47.562: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf container client-container: <nil>
    STEP: delete the pod 03/30/23 13:32:47.565
    Mar 30 13:32:47.570: INFO: Waiting for pod downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf to disappear
    Mar 30 13:32:47.571: INFO: Pod downwardapi-volume-d4366e91-c029-43d6-adbb-086da681f5cf no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:32:47.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9429" for this suite. 03/30/23 13:32:47.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:32:47.576
Mar 30 13:32:47.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename dns 03/30/23 13:32:47.577
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:47.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:47.585
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6476.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6476.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 03/30/23 13:32:47.587
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6476.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6476.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 03/30/23 13:32:47.587
STEP: creating a pod to probe /etc/hosts 03/30/23 13:32:47.587
STEP: submitting the pod to kubernetes 03/30/23 13:32:47.587
Mar 30 13:32:47.591: INFO: Waiting up to 15m0s for pod "dns-test-4a869af3-df1f-429c-b8ef-24a12fb8c6c5" in namespace "dns-6476" to be "running"
Mar 30 13:32:47.592: INFO: Pod "dns-test-4a869af3-df1f-429c-b8ef-24a12fb8c6c5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.272492ms
Mar 30 13:32:49.595: INFO: Pod "dns-test-4a869af3-df1f-429c-b8ef-24a12fb8c6c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004423273s
Mar 30 13:32:49.595: INFO: Pod "dns-test-4a869af3-df1f-429c-b8ef-24a12fb8c6c5" satisfied condition "running"
STEP: retrieving the pod 03/30/23 13:32:49.595
STEP: looking for the results for each expected name from probers 03/30/23 13:32:49.597
Mar 30 13:32:49.605: INFO: DNS probes using dns-6476/dns-test-4a869af3-df1f-429c-b8ef-24a12fb8c6c5 succeeded

STEP: deleting the pod 03/30/23 13:32:49.605
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 30 13:32:49.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6476" for this suite. 03/30/23 13:32:49.612
------------------------------
â€¢ [2.038 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:32:47.576
    Mar 30 13:32:47.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename dns 03/30/23 13:32:47.577
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:47.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:47.585
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6476.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6476.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     03/30/23 13:32:47.587
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6476.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6476.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     03/30/23 13:32:47.587
    STEP: creating a pod to probe /etc/hosts 03/30/23 13:32:47.587
    STEP: submitting the pod to kubernetes 03/30/23 13:32:47.587
    Mar 30 13:32:47.591: INFO: Waiting up to 15m0s for pod "dns-test-4a869af3-df1f-429c-b8ef-24a12fb8c6c5" in namespace "dns-6476" to be "running"
    Mar 30 13:32:47.592: INFO: Pod "dns-test-4a869af3-df1f-429c-b8ef-24a12fb8c6c5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.272492ms
    Mar 30 13:32:49.595: INFO: Pod "dns-test-4a869af3-df1f-429c-b8ef-24a12fb8c6c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004423273s
    Mar 30 13:32:49.595: INFO: Pod "dns-test-4a869af3-df1f-429c-b8ef-24a12fb8c6c5" satisfied condition "running"
    STEP: retrieving the pod 03/30/23 13:32:49.595
    STEP: looking for the results for each expected name from probers 03/30/23 13:32:49.597
    Mar 30 13:32:49.605: INFO: DNS probes using dns-6476/dns-test-4a869af3-df1f-429c-b8ef-24a12fb8c6c5 succeeded

    STEP: deleting the pod 03/30/23 13:32:49.605
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:32:49.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6476" for this suite. 03/30/23 13:32:49.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:32:49.615
Mar 30 13:32:49.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 13:32:49.616
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:49.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:49.623
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 13:32:49.63
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:32:50.582
STEP: Deploying the webhook pod 03/30/23 13:32:50.585
STEP: Wait for the deployment to be ready 03/30/23 13:32:50.59
Mar 30 13:32:50.593: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/30/23 13:32:52.599
STEP: Verifying the service has paired with the endpoint 03/30/23 13:32:52.604
Mar 30 13:32:53.604: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/30/23 13:32:53.606
STEP: create a configmap that should be updated by the webhook 03/30/23 13:32:53.616
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:32:53.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-795" for this suite. 03/30/23 13:32:53.644
STEP: Destroying namespace "webhook-795-markers" for this suite. 03/30/23 13:32:53.647
------------------------------
â€¢ [4.035 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:32:49.615
    Mar 30 13:32:49.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 13:32:49.616
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:49.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:49.623
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 13:32:49.63
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:32:50.582
    STEP: Deploying the webhook pod 03/30/23 13:32:50.585
    STEP: Wait for the deployment to be ready 03/30/23 13:32:50.59
    Mar 30 13:32:50.593: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/30/23 13:32:52.599
    STEP: Verifying the service has paired with the endpoint 03/30/23 13:32:52.604
    Mar 30 13:32:53.604: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/30/23 13:32:53.606
    STEP: create a configmap that should be updated by the webhook 03/30/23 13:32:53.616
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:32:53.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-795" for this suite. 03/30/23 13:32:53.644
    STEP: Destroying namespace "webhook-795-markers" for this suite. 03/30/23 13:32:53.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:32:53.65
Mar 30 13:32:53.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 13:32:53.651
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:53.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:53.66
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:32:53.661
Mar 30 13:32:53.665: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f" in namespace "downward-api-3278" to be "Succeeded or Failed"
Mar 30 13:32:53.667: INFO: Pod "downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.414296ms
Mar 30 13:32:55.669: INFO: Pod "downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003381625s
Mar 30 13:32:57.669: INFO: Pod "downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003691291s
STEP: Saw pod success 03/30/23 13:32:57.669
Mar 30 13:32:57.669: INFO: Pod "downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f" satisfied condition "Succeeded or Failed"
Mar 30 13:32:57.671: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f container client-container: <nil>
STEP: delete the pod 03/30/23 13:32:57.674
Mar 30 13:32:57.679: INFO: Waiting for pod downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f to disappear
Mar 30 13:32:57.680: INFO: Pod downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 30 13:32:57.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3278" for this suite. 03/30/23 13:32:57.682
------------------------------
â€¢ [4.035 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:32:53.65
    Mar 30 13:32:53.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 13:32:53.651
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:53.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:53.66
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:32:53.661
    Mar 30 13:32:53.665: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f" in namespace "downward-api-3278" to be "Succeeded or Failed"
    Mar 30 13:32:53.667: INFO: Pod "downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.414296ms
    Mar 30 13:32:55.669: INFO: Pod "downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003381625s
    Mar 30 13:32:57.669: INFO: Pod "downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003691291s
    STEP: Saw pod success 03/30/23 13:32:57.669
    Mar 30 13:32:57.669: INFO: Pod "downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f" satisfied condition "Succeeded or Failed"
    Mar 30 13:32:57.671: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f container client-container: <nil>
    STEP: delete the pod 03/30/23 13:32:57.674
    Mar 30 13:32:57.679: INFO: Waiting for pod downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f to disappear
    Mar 30 13:32:57.680: INFO: Pod downwardapi-volume-bf7e665e-9a83-43ee-922d-2b45121af89f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:32:57.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3278" for this suite. 03/30/23 13:32:57.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:32:57.687
Mar 30 13:32:57.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 13:32:57.688
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:57.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:57.695
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 03/30/23 13:32:57.697
Mar 30 13:32:57.700: INFO: Waiting up to 5m0s for pod "downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a" in namespace "downward-api-1875" to be "Succeeded or Failed"
Mar 30 13:32:57.701: INFO: Pod "downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.254336ms
Mar 30 13:32:59.703: INFO: Pod "downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003277473s
Mar 30 13:33:01.704: INFO: Pod "downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004142082s
STEP: Saw pod success 03/30/23 13:33:01.704
Mar 30 13:33:01.704: INFO: Pod "downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a" satisfied condition "Succeeded or Failed"
Mar 30 13:33:01.706: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a container dapi-container: <nil>
STEP: delete the pod 03/30/23 13:33:01.71
Mar 30 13:33:01.714: INFO: Waiting for pod downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a to disappear
Mar 30 13:33:01.715: INFO: Pod downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 30 13:33:01.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1875" for this suite. 03/30/23 13:33:01.717
------------------------------
â€¢ [4.032 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:32:57.687
    Mar 30 13:32:57.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 13:32:57.688
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:32:57.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:32:57.695
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 03/30/23 13:32:57.697
    Mar 30 13:32:57.700: INFO: Waiting up to 5m0s for pod "downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a" in namespace "downward-api-1875" to be "Succeeded or Failed"
    Mar 30 13:32:57.701: INFO: Pod "downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.254336ms
    Mar 30 13:32:59.703: INFO: Pod "downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003277473s
    Mar 30 13:33:01.704: INFO: Pod "downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004142082s
    STEP: Saw pod success 03/30/23 13:33:01.704
    Mar 30 13:33:01.704: INFO: Pod "downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a" satisfied condition "Succeeded or Failed"
    Mar 30 13:33:01.706: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a container dapi-container: <nil>
    STEP: delete the pod 03/30/23 13:33:01.71
    Mar 30 13:33:01.714: INFO: Waiting for pod downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a to disappear
    Mar 30 13:33:01.715: INFO: Pod downward-api-5a26d871-9eb2-4639-be43-7cc2af3cba4a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:33:01.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1875" for this suite. 03/30/23 13:33:01.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:33:01.721
Mar 30 13:33:01.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename containers 03/30/23 13:33:01.721
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:33:01.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:33:01.729
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Mar 30 13:33:01.734: INFO: Waiting up to 5m0s for pod "client-containers-a8c9f622-b422-4528-a39e-b00e9f9136b0" in namespace "containers-1747" to be "running"
Mar 30 13:33:01.736: INFO: Pod "client-containers-a8c9f622-b422-4528-a39e-b00e9f9136b0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.405153ms
Mar 30 13:33:03.738: INFO: Pod "client-containers-a8c9f622-b422-4528-a39e-b00e9f9136b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.004124372s
Mar 30 13:33:03.738: INFO: Pod "client-containers-a8c9f622-b422-4528-a39e-b00e9f9136b0" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 30 13:33:03.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1747" for this suite. 03/30/23 13:33:03.744
------------------------------
â€¢ [2.026 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:33:01.721
    Mar 30 13:33:01.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename containers 03/30/23 13:33:01.721
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:33:01.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:33:01.729
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Mar 30 13:33:01.734: INFO: Waiting up to 5m0s for pod "client-containers-a8c9f622-b422-4528-a39e-b00e9f9136b0" in namespace "containers-1747" to be "running"
    Mar 30 13:33:01.736: INFO: Pod "client-containers-a8c9f622-b422-4528-a39e-b00e9f9136b0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.405153ms
    Mar 30 13:33:03.738: INFO: Pod "client-containers-a8c9f622-b422-4528-a39e-b00e9f9136b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.004124372s
    Mar 30 13:33:03.738: INFO: Pod "client-containers-a8c9f622-b422-4528-a39e-b00e9f9136b0" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:33:03.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1747" for this suite. 03/30/23 13:33:03.744
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:33:03.747
Mar 30 13:33:03.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 13:33:03.748
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:33:03.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:33:03.756
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 03/30/23 13:33:03.758
Mar 30 13:33:03.761: INFO: Waiting up to 5m0s for pod "pod-22673c05-187f-4e33-b8c5-2c19fde08999" in namespace "emptydir-8345" to be "Succeeded or Failed"
Mar 30 13:33:03.763: INFO: Pod "pod-22673c05-187f-4e33-b8c5-2c19fde08999": Phase="Pending", Reason="", readiness=false. Elapsed: 1.392011ms
Mar 30 13:33:05.765: INFO: Pod "pod-22673c05-187f-4e33-b8c5-2c19fde08999": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003558953s
Mar 30 13:33:07.765: INFO: Pod "pod-22673c05-187f-4e33-b8c5-2c19fde08999": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003495941s
STEP: Saw pod success 03/30/23 13:33:07.765
Mar 30 13:33:07.765: INFO: Pod "pod-22673c05-187f-4e33-b8c5-2c19fde08999" satisfied condition "Succeeded or Failed"
Mar 30 13:33:07.766: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-22673c05-187f-4e33-b8c5-2c19fde08999 container test-container: <nil>
STEP: delete the pod 03/30/23 13:33:07.77
Mar 30 13:33:07.775: INFO: Waiting for pod pod-22673c05-187f-4e33-b8c5-2c19fde08999 to disappear
Mar 30 13:33:07.776: INFO: Pod pod-22673c05-187f-4e33-b8c5-2c19fde08999 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 13:33:07.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8345" for this suite. 03/30/23 13:33:07.778
------------------------------
â€¢ [4.033 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:33:03.747
    Mar 30 13:33:03.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 13:33:03.748
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:33:03.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:33:03.756
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/30/23 13:33:03.758
    Mar 30 13:33:03.761: INFO: Waiting up to 5m0s for pod "pod-22673c05-187f-4e33-b8c5-2c19fde08999" in namespace "emptydir-8345" to be "Succeeded or Failed"
    Mar 30 13:33:03.763: INFO: Pod "pod-22673c05-187f-4e33-b8c5-2c19fde08999": Phase="Pending", Reason="", readiness=false. Elapsed: 1.392011ms
    Mar 30 13:33:05.765: INFO: Pod "pod-22673c05-187f-4e33-b8c5-2c19fde08999": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003558953s
    Mar 30 13:33:07.765: INFO: Pod "pod-22673c05-187f-4e33-b8c5-2c19fde08999": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003495941s
    STEP: Saw pod success 03/30/23 13:33:07.765
    Mar 30 13:33:07.765: INFO: Pod "pod-22673c05-187f-4e33-b8c5-2c19fde08999" satisfied condition "Succeeded or Failed"
    Mar 30 13:33:07.766: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-22673c05-187f-4e33-b8c5-2c19fde08999 container test-container: <nil>
    STEP: delete the pod 03/30/23 13:33:07.77
    Mar 30 13:33:07.775: INFO: Waiting for pod pod-22673c05-187f-4e33-b8c5-2c19fde08999 to disappear
    Mar 30 13:33:07.776: INFO: Pod pod-22673c05-187f-4e33-b8c5-2c19fde08999 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:33:07.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8345" for this suite. 03/30/23 13:33:07.778
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:33:07.78
Mar 30 13:33:07.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:33:07.781
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:33:07.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:33:07.789
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-5be6f17f-041f-4e56-a7e0-f24bc0750d78 03/30/23 13:33:07.791
STEP: Creating a pod to test consume configMaps 03/30/23 13:33:07.793
Mar 30 13:33:07.796: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c" in namespace "configmap-8244" to be "Succeeded or Failed"
Mar 30 13:33:07.798: INFO: Pod "pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.373183ms
Mar 30 13:33:09.800: INFO: Pod "pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003907556s
Mar 30 13:33:11.801: INFO: Pod "pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004253884s
STEP: Saw pod success 03/30/23 13:33:11.801
Mar 30 13:33:11.801: INFO: Pod "pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c" satisfied condition "Succeeded or Failed"
Mar 30 13:33:11.802: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c container agnhost-container: <nil>
STEP: delete the pod 03/30/23 13:33:11.806
Mar 30 13:33:11.811: INFO: Waiting for pod pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c to disappear
Mar 30 13:33:11.813: INFO: Pod pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:33:11.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8244" for this suite. 03/30/23 13:33:11.816
------------------------------
â€¢ [4.038 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:33:07.78
    Mar 30 13:33:07.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:33:07.781
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:33:07.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:33:07.789
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-5be6f17f-041f-4e56-a7e0-f24bc0750d78 03/30/23 13:33:07.791
    STEP: Creating a pod to test consume configMaps 03/30/23 13:33:07.793
    Mar 30 13:33:07.796: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c" in namespace "configmap-8244" to be "Succeeded or Failed"
    Mar 30 13:33:07.798: INFO: Pod "pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.373183ms
    Mar 30 13:33:09.800: INFO: Pod "pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003907556s
    Mar 30 13:33:11.801: INFO: Pod "pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004253884s
    STEP: Saw pod success 03/30/23 13:33:11.801
    Mar 30 13:33:11.801: INFO: Pod "pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c" satisfied condition "Succeeded or Failed"
    Mar 30 13:33:11.802: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 13:33:11.806
    Mar 30 13:33:11.811: INFO: Waiting for pod pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c to disappear
    Mar 30 13:33:11.813: INFO: Pod pod-configmaps-9ec80f23-be6b-45f7-a037-59eaa293124c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:33:11.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8244" for this suite. 03/30/23 13:33:11.816
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:33:11.819
Mar 30 13:33:11.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename statefulset 03/30/23 13:33:11.819
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:33:11.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:33:11.841
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9555 03/30/23 13:33:11.843
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-9555 03/30/23 13:33:11.845
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9555 03/30/23 13:33:11.848
Mar 30 13:33:11.849: INFO: Found 0 stateful pods, waiting for 1
Mar 30 13:33:21.852: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/30/23 13:33:21.852
Mar 30 13:33:21.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 13:33:21.946: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 13:33:21.946: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 13:33:21.946: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 13:33:21.948: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 30 13:33:31.951: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 13:33:31.951: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 13:33:31.958: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Mar 30 13:33:31.958: INFO: ss-0  cn-hongkong.192.168.0.5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:11 +0000 UTC  }]
Mar 30 13:33:31.958: INFO: 
Mar 30 13:33:31.958: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 30 13:33:32.961: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997818285s
Mar 30 13:33:33.964: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994820225s
Mar 30 13:33:34.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991504568s
Mar 30 13:33:35.970: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988823134s
Mar 30 13:33:36.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.985821997s
Mar 30 13:33:37.976: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982661924s
Mar 30 13:33:38.979: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97981915s
Mar 30 13:33:39.982: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.97682137s
Mar 30 13:33:40.985: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.818703ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9555 03/30/23 13:33:41.986
Mar 30 13:33:41.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 13:33:42.082: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 13:33:42.083: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 13:33:42.083: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 13:33:42.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 13:33:42.173: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 30 13:33:42.173: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 13:33:42.173: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 13:33:42.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 13:33:42.259: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 30 13:33:42.259: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 13:33:42.259: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 13:33:42.261: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 13:33:42.261: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 13:33:42.261: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 03/30/23 13:33:42.261
Mar 30 13:33:42.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 13:33:42.358: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 13:33:42.358: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 13:33:42.358: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 13:33:42.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 13:33:42.453: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 13:33:42.453: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 13:33:42.453: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 13:33:42.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 13:33:42.548: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 13:33:42.548: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 13:33:42.548: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 13:33:42.548: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 13:33:42.550: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 30 13:33:52.554: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 13:33:52.554: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 13:33:52.554: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 30 13:33:52.560: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Mar 30 13:33:52.560: INFO: ss-0  cn-hongkong.192.168.0.5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:11 +0000 UTC  }]
Mar 30 13:33:52.560: INFO: ss-1  cn-hongkong.192.168.0.3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:31 +0000 UTC  }]
Mar 30 13:33:52.560: INFO: ss-2  cn-hongkong.192.168.0.4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:31 +0000 UTC  }]
Mar 30 13:33:52.560: INFO: 
Mar 30 13:33:52.560: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 30 13:33:53.562: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Mar 30 13:33:53.562: INFO: ss-0  cn-hongkong.192.168.0.5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:11 +0000 UTC  }]
Mar 30 13:33:53.562: INFO: ss-1  cn-hongkong.192.168.0.3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:31 +0000 UTC  }]
Mar 30 13:33:53.562: INFO: 
Mar 30 13:33:53.562: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 30 13:33:54.564: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.995631106s
Mar 30 13:33:55.566: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.992972866s
Mar 30 13:33:56.569: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.990638813s
Mar 30 13:33:57.571: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.988692479s
Mar 30 13:33:58.573: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.986637804s
Mar 30 13:33:59.576: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.983638738s
Mar 30 13:34:00.578: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.981635525s
Mar 30 13:34:01.580: INFO: Verifying statefulset ss doesn't scale past 0 for another 979.56678ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9555 03/30/23 13:34:02.58
Mar 30 13:34:02.582: INFO: Scaling statefulset ss to 0
Mar 30 13:34:02.587: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 30 13:34:02.589: INFO: Deleting all statefulset in ns statefulset-9555
Mar 30 13:34:02.590: INFO: Scaling statefulset ss to 0
Mar 30 13:34:02.595: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 13:34:02.596: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:02.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9555" for this suite. 03/30/23 13:34:02.604
------------------------------
â€¢ [SLOW TEST] [50.788 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:33:11.819
    Mar 30 13:33:11.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename statefulset 03/30/23 13:33:11.819
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:33:11.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:33:11.841
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9555 03/30/23 13:33:11.843
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-9555 03/30/23 13:33:11.845
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9555 03/30/23 13:33:11.848
    Mar 30 13:33:11.849: INFO: Found 0 stateful pods, waiting for 1
    Mar 30 13:33:21.852: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/30/23 13:33:21.852
    Mar 30 13:33:21.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 30 13:33:21.946: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 30 13:33:21.946: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 30 13:33:21.946: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 30 13:33:21.948: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 30 13:33:31.951: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 30 13:33:31.951: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 30 13:33:31.958: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
    Mar 30 13:33:31.958: INFO: ss-0  cn-hongkong.192.168.0.5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:11 +0000 UTC  }]
    Mar 30 13:33:31.958: INFO: 
    Mar 30 13:33:31.958: INFO: StatefulSet ss has not reached scale 3, at 1
    Mar 30 13:33:32.961: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997818285s
    Mar 30 13:33:33.964: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994820225s
    Mar 30 13:33:34.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991504568s
    Mar 30 13:33:35.970: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988823134s
    Mar 30 13:33:36.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.985821997s
    Mar 30 13:33:37.976: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982661924s
    Mar 30 13:33:38.979: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97981915s
    Mar 30 13:33:39.982: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.97682137s
    Mar 30 13:33:40.985: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.818703ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9555 03/30/23 13:33:41.986
    Mar 30 13:33:41.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 30 13:33:42.082: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 30 13:33:42.083: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 30 13:33:42.083: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 30 13:33:42.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 30 13:33:42.173: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 30 13:33:42.173: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 30 13:33:42.173: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 30 13:33:42.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 30 13:33:42.259: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 30 13:33:42.259: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 30 13:33:42.259: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 30 13:33:42.261: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 30 13:33:42.261: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 30 13:33:42.261: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 03/30/23 13:33:42.261
    Mar 30 13:33:42.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 30 13:33:42.358: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 30 13:33:42.358: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 30 13:33:42.358: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 30 13:33:42.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 30 13:33:42.453: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 30 13:33:42.453: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 30 13:33:42.453: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 30 13:33:42.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9555 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 30 13:33:42.548: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 30 13:33:42.548: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 30 13:33:42.548: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 30 13:33:42.548: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 30 13:33:42.550: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Mar 30 13:33:52.554: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 30 13:33:52.554: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 30 13:33:52.554: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 30 13:33:52.560: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
    Mar 30 13:33:52.560: INFO: ss-0  cn-hongkong.192.168.0.5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:11 +0000 UTC  }]
    Mar 30 13:33:52.560: INFO: ss-1  cn-hongkong.192.168.0.3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:31 +0000 UTC  }]
    Mar 30 13:33:52.560: INFO: ss-2  cn-hongkong.192.168.0.4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:31 +0000 UTC  }]
    Mar 30 13:33:52.560: INFO: 
    Mar 30 13:33:52.560: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar 30 13:33:53.562: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
    Mar 30 13:33:53.562: INFO: ss-0  cn-hongkong.192.168.0.5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:11 +0000 UTC  }]
    Mar 30 13:33:53.562: INFO: ss-1  cn-hongkong.192.168.0.3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:33:31 +0000 UTC  }]
    Mar 30 13:33:53.562: INFO: 
    Mar 30 13:33:53.562: INFO: StatefulSet ss has not reached scale 0, at 2
    Mar 30 13:33:54.564: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.995631106s
    Mar 30 13:33:55.566: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.992972866s
    Mar 30 13:33:56.569: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.990638813s
    Mar 30 13:33:57.571: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.988692479s
    Mar 30 13:33:58.573: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.986637804s
    Mar 30 13:33:59.576: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.983638738s
    Mar 30 13:34:00.578: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.981635525s
    Mar 30 13:34:01.580: INFO: Verifying statefulset ss doesn't scale past 0 for another 979.56678ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9555 03/30/23 13:34:02.58
    Mar 30 13:34:02.582: INFO: Scaling statefulset ss to 0
    Mar 30 13:34:02.587: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 30 13:34:02.589: INFO: Deleting all statefulset in ns statefulset-9555
    Mar 30 13:34:02.590: INFO: Scaling statefulset ss to 0
    Mar 30 13:34:02.595: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 30 13:34:02.596: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:02.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9555" for this suite. 03/30/23 13:34:02.604
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:02.607
Mar 30 13:34:02.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 13:34:02.608
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:02.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:02.616
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/30/23 13:34:02.617
Mar 30 13:34:02.621: INFO: Waiting up to 5m0s for pod "pod-9166276b-6208-4d96-93e8-99f3082eefad" in namespace "emptydir-8133" to be "Succeeded or Failed"
Mar 30 13:34:02.622: INFO: Pod "pod-9166276b-6208-4d96-93e8-99f3082eefad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.440523ms
Mar 30 13:34:04.625: INFO: Pod "pod-9166276b-6208-4d96-93e8-99f3082eefad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004205628s
Mar 30 13:34:06.625: INFO: Pod "pod-9166276b-6208-4d96-93e8-99f3082eefad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00452077s
STEP: Saw pod success 03/30/23 13:34:06.625
Mar 30 13:34:06.625: INFO: Pod "pod-9166276b-6208-4d96-93e8-99f3082eefad" satisfied condition "Succeeded or Failed"
Mar 30 13:34:06.627: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-9166276b-6208-4d96-93e8-99f3082eefad container test-container: <nil>
STEP: delete the pod 03/30/23 13:34:06.63
Mar 30 13:34:06.635: INFO: Waiting for pod pod-9166276b-6208-4d96-93e8-99f3082eefad to disappear
Mar 30 13:34:06.636: INFO: Pod pod-9166276b-6208-4d96-93e8-99f3082eefad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:06.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8133" for this suite. 03/30/23 13:34:06.639
------------------------------
â€¢ [4.034 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:02.607
    Mar 30 13:34:02.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 13:34:02.608
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:02.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:02.616
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/30/23 13:34:02.617
    Mar 30 13:34:02.621: INFO: Waiting up to 5m0s for pod "pod-9166276b-6208-4d96-93e8-99f3082eefad" in namespace "emptydir-8133" to be "Succeeded or Failed"
    Mar 30 13:34:02.622: INFO: Pod "pod-9166276b-6208-4d96-93e8-99f3082eefad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.440523ms
    Mar 30 13:34:04.625: INFO: Pod "pod-9166276b-6208-4d96-93e8-99f3082eefad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004205628s
    Mar 30 13:34:06.625: INFO: Pod "pod-9166276b-6208-4d96-93e8-99f3082eefad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00452077s
    STEP: Saw pod success 03/30/23 13:34:06.625
    Mar 30 13:34:06.625: INFO: Pod "pod-9166276b-6208-4d96-93e8-99f3082eefad" satisfied condition "Succeeded or Failed"
    Mar 30 13:34:06.627: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-9166276b-6208-4d96-93e8-99f3082eefad container test-container: <nil>
    STEP: delete the pod 03/30/23 13:34:06.63
    Mar 30 13:34:06.635: INFO: Waiting for pod pod-9166276b-6208-4d96-93e8-99f3082eefad to disappear
    Mar 30 13:34:06.636: INFO: Pod pod-9166276b-6208-4d96-93e8-99f3082eefad no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:06.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8133" for this suite. 03/30/23 13:34:06.639
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:06.641
Mar 30 13:34:06.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename svcaccounts 03/30/23 13:34:06.642
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:06.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:06.649
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Mar 30 13:34:06.652: INFO: Got root ca configmap in namespace "svcaccounts-7834"
Mar 30 13:34:06.654: INFO: Deleted root ca configmap in namespace "svcaccounts-7834"
STEP: waiting for a new root ca configmap created 03/30/23 13:34:07.155
Mar 30 13:34:07.157: INFO: Recreated root ca configmap in namespace "svcaccounts-7834"
Mar 30 13:34:07.160: INFO: Updated root ca configmap in namespace "svcaccounts-7834"
STEP: waiting for the root ca configmap reconciled 03/30/23 13:34:07.66
Mar 30 13:34:07.662: INFO: Reconciled root ca configmap in namespace "svcaccounts-7834"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:07.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7834" for this suite. 03/30/23 13:34:07.664
------------------------------
â€¢ [1.025 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:06.641
    Mar 30 13:34:06.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename svcaccounts 03/30/23 13:34:06.642
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:06.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:06.649
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Mar 30 13:34:06.652: INFO: Got root ca configmap in namespace "svcaccounts-7834"
    Mar 30 13:34:06.654: INFO: Deleted root ca configmap in namespace "svcaccounts-7834"
    STEP: waiting for a new root ca configmap created 03/30/23 13:34:07.155
    Mar 30 13:34:07.157: INFO: Recreated root ca configmap in namespace "svcaccounts-7834"
    Mar 30 13:34:07.160: INFO: Updated root ca configmap in namespace "svcaccounts-7834"
    STEP: waiting for the root ca configmap reconciled 03/30/23 13:34:07.66
    Mar 30 13:34:07.662: INFO: Reconciled root ca configmap in namespace "svcaccounts-7834"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:07.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7834" for this suite. 03/30/23 13:34:07.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:07.667
Mar 30 13:34:07.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename server-version 03/30/23 13:34:07.668
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:07.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:07.675
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 03/30/23 13:34:07.677
STEP: Confirm major version 03/30/23 13:34:07.677
Mar 30 13:34:07.677: INFO: Major version: 1
STEP: Confirm minor version 03/30/23 13:34:07.677
Mar 30 13:34:07.677: INFO: cleanMinorVersion: 26
Mar 30 13:34:07.677: INFO: Minor version: 26+
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:07.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-8755" for this suite. 03/30/23 13:34:07.679
------------------------------
â€¢ [0.015 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:07.667
    Mar 30 13:34:07.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename server-version 03/30/23 13:34:07.668
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:07.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:07.675
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 03/30/23 13:34:07.677
    STEP: Confirm major version 03/30/23 13:34:07.677
    Mar 30 13:34:07.677: INFO: Major version: 1
    STEP: Confirm minor version 03/30/23 13:34:07.677
    Mar 30 13:34:07.677: INFO: cleanMinorVersion: 26
    Mar 30 13:34:07.677: INFO: Minor version: 26+
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:07.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-8755" for this suite. 03/30/23 13:34:07.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:07.682
Mar 30 13:34:07.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 13:34:07.683
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:07.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:07.69
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-f96e3037-30ae-4659-80c2-96fe8383f9e0 03/30/23 13:34:07.694
STEP: Creating secret with name s-test-opt-upd-685e73d5-6d2f-4987-8f51-e9b9c78e54c7 03/30/23 13:34:07.695
STEP: Creating the pod 03/30/23 13:34:07.697
Mar 30 13:34:07.701: INFO: Waiting up to 5m0s for pod "pod-secrets-9776bae9-d63d-41b4-96e0-de3367743fae" in namespace "secrets-5378" to be "running and ready"
Mar 30 13:34:07.702: INFO: Pod "pod-secrets-9776bae9-d63d-41b4-96e0-de3367743fae": Phase="Pending", Reason="", readiness=false. Elapsed: 1.620567ms
Mar 30 13:34:07.702: INFO: The phase of Pod pod-secrets-9776bae9-d63d-41b4-96e0-de3367743fae is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:34:09.705: INFO: Pod "pod-secrets-9776bae9-d63d-41b4-96e0-de3367743fae": Phase="Running", Reason="", readiness=true. Elapsed: 2.003938032s
Mar 30 13:34:09.705: INFO: The phase of Pod pod-secrets-9776bae9-d63d-41b4-96e0-de3367743fae is Running (Ready = true)
Mar 30 13:34:09.705: INFO: Pod "pod-secrets-9776bae9-d63d-41b4-96e0-de3367743fae" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-f96e3037-30ae-4659-80c2-96fe8383f9e0 03/30/23 13:34:09.715
STEP: Updating secret s-test-opt-upd-685e73d5-6d2f-4987-8f51-e9b9c78e54c7 03/30/23 13:34:09.717
STEP: Creating secret with name s-test-opt-create-4aac8d37-a0c4-4e8b-b885-2b4ea858a05d 03/30/23 13:34:09.719
STEP: waiting to observe update in volume 03/30/23 13:34:09.721
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:11.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5378" for this suite. 03/30/23 13:34:11.736
------------------------------
â€¢ [4.056 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:07.682
    Mar 30 13:34:07.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 13:34:07.683
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:07.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:07.69
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-f96e3037-30ae-4659-80c2-96fe8383f9e0 03/30/23 13:34:07.694
    STEP: Creating secret with name s-test-opt-upd-685e73d5-6d2f-4987-8f51-e9b9c78e54c7 03/30/23 13:34:07.695
    STEP: Creating the pod 03/30/23 13:34:07.697
    Mar 30 13:34:07.701: INFO: Waiting up to 5m0s for pod "pod-secrets-9776bae9-d63d-41b4-96e0-de3367743fae" in namespace "secrets-5378" to be "running and ready"
    Mar 30 13:34:07.702: INFO: Pod "pod-secrets-9776bae9-d63d-41b4-96e0-de3367743fae": Phase="Pending", Reason="", readiness=false. Elapsed: 1.620567ms
    Mar 30 13:34:07.702: INFO: The phase of Pod pod-secrets-9776bae9-d63d-41b4-96e0-de3367743fae is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:34:09.705: INFO: Pod "pod-secrets-9776bae9-d63d-41b4-96e0-de3367743fae": Phase="Running", Reason="", readiness=true. Elapsed: 2.003938032s
    Mar 30 13:34:09.705: INFO: The phase of Pod pod-secrets-9776bae9-d63d-41b4-96e0-de3367743fae is Running (Ready = true)
    Mar 30 13:34:09.705: INFO: Pod "pod-secrets-9776bae9-d63d-41b4-96e0-de3367743fae" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-f96e3037-30ae-4659-80c2-96fe8383f9e0 03/30/23 13:34:09.715
    STEP: Updating secret s-test-opt-upd-685e73d5-6d2f-4987-8f51-e9b9c78e54c7 03/30/23 13:34:09.717
    STEP: Creating secret with name s-test-opt-create-4aac8d37-a0c4-4e8b-b885-2b4ea858a05d 03/30/23 13:34:09.719
    STEP: waiting to observe update in volume 03/30/23 13:34:09.721
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:11.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5378" for this suite. 03/30/23 13:34:11.736
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:11.739
Mar 30 13:34:11.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename job 03/30/23 13:34:11.739
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:11.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:11.747
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 03/30/23 13:34:11.749
STEP: Patching the Job 03/30/23 13:34:11.752
STEP: Watching for Job to be patched 03/30/23 13:34:11.763
Mar 30 13:34:11.764: INFO: Event ADDED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 30 13:34:11.764: INFO: Event MODIFIED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 30 13:34:11.764: INFO: Event MODIFIED found for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 03/30/23 13:34:11.764
STEP: Watching for Job to be updated 03/30/23 13:34:11.769
Mar 30 13:34:11.769: INFO: Event MODIFIED found for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 30 13:34:11.769: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 03/30/23 13:34:11.769
Mar 30 13:34:11.771: INFO: Job: e2e-tnlpj as labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched]
STEP: Waiting for job to complete 03/30/23 13:34:11.771
STEP: Delete a job collection with a labelselector 03/30/23 13:34:19.773
STEP: Watching for Job to be deleted 03/30/23 13:34:19.777
Mar 30 13:34:19.778: INFO: Event MODIFIED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 30 13:34:19.778: INFO: Event MODIFIED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 30 13:34:19.778: INFO: Event MODIFIED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 30 13:34:19.778: INFO: Event MODIFIED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 30 13:34:19.778: INFO: Event MODIFIED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 30 13:34:19.778: INFO: Event DELETED found for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 03/30/23 13:34:19.778
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:19.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3443" for this suite. 03/30/23 13:34:19.783
------------------------------
â€¢ [SLOW TEST] [8.047 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:11.739
    Mar 30 13:34:11.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename job 03/30/23 13:34:11.739
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:11.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:11.747
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 03/30/23 13:34:11.749
    STEP: Patching the Job 03/30/23 13:34:11.752
    STEP: Watching for Job to be patched 03/30/23 13:34:11.763
    Mar 30 13:34:11.764: INFO: Event ADDED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 30 13:34:11.764: INFO: Event MODIFIED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 30 13:34:11.764: INFO: Event MODIFIED found for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 03/30/23 13:34:11.764
    STEP: Watching for Job to be updated 03/30/23 13:34:11.769
    Mar 30 13:34:11.769: INFO: Event MODIFIED found for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 30 13:34:11.769: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 03/30/23 13:34:11.769
    Mar 30 13:34:11.771: INFO: Job: e2e-tnlpj as labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched]
    STEP: Waiting for job to complete 03/30/23 13:34:11.771
    STEP: Delete a job collection with a labelselector 03/30/23 13:34:19.773
    STEP: Watching for Job to be deleted 03/30/23 13:34:19.777
    Mar 30 13:34:19.778: INFO: Event MODIFIED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 30 13:34:19.778: INFO: Event MODIFIED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 30 13:34:19.778: INFO: Event MODIFIED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 30 13:34:19.778: INFO: Event MODIFIED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 30 13:34:19.778: INFO: Event MODIFIED observed for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 30 13:34:19.778: INFO: Event DELETED found for Job e2e-tnlpj in namespace job-3443 with labels: map[e2e-job-label:e2e-tnlpj e2e-tnlpj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 03/30/23 13:34:19.778
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:19.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3443" for this suite. 03/30/23 13:34:19.783
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:19.786
Mar 30 13:34:19.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pods 03/30/23 13:34:19.787
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:19.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:19.795
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Mar 30 13:34:19.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: creating the pod 03/30/23 13:34:19.797
STEP: submitting the pod to kubernetes 03/30/23 13:34:19.797
Mar 30 13:34:19.800: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb" in namespace "pods-3088" to be "running and ready"
Mar 30 13:34:19.801: INFO: Pod "pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.444479ms
Mar 30 13:34:19.801: INFO: The phase of Pod pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:34:21.803: INFO: Pod "pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb": Phase="Running", Reason="", readiness=true. Elapsed: 2.003767161s
Mar 30 13:34:21.803: INFO: The phase of Pod pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb is Running (Ready = true)
Mar 30 13:34:21.803: INFO: Pod "pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:21.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3088" for this suite. 03/30/23 13:34:21.864
------------------------------
â€¢ [2.080 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:19.786
    Mar 30 13:34:19.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pods 03/30/23 13:34:19.787
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:19.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:19.795
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Mar 30 13:34:19.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: creating the pod 03/30/23 13:34:19.797
    STEP: submitting the pod to kubernetes 03/30/23 13:34:19.797
    Mar 30 13:34:19.800: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb" in namespace "pods-3088" to be "running and ready"
    Mar 30 13:34:19.801: INFO: Pod "pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.444479ms
    Mar 30 13:34:19.801: INFO: The phase of Pod pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:34:21.803: INFO: Pod "pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb": Phase="Running", Reason="", readiness=true. Elapsed: 2.003767161s
    Mar 30 13:34:21.803: INFO: The phase of Pod pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb is Running (Ready = true)
    Mar 30 13:34:21.803: INFO: Pod "pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:21.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3088" for this suite. 03/30/23 13:34:21.864
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:21.866
Mar 30 13:34:21.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename security-context-test 03/30/23 13:34:21.867
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:21.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:21.875
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Mar 30 13:34:21.880: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0" in namespace "security-context-test-4428" to be "Succeeded or Failed"
Mar 30 13:34:21.882: INFO: Pod "busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.453525ms
Mar 30 13:34:23.884: INFO: Pod "busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003950867s
Mar 30 13:34:25.885: INFO: Pod "busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004300342s
Mar 30 13:34:25.885: INFO: Pod "busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:25.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4428" for this suite. 03/30/23 13:34:25.887
------------------------------
â€¢ [4.024 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:21.866
    Mar 30 13:34:21.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename security-context-test 03/30/23 13:34:21.867
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:21.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:21.875
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Mar 30 13:34:21.880: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0" in namespace "security-context-test-4428" to be "Succeeded or Failed"
    Mar 30 13:34:21.882: INFO: Pod "busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.453525ms
    Mar 30 13:34:23.884: INFO: Pod "busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003950867s
    Mar 30 13:34:25.885: INFO: Pod "busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004300342s
    Mar 30 13:34:25.885: INFO: Pod "busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:25.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4428" for this suite. 03/30/23 13:34:25.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:25.891
Mar 30 13:34:25.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename sched-pred 03/30/23 13:34:25.891
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:25.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:25.899
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 30 13:34:25.901: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 30 13:34:25.905: INFO: Waiting for terminating namespaces to be deleted...
Mar 30 13:34:25.906: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.3 before test
Mar 30 13:34:25.910: INFO: csi-plugin-2x6v8 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (4 container statuses recorded)
Mar 30 13:34:25.910: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 30 13:34:25.910: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 30 13:34:25.910: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 30 13:34:25.910: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 30 13:34:25.910: INFO: kube-flannel-ds-swjx9 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.910: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 30 13:34:25.910: INFO: kube-proxy-worker-rxqts from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.910: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 30 13:34:25.910: INFO: storage-cnfs-5bbdf677b6-s48rj from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.910: INFO: 	Container storage-cnfs ready: true, restart count 0
Mar 30 13:34:25.910: INFO: storage-snapshot-manager-55c574dc6c-ths98 from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.910: INFO: 	Container storage-snapshot-manager ready: true, restart count 0
Mar 30 13:34:25.910: INFO: busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0 from security-context-test-4428 started at 2023-03-30 13:34:21 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.910: INFO: 	Container busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0 ready: false, restart count 0
Mar 30 13:34:25.910: INFO: sonobuoy-e2e-job-5f38eb9e10894fe7 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:34:25.910: INFO: 	Container e2e ready: true, restart count 0
Mar 30 13:34:25.910: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:34:25.910: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-bn4wq from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:34:25.910: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:34:25.911: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 13:34:25.911: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.4 before test
Mar 30 13:34:25.915: INFO: alicloud-monitor-controller-77f876c7d8-kqc5x from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.915: INFO: 	Container alicloud-monitor-controller ready: true, restart count 0
Mar 30 13:34:25.915: INFO: coredns-5ff46f8d6f-llbsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.915: INFO: 	Container coredns ready: true, restart count 0
Mar 30 13:34:25.915: INFO: csi-plugin-pgrk2 from kube-system started at 2023-03-30 10:07:49 +0000 UTC (4 container statuses recorded)
Mar 30 13:34:25.915: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 30 13:34:25.915: INFO: csi-provisioner-648c597bcb-67xzk from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
Mar 30 13:34:25.915: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-nas-resizer ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-oss-provisioner ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 30 13:34:25.915: INFO: csi-provisioner-648c597bcb-jwm9r from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
Mar 30 13:34:25.915: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-nas-resizer ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-oss-provisioner ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 30 13:34:25.915: INFO: kube-flannel-ds-dftjj from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.915: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 30 13:34:25.915: INFO: kube-proxy-worker-2mrsr from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.915: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 30 13:34:25.915: INFO: metrics-server-5c58794dd-cbrsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.915: INFO: 	Container metrics-server ready: true, restart count 0
Mar 30 13:34:25.915: INFO: storage-operator-5f775996-dphqv from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.915: INFO: 	Container storage-operator ready: true, restart count 0
Mar 30 13:34:25.915: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-prg6s from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:34:25.915: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 13:34:25.915: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.5 before test
Mar 30 13:34:25.919: INFO: coredns-5ff46f8d6f-zz9zg from kube-system started at 2023-03-30 10:07:58 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.919: INFO: 	Container coredns ready: true, restart count 0
Mar 30 13:34:25.919: INFO: csi-plugin-rgcxj from kube-system started at 2023-03-30 10:07:52 +0000 UTC (4 container statuses recorded)
Mar 30 13:34:25.919: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 30 13:34:25.919: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 30 13:34:25.919: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 30 13:34:25.919: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 30 13:34:25.919: INFO: kube-flannel-ds-dspdm from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.919: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 30 13:34:25.919: INFO: kube-proxy-worker-h688q from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.919: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 30 13:34:25.919: INFO: storage-auto-expander-7fd5f8f78-jl4ph from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.919: INFO: 	Container storage-auto-expander ready: true, restart count 0
Mar 30 13:34:25.919: INFO: storage-monitor-8554bcf4c7-c8w4l from kube-system started at 2023-03-30 10:09:36 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.919: INFO: 	Container storage-monitor ready: true, restart count 0
Mar 30 13:34:25.919: INFO: pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb from pods-3088 started at 2023-03-30 13:34:19 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.919: INFO: 	Container main ready: true, restart count 0
Mar 30 13:34:25.919: INFO: sonobuoy from sonobuoy started at 2023-03-30 13:01:31 +0000 UTC (1 container statuses recorded)
Mar 30 13:34:25.919: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 30 13:34:25.919: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-87ph2 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:34:25.919: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:34:25.919: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/30/23 13:34:25.919
Mar 30 13:34:25.923: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-507" to be "running"
Mar 30 13:34:25.925: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481968ms
Mar 30 13:34:27.927: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.00377894s
Mar 30 13:34:27.927: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/30/23 13:34:27.929
STEP: Trying to apply a random label on the found node. 03/30/23 13:34:27.934
STEP: verifying the node has the label kubernetes.io/e2e-aaa4ea29-1d27-438c-8db9-eb447f02ff6c 42 03/30/23 13:34:27.939
STEP: Trying to relaunch the pod, now with labels. 03/30/23 13:34:27.94
Mar 30 13:34:27.942: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-507" to be "not pending"
Mar 30 13:34:27.944: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.44502ms
Mar 30 13:34:29.946: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.003744443s
Mar 30 13:34:29.946: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-aaa4ea29-1d27-438c-8db9-eb447f02ff6c off the node cn-hongkong.192.168.0.5 03/30/23 13:34:29.948
STEP: verifying the node doesn't have the label kubernetes.io/e2e-aaa4ea29-1d27-438c-8db9-eb447f02ff6c 03/30/23 13:34:29.954
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:29.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-507" for this suite. 03/30/23 13:34:29.958
------------------------------
â€¢ [4.071 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:25.891
    Mar 30 13:34:25.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename sched-pred 03/30/23 13:34:25.891
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:25.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:25.899
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 30 13:34:25.901: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 30 13:34:25.905: INFO: Waiting for terminating namespaces to be deleted...
    Mar 30 13:34:25.906: INFO: 
    Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.3 before test
    Mar 30 13:34:25.910: INFO: csi-plugin-2x6v8 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (4 container statuses recorded)
    Mar 30 13:34:25.910: INFO: 	Container csi-plugin ready: true, restart count 0
    Mar 30 13:34:25.910: INFO: 	Container disk-driver-registrar ready: true, restart count 0
    Mar 30 13:34:25.910: INFO: 	Container nas-driver-registrar ready: true, restart count 0
    Mar 30 13:34:25.910: INFO: 	Container oss-driver-registrar ready: true, restart count 0
    Mar 30 13:34:25.910: INFO: kube-flannel-ds-swjx9 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.910: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 30 13:34:25.910: INFO: kube-proxy-worker-rxqts from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.910: INFO: 	Container kube-proxy-worker ready: true, restart count 0
    Mar 30 13:34:25.910: INFO: storage-cnfs-5bbdf677b6-s48rj from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.910: INFO: 	Container storage-cnfs ready: true, restart count 0
    Mar 30 13:34:25.910: INFO: storage-snapshot-manager-55c574dc6c-ths98 from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.910: INFO: 	Container storage-snapshot-manager ready: true, restart count 0
    Mar 30 13:34:25.910: INFO: busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0 from security-context-test-4428 started at 2023-03-30 13:34:21 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.910: INFO: 	Container busybox-user-65534-a654149c-f371-45d2-ac70-c887264fcfd0 ready: false, restart count 0
    Mar 30 13:34:25.910: INFO: sonobuoy-e2e-job-5f38eb9e10894fe7 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:34:25.910: INFO: 	Container e2e ready: true, restart count 0
    Mar 30 13:34:25.910: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:34:25.910: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-bn4wq from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:34:25.910: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:34:25.911: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 30 13:34:25.911: INFO: 
    Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.4 before test
    Mar 30 13:34:25.915: INFO: alicloud-monitor-controller-77f876c7d8-kqc5x from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.915: INFO: 	Container alicloud-monitor-controller ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: coredns-5ff46f8d6f-llbsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.915: INFO: 	Container coredns ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: csi-plugin-pgrk2 from kube-system started at 2023-03-30 10:07:49 +0000 UTC (4 container statuses recorded)
    Mar 30 13:34:25.915: INFO: 	Container csi-plugin ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container disk-driver-registrar ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container nas-driver-registrar ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container oss-driver-registrar ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: csi-provisioner-648c597bcb-67xzk from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
    Mar 30 13:34:25.915: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-disk-attacher ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-disk-provisioner ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-disk-resizer ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-nas-provisioner ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-nas-resizer ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-oss-provisioner ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-snapshot-controller ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: csi-provisioner-648c597bcb-jwm9r from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
    Mar 30 13:34:25.915: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-disk-attacher ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-disk-provisioner ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-disk-resizer ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-nas-provisioner ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-nas-resizer ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-oss-provisioner ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container external-snapshot-controller ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: kube-flannel-ds-dftjj from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.915: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: kube-proxy-worker-2mrsr from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.915: INFO: 	Container kube-proxy-worker ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: metrics-server-5c58794dd-cbrsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.915: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: storage-operator-5f775996-dphqv from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.915: INFO: 	Container storage-operator ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-prg6s from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:34:25.915: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 30 13:34:25.915: INFO: 
    Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.5 before test
    Mar 30 13:34:25.919: INFO: coredns-5ff46f8d6f-zz9zg from kube-system started at 2023-03-30 10:07:58 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.919: INFO: 	Container coredns ready: true, restart count 0
    Mar 30 13:34:25.919: INFO: csi-plugin-rgcxj from kube-system started at 2023-03-30 10:07:52 +0000 UTC (4 container statuses recorded)
    Mar 30 13:34:25.919: INFO: 	Container csi-plugin ready: true, restart count 0
    Mar 30 13:34:25.919: INFO: 	Container disk-driver-registrar ready: true, restart count 0
    Mar 30 13:34:25.919: INFO: 	Container nas-driver-registrar ready: true, restart count 0
    Mar 30 13:34:25.919: INFO: 	Container oss-driver-registrar ready: true, restart count 0
    Mar 30 13:34:25.919: INFO: kube-flannel-ds-dspdm from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.919: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 30 13:34:25.919: INFO: kube-proxy-worker-h688q from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.919: INFO: 	Container kube-proxy-worker ready: true, restart count 0
    Mar 30 13:34:25.919: INFO: storage-auto-expander-7fd5f8f78-jl4ph from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.919: INFO: 	Container storage-auto-expander ready: true, restart count 0
    Mar 30 13:34:25.919: INFO: storage-monitor-8554bcf4c7-c8w4l from kube-system started at 2023-03-30 10:09:36 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.919: INFO: 	Container storage-monitor ready: true, restart count 0
    Mar 30 13:34:25.919: INFO: pod-exec-websocket-7e5beb45-5473-49bd-a432-7c31786ddfdb from pods-3088 started at 2023-03-30 13:34:19 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.919: INFO: 	Container main ready: true, restart count 0
    Mar 30 13:34:25.919: INFO: sonobuoy from sonobuoy started at 2023-03-30 13:01:31 +0000 UTC (1 container statuses recorded)
    Mar 30 13:34:25.919: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 30 13:34:25.919: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-87ph2 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:34:25.919: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:34:25.919: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/30/23 13:34:25.919
    Mar 30 13:34:25.923: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-507" to be "running"
    Mar 30 13:34:25.925: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481968ms
    Mar 30 13:34:27.927: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.00377894s
    Mar 30 13:34:27.927: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/30/23 13:34:27.929
    STEP: Trying to apply a random label on the found node. 03/30/23 13:34:27.934
    STEP: verifying the node has the label kubernetes.io/e2e-aaa4ea29-1d27-438c-8db9-eb447f02ff6c 42 03/30/23 13:34:27.939
    STEP: Trying to relaunch the pod, now with labels. 03/30/23 13:34:27.94
    Mar 30 13:34:27.942: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-507" to be "not pending"
    Mar 30 13:34:27.944: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.44502ms
    Mar 30 13:34:29.946: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.003744443s
    Mar 30 13:34:29.946: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-aaa4ea29-1d27-438c-8db9-eb447f02ff6c off the node cn-hongkong.192.168.0.5 03/30/23 13:34:29.948
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-aaa4ea29-1d27-438c-8db9-eb447f02ff6c 03/30/23 13:34:29.954
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:29.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-507" for this suite. 03/30/23 13:34:29.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:29.963
Mar 30 13:34:29.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 13:34:29.964
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:29.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:29.971
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 13:34:29.978
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:34:30.121
STEP: Deploying the webhook pod 03/30/23 13:34:30.123
STEP: Wait for the deployment to be ready 03/30/23 13:34:30.128
Mar 30 13:34:30.131: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/30/23 13:34:32.136
STEP: Verifying the service has paired with the endpoint 03/30/23 13:34:32.141
Mar 30 13:34:33.141: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 03/30/23 13:34:33.143
STEP: create a pod 03/30/23 13:34:33.152
Mar 30 13:34:33.156: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4813" to be "running"
Mar 30 13:34:33.157: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.544141ms
Mar 30 13:34:35.160: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004454041s
Mar 30 13:34:35.160: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 03/30/23 13:34:35.16
Mar 30 13:34:35.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=webhook-4813 attach --namespace=webhook-4813 to-be-attached-pod -i -c=container1'
Mar 30 13:34:35.219: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:35.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4813" for this suite. 03/30/23 13:34:35.24
STEP: Destroying namespace "webhook-4813-markers" for this suite. 03/30/23 13:34:35.245
------------------------------
â€¢ [SLOW TEST] [5.285 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:29.963
    Mar 30 13:34:29.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 13:34:29.964
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:29.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:29.971
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 13:34:29.978
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:34:30.121
    STEP: Deploying the webhook pod 03/30/23 13:34:30.123
    STEP: Wait for the deployment to be ready 03/30/23 13:34:30.128
    Mar 30 13:34:30.131: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/30/23 13:34:32.136
    STEP: Verifying the service has paired with the endpoint 03/30/23 13:34:32.141
    Mar 30 13:34:33.141: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 03/30/23 13:34:33.143
    STEP: create a pod 03/30/23 13:34:33.152
    Mar 30 13:34:33.156: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4813" to be "running"
    Mar 30 13:34:33.157: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.544141ms
    Mar 30 13:34:35.160: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004454041s
    Mar 30 13:34:35.160: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 03/30/23 13:34:35.16
    Mar 30 13:34:35.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=webhook-4813 attach --namespace=webhook-4813 to-be-attached-pod -i -c=container1'
    Mar 30 13:34:35.219: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:35.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4813" for this suite. 03/30/23 13:34:35.24
    STEP: Destroying namespace "webhook-4813-markers" for this suite. 03/30/23 13:34:35.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:35.248
Mar 30 13:34:35.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename var-expansion 03/30/23 13:34:35.25
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:35.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:35.26
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Mar 30 13:34:35.266: INFO: Waiting up to 2m0s for pod "var-expansion-2f1e9f41-e0c7-4ecd-8d37-4ede74945f76" in namespace "var-expansion-5097" to be "container 0 failed with reason CreateContainerConfigError"
Mar 30 13:34:35.268: INFO: Pod "var-expansion-2f1e9f41-e0c7-4ecd-8d37-4ede74945f76": Phase="Pending", Reason="", readiness=false. Elapsed: 1.540845ms
Mar 30 13:34:37.270: INFO: Pod "var-expansion-2f1e9f41-e0c7-4ecd-8d37-4ede74945f76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003972904s
Mar 30 13:34:37.270: INFO: Pod "var-expansion-2f1e9f41-e0c7-4ecd-8d37-4ede74945f76" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 30 13:34:37.270: INFO: Deleting pod "var-expansion-2f1e9f41-e0c7-4ecd-8d37-4ede74945f76" in namespace "var-expansion-5097"
Mar 30 13:34:37.277: INFO: Wait up to 5m0s for pod "var-expansion-2f1e9f41-e0c7-4ecd-8d37-4ede74945f76" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:39.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5097" for this suite. 03/30/23 13:34:39.283
------------------------------
â€¢ [4.037 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:35.248
    Mar 30 13:34:35.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename var-expansion 03/30/23 13:34:35.25
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:35.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:35.26
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Mar 30 13:34:35.266: INFO: Waiting up to 2m0s for pod "var-expansion-2f1e9f41-e0c7-4ecd-8d37-4ede74945f76" in namespace "var-expansion-5097" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 30 13:34:35.268: INFO: Pod "var-expansion-2f1e9f41-e0c7-4ecd-8d37-4ede74945f76": Phase="Pending", Reason="", readiness=false. Elapsed: 1.540845ms
    Mar 30 13:34:37.270: INFO: Pod "var-expansion-2f1e9f41-e0c7-4ecd-8d37-4ede74945f76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003972904s
    Mar 30 13:34:37.270: INFO: Pod "var-expansion-2f1e9f41-e0c7-4ecd-8d37-4ede74945f76" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 30 13:34:37.270: INFO: Deleting pod "var-expansion-2f1e9f41-e0c7-4ecd-8d37-4ede74945f76" in namespace "var-expansion-5097"
    Mar 30 13:34:37.277: INFO: Wait up to 5m0s for pod "var-expansion-2f1e9f41-e0c7-4ecd-8d37-4ede74945f76" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:39.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5097" for this suite. 03/30/23 13:34:39.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:39.286
Mar 30 13:34:39.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename gc 03/30/23 13:34:39.287
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:39.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:39.295
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 03/30/23 13:34:39.297
STEP: delete the rc 03/30/23 13:34:44.305
STEP: wait for all pods to be garbage collected 03/30/23 13:34:44.308
STEP: Gathering metrics 03/30/23 13:34:49.312
Mar 30 13:34:49.325: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cn-hongkong.192.168.0.252" in namespace "kube-system" to be "running and ready"
Mar 30 13:34:49.326: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252": Phase="Running", Reason="", readiness=true. Elapsed: 1.520108ms
Mar 30 13:34:49.326: INFO: The phase of Pod kube-controller-manager-cn-hongkong.192.168.0.252 is Running (Ready = true)
Mar 30 13:34:49.326: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252" satisfied condition "running and ready"
Mar 30 13:34:49.363: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:49.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-908" for this suite. 03/30/23 13:34:49.365
------------------------------
â€¢ [SLOW TEST] [10.081 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:39.286
    Mar 30 13:34:39.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename gc 03/30/23 13:34:39.287
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:39.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:39.295
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 03/30/23 13:34:39.297
    STEP: delete the rc 03/30/23 13:34:44.305
    STEP: wait for all pods to be garbage collected 03/30/23 13:34:44.308
    STEP: Gathering metrics 03/30/23 13:34:49.312
    Mar 30 13:34:49.325: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cn-hongkong.192.168.0.252" in namespace "kube-system" to be "running and ready"
    Mar 30 13:34:49.326: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252": Phase="Running", Reason="", readiness=true. Elapsed: 1.520108ms
    Mar 30 13:34:49.326: INFO: The phase of Pod kube-controller-manager-cn-hongkong.192.168.0.252 is Running (Ready = true)
    Mar 30 13:34:49.326: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252" satisfied condition "running and ready"
    Mar 30 13:34:49.363: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:49.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-908" for this suite. 03/30/23 13:34:49.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:49.369
Mar 30 13:34:49.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename endpointslice 03/30/23 13:34:49.37
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:49.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:49.387
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Mar 30 13:34:49.393: INFO: Endpoints addresses: [192.168.0.1 192.168.0.2 192.168.0.252] , ports: [6443]
Mar 30 13:34:49.393: INFO: EndpointSlices addresses: [192.168.0.1 192.168.0.2 192.168.0.252] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:49.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1199" for this suite. 03/30/23 13:34:49.395
------------------------------
â€¢ [0.028 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:49.369
    Mar 30 13:34:49.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename endpointslice 03/30/23 13:34:49.37
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:49.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:49.387
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Mar 30 13:34:49.393: INFO: Endpoints addresses: [192.168.0.1 192.168.0.2 192.168.0.252] , ports: [6443]
    Mar 30 13:34:49.393: INFO: EndpointSlices addresses: [192.168.0.1 192.168.0.2 192.168.0.252] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:49.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1199" for this suite. 03/30/23 13:34:49.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:49.398
Mar 30 13:34:49.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename discovery 03/30/23 13:34:49.399
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:49.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:49.406
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 03/30/23 13:34:49.408
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Mar 30 13:34:49.918: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 30 13:34:49.919: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 30 13:34:49.919: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar 30 13:34:49.919: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 30 13:34:49.919: INFO: Checking APIGroup: apps
Mar 30 13:34:49.920: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 30 13:34:49.920: INFO: Versions found [{apps/v1 v1}]
Mar 30 13:34:49.920: INFO: apps/v1 matches apps/v1
Mar 30 13:34:49.920: INFO: Checking APIGroup: events.k8s.io
Mar 30 13:34:49.920: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 30 13:34:49.920: INFO: Versions found [{events.k8s.io/v1 v1}]
Mar 30 13:34:49.920: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 30 13:34:49.920: INFO: Checking APIGroup: authentication.k8s.io
Mar 30 13:34:49.921: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 30 13:34:49.921: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar 30 13:34:49.921: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 30 13:34:49.921: INFO: Checking APIGroup: authorization.k8s.io
Mar 30 13:34:49.921: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 30 13:34:49.921: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar 30 13:34:49.921: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 30 13:34:49.921: INFO: Checking APIGroup: autoscaling
Mar 30 13:34:49.922: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar 30 13:34:49.922: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Mar 30 13:34:49.922: INFO: autoscaling/v2 matches autoscaling/v2
Mar 30 13:34:49.922: INFO: Checking APIGroup: batch
Mar 30 13:34:49.923: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 30 13:34:49.923: INFO: Versions found [{batch/v1 v1}]
Mar 30 13:34:49.923: INFO: batch/v1 matches batch/v1
Mar 30 13:34:49.923: INFO: Checking APIGroup: certificates.k8s.io
Mar 30 13:34:49.923: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 30 13:34:49.923: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar 30 13:34:49.923: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 30 13:34:49.923: INFO: Checking APIGroup: networking.k8s.io
Mar 30 13:34:49.924: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 30 13:34:49.924: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar 30 13:34:49.924: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 30 13:34:49.924: INFO: Checking APIGroup: policy
Mar 30 13:34:49.924: INFO: PreferredVersion.GroupVersion: policy/v1
Mar 30 13:34:49.924: INFO: Versions found [{policy/v1 v1}]
Mar 30 13:34:49.924: INFO: policy/v1 matches policy/v1
Mar 30 13:34:49.924: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 30 13:34:49.925: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 30 13:34:49.925: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar 30 13:34:49.925: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 30 13:34:49.925: INFO: Checking APIGroup: storage.k8s.io
Mar 30 13:34:49.925: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 30 13:34:49.925: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 30 13:34:49.925: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 30 13:34:49.925: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 30 13:34:49.926: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 30 13:34:49.926: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar 30 13:34:49.926: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 30 13:34:49.926: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 30 13:34:49.926: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 30 13:34:49.926: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar 30 13:34:49.926: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 30 13:34:49.926: INFO: Checking APIGroup: scheduling.k8s.io
Mar 30 13:34:49.927: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 30 13:34:49.927: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar 30 13:34:49.927: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 30 13:34:49.927: INFO: Checking APIGroup: coordination.k8s.io
Mar 30 13:34:49.928: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 30 13:34:49.928: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar 30 13:34:49.928: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 30 13:34:49.928: INFO: Checking APIGroup: node.k8s.io
Mar 30 13:34:49.928: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 30 13:34:49.928: INFO: Versions found [{node.k8s.io/v1 v1}]
Mar 30 13:34:49.928: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 30 13:34:49.928: INFO: Checking APIGroup: discovery.k8s.io
Mar 30 13:34:49.929: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar 30 13:34:49.929: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Mar 30 13:34:49.929: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar 30 13:34:49.929: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 30 13:34:49.929: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Mar 30 13:34:49.929: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Mar 30 13:34:49.929: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Mar 30 13:34:49.929: INFO: Checking APIGroup: snapshot.storage.k8s.io
Mar 30 13:34:49.930: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Mar 30 13:34:49.930: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Mar 30 13:34:49.930: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Mar 30 13:34:49.930: INFO: Checking APIGroup: storage.alibabacloud.com
Mar 30 13:34:49.930: INFO: PreferredVersion.GroupVersion: storage.alibabacloud.com/v1beta1
Mar 30 13:34:49.930: INFO: Versions found [{storage.alibabacloud.com/v1beta1 v1beta1} {storage.alibabacloud.com/v1alpha1 v1alpha1}]
Mar 30 13:34:49.930: INFO: storage.alibabacloud.com/v1beta1 matches storage.alibabacloud.com/v1beta1
Mar 30 13:34:49.930: INFO: Checking APIGroup: gateway.networking.k8s.io
Mar 30 13:34:49.931: INFO: PreferredVersion.GroupVersion: gateway.networking.k8s.io/v1beta1
Mar 30 13:34:49.931: INFO: Versions found [{gateway.networking.k8s.io/v1beta1 v1beta1} {gateway.networking.k8s.io/v1alpha2 v1alpha2}]
Mar 30 13:34:49.931: INFO: gateway.networking.k8s.io/v1beta1 matches gateway.networking.k8s.io/v1beta1
Mar 30 13:34:49.931: INFO: Checking APIGroup: alert.alibabacloud.com
Mar 30 13:34:49.931: INFO: PreferredVersion.GroupVersion: alert.alibabacloud.com/v1beta1
Mar 30 13:34:49.931: INFO: Versions found [{alert.alibabacloud.com/v1beta1 v1beta1}]
Mar 30 13:34:49.931: INFO: alert.alibabacloud.com/v1beta1 matches alert.alibabacloud.com/v1beta1
Mar 30 13:34:49.931: INFO: Checking APIGroup: metrics.k8s.io
Mar 30 13:34:49.932: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar 30 13:34:49.932: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar 30 13:34:49.932: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Mar 30 13:34:49.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-9331" for this suite. 03/30/23 13:34:49.934
------------------------------
â€¢ [0.539 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:49.398
    Mar 30 13:34:49.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename discovery 03/30/23 13:34:49.399
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:49.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:49.406
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 03/30/23 13:34:49.408
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Mar 30 13:34:49.918: INFO: Checking APIGroup: apiregistration.k8s.io
    Mar 30 13:34:49.919: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Mar 30 13:34:49.919: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Mar 30 13:34:49.919: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Mar 30 13:34:49.919: INFO: Checking APIGroup: apps
    Mar 30 13:34:49.920: INFO: PreferredVersion.GroupVersion: apps/v1
    Mar 30 13:34:49.920: INFO: Versions found [{apps/v1 v1}]
    Mar 30 13:34:49.920: INFO: apps/v1 matches apps/v1
    Mar 30 13:34:49.920: INFO: Checking APIGroup: events.k8s.io
    Mar 30 13:34:49.920: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Mar 30 13:34:49.920: INFO: Versions found [{events.k8s.io/v1 v1}]
    Mar 30 13:34:49.920: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Mar 30 13:34:49.920: INFO: Checking APIGroup: authentication.k8s.io
    Mar 30 13:34:49.921: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Mar 30 13:34:49.921: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Mar 30 13:34:49.921: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Mar 30 13:34:49.921: INFO: Checking APIGroup: authorization.k8s.io
    Mar 30 13:34:49.921: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Mar 30 13:34:49.921: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Mar 30 13:34:49.921: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Mar 30 13:34:49.921: INFO: Checking APIGroup: autoscaling
    Mar 30 13:34:49.922: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Mar 30 13:34:49.922: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Mar 30 13:34:49.922: INFO: autoscaling/v2 matches autoscaling/v2
    Mar 30 13:34:49.922: INFO: Checking APIGroup: batch
    Mar 30 13:34:49.923: INFO: PreferredVersion.GroupVersion: batch/v1
    Mar 30 13:34:49.923: INFO: Versions found [{batch/v1 v1}]
    Mar 30 13:34:49.923: INFO: batch/v1 matches batch/v1
    Mar 30 13:34:49.923: INFO: Checking APIGroup: certificates.k8s.io
    Mar 30 13:34:49.923: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Mar 30 13:34:49.923: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Mar 30 13:34:49.923: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Mar 30 13:34:49.923: INFO: Checking APIGroup: networking.k8s.io
    Mar 30 13:34:49.924: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Mar 30 13:34:49.924: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Mar 30 13:34:49.924: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Mar 30 13:34:49.924: INFO: Checking APIGroup: policy
    Mar 30 13:34:49.924: INFO: PreferredVersion.GroupVersion: policy/v1
    Mar 30 13:34:49.924: INFO: Versions found [{policy/v1 v1}]
    Mar 30 13:34:49.924: INFO: policy/v1 matches policy/v1
    Mar 30 13:34:49.924: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Mar 30 13:34:49.925: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Mar 30 13:34:49.925: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Mar 30 13:34:49.925: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Mar 30 13:34:49.925: INFO: Checking APIGroup: storage.k8s.io
    Mar 30 13:34:49.925: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Mar 30 13:34:49.925: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Mar 30 13:34:49.925: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Mar 30 13:34:49.925: INFO: Checking APIGroup: admissionregistration.k8s.io
    Mar 30 13:34:49.926: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Mar 30 13:34:49.926: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Mar 30 13:34:49.926: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Mar 30 13:34:49.926: INFO: Checking APIGroup: apiextensions.k8s.io
    Mar 30 13:34:49.926: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Mar 30 13:34:49.926: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Mar 30 13:34:49.926: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Mar 30 13:34:49.926: INFO: Checking APIGroup: scheduling.k8s.io
    Mar 30 13:34:49.927: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Mar 30 13:34:49.927: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Mar 30 13:34:49.927: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Mar 30 13:34:49.927: INFO: Checking APIGroup: coordination.k8s.io
    Mar 30 13:34:49.928: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Mar 30 13:34:49.928: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Mar 30 13:34:49.928: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Mar 30 13:34:49.928: INFO: Checking APIGroup: node.k8s.io
    Mar 30 13:34:49.928: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Mar 30 13:34:49.928: INFO: Versions found [{node.k8s.io/v1 v1}]
    Mar 30 13:34:49.928: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Mar 30 13:34:49.928: INFO: Checking APIGroup: discovery.k8s.io
    Mar 30 13:34:49.929: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Mar 30 13:34:49.929: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Mar 30 13:34:49.929: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Mar 30 13:34:49.929: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Mar 30 13:34:49.929: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Mar 30 13:34:49.929: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Mar 30 13:34:49.929: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Mar 30 13:34:49.929: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Mar 30 13:34:49.930: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Mar 30 13:34:49.930: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Mar 30 13:34:49.930: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Mar 30 13:34:49.930: INFO: Checking APIGroup: storage.alibabacloud.com
    Mar 30 13:34:49.930: INFO: PreferredVersion.GroupVersion: storage.alibabacloud.com/v1beta1
    Mar 30 13:34:49.930: INFO: Versions found [{storage.alibabacloud.com/v1beta1 v1beta1} {storage.alibabacloud.com/v1alpha1 v1alpha1}]
    Mar 30 13:34:49.930: INFO: storage.alibabacloud.com/v1beta1 matches storage.alibabacloud.com/v1beta1
    Mar 30 13:34:49.930: INFO: Checking APIGroup: gateway.networking.k8s.io
    Mar 30 13:34:49.931: INFO: PreferredVersion.GroupVersion: gateway.networking.k8s.io/v1beta1
    Mar 30 13:34:49.931: INFO: Versions found [{gateway.networking.k8s.io/v1beta1 v1beta1} {gateway.networking.k8s.io/v1alpha2 v1alpha2}]
    Mar 30 13:34:49.931: INFO: gateway.networking.k8s.io/v1beta1 matches gateway.networking.k8s.io/v1beta1
    Mar 30 13:34:49.931: INFO: Checking APIGroup: alert.alibabacloud.com
    Mar 30 13:34:49.931: INFO: PreferredVersion.GroupVersion: alert.alibabacloud.com/v1beta1
    Mar 30 13:34:49.931: INFO: Versions found [{alert.alibabacloud.com/v1beta1 v1beta1}]
    Mar 30 13:34:49.931: INFO: alert.alibabacloud.com/v1beta1 matches alert.alibabacloud.com/v1beta1
    Mar 30 13:34:49.931: INFO: Checking APIGroup: metrics.k8s.io
    Mar 30 13:34:49.932: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Mar 30 13:34:49.932: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Mar 30 13:34:49.932: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:34:49.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-9331" for this suite. 03/30/23 13:34:49.934
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:34:49.937
Mar 30 13:34:49.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename gc 03/30/23 13:34:49.938
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:49.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:49.945
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 03/30/23 13:34:49.949
STEP: delete the rc 03/30/23 13:34:54.953
STEP: wait for the rc to be deleted 03/30/23 13:34:54.955
Mar 30 13:34:55.966: INFO: 76 pods remaining
Mar 30 13:34:55.966: INFO: 76 pods has nil DeletionTimestamp
Mar 30 13:34:55.966: INFO: 
Mar 30 13:34:56.961: INFO: 66 pods remaining
Mar 30 13:34:56.961: INFO: 66 pods has nil DeletionTimestamp
Mar 30 13:34:56.961: INFO: 
Mar 30 13:34:57.961: INFO: 56 pods remaining
Mar 30 13:34:57.961: INFO: 56 pods has nil DeletionTimestamp
Mar 30 13:34:57.961: INFO: 
Mar 30 13:34:58.960: INFO: 36 pods remaining
Mar 30 13:34:58.960: INFO: 36 pods has nil DeletionTimestamp
Mar 30 13:34:58.960: INFO: 
Mar 30 13:34:59.960: INFO: 27 pods remaining
Mar 30 13:34:59.960: INFO: 26 pods has nil DeletionTimestamp
Mar 30 13:34:59.960: INFO: 
Mar 30 13:35:00.960: INFO: 16 pods remaining
Mar 30 13:35:00.960: INFO: 16 pods has nil DeletionTimestamp
Mar 30 13:35:00.960: INFO: 
STEP: Gathering metrics 03/30/23 13:35:01.958
Mar 30 13:35:01.968: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cn-hongkong.192.168.0.252" in namespace "kube-system" to be "running and ready"
Mar 30 13:35:01.970: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252": Phase="Running", Reason="", readiness=true. Elapsed: 1.438267ms
Mar 30 13:35:01.970: INFO: The phase of Pod kube-controller-manager-cn-hongkong.192.168.0.252 is Running (Ready = true)
Mar 30 13:35:01.970: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252" satisfied condition "running and ready"
Mar 30 13:35:02.003: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 30 13:35:02.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9930" for this suite. 03/30/23 13:35:02.005
------------------------------
â€¢ [SLOW TEST] [12.071 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:34:49.937
    Mar 30 13:34:49.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename gc 03/30/23 13:34:49.938
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:34:49.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:34:49.945
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 03/30/23 13:34:49.949
    STEP: delete the rc 03/30/23 13:34:54.953
    STEP: wait for the rc to be deleted 03/30/23 13:34:54.955
    Mar 30 13:34:55.966: INFO: 76 pods remaining
    Mar 30 13:34:55.966: INFO: 76 pods has nil DeletionTimestamp
    Mar 30 13:34:55.966: INFO: 
    Mar 30 13:34:56.961: INFO: 66 pods remaining
    Mar 30 13:34:56.961: INFO: 66 pods has nil DeletionTimestamp
    Mar 30 13:34:56.961: INFO: 
    Mar 30 13:34:57.961: INFO: 56 pods remaining
    Mar 30 13:34:57.961: INFO: 56 pods has nil DeletionTimestamp
    Mar 30 13:34:57.961: INFO: 
    Mar 30 13:34:58.960: INFO: 36 pods remaining
    Mar 30 13:34:58.960: INFO: 36 pods has nil DeletionTimestamp
    Mar 30 13:34:58.960: INFO: 
    Mar 30 13:34:59.960: INFO: 27 pods remaining
    Mar 30 13:34:59.960: INFO: 26 pods has nil DeletionTimestamp
    Mar 30 13:34:59.960: INFO: 
    Mar 30 13:35:00.960: INFO: 16 pods remaining
    Mar 30 13:35:00.960: INFO: 16 pods has nil DeletionTimestamp
    Mar 30 13:35:00.960: INFO: 
    STEP: Gathering metrics 03/30/23 13:35:01.958
    Mar 30 13:35:01.968: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cn-hongkong.192.168.0.252" in namespace "kube-system" to be "running and ready"
    Mar 30 13:35:01.970: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252": Phase="Running", Reason="", readiness=true. Elapsed: 1.438267ms
    Mar 30 13:35:01.970: INFO: The phase of Pod kube-controller-manager-cn-hongkong.192.168.0.252 is Running (Ready = true)
    Mar 30 13:35:01.970: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252" satisfied condition "running and ready"
    Mar 30 13:35:02.003: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:35:02.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9930" for this suite. 03/30/23 13:35:02.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:35:02.009
Mar 30 13:35:02.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename watch 03/30/23 13:35:02.009
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:35:02.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:35:02.017
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 03/30/23 13:35:02.019
STEP: creating a watch on configmaps with label B 03/30/23 13:35:02.019
STEP: creating a watch on configmaps with label A or B 03/30/23 13:35:02.02
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/30/23 13:35:02.021
Mar 30 13:35:02.022: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71024 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 13:35:02.023: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71024 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/30/23 13:35:02.023
Mar 30 13:35:02.026: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71025 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 13:35:02.026: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71025 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/30/23 13:35:02.026
Mar 30 13:35:02.029: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71026 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 13:35:02.029: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71026 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/30/23 13:35:02.029
Mar 30 13:35:02.031: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71027 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 13:35:02.031: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71027 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/30/23 13:35:02.031
Mar 30 13:35:02.033: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5883  bc76d092-0155-4573-af07-af6d1ffea1de 71028 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 13:35:02.033: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5883  bc76d092-0155-4573-af07-af6d1ffea1de 71028 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/30/23 13:35:12.033
Mar 30 13:35:12.036: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5883  bc76d092-0155-4573-af07-af6d1ffea1de 71391 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 13:35:12.036: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5883  bc76d092-0155-4573-af07-af6d1ffea1de 71391 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 30 13:35:22.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5883" for this suite. 03/30/23 13:35:22.04
------------------------------
â€¢ [SLOW TEST] [20.034 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:35:02.009
    Mar 30 13:35:02.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename watch 03/30/23 13:35:02.009
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:35:02.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:35:02.017
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 03/30/23 13:35:02.019
    STEP: creating a watch on configmaps with label B 03/30/23 13:35:02.019
    STEP: creating a watch on configmaps with label A or B 03/30/23 13:35:02.02
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/30/23 13:35:02.021
    Mar 30 13:35:02.022: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71024 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 13:35:02.023: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71024 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/30/23 13:35:02.023
    Mar 30 13:35:02.026: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71025 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 13:35:02.026: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71025 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/30/23 13:35:02.026
    Mar 30 13:35:02.029: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71026 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 13:35:02.029: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71026 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/30/23 13:35:02.029
    Mar 30 13:35:02.031: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71027 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 13:35:02.031: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5883  fc73b4ed-2fb7-4eda-9039-c029f537a17b 71027 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/30/23 13:35:02.031
    Mar 30 13:35:02.033: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5883  bc76d092-0155-4573-af07-af6d1ffea1de 71028 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 13:35:02.033: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5883  bc76d092-0155-4573-af07-af6d1ffea1de 71028 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/30/23 13:35:12.033
    Mar 30 13:35:12.036: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5883  bc76d092-0155-4573-af07-af6d1ffea1de 71391 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 13:35:12.036: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5883  bc76d092-0155-4573-af07-af6d1ffea1de 71391 0 2023-03-30 13:35:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-30 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:35:22.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5883" for this suite. 03/30/23 13:35:22.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:35:22.043
Mar 30 13:35:22.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-lifecycle-hook 03/30/23 13:35:22.044
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:35:22.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:35:22.052
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/30/23 13:35:22.055
Mar 30 13:35:22.059: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3472" to be "running and ready"
Mar 30 13:35:22.061: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.385761ms
Mar 30 13:35:22.061: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:35:24.063: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003824926s
Mar 30 13:35:24.063: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 30 13:35:24.063: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 03/30/23 13:35:24.065
Mar 30 13:35:24.067: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3472" to be "running and ready"
Mar 30 13:35:24.069: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.302024ms
Mar 30 13:35:24.069: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:35:26.072: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004824511s
Mar 30 13:35:26.072: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Mar 30 13:35:26.072: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/30/23 13:35:26.074
Mar 30 13:35:26.077: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 30 13:35:26.079: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 30 13:35:28.079: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 30 13:35:28.081: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 03/30/23 13:35:28.081
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 30 13:35:28.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3472" for this suite. 03/30/23 13:35:28.093
------------------------------
â€¢ [SLOW TEST] [6.052 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:35:22.043
    Mar 30 13:35:22.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/30/23 13:35:22.044
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:35:22.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:35:22.052
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/30/23 13:35:22.055
    Mar 30 13:35:22.059: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3472" to be "running and ready"
    Mar 30 13:35:22.061: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.385761ms
    Mar 30 13:35:22.061: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:35:24.063: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003824926s
    Mar 30 13:35:24.063: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 30 13:35:24.063: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 03/30/23 13:35:24.065
    Mar 30 13:35:24.067: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3472" to be "running and ready"
    Mar 30 13:35:24.069: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.302024ms
    Mar 30 13:35:24.069: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:35:26.072: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004824511s
    Mar 30 13:35:26.072: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Mar 30 13:35:26.072: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/30/23 13:35:26.074
    Mar 30 13:35:26.077: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 30 13:35:26.079: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 30 13:35:28.079: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 30 13:35:28.081: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 03/30/23 13:35:28.081
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:35:28.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3472" for this suite. 03/30/23 13:35:28.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:35:28.096
Mar 30 13:35:28.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 13:35:28.096
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:35:28.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:35:28.104
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-1594 03/30/23 13:35:28.106
STEP: creating service affinity-nodeport in namespace services-1594 03/30/23 13:35:28.106
STEP: creating replication controller affinity-nodeport in namespace services-1594 03/30/23 13:35:28.112
I0330 13:35:28.115313      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1594, replica count: 3
I0330 13:35:31.167553      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 13:35:31.173: INFO: Creating new exec pod
Mar 30 13:35:31.176: INFO: Waiting up to 5m0s for pod "execpod-affinitykz7dn" in namespace "services-1594" to be "running"
Mar 30 13:35:31.177: INFO: Pod "execpod-affinitykz7dn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.492917ms
Mar 30 13:35:33.180: INFO: Pod "execpod-affinitykz7dn": Phase="Running", Reason="", readiness=true. Elapsed: 2.004213902s
Mar 30 13:35:33.180: INFO: Pod "execpod-affinitykz7dn" satisfied condition "running"
Mar 30 13:35:34.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1594 exec execpod-affinitykz7dn -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Mar 30 13:35:34.275: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 30 13:35:34.275: INFO: stdout: ""
Mar 30 13:35:34.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1594 exec execpod-affinitykz7dn -- /bin/sh -x -c nc -v -z -w 2 172.16.224.67 80'
Mar 30 13:35:34.367: INFO: stderr: "+ nc -v -z -w 2 172.16.224.67 80\nConnection to 172.16.224.67 80 port [tcp/http] succeeded!\n"
Mar 30 13:35:34.367: INFO: stdout: ""
Mar 30 13:35:34.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1594 exec execpod-affinitykz7dn -- /bin/sh -x -c nc -v -z -w 2 192.168.0.5 31938'
Mar 30 13:35:34.460: INFO: stderr: "+ nc -v -z -w 2 192.168.0.5 31938\nConnection to 192.168.0.5 31938 port [tcp/*] succeeded!\n"
Mar 30 13:35:34.460: INFO: stdout: ""
Mar 30 13:35:34.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1594 exec execpod-affinitykz7dn -- /bin/sh -x -c nc -v -z -w 2 192.168.0.3 31938'
Mar 30 13:35:34.551: INFO: stderr: "+ nc -v -z -w 2 192.168.0.3 31938\nConnection to 192.168.0.3 31938 port [tcp/*] succeeded!\n"
Mar 30 13:35:34.551: INFO: stdout: ""
Mar 30 13:35:34.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1594 exec execpod-affinitykz7dn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.3:31938/ ; done'
Mar 30 13:35:34.698: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n"
Mar 30 13:35:34.698: INFO: stdout: "\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts"
Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.699: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.699: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.699: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.699: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.699: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.699: INFO: Received response from host: affinity-nodeport-pvhts
Mar 30 13:35:34.699: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1594, will wait for the garbage collector to delete the pods 03/30/23 13:35:34.703
Mar 30 13:35:34.758: INFO: Deleting ReplicationController affinity-nodeport took: 2.881064ms
Mar 30 13:35:34.858: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.066505ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 13:35:36.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1594" for this suite. 03/30/23 13:35:36.97
------------------------------
â€¢ [SLOW TEST] [8.878 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:35:28.096
    Mar 30 13:35:28.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 13:35:28.096
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:35:28.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:35:28.104
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-1594 03/30/23 13:35:28.106
    STEP: creating service affinity-nodeport in namespace services-1594 03/30/23 13:35:28.106
    STEP: creating replication controller affinity-nodeport in namespace services-1594 03/30/23 13:35:28.112
    I0330 13:35:28.115313      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1594, replica count: 3
    I0330 13:35:31.167553      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 30 13:35:31.173: INFO: Creating new exec pod
    Mar 30 13:35:31.176: INFO: Waiting up to 5m0s for pod "execpod-affinitykz7dn" in namespace "services-1594" to be "running"
    Mar 30 13:35:31.177: INFO: Pod "execpod-affinitykz7dn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.492917ms
    Mar 30 13:35:33.180: INFO: Pod "execpod-affinitykz7dn": Phase="Running", Reason="", readiness=true. Elapsed: 2.004213902s
    Mar 30 13:35:33.180: INFO: Pod "execpod-affinitykz7dn" satisfied condition "running"
    Mar 30 13:35:34.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1594 exec execpod-affinitykz7dn -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Mar 30 13:35:34.275: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Mar 30 13:35:34.275: INFO: stdout: ""
    Mar 30 13:35:34.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1594 exec execpod-affinitykz7dn -- /bin/sh -x -c nc -v -z -w 2 172.16.224.67 80'
    Mar 30 13:35:34.367: INFO: stderr: "+ nc -v -z -w 2 172.16.224.67 80\nConnection to 172.16.224.67 80 port [tcp/http] succeeded!\n"
    Mar 30 13:35:34.367: INFO: stdout: ""
    Mar 30 13:35:34.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1594 exec execpod-affinitykz7dn -- /bin/sh -x -c nc -v -z -w 2 192.168.0.5 31938'
    Mar 30 13:35:34.460: INFO: stderr: "+ nc -v -z -w 2 192.168.0.5 31938\nConnection to 192.168.0.5 31938 port [tcp/*] succeeded!\n"
    Mar 30 13:35:34.460: INFO: stdout: ""
    Mar 30 13:35:34.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1594 exec execpod-affinitykz7dn -- /bin/sh -x -c nc -v -z -w 2 192.168.0.3 31938'
    Mar 30 13:35:34.551: INFO: stderr: "+ nc -v -z -w 2 192.168.0.3 31938\nConnection to 192.168.0.3 31938 port [tcp/*] succeeded!\n"
    Mar 30 13:35:34.551: INFO: stdout: ""
    Mar 30 13:35:34.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-1594 exec execpod-affinitykz7dn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.3:31938/ ; done'
    Mar 30 13:35:34.698: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31938/\n"
    Mar 30 13:35:34.698: INFO: stdout: "\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts\naffinity-nodeport-pvhts"
    Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.698: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.699: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.699: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.699: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.699: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.699: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.699: INFO: Received response from host: affinity-nodeport-pvhts
    Mar 30 13:35:34.699: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-1594, will wait for the garbage collector to delete the pods 03/30/23 13:35:34.703
    Mar 30 13:35:34.758: INFO: Deleting ReplicationController affinity-nodeport took: 2.881064ms
    Mar 30 13:35:34.858: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.066505ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:35:36.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1594" for this suite. 03/30/23 13:35:36.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:35:36.974
Mar 30 13:35:36.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename runtimeclass 03/30/23 13:35:36.975
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:35:36.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:35:36.984
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Mar 30 13:35:36.992: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9959 to be scheduled
Mar 30 13:35:36.993: INFO: 1 pods are not scheduled: [runtimeclass-9959/test-runtimeclass-runtimeclass-9959-preconfigured-handler-qvnvs(585269ef-8a34-4a18-895d-ba6b6cc073be)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 30 13:35:38.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9959" for this suite. 03/30/23 13:35:39.001
------------------------------
â€¢ [2.029 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:35:36.974
    Mar 30 13:35:36.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename runtimeclass 03/30/23 13:35:36.975
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:35:36.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:35:36.984
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Mar 30 13:35:36.992: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9959 to be scheduled
    Mar 30 13:35:36.993: INFO: 1 pods are not scheduled: [runtimeclass-9959/test-runtimeclass-runtimeclass-9959-preconfigured-handler-qvnvs(585269ef-8a34-4a18-895d-ba6b6cc073be)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:35:38.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9959" for this suite. 03/30/23 13:35:39.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:35:39.004
Mar 30 13:35:39.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename runtimeclass 03/30/23 13:35:39.004
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:35:39.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:35:39.015
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 30 13:35:39.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8539" for this suite. 03/30/23 13:35:39.021
------------------------------
â€¢ [0.020 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:35:39.004
    Mar 30 13:35:39.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename runtimeclass 03/30/23 13:35:39.004
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:35:39.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:35:39.015
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:35:39.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8539" for this suite. 03/30/23 13:35:39.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:35:39.024
Mar 30 13:35:39.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename var-expansion 03/30/23 13:35:39.025
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:35:39.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:35:39.032
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 03/30/23 13:35:39.033
Mar 30 13:35:39.037: INFO: Waiting up to 2m0s for pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74" in namespace "var-expansion-5154" to be "running"
Mar 30 13:35:39.038: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574732ms
Mar 30 13:35:41.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004321435s
Mar 30 13:35:43.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004326985s
Mar 30 13:35:45.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00455396s
Mar 30 13:35:47.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004249487s
Mar 30 13:35:49.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00442321s
Mar 30 13:35:51.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004177363s
Mar 30 13:35:53.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004293447s
Mar 30 13:35:55.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004354325s
Mar 30 13:35:57.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 18.003929097s
Mar 30 13:35:59.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 20.003716914s
Mar 30 13:36:01.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005139268s
Mar 30 13:36:03.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004508738s
Mar 30 13:36:05.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004473649s
Mar 30 13:36:07.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004097656s
Mar 30 13:36:09.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004413429s
Mar 30 13:36:11.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00453002s
Mar 30 13:36:13.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 34.0040914s
Mar 30 13:36:15.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004238693s
Mar 30 13:36:17.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004070627s
Mar 30 13:36:19.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004188889s
Mar 30 13:36:21.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004263389s
Mar 30 13:36:23.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 44.004183877s
Mar 30 13:36:25.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 46.003929066s
Mar 30 13:36:27.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004437222s
Mar 30 13:36:29.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005474652s
Mar 30 13:36:31.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005231915s
Mar 30 13:36:33.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004647333s
Mar 30 13:36:35.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005328558s
Mar 30 13:36:37.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 58.003968509s
Mar 30 13:36:39.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004723672s
Mar 30 13:36:41.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004517452s
Mar 30 13:36:43.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004258094s
Mar 30 13:36:45.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004411273s
Mar 30 13:36:47.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004673125s
Mar 30 13:36:49.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.00387715s
Mar 30 13:36:51.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004342595s
Mar 30 13:36:53.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004477012s
Mar 30 13:36:55.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005167988s
Mar 30 13:36:57.040: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.003602696s
Mar 30 13:36:59.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005112991s
Mar 30 13:37:01.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004230673s
Mar 30 13:37:03.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005201922s
Mar 30 13:37:05.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004584691s
Mar 30 13:37:07.040: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.003478514s
Mar 30 13:37:09.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004161606s
Mar 30 13:37:11.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004412928s
Mar 30 13:37:13.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004276238s
Mar 30 13:37:15.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004485872s
Mar 30 13:37:17.040: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.003643642s
Mar 30 13:37:19.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005009256s
Mar 30 13:37:21.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00438764s
Mar 30 13:37:23.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004189395s
Mar 30 13:37:25.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.00409408s
Mar 30 13:37:27.040: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.003676588s
Mar 30 13:37:29.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00411017s
Mar 30 13:37:31.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004288723s
Mar 30 13:37:33.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004256057s
Mar 30 13:37:35.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004102631s
Mar 30 13:37:37.040: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.003512276s
Mar 30 13:37:39.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005504935s
Mar 30 13:37:39.044: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007073865s
STEP: updating the pod 03/30/23 13:37:39.044
Mar 30 13:37:39.551: INFO: Successfully updated pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74"
STEP: waiting for pod running 03/30/23 13:37:39.551
Mar 30 13:37:39.551: INFO: Waiting up to 2m0s for pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74" in namespace "var-expansion-5154" to be "running"
Mar 30 13:37:39.552: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47738ms
Mar 30 13:37:41.555: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Running", Reason="", readiness=true. Elapsed: 2.004459864s
Mar 30 13:37:41.555: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74" satisfied condition "running"
STEP: deleting the pod gracefully 03/30/23 13:37:41.555
Mar 30 13:37:41.555: INFO: Deleting pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74" in namespace "var-expansion-5154"
Mar 30 13:37:41.559: INFO: Wait up to 5m0s for pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 30 13:38:13.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5154" for this suite. 03/30/23 13:38:13.566
------------------------------
â€¢ [SLOW TEST] [154.544 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:35:39.024
    Mar 30 13:35:39.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename var-expansion 03/30/23 13:35:39.025
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:35:39.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:35:39.032
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 03/30/23 13:35:39.033
    Mar 30 13:35:39.037: INFO: Waiting up to 2m0s for pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74" in namespace "var-expansion-5154" to be "running"
    Mar 30 13:35:39.038: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574732ms
    Mar 30 13:35:41.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004321435s
    Mar 30 13:35:43.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004326985s
    Mar 30 13:35:45.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00455396s
    Mar 30 13:35:47.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004249487s
    Mar 30 13:35:49.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00442321s
    Mar 30 13:35:51.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004177363s
    Mar 30 13:35:53.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004293447s
    Mar 30 13:35:55.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004354325s
    Mar 30 13:35:57.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 18.003929097s
    Mar 30 13:35:59.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 20.003716914s
    Mar 30 13:36:01.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005139268s
    Mar 30 13:36:03.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004508738s
    Mar 30 13:36:05.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004473649s
    Mar 30 13:36:07.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004097656s
    Mar 30 13:36:09.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004413429s
    Mar 30 13:36:11.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00453002s
    Mar 30 13:36:13.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 34.0040914s
    Mar 30 13:36:15.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004238693s
    Mar 30 13:36:17.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004070627s
    Mar 30 13:36:19.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004188889s
    Mar 30 13:36:21.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004263389s
    Mar 30 13:36:23.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 44.004183877s
    Mar 30 13:36:25.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 46.003929066s
    Mar 30 13:36:27.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004437222s
    Mar 30 13:36:29.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005474652s
    Mar 30 13:36:31.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005231915s
    Mar 30 13:36:33.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004647333s
    Mar 30 13:36:35.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005328558s
    Mar 30 13:36:37.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 58.003968509s
    Mar 30 13:36:39.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004723672s
    Mar 30 13:36:41.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004517452s
    Mar 30 13:36:43.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004258094s
    Mar 30 13:36:45.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004411273s
    Mar 30 13:36:47.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004673125s
    Mar 30 13:36:49.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.00387715s
    Mar 30 13:36:51.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004342595s
    Mar 30 13:36:53.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004477012s
    Mar 30 13:36:55.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005167988s
    Mar 30 13:36:57.040: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.003602696s
    Mar 30 13:36:59.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005112991s
    Mar 30 13:37:01.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004230673s
    Mar 30 13:37:03.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005201922s
    Mar 30 13:37:05.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004584691s
    Mar 30 13:37:07.040: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.003478514s
    Mar 30 13:37:09.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004161606s
    Mar 30 13:37:11.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004412928s
    Mar 30 13:37:13.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004276238s
    Mar 30 13:37:15.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004485872s
    Mar 30 13:37:17.040: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.003643642s
    Mar 30 13:37:19.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005009256s
    Mar 30 13:37:21.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00438764s
    Mar 30 13:37:23.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004189395s
    Mar 30 13:37:25.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.00409408s
    Mar 30 13:37:27.040: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.003676588s
    Mar 30 13:37:29.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00411017s
    Mar 30 13:37:31.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004288723s
    Mar 30 13:37:33.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004256057s
    Mar 30 13:37:35.041: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004102631s
    Mar 30 13:37:37.040: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.003512276s
    Mar 30 13:37:39.042: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005504935s
    Mar 30 13:37:39.044: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007073865s
    STEP: updating the pod 03/30/23 13:37:39.044
    Mar 30 13:37:39.551: INFO: Successfully updated pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74"
    STEP: waiting for pod running 03/30/23 13:37:39.551
    Mar 30 13:37:39.551: INFO: Waiting up to 2m0s for pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74" in namespace "var-expansion-5154" to be "running"
    Mar 30 13:37:39.552: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47738ms
    Mar 30 13:37:41.555: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74": Phase="Running", Reason="", readiness=true. Elapsed: 2.004459864s
    Mar 30 13:37:41.555: INFO: Pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74" satisfied condition "running"
    STEP: deleting the pod gracefully 03/30/23 13:37:41.555
    Mar 30 13:37:41.555: INFO: Deleting pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74" in namespace "var-expansion-5154"
    Mar 30 13:37:41.559: INFO: Wait up to 5m0s for pod "var-expansion-22e83d2e-2b1d-4774-a784-7169c0329b74" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:38:13.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5154" for this suite. 03/30/23 13:38:13.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:38:13.569
Mar 30 13:38:13.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 13:38:13.57
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:38:13.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:38:13.578
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 13:38:13.585
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:38:13.888
STEP: Deploying the webhook pod 03/30/23 13:38:13.891
STEP: Wait for the deployment to be ready 03/30/23 13:38:13.896
Mar 30 13:38:13.899: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/30/23 13:38:15.905
STEP: Verifying the service has paired with the endpoint 03/30/23 13:38:15.909
Mar 30 13:38:16.910: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 03/30/23 13:38:16.911
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/30/23 13:38:16.912
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/30/23 13:38:16.912
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/30/23 13:38:16.912
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/30/23 13:38:16.913
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/30/23 13:38:16.913
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/30/23 13:38:16.914
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:38:16.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4806" for this suite. 03/30/23 13:38:16.931
STEP: Destroying namespace "webhook-4806-markers" for this suite. 03/30/23 13:38:16.934
------------------------------
â€¢ [3.368 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:38:13.569
    Mar 30 13:38:13.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 13:38:13.57
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:38:13.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:38:13.578
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 13:38:13.585
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:38:13.888
    STEP: Deploying the webhook pod 03/30/23 13:38:13.891
    STEP: Wait for the deployment to be ready 03/30/23 13:38:13.896
    Mar 30 13:38:13.899: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/30/23 13:38:15.905
    STEP: Verifying the service has paired with the endpoint 03/30/23 13:38:15.909
    Mar 30 13:38:16.910: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 03/30/23 13:38:16.911
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/30/23 13:38:16.912
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/30/23 13:38:16.912
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/30/23 13:38:16.912
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/30/23 13:38:16.913
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/30/23 13:38:16.913
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/30/23 13:38:16.914
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:38:16.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4806" for this suite. 03/30/23 13:38:16.931
    STEP: Destroying namespace "webhook-4806-markers" for this suite. 03/30/23 13:38:16.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:38:16.937
Mar 30 13:38:16.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 13:38:16.938
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:38:16.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:38:16.946
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Mar 30 13:38:16.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/30/23 13:38:24.079
Mar 30 13:38:24.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-3233 --namespace=crd-publish-openapi-3233 create -f -'
Mar 30 13:38:24.519: INFO: stderr: ""
Mar 30 13:38:24.519: INFO: stdout: "e2e-test-crd-publish-openapi-5373-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 30 13:38:24.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-3233 --namespace=crd-publish-openapi-3233 delete e2e-test-crd-publish-openapi-5373-crds test-cr'
Mar 30 13:38:24.587: INFO: stderr: ""
Mar 30 13:38:24.587: INFO: stdout: "e2e-test-crd-publish-openapi-5373-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 30 13:38:24.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-3233 --namespace=crd-publish-openapi-3233 apply -f -'
Mar 30 13:38:24.954: INFO: stderr: ""
Mar 30 13:38:24.954: INFO: stdout: "e2e-test-crd-publish-openapi-5373-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 30 13:38:24.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-3233 --namespace=crd-publish-openapi-3233 delete e2e-test-crd-publish-openapi-5373-crds test-cr'
Mar 30 13:38:25.007: INFO: stderr: ""
Mar 30 13:38:25.007: INFO: stdout: "e2e-test-crd-publish-openapi-5373-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 03/30/23 13:38:25.007
Mar 30 13:38:25.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-3233 explain e2e-test-crd-publish-openapi-5373-crds'
Mar 30 13:38:25.371: INFO: stderr: ""
Mar 30 13:38:25.371: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5373-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:38:27.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3233" for this suite. 03/30/23 13:38:27.01
------------------------------
â€¢ [SLOW TEST] [10.076 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:38:16.937
    Mar 30 13:38:16.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 13:38:16.938
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:38:16.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:38:16.946
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Mar 30 13:38:16.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/30/23 13:38:24.079
    Mar 30 13:38:24.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-3233 --namespace=crd-publish-openapi-3233 create -f -'
    Mar 30 13:38:24.519: INFO: stderr: ""
    Mar 30 13:38:24.519: INFO: stdout: "e2e-test-crd-publish-openapi-5373-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 30 13:38:24.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-3233 --namespace=crd-publish-openapi-3233 delete e2e-test-crd-publish-openapi-5373-crds test-cr'
    Mar 30 13:38:24.587: INFO: stderr: ""
    Mar 30 13:38:24.587: INFO: stdout: "e2e-test-crd-publish-openapi-5373-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Mar 30 13:38:24.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-3233 --namespace=crd-publish-openapi-3233 apply -f -'
    Mar 30 13:38:24.954: INFO: stderr: ""
    Mar 30 13:38:24.954: INFO: stdout: "e2e-test-crd-publish-openapi-5373-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 30 13:38:24.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-3233 --namespace=crd-publish-openapi-3233 delete e2e-test-crd-publish-openapi-5373-crds test-cr'
    Mar 30 13:38:25.007: INFO: stderr: ""
    Mar 30 13:38:25.007: INFO: stdout: "e2e-test-crd-publish-openapi-5373-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 03/30/23 13:38:25.007
    Mar 30 13:38:25.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-3233 explain e2e-test-crd-publish-openapi-5373-crds'
    Mar 30 13:38:25.371: INFO: stderr: ""
    Mar 30 13:38:25.371: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5373-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:38:27.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3233" for this suite. 03/30/23 13:38:27.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:38:27.014
Mar 30 13:38:27.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename events 03/30/23 13:38:27.014
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:38:27.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:38:27.023
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 03/30/23 13:38:27.024
STEP: get a list of Events with a label in the current namespace 03/30/23 13:38:27.031
STEP: delete a list of events 03/30/23 13:38:27.033
Mar 30 13:38:27.033: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/30/23 13:38:27.039
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Mar 30 13:38:27.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9980" for this suite. 03/30/23 13:38:27.043
------------------------------
â€¢ [0.031 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:38:27.014
    Mar 30 13:38:27.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename events 03/30/23 13:38:27.014
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:38:27.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:38:27.023
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 03/30/23 13:38:27.024
    STEP: get a list of Events with a label in the current namespace 03/30/23 13:38:27.031
    STEP: delete a list of events 03/30/23 13:38:27.033
    Mar 30 13:38:27.033: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/30/23 13:38:27.039
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:38:27.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9980" for this suite. 03/30/23 13:38:27.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:38:27.045
Mar 30 13:38:27.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename resourcequota 03/30/23 13:38:27.046
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:38:27.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:38:27.053
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 03/30/23 13:38:27.054
STEP: Counting existing ResourceQuota 03/30/23 13:38:32.056
STEP: Creating a ResourceQuota 03/30/23 13:38:37.058
STEP: Ensuring resource quota status is calculated 03/30/23 13:38:37.061
STEP: Creating a Secret 03/30/23 13:38:39.063
STEP: Ensuring resource quota status captures secret creation 03/30/23 13:38:39.068
STEP: Deleting a secret 03/30/23 13:38:41.071
STEP: Ensuring resource quota status released usage 03/30/23 13:38:41.073
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 30 13:38:43.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1089" for this suite. 03/30/23 13:38:43.078
------------------------------
â€¢ [SLOW TEST] [16.036 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:38:27.045
    Mar 30 13:38:27.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename resourcequota 03/30/23 13:38:27.046
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:38:27.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:38:27.053
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 03/30/23 13:38:27.054
    STEP: Counting existing ResourceQuota 03/30/23 13:38:32.056
    STEP: Creating a ResourceQuota 03/30/23 13:38:37.058
    STEP: Ensuring resource quota status is calculated 03/30/23 13:38:37.061
    STEP: Creating a Secret 03/30/23 13:38:39.063
    STEP: Ensuring resource quota status captures secret creation 03/30/23 13:38:39.068
    STEP: Deleting a secret 03/30/23 13:38:41.071
    STEP: Ensuring resource quota status released usage 03/30/23 13:38:41.073
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:38:43.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1089" for this suite. 03/30/23 13:38:43.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:38:43.081
Mar 30 13:38:43.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename disruption 03/30/23 13:38:43.082
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:38:43.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:38:43.09
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 03/30/23 13:38:43.094
STEP: Updating PodDisruptionBudget status 03/30/23 13:38:45.098
STEP: Waiting for all pods to be running 03/30/23 13:38:45.102
Mar 30 13:38:45.103: INFO: running pods: 0 < 1
STEP: locating a running pod 03/30/23 13:38:47.106
STEP: Waiting for the pdb to be processed 03/30/23 13:38:47.111
STEP: Patching PodDisruptionBudget status 03/30/23 13:38:47.115
STEP: Waiting for the pdb to be processed 03/30/23 13:38:47.119
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 30 13:38:47.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-896" for this suite. 03/30/23 13:38:47.122
------------------------------
â€¢ [4.043 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:38:43.081
    Mar 30 13:38:43.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename disruption 03/30/23 13:38:43.082
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:38:43.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:38:43.09
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 03/30/23 13:38:43.094
    STEP: Updating PodDisruptionBudget status 03/30/23 13:38:45.098
    STEP: Waiting for all pods to be running 03/30/23 13:38:45.102
    Mar 30 13:38:45.103: INFO: running pods: 0 < 1
    STEP: locating a running pod 03/30/23 13:38:47.106
    STEP: Waiting for the pdb to be processed 03/30/23 13:38:47.111
    STEP: Patching PodDisruptionBudget status 03/30/23 13:38:47.115
    STEP: Waiting for the pdb to be processed 03/30/23 13:38:47.119
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:38:47.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-896" for this suite. 03/30/23 13:38:47.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:38:47.126
Mar 30 13:38:47.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename taint-single-pod 03/30/23 13:38:47.127
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:38:47.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:38:47.134
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Mar 30 13:38:47.135: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 30 13:39:47.159: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Mar 30 13:39:47.161: INFO: Starting informer...
STEP: Starting pod... 03/30/23 13:39:47.161
Mar 30 13:39:47.368: INFO: Pod is running on cn-hongkong.192.168.0.5. Tainting Node
STEP: Trying to apply a taint on the Node 03/30/23 13:39:47.368
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/30/23 13:39:47.375
STEP: Waiting short time to make sure Pod is queued for deletion 03/30/23 13:39:47.377
Mar 30 13:39:47.377: INFO: Pod wasn't evicted. Proceeding
Mar 30 13:39:47.377: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/30/23 13:39:47.383
STEP: Waiting some time to make sure that toleration time passed. 03/30/23 13:39:47.385
Mar 30 13:41:02.385: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:02.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-4331" for this suite. 03/30/23 13:41:02.388
------------------------------
â€¢ [SLOW TEST] [135.264 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:38:47.126
    Mar 30 13:38:47.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename taint-single-pod 03/30/23 13:38:47.127
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:38:47.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:38:47.134
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Mar 30 13:38:47.135: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 30 13:39:47.159: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Mar 30 13:39:47.161: INFO: Starting informer...
    STEP: Starting pod... 03/30/23 13:39:47.161
    Mar 30 13:39:47.368: INFO: Pod is running on cn-hongkong.192.168.0.5. Tainting Node
    STEP: Trying to apply a taint on the Node 03/30/23 13:39:47.368
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/30/23 13:39:47.375
    STEP: Waiting short time to make sure Pod is queued for deletion 03/30/23 13:39:47.377
    Mar 30 13:39:47.377: INFO: Pod wasn't evicted. Proceeding
    Mar 30 13:39:47.377: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/30/23 13:39:47.383
    STEP: Waiting some time to make sure that toleration time passed. 03/30/23 13:39:47.385
    Mar 30 13:41:02.385: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:02.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-4331" for this suite. 03/30/23 13:41:02.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:02.391
Mar 30 13:41:02.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename lease-test 03/30/23 13:41:02.392
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:02.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:02.4
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:02.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-2819" for this suite. 03/30/23 13:41:02.426
------------------------------
â€¢ [0.037 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:02.391
    Mar 30 13:41:02.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename lease-test 03/30/23 13:41:02.392
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:02.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:02.4
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:02.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-2819" for this suite. 03/30/23 13:41:02.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:02.429
Mar 30 13:41:02.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename csistoragecapacity 03/30/23 13:41:02.43
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:02.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:02.437
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 03/30/23 13:41:02.439
STEP: getting /apis/storage.k8s.io 03/30/23 13:41:02.44
STEP: getting /apis/storage.k8s.io/v1 03/30/23 13:41:02.441
STEP: creating 03/30/23 13:41:02.442
STEP: watching 03/30/23 13:41:02.448
Mar 30 13:41:02.448: INFO: starting watch
STEP: getting 03/30/23 13:41:02.451
STEP: listing in namespace 03/30/23 13:41:02.452
STEP: listing across namespaces 03/30/23 13:41:02.455
STEP: patching 03/30/23 13:41:02.457
STEP: updating 03/30/23 13:41:02.459
Mar 30 13:41:02.461: INFO: waiting for watch events with expected annotations in namespace
Mar 30 13:41:02.461: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 03/30/23 13:41:02.461
STEP: deleting a collection 03/30/23 13:41:02.466
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:02.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-8475" for this suite. 03/30/23 13:41:02.474
------------------------------
â€¢ [0.047 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:02.429
    Mar 30 13:41:02.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename csistoragecapacity 03/30/23 13:41:02.43
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:02.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:02.437
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 03/30/23 13:41:02.439
    STEP: getting /apis/storage.k8s.io 03/30/23 13:41:02.44
    STEP: getting /apis/storage.k8s.io/v1 03/30/23 13:41:02.441
    STEP: creating 03/30/23 13:41:02.442
    STEP: watching 03/30/23 13:41:02.448
    Mar 30 13:41:02.448: INFO: starting watch
    STEP: getting 03/30/23 13:41:02.451
    STEP: listing in namespace 03/30/23 13:41:02.452
    STEP: listing across namespaces 03/30/23 13:41:02.455
    STEP: patching 03/30/23 13:41:02.457
    STEP: updating 03/30/23 13:41:02.459
    Mar 30 13:41:02.461: INFO: waiting for watch events with expected annotations in namespace
    Mar 30 13:41:02.461: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 03/30/23 13:41:02.461
    STEP: deleting a collection 03/30/23 13:41:02.466
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:02.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-8475" for this suite. 03/30/23 13:41:02.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:02.477
Mar 30 13:41:02.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename limitrange 03/30/23 13:41:02.478
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:02.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:02.485
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-88jz7" in namespace "limitrange-5995" 03/30/23 13:41:02.486
STEP: Creating another limitRange in another namespace 03/30/23 13:41:02.489
Mar 30 13:41:02.494: INFO: Namespace "e2e-limitrange-88jz7-7119" created
Mar 30 13:41:02.494: INFO: Creating LimitRange "e2e-limitrange-88jz7" in namespace "e2e-limitrange-88jz7-7119"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-88jz7" 03/30/23 13:41:02.496
Mar 30 13:41:02.498: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-88jz7" in "limitrange-5995" namespace 03/30/23 13:41:02.498
Mar 30 13:41:02.500: INFO: LimitRange "e2e-limitrange-88jz7" has been patched
STEP: Delete LimitRange "e2e-limitrange-88jz7" by Collection with labelSelector: "e2e-limitrange-88jz7=patched" 03/30/23 13:41:02.5
STEP: Confirm that the limitRange "e2e-limitrange-88jz7" has been deleted 03/30/23 13:41:02.503
Mar 30 13:41:02.503: INFO: Requesting list of LimitRange to confirm quantity
Mar 30 13:41:02.505: INFO: Found 0 LimitRange with label "e2e-limitrange-88jz7=patched"
Mar 30 13:41:02.505: INFO: LimitRange "e2e-limitrange-88jz7" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-88jz7" 03/30/23 13:41:02.505
Mar 30 13:41:02.506: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:02.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5995" for this suite. 03/30/23 13:41:02.508
STEP: Destroying namespace "e2e-limitrange-88jz7-7119" for this suite. 03/30/23 13:41:02.51
------------------------------
â€¢ [0.036 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:02.477
    Mar 30 13:41:02.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename limitrange 03/30/23 13:41:02.478
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:02.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:02.485
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-88jz7" in namespace "limitrange-5995" 03/30/23 13:41:02.486
    STEP: Creating another limitRange in another namespace 03/30/23 13:41:02.489
    Mar 30 13:41:02.494: INFO: Namespace "e2e-limitrange-88jz7-7119" created
    Mar 30 13:41:02.494: INFO: Creating LimitRange "e2e-limitrange-88jz7" in namespace "e2e-limitrange-88jz7-7119"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-88jz7" 03/30/23 13:41:02.496
    Mar 30 13:41:02.498: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-88jz7" in "limitrange-5995" namespace 03/30/23 13:41:02.498
    Mar 30 13:41:02.500: INFO: LimitRange "e2e-limitrange-88jz7" has been patched
    STEP: Delete LimitRange "e2e-limitrange-88jz7" by Collection with labelSelector: "e2e-limitrange-88jz7=patched" 03/30/23 13:41:02.5
    STEP: Confirm that the limitRange "e2e-limitrange-88jz7" has been deleted 03/30/23 13:41:02.503
    Mar 30 13:41:02.503: INFO: Requesting list of LimitRange to confirm quantity
    Mar 30 13:41:02.505: INFO: Found 0 LimitRange with label "e2e-limitrange-88jz7=patched"
    Mar 30 13:41:02.505: INFO: LimitRange "e2e-limitrange-88jz7" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-88jz7" 03/30/23 13:41:02.505
    Mar 30 13:41:02.506: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:02.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5995" for this suite. 03/30/23 13:41:02.508
    STEP: Destroying namespace "e2e-limitrange-88jz7-7119" for this suite. 03/30/23 13:41:02.51
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:02.513
Mar 30 13:41:02.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename custom-resource-definition 03/30/23 13:41:02.514
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:02.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:02.52
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 03/30/23 13:41:02.522
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/30/23 13:41:02.523
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/30/23 13:41:02.523
STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/30/23 13:41:02.523
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/30/23 13:41:02.524
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/30/23 13:41:02.524
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/30/23 13:41:02.524
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:02.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5946" for this suite. 03/30/23 13:41:02.526
------------------------------
â€¢ [0.016 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:02.513
    Mar 30 13:41:02.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename custom-resource-definition 03/30/23 13:41:02.514
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:02.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:02.52
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 03/30/23 13:41:02.522
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/30/23 13:41:02.523
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/30/23 13:41:02.523
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/30/23 13:41:02.523
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/30/23 13:41:02.524
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/30/23 13:41:02.524
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/30/23 13:41:02.524
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:02.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5946" for this suite. 03/30/23 13:41:02.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:02.529
Mar 30 13:41:02.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename security-context-test 03/30/23 13:41:02.53
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:02.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:02.537
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Mar 30 13:41:02.542: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-bb46cfde-90fb-451d-a08e-944584cfad19" in namespace "security-context-test-9629" to be "Succeeded or Failed"
Mar 30 13:41:02.543: INFO: Pod "busybox-privileged-false-bb46cfde-90fb-451d-a08e-944584cfad19": Phase="Pending", Reason="", readiness=false. Elapsed: 1.348122ms
Mar 30 13:41:04.546: INFO: Pod "busybox-privileged-false-bb46cfde-90fb-451d-a08e-944584cfad19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004062448s
Mar 30 13:41:06.546: INFO: Pod "busybox-privileged-false-bb46cfde-90fb-451d-a08e-944584cfad19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004124004s
Mar 30 13:41:06.546: INFO: Pod "busybox-privileged-false-bb46cfde-90fb-451d-a08e-944584cfad19" satisfied condition "Succeeded or Failed"
Mar 30 13:41:06.556: INFO: Got logs for pod "busybox-privileged-false-bb46cfde-90fb-451d-a08e-944584cfad19": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:06.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9629" for this suite. 03/30/23 13:41:06.558
------------------------------
â€¢ [4.032 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:02.529
    Mar 30 13:41:02.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename security-context-test 03/30/23 13:41:02.53
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:02.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:02.537
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Mar 30 13:41:02.542: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-bb46cfde-90fb-451d-a08e-944584cfad19" in namespace "security-context-test-9629" to be "Succeeded or Failed"
    Mar 30 13:41:02.543: INFO: Pod "busybox-privileged-false-bb46cfde-90fb-451d-a08e-944584cfad19": Phase="Pending", Reason="", readiness=false. Elapsed: 1.348122ms
    Mar 30 13:41:04.546: INFO: Pod "busybox-privileged-false-bb46cfde-90fb-451d-a08e-944584cfad19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004062448s
    Mar 30 13:41:06.546: INFO: Pod "busybox-privileged-false-bb46cfde-90fb-451d-a08e-944584cfad19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004124004s
    Mar 30 13:41:06.546: INFO: Pod "busybox-privileged-false-bb46cfde-90fb-451d-a08e-944584cfad19" satisfied condition "Succeeded or Failed"
    Mar 30 13:41:06.556: INFO: Got logs for pod "busybox-privileged-false-bb46cfde-90fb-451d-a08e-944584cfad19": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:06.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9629" for this suite. 03/30/23 13:41:06.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:06.561
Mar 30 13:41:06.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 13:41:06.562
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:06.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:06.57
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-65c83eac-9580-45a5-a52b-fb6bc543c438 03/30/23 13:41:06.571
STEP: Creating a pod to test consume secrets 03/30/23 13:41:06.573
Mar 30 13:41:06.577: INFO: Waiting up to 5m0s for pod "pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551" in namespace "secrets-806" to be "Succeeded or Failed"
Mar 30 13:41:06.578: INFO: Pod "pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551": Phase="Pending", Reason="", readiness=false. Elapsed: 1.532812ms
Mar 30 13:41:08.581: INFO: Pod "pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004076835s
Mar 30 13:41:10.581: INFO: Pod "pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004110922s
STEP: Saw pod success 03/30/23 13:41:10.581
Mar 30 13:41:10.581: INFO: Pod "pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551" satisfied condition "Succeeded or Failed"
Mar 30 13:41:10.583: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551 container secret-volume-test: <nil>
STEP: delete the pod 03/30/23 13:41:10.586
Mar 30 13:41:10.590: INFO: Waiting for pod pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551 to disappear
Mar 30 13:41:10.591: INFO: Pod pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:10.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-806" for this suite. 03/30/23 13:41:10.594
------------------------------
â€¢ [4.035 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:06.561
    Mar 30 13:41:06.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 13:41:06.562
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:06.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:06.57
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-65c83eac-9580-45a5-a52b-fb6bc543c438 03/30/23 13:41:06.571
    STEP: Creating a pod to test consume secrets 03/30/23 13:41:06.573
    Mar 30 13:41:06.577: INFO: Waiting up to 5m0s for pod "pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551" in namespace "secrets-806" to be "Succeeded or Failed"
    Mar 30 13:41:06.578: INFO: Pod "pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551": Phase="Pending", Reason="", readiness=false. Elapsed: 1.532812ms
    Mar 30 13:41:08.581: INFO: Pod "pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004076835s
    Mar 30 13:41:10.581: INFO: Pod "pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004110922s
    STEP: Saw pod success 03/30/23 13:41:10.581
    Mar 30 13:41:10.581: INFO: Pod "pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551" satisfied condition "Succeeded or Failed"
    Mar 30 13:41:10.583: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551 container secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 13:41:10.586
    Mar 30 13:41:10.590: INFO: Waiting for pod pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551 to disappear
    Mar 30 13:41:10.591: INFO: Pod pod-secrets-aa3e7108-c2d4-4381-9ce9-b75e197e7551 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:10.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-806" for this suite. 03/30/23 13:41:10.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:10.597
Mar 30 13:41:10.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 13:41:10.598
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:10.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:10.605
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/30/23 13:41:10.607
Mar 30 13:41:10.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-7297 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Mar 30 13:41:10.661: INFO: stderr: ""
Mar 30 13:41:10.661: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 03/30/23 13:41:10.661
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Mar 30 13:41:10.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-7297 delete pods e2e-test-httpd-pod'
Mar 30 13:41:12.377: INFO: stderr: ""
Mar 30 13:41:12.377: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:12.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7297" for this suite. 03/30/23 13:41:12.38
------------------------------
â€¢ [1.785 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:10.597
    Mar 30 13:41:10.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 13:41:10.598
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:10.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:10.605
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/30/23 13:41:10.607
    Mar 30 13:41:10.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-7297 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Mar 30 13:41:10.661: INFO: stderr: ""
    Mar 30 13:41:10.661: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 03/30/23 13:41:10.661
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Mar 30 13:41:10.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-7297 delete pods e2e-test-httpd-pod'
    Mar 30 13:41:12.377: INFO: stderr: ""
    Mar 30 13:41:12.377: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:12.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7297" for this suite. 03/30/23 13:41:12.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:12.383
Mar 30 13:41:12.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 13:41:12.383
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:12.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:12.392
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/30/23 13:41:12.393
Mar 30 13:41:12.396: INFO: Waiting up to 5m0s for pod "pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634" in namespace "emptydir-3431" to be "Succeeded or Failed"
Mar 30 13:41:12.398: INFO: Pod "pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634": Phase="Pending", Reason="", readiness=false. Elapsed: 1.465669ms
Mar 30 13:41:14.401: INFO: Pod "pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004451809s
Mar 30 13:41:16.401: INFO: Pod "pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00497479s
STEP: Saw pod success 03/30/23 13:41:16.401
Mar 30 13:41:16.401: INFO: Pod "pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634" satisfied condition "Succeeded or Failed"
Mar 30 13:41:16.403: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634 container test-container: <nil>
STEP: delete the pod 03/30/23 13:41:16.406
Mar 30 13:41:16.411: INFO: Waiting for pod pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634 to disappear
Mar 30 13:41:16.412: INFO: Pod pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:16.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3431" for this suite. 03/30/23 13:41:16.414
------------------------------
â€¢ [4.034 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:12.383
    Mar 30 13:41:12.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 13:41:12.383
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:12.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:12.392
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/30/23 13:41:12.393
    Mar 30 13:41:12.396: INFO: Waiting up to 5m0s for pod "pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634" in namespace "emptydir-3431" to be "Succeeded or Failed"
    Mar 30 13:41:12.398: INFO: Pod "pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634": Phase="Pending", Reason="", readiness=false. Elapsed: 1.465669ms
    Mar 30 13:41:14.401: INFO: Pod "pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004451809s
    Mar 30 13:41:16.401: INFO: Pod "pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00497479s
    STEP: Saw pod success 03/30/23 13:41:16.401
    Mar 30 13:41:16.401: INFO: Pod "pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634" satisfied condition "Succeeded or Failed"
    Mar 30 13:41:16.403: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634 container test-container: <nil>
    STEP: delete the pod 03/30/23 13:41:16.406
    Mar 30 13:41:16.411: INFO: Waiting for pod pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634 to disappear
    Mar 30 13:41:16.412: INFO: Pod pod-0206a8d5-15e5-4022-a711-fe3c4e9ed634 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:16.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3431" for this suite. 03/30/23 13:41:16.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:16.417
Mar 30 13:41:16.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 13:41:16.418
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:16.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:16.426
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 03/30/23 13:41:16.428
Mar 30 13:41:16.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4697 create -f -'
Mar 30 13:41:16.835: INFO: stderr: ""
Mar 30 13:41:16.835: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/30/23 13:41:16.835
Mar 30 13:41:17.838: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 13:41:17.838: INFO: Found 1 / 1
Mar 30 13:41:17.838: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 03/30/23 13:41:17.838
Mar 30 13:41:17.840: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 13:41:17.840: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 30 13:41:17.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4697 patch pod agnhost-primary-8r99k -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 30 13:41:17.895: INFO: stderr: ""
Mar 30 13:41:17.895: INFO: stdout: "pod/agnhost-primary-8r99k patched\n"
STEP: checking annotations 03/30/23 13:41:17.895
Mar 30 13:41:17.897: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 13:41:17.897: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:17.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4697" for this suite. 03/30/23 13:41:17.899
------------------------------
â€¢ [1.484 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:16.417
    Mar 30 13:41:16.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 13:41:16.418
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:16.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:16.426
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 03/30/23 13:41:16.428
    Mar 30 13:41:16.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4697 create -f -'
    Mar 30 13:41:16.835: INFO: stderr: ""
    Mar 30 13:41:16.835: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/30/23 13:41:16.835
    Mar 30 13:41:17.838: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 30 13:41:17.838: INFO: Found 1 / 1
    Mar 30 13:41:17.838: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 03/30/23 13:41:17.838
    Mar 30 13:41:17.840: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 30 13:41:17.840: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 30 13:41:17.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-4697 patch pod agnhost-primary-8r99k -p {"metadata":{"annotations":{"x":"y"}}}'
    Mar 30 13:41:17.895: INFO: stderr: ""
    Mar 30 13:41:17.895: INFO: stdout: "pod/agnhost-primary-8r99k patched\n"
    STEP: checking annotations 03/30/23 13:41:17.895
    Mar 30 13:41:17.897: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 30 13:41:17.897: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:17.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4697" for this suite. 03/30/23 13:41:17.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:17.902
Mar 30 13:41:17.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 13:41:17.902
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:17.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:17.91
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 03/30/23 13:41:17.912
Mar 30 13:41:17.915: INFO: Waiting up to 5m0s for pod "downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2" in namespace "downward-api-1298" to be "Succeeded or Failed"
Mar 30 13:41:17.917: INFO: Pod "downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.467682ms
Mar 30 13:41:19.919: INFO: Pod "downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003695544s
Mar 30 13:41:21.919: INFO: Pod "downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004204797s
STEP: Saw pod success 03/30/23 13:41:21.919
Mar 30 13:41:21.919: INFO: Pod "downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2" satisfied condition "Succeeded or Failed"
Mar 30 13:41:21.921: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2 container dapi-container: <nil>
STEP: delete the pod 03/30/23 13:41:21.924
Mar 30 13:41:21.928: INFO: Waiting for pod downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2 to disappear
Mar 30 13:41:21.930: INFO: Pod downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:21.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1298" for this suite. 03/30/23 13:41:21.932
------------------------------
â€¢ [4.033 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:17.902
    Mar 30 13:41:17.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 13:41:17.902
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:17.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:17.91
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 03/30/23 13:41:17.912
    Mar 30 13:41:17.915: INFO: Waiting up to 5m0s for pod "downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2" in namespace "downward-api-1298" to be "Succeeded or Failed"
    Mar 30 13:41:17.917: INFO: Pod "downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.467682ms
    Mar 30 13:41:19.919: INFO: Pod "downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003695544s
    Mar 30 13:41:21.919: INFO: Pod "downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004204797s
    STEP: Saw pod success 03/30/23 13:41:21.919
    Mar 30 13:41:21.919: INFO: Pod "downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2" satisfied condition "Succeeded or Failed"
    Mar 30 13:41:21.921: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2 container dapi-container: <nil>
    STEP: delete the pod 03/30/23 13:41:21.924
    Mar 30 13:41:21.928: INFO: Waiting for pod downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2 to disappear
    Mar 30 13:41:21.930: INFO: Pod downward-api-78238fe6-7f2a-45f7-bcfe-71994bf591c2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:21.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1298" for this suite. 03/30/23 13:41:21.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:21.937
Mar 30 13:41:21.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubelet-test 03/30/23 13:41:21.938
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:21.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:21.946
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:21.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3727" for this suite. 03/30/23 13:41:21.958
------------------------------
â€¢ [0.023 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:21.937
    Mar 30 13:41:21.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubelet-test 03/30/23 13:41:21.938
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:21.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:21.946
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:21.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3727" for this suite. 03/30/23 13:41:21.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:21.96
Mar 30 13:41:21.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:41:21.961
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:21.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:21.968
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 03/30/23 13:41:21.97
Mar 30 13:41:21.973: INFO: Waiting up to 5m0s for pod "labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66" in namespace "projected-9120" to be "running and ready"
Mar 30 13:41:21.978: INFO: Pod "labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.870239ms
Mar 30 13:41:21.978: INFO: The phase of Pod labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:41:23.980: INFO: Pod "labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66": Phase="Running", Reason="", readiness=true. Elapsed: 2.007033761s
Mar 30 13:41:23.980: INFO: The phase of Pod labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66 is Running (Ready = true)
Mar 30 13:41:23.980: INFO: Pod "labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66" satisfied condition "running and ready"
Mar 30 13:41:24.490: INFO: Successfully updated pod "labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:28.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9120" for this suite. 03/30/23 13:41:28.503
------------------------------
â€¢ [SLOW TEST] [6.546 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:21.96
    Mar 30 13:41:21.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:41:21.961
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:21.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:21.968
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 03/30/23 13:41:21.97
    Mar 30 13:41:21.973: INFO: Waiting up to 5m0s for pod "labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66" in namespace "projected-9120" to be "running and ready"
    Mar 30 13:41:21.978: INFO: Pod "labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.870239ms
    Mar 30 13:41:21.978: INFO: The phase of Pod labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:41:23.980: INFO: Pod "labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66": Phase="Running", Reason="", readiness=true. Elapsed: 2.007033761s
    Mar 30 13:41:23.980: INFO: The phase of Pod labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66 is Running (Ready = true)
    Mar 30 13:41:23.980: INFO: Pod "labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66" satisfied condition "running and ready"
    Mar 30 13:41:24.490: INFO: Successfully updated pod "labelsupdate884391d0-f8db-4523-9180-a9c798c4cd66"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:28.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9120" for this suite. 03/30/23 13:41:28.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:28.507
Mar 30 13:41:28.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 13:41:28.507
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:28.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:28.515
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/30/23 13:41:28.517
Mar 30 13:41:28.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:41:35.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:46.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1675" for this suite. 03/30/23 13:41:46.304
------------------------------
â€¢ [SLOW TEST] [17.801 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:28.507
    Mar 30 13:41:28.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 13:41:28.507
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:28.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:28.515
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/30/23 13:41:28.517
    Mar 30 13:41:28.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:41:35.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:46.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1675" for this suite. 03/30/23 13:41:46.304
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:46.308
Mar 30 13:41:46.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pods 03/30/23 13:41:46.308
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:46.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:46.315
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 03/30/23 13:41:46.32
STEP: watching for Pod to be ready 03/30/23 13:41:46.324
Mar 30 13:41:46.325: INFO: observed Pod pod-test in namespace pods-908 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar 30 13:41:46.327: INFO: observed Pod pod-test in namespace pods-908 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC  }]
Mar 30 13:41:46.335: INFO: observed Pod pod-test in namespace pods-908 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC  }]
Mar 30 13:41:47.432: INFO: Found Pod pod-test in namespace pods-908 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 03/30/23 13:41:47.433
STEP: getting the Pod and ensuring that it's patched 03/30/23 13:41:47.438
STEP: replacing the Pod's status Ready condition to False 03/30/23 13:41:47.44
STEP: check the Pod again to ensure its Ready conditions are False 03/30/23 13:41:47.444
STEP: deleting the Pod via a Collection with a LabelSelector 03/30/23 13:41:47.444
STEP: watching for the Pod to be deleted 03/30/23 13:41:47.448
Mar 30 13:41:47.449: INFO: observed event type MODIFIED
Mar 30 13:41:49.435: INFO: observed event type MODIFIED
Mar 30 13:41:50.437: INFO: observed event type MODIFIED
Mar 30 13:41:50.441: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:50.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-908" for this suite. 03/30/23 13:41:50.446
------------------------------
â€¢ [4.141 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:46.308
    Mar 30 13:41:46.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pods 03/30/23 13:41:46.308
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:46.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:46.315
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 03/30/23 13:41:46.32
    STEP: watching for Pod to be ready 03/30/23 13:41:46.324
    Mar 30 13:41:46.325: INFO: observed Pod pod-test in namespace pods-908 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Mar 30 13:41:46.327: INFO: observed Pod pod-test in namespace pods-908 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC  }]
    Mar 30 13:41:46.335: INFO: observed Pod pod-test in namespace pods-908 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC  }]
    Mar 30 13:41:47.432: INFO: Found Pod pod-test in namespace pods-908 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 13:41:46 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 03/30/23 13:41:47.433
    STEP: getting the Pod and ensuring that it's patched 03/30/23 13:41:47.438
    STEP: replacing the Pod's status Ready condition to False 03/30/23 13:41:47.44
    STEP: check the Pod again to ensure its Ready conditions are False 03/30/23 13:41:47.444
    STEP: deleting the Pod via a Collection with a LabelSelector 03/30/23 13:41:47.444
    STEP: watching for the Pod to be deleted 03/30/23 13:41:47.448
    Mar 30 13:41:47.449: INFO: observed event type MODIFIED
    Mar 30 13:41:49.435: INFO: observed event type MODIFIED
    Mar 30 13:41:50.437: INFO: observed event type MODIFIED
    Mar 30 13:41:50.441: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:50.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-908" for this suite. 03/30/23 13:41:50.446
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:50.449
Mar 30 13:41:50.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:41:50.449
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:50.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:50.456
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-2d36102e-87ee-40ca-aa06-ee848b4fd9ba 03/30/23 13:41:50.459
STEP: Creating secret with name s-test-opt-upd-02bdcae5-3d6d-40ad-aa9c-226922215dd9 03/30/23 13:41:50.461
STEP: Creating the pod 03/30/23 13:41:50.463
Mar 30 13:41:50.467: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1c5ea8c6-145b-4038-b04f-368f08026c8d" in namespace "projected-1149" to be "running and ready"
Mar 30 13:41:50.468: INFO: Pod "pod-projected-secrets-1c5ea8c6-145b-4038-b04f-368f08026c8d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.480807ms
Mar 30 13:41:50.468: INFO: The phase of Pod pod-projected-secrets-1c5ea8c6-145b-4038-b04f-368f08026c8d is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:41:52.471: INFO: Pod "pod-projected-secrets-1c5ea8c6-145b-4038-b04f-368f08026c8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004205916s
Mar 30 13:41:52.471: INFO: The phase of Pod pod-projected-secrets-1c5ea8c6-145b-4038-b04f-368f08026c8d is Running (Ready = true)
Mar 30 13:41:52.471: INFO: Pod "pod-projected-secrets-1c5ea8c6-145b-4038-b04f-368f08026c8d" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-2d36102e-87ee-40ca-aa06-ee848b4fd9ba 03/30/23 13:41:52.486
STEP: Updating secret s-test-opt-upd-02bdcae5-3d6d-40ad-aa9c-226922215dd9 03/30/23 13:41:52.488
STEP: Creating secret with name s-test-opt-create-0af67e7b-38ba-4925-b5bd-64d16337a5c0 03/30/23 13:41:52.49
STEP: waiting to observe update in volume 03/30/23 13:41:52.492
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:54.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1149" for this suite. 03/30/23 13:41:54.507
------------------------------
â€¢ [4.062 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:50.449
    Mar 30 13:41:50.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:41:50.449
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:50.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:50.456
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-2d36102e-87ee-40ca-aa06-ee848b4fd9ba 03/30/23 13:41:50.459
    STEP: Creating secret with name s-test-opt-upd-02bdcae5-3d6d-40ad-aa9c-226922215dd9 03/30/23 13:41:50.461
    STEP: Creating the pod 03/30/23 13:41:50.463
    Mar 30 13:41:50.467: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1c5ea8c6-145b-4038-b04f-368f08026c8d" in namespace "projected-1149" to be "running and ready"
    Mar 30 13:41:50.468: INFO: Pod "pod-projected-secrets-1c5ea8c6-145b-4038-b04f-368f08026c8d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.480807ms
    Mar 30 13:41:50.468: INFO: The phase of Pod pod-projected-secrets-1c5ea8c6-145b-4038-b04f-368f08026c8d is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:41:52.471: INFO: Pod "pod-projected-secrets-1c5ea8c6-145b-4038-b04f-368f08026c8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004205916s
    Mar 30 13:41:52.471: INFO: The phase of Pod pod-projected-secrets-1c5ea8c6-145b-4038-b04f-368f08026c8d is Running (Ready = true)
    Mar 30 13:41:52.471: INFO: Pod "pod-projected-secrets-1c5ea8c6-145b-4038-b04f-368f08026c8d" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-2d36102e-87ee-40ca-aa06-ee848b4fd9ba 03/30/23 13:41:52.486
    STEP: Updating secret s-test-opt-upd-02bdcae5-3d6d-40ad-aa9c-226922215dd9 03/30/23 13:41:52.488
    STEP: Creating secret with name s-test-opt-create-0af67e7b-38ba-4925-b5bd-64d16337a5c0 03/30/23 13:41:52.49
    STEP: waiting to observe update in volume 03/30/23 13:41:52.492
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:54.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1149" for this suite. 03/30/23 13:41:54.507
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:54.512
Mar 30 13:41:54.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename deployment 03/30/23 13:41:54.512
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:54.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:54.518
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 03/30/23 13:41:54.522
STEP: waiting for Deployment to be created 03/30/23 13:41:54.524
STEP: waiting for all Replicas to be Ready 03/30/23 13:41:54.525
Mar 30 13:41:54.526: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 30 13:41:54.526: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 30 13:41:54.529: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 30 13:41:54.529: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 30 13:41:54.534: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 30 13:41:54.534: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 30 13:41:54.542: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 30 13:41:54.542: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 30 13:41:55.434: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 30 13:41:55.434: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 30 13:41:55.453: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 03/30/23 13:41:55.453
W0330 13:41:55.458941      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 30 13:41:55.459: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 03/30/23 13:41:55.459
Mar 30 13:41:55.460: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
Mar 30 13:41:55.460: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
Mar 30 13:41:55.460: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
Mar 30 13:41:55.460: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
Mar 30 13:41:55.460: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
Mar 30 13:41:55.460: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:55.464: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:55.464: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:55.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:55.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:55.473: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
Mar 30 13:41:55.473: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
Mar 30 13:41:55.476: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
Mar 30 13:41:55.476: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
Mar 30 13:41:56.458: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:56.458: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:56.465: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
STEP: listing Deployments 03/30/23 13:41:56.465
Mar 30 13:41:56.467: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 03/30/23 13:41:56.467
Mar 30 13:41:56.474: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 03/30/23 13:41:56.474
Mar 30 13:41:56.478: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 30 13:41:56.478: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 30 13:41:56.484: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 30 13:41:56.490: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 30 13:41:56.493: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 30 13:41:57.440: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 30 13:41:57.459: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Mar 30 13:41:57.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 30 13:41:57.472: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 30 13:41:58.456: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 03/30/23 13:41:58.464
STEP: fetching the DeploymentStatus 03/30/23 13:41:58.467
Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 3
Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 3
STEP: deleting the Deployment 03/30/23 13:41:58.47
Mar 30 13:41:58.474: INFO: observed event type MODIFIED
Mar 30 13:41:58.474: INFO: observed event type MODIFIED
Mar 30 13:41:58.474: INFO: observed event type MODIFIED
Mar 30 13:41:58.474: INFO: observed event type MODIFIED
Mar 30 13:41:58.474: INFO: observed event type MODIFIED
Mar 30 13:41:58.474: INFO: observed event type MODIFIED
Mar 30 13:41:58.474: INFO: observed event type MODIFIED
Mar 30 13:41:58.474: INFO: observed event type MODIFIED
Mar 30 13:41:58.474: INFO: observed event type MODIFIED
Mar 30 13:41:58.474: INFO: observed event type MODIFIED
Mar 30 13:41:58.474: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 30 13:41:58.476: INFO: Log out all the ReplicaSets if there is no deployment created
Mar 30 13:41:58.481: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-5731  797674fb-162e-410a-8b21-70e67d90f0d8 74274 2 2023-03-30 13:41:56 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 88d9725e-c133-405d-b453-17a35d180e4d 0xc0052b6467 0xc0052b6468}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:41:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d9725e-c133-405d-b453-17a35d180e4d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:41:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052b64f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Mar 30 13:41:58.483: INFO: pod: "test-deployment-7b7876f9d6-lpm7q":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-lpm7q test-deployment-7b7876f9d6- deployment-5731  73dc4bf0-099f-40dd-a6c4-980d19938997 74272 0 2023-03-30 13:41:57 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 797674fb-162e-410a-8b21-70e67d90f0d8 0xc005373787 0xc005373788}] [] [{kube-controller-manager Update v1 2023-03-30 13:41:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797674fb-162e-410a-8b21-70e67d90f0d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:41:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ljhjp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ljhjp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:10.29.1.111,StartTime:2023-03-30 13:41:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:41:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e661f71b1f01a6ea1ed155e1dffb6f407c10b1f7fb2431b20851cf198d3bc22b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 30 13:41:58.483: INFO: pod: "test-deployment-7b7876f9d6-t476h":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-t476h test-deployment-7b7876f9d6- deployment-5731  3e29fe31-17f3-4c6f-8c01-491653b6d71c 74245 0 2023-03-30 13:41:56 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 797674fb-162e-410a-8b21-70e67d90f0d8 0xc005373967 0xc005373968}] [] [{kube-controller-manager Update v1 2023-03-30 13:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797674fb-162e-410a-8b21-70e67d90f0d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:41:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-64f55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-64f55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.7,StartTime:2023-03-30 13:41:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:41:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://36e5f9bd9a69ed9e460f42a966240b9f4bad811612419c25bf7bb84c3a2db50f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 30 13:41:58.483: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-5731  b8647abb-47c6-4b83-b13c-57d1da2a7d07 74283 4 2023-03-30 13:41:55 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 88d9725e-c133-405d-b453-17a35d180e4d 0xc0052b6557 0xc0052b6558}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:41:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d9725e-c133-405d-b453-17a35d180e4d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:41:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052b65f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar 30 13:41:58.485: INFO: pod: "test-deployment-7df74c55ff-dj84n":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-dj84n test-deployment-7df74c55ff- deployment-5731  ed2f1513-a1a6-4799-8497-a27dd4867e6a 74250 0 2023-03-30 13:41:55 +0000 UTC 2023-03-30 13:41:58 +0000 UTC 0xc005cda9e8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff b8647abb-47c6-4b83-b13c-57d1da2a7d07 0xc005cdaa17 0xc005cdaa18}] [] [{kube-controller-manager Update v1 2023-03-30 13:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8647abb-47c6-4b83-b13c-57d1da2a7d07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:41:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wv2jq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wv2jq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.6,StartTime:2023-03-30 13:41:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:41:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://3e34de971b3884b0a8edbbbe3daebd80ef47cd9357619c55e5f0960aa7818d0c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 30 13:41:58.485: INFO: pod: "test-deployment-7df74c55ff-wn9d7":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-wn9d7 test-deployment-7df74c55ff- deployment-5731  905e852f-54ee-43d8-9c4e-6ab77b97fc3e 74277 0 2023-03-30 13:41:56 +0000 UTC 2023-03-30 13:41:59 +0000 UTC 0xc005cdabd0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff b8647abb-47c6-4b83-b13c-57d1da2a7d07 0xc005cdac07 0xc005cdac08}] [] [{kube-controller-manager Update v1 2023-03-30 13:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8647abb-47c6-4b83-b13c-57d1da2a7d07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:41:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42pqj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42pqj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:10.29.1.110,StartTime:2023-03-30 13:41:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:41:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://155da02901fd804c2bb900c98a83c56592b6947469d27566a3d11a8b38a5af0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 30 13:41:58.485: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-5731  8eebfe4a-5ee5-407e-a5d9-d0e5a0543f3f 74204 3 2023-03-30 13:41:54 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 88d9725e-c133-405d-b453-17a35d180e4d 0xc0052b6657 0xc0052b6658}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d9725e-c133-405d-b453-17a35d180e4d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:41:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052b66e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:58.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5731" for this suite. 03/30/23 13:41:58.501
------------------------------
â€¢ [3.992 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:54.512
    Mar 30 13:41:54.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename deployment 03/30/23 13:41:54.512
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:54.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:54.518
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 03/30/23 13:41:54.522
    STEP: waiting for Deployment to be created 03/30/23 13:41:54.524
    STEP: waiting for all Replicas to be Ready 03/30/23 13:41:54.525
    Mar 30 13:41:54.526: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 30 13:41:54.526: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 30 13:41:54.529: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 30 13:41:54.529: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 30 13:41:54.534: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 30 13:41:54.534: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 30 13:41:54.542: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 30 13:41:54.542: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 30 13:41:55.434: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 30 13:41:55.434: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 30 13:41:55.453: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 03/30/23 13:41:55.453
    W0330 13:41:55.458941      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 30 13:41:55.459: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 03/30/23 13:41:55.459
    Mar 30 13:41:55.460: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
    Mar 30 13:41:55.460: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
    Mar 30 13:41:55.460: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
    Mar 30 13:41:55.460: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
    Mar 30 13:41:55.460: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
    Mar 30 13:41:55.460: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
    Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
    Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 0
    Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:55.461: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:55.464: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:55.464: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:55.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:55.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:55.473: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    Mar 30 13:41:55.473: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    Mar 30 13:41:55.476: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    Mar 30 13:41:55.476: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    Mar 30 13:41:56.458: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:56.458: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:56.465: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    STEP: listing Deployments 03/30/23 13:41:56.465
    Mar 30 13:41:56.467: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 03/30/23 13:41:56.467
    Mar 30 13:41:56.474: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 03/30/23 13:41:56.474
    Mar 30 13:41:56.478: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 30 13:41:56.478: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 30 13:41:56.484: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 30 13:41:56.490: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 30 13:41:56.493: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 30 13:41:57.440: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 30 13:41:57.459: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 30 13:41:57.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 30 13:41:57.472: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 30 13:41:58.456: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 03/30/23 13:41:58.464
    STEP: fetching the DeploymentStatus 03/30/23 13:41:58.467
    Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 1
    Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 3
    Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 2
    Mar 30 13:41:58.470: INFO: observed Deployment test-deployment in namespace deployment-5731 with ReadyReplicas 3
    STEP: deleting the Deployment 03/30/23 13:41:58.47
    Mar 30 13:41:58.474: INFO: observed event type MODIFIED
    Mar 30 13:41:58.474: INFO: observed event type MODIFIED
    Mar 30 13:41:58.474: INFO: observed event type MODIFIED
    Mar 30 13:41:58.474: INFO: observed event type MODIFIED
    Mar 30 13:41:58.474: INFO: observed event type MODIFIED
    Mar 30 13:41:58.474: INFO: observed event type MODIFIED
    Mar 30 13:41:58.474: INFO: observed event type MODIFIED
    Mar 30 13:41:58.474: INFO: observed event type MODIFIED
    Mar 30 13:41:58.474: INFO: observed event type MODIFIED
    Mar 30 13:41:58.474: INFO: observed event type MODIFIED
    Mar 30 13:41:58.474: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 30 13:41:58.476: INFO: Log out all the ReplicaSets if there is no deployment created
    Mar 30 13:41:58.481: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-5731  797674fb-162e-410a-8b21-70e67d90f0d8 74274 2 2023-03-30 13:41:56 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 88d9725e-c133-405d-b453-17a35d180e4d 0xc0052b6467 0xc0052b6468}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:41:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d9725e-c133-405d-b453-17a35d180e4d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:41:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052b64f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Mar 30 13:41:58.483: INFO: pod: "test-deployment-7b7876f9d6-lpm7q":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-lpm7q test-deployment-7b7876f9d6- deployment-5731  73dc4bf0-099f-40dd-a6c4-980d19938997 74272 0 2023-03-30 13:41:57 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 797674fb-162e-410a-8b21-70e67d90f0d8 0xc005373787 0xc005373788}] [] [{kube-controller-manager Update v1 2023-03-30 13:41:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797674fb-162e-410a-8b21-70e67d90f0d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:41:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ljhjp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ljhjp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:10.29.1.111,StartTime:2023-03-30 13:41:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:41:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e661f71b1f01a6ea1ed155e1dffb6f407c10b1f7fb2431b20851cf198d3bc22b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 30 13:41:58.483: INFO: pod: "test-deployment-7b7876f9d6-t476h":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-t476h test-deployment-7b7876f9d6- deployment-5731  3e29fe31-17f3-4c6f-8c01-491653b6d71c 74245 0 2023-03-30 13:41:56 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 797674fb-162e-410a-8b21-70e67d90f0d8 0xc005373967 0xc005373968}] [] [{kube-controller-manager Update v1 2023-03-30 13:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797674fb-162e-410a-8b21-70e67d90f0d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:41:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-64f55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-64f55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.7,StartTime:2023-03-30 13:41:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:41:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://36e5f9bd9a69ed9e460f42a966240b9f4bad811612419c25bf7bb84c3a2db50f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 30 13:41:58.483: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-5731  b8647abb-47c6-4b83-b13c-57d1da2a7d07 74283 4 2023-03-30 13:41:55 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 88d9725e-c133-405d-b453-17a35d180e4d 0xc0052b6557 0xc0052b6558}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:41:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d9725e-c133-405d-b453-17a35d180e4d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:41:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052b65f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Mar 30 13:41:58.485: INFO: pod: "test-deployment-7df74c55ff-dj84n":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-dj84n test-deployment-7df74c55ff- deployment-5731  ed2f1513-a1a6-4799-8497-a27dd4867e6a 74250 0 2023-03-30 13:41:55 +0000 UTC 2023-03-30 13:41:58 +0000 UTC 0xc005cda9e8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff b8647abb-47c6-4b83-b13c-57d1da2a7d07 0xc005cdaa17 0xc005cdaa18}] [] [{kube-controller-manager Update v1 2023-03-30 13:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8647abb-47c6-4b83-b13c-57d1da2a7d07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:41:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wv2jq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wv2jq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.6,StartTime:2023-03-30 13:41:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:41:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://3e34de971b3884b0a8edbbbe3daebd80ef47cd9357619c55e5f0960aa7818d0c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 30 13:41:58.485: INFO: pod: "test-deployment-7df74c55ff-wn9d7":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-wn9d7 test-deployment-7df74c55ff- deployment-5731  905e852f-54ee-43d8-9c4e-6ab77b97fc3e 74277 0 2023-03-30 13:41:56 +0000 UTC 2023-03-30 13:41:59 +0000 UTC 0xc005cdabd0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff b8647abb-47c6-4b83-b13c-57d1da2a7d07 0xc005cdac07 0xc005cdac08}] [] [{kube-controller-manager Update v1 2023-03-30 13:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8647abb-47c6-4b83-b13c-57d1da2a7d07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:41:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42pqj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42pqj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.3,PodIP:10.29.1.110,StartTime:2023-03-30 13:41:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:41:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://155da02901fd804c2bb900c98a83c56592b6947469d27566a3d11a8b38a5af0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 30 13:41:58.485: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-5731  8eebfe4a-5ee5-407e-a5d9-d0e5a0543f3f 74204 3 2023-03-30 13:41:54 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 88d9725e-c133-405d-b453-17a35d180e4d 0xc0052b6657 0xc0052b6658}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88d9725e-c133-405d-b453-17a35d180e4d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:41:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052b66e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:58.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5731" for this suite. 03/30/23 13:41:58.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:58.504
Mar 30 13:41:58.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename daemonsets 03/30/23 13:41:58.505
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:58.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:58.512
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 03/30/23 13:41:58.522
STEP: Check that daemon pods launch on every node of the cluster. 03/30/23 13:41:58.525
Mar 30 13:41:58.527: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:41:58.527: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:41:58.527: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:41:58.528: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 13:41:58.528: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
Mar 30 13:41:59.532: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:41:59.532: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:41:59.532: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 13:41:59.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 30 13:41:59.534: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 03/30/23 13:41:59.535
STEP: DeleteCollection of the DaemonSets 03/30/23 13:41:59.537
STEP: Verify that ReplicaSets have been deleted 03/30/23 13:41:59.54
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Mar 30 13:41:59.546: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74344"},"items":null}

Mar 30 13:41:59.549: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74346"},"items":[{"metadata":{"name":"daemon-set-fkcst","generateName":"daemon-set-","namespace":"daemonsets-4950","uid":"2f5f274f-56cb-4111-ba18-51042da1718a","resourceVersion":"74339","creationTimestamp":"2023-03-30T13:41:58Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"60d34e4b-867c-462b-8716-2118f69daf51","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-30T13:41:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d34e4b-867c-462b-8716-2118f69daf51\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-30T13:41:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4q4xf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4q4xf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cn-hongkong.192.168.0.5","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cn-hongkong.192.168.0.5"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:58Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:59Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:59Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:58Z"}],"hostIP":"192.168.0.5","podIP":"10.29.1.8","podIPs":[{"ip":"10.29.1.8"}],"startTime":"2023-03-30T13:41:58Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-30T13:41:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b11bf65a0252ce0df4f5332666eb313004d808ee005057a4a79f8b2693c5d1c9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kdxl7","generateName":"daemon-set-","namespace":"daemonsets-4950","uid":"8365246c-8972-4678-a4d7-974f52188103","resourceVersion":"74332","creationTimestamp":"2023-03-30T13:41:58Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"60d34e4b-867c-462b-8716-2118f69daf51","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-30T13:41:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d34e4b-867c-462b-8716-2118f69daf51\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-30T13:41:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.0.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rs9wk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rs9wk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cn-hongkong.192.168.0.4","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cn-hongkong.192.168.0.4"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:58Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:59Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:59Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:58Z"}],"hostIP":"192.168.0.4","podIP":"10.29.0.201","podIPs":[{"ip":"10.29.0.201"}],"startTime":"2023-03-30T13:41:58Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-30T13:41:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://82989b5cd5e24d5ada26f0e256f09e582683eb976ea31737c53286d5c16205d5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rcw7s","generateName":"daemon-set-","namespace":"daemonsets-4950","uid":"a6e87d70-e0f9-4b4c-a01d-253d40840276","resourceVersion":"74334","creationTimestamp":"2023-03-30T13:41:58Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"60d34e4b-867c-462b-8716-2118f69daf51","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-30T13:41:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d34e4b-867c-462b-8716-2118f69daf51\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-30T13:41:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l46gl","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l46gl","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cn-hongkong.192.168.0.3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cn-hongkong.192.168.0.3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:58Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:59Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:59Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:58Z"}],"hostIP":"192.168.0.3","podIP":"10.29.1.112","podIPs":[{"ip":"10.29.1.112"}],"startTime":"2023-03-30T13:41:58Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-30T13:41:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d64a07dee63fa34b2d8263274a81db798d12b5892ebcf5968762259614f326a0","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:41:59.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4950" for this suite. 03/30/23 13:41:59.557
------------------------------
â€¢ [1.056 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:58.504
    Mar 30 13:41:58.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename daemonsets 03/30/23 13:41:58.505
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:58.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:58.512
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 03/30/23 13:41:58.522
    STEP: Check that daemon pods launch on every node of the cluster. 03/30/23 13:41:58.525
    Mar 30 13:41:58.527: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:41:58.527: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:41:58.527: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:41:58.528: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 13:41:58.528: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
    Mar 30 13:41:59.532: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:41:59.532: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:41:59.532: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 13:41:59.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 30 13:41:59.534: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 03/30/23 13:41:59.535
    STEP: DeleteCollection of the DaemonSets 03/30/23 13:41:59.537
    STEP: Verify that ReplicaSets have been deleted 03/30/23 13:41:59.54
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Mar 30 13:41:59.546: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74344"},"items":null}

    Mar 30 13:41:59.549: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74346"},"items":[{"metadata":{"name":"daemon-set-fkcst","generateName":"daemon-set-","namespace":"daemonsets-4950","uid":"2f5f274f-56cb-4111-ba18-51042da1718a","resourceVersion":"74339","creationTimestamp":"2023-03-30T13:41:58Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"60d34e4b-867c-462b-8716-2118f69daf51","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-30T13:41:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d34e4b-867c-462b-8716-2118f69daf51\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-30T13:41:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4q4xf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4q4xf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cn-hongkong.192.168.0.5","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cn-hongkong.192.168.0.5"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:58Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:59Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:59Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:58Z"}],"hostIP":"192.168.0.5","podIP":"10.29.1.8","podIPs":[{"ip":"10.29.1.8"}],"startTime":"2023-03-30T13:41:58Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-30T13:41:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b11bf65a0252ce0df4f5332666eb313004d808ee005057a4a79f8b2693c5d1c9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kdxl7","generateName":"daemon-set-","namespace":"daemonsets-4950","uid":"8365246c-8972-4678-a4d7-974f52188103","resourceVersion":"74332","creationTimestamp":"2023-03-30T13:41:58Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"60d34e4b-867c-462b-8716-2118f69daf51","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-30T13:41:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d34e4b-867c-462b-8716-2118f69daf51\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-30T13:41:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.0.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rs9wk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rs9wk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cn-hongkong.192.168.0.4","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cn-hongkong.192.168.0.4"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:58Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:59Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:59Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:58Z"}],"hostIP":"192.168.0.4","podIP":"10.29.0.201","podIPs":[{"ip":"10.29.0.201"}],"startTime":"2023-03-30T13:41:58Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-30T13:41:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://82989b5cd5e24d5ada26f0e256f09e582683eb976ea31737c53286d5c16205d5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rcw7s","generateName":"daemon-set-","namespace":"daemonsets-4950","uid":"a6e87d70-e0f9-4b4c-a01d-253d40840276","resourceVersion":"74334","creationTimestamp":"2023-03-30T13:41:58Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"60d34e4b-867c-462b-8716-2118f69daf51","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-30T13:41:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d34e4b-867c-462b-8716-2118f69daf51\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-30T13:41:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l46gl","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l46gl","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cn-hongkong.192.168.0.3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cn-hongkong.192.168.0.3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:58Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:59Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:59Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-30T13:41:58Z"}],"hostIP":"192.168.0.3","podIP":"10.29.1.112","podIPs":[{"ip":"10.29.1.112"}],"startTime":"2023-03-30T13:41:58Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-30T13:41:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d64a07dee63fa34b2d8263274a81db798d12b5892ebcf5968762259614f326a0","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:41:59.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4950" for this suite. 03/30/23 13:41:59.557
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:41:59.56
Mar 30 13:41:59.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:41:59.561
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:59.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:59.567
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-66f64f6a-4609-4523-837a-bb303babcde0 03/30/23 13:41:59.569
STEP: Creating a pod to test consume configMaps 03/30/23 13:41:59.571
Mar 30 13:41:59.575: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941" in namespace "projected-6252" to be "Succeeded or Failed"
Mar 30 13:41:59.577: INFO: Pod "pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941": Phase="Pending", Reason="", readiness=false. Elapsed: 1.536637ms
Mar 30 13:42:01.579: INFO: Pod "pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003919874s
Mar 30 13:42:03.579: INFO: Pod "pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0037664s
Mar 30 13:42:05.579: INFO: Pod "pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004135326s
STEP: Saw pod success 03/30/23 13:42:05.579
Mar 30 13:42:05.579: INFO: Pod "pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941" satisfied condition "Succeeded or Failed"
Mar 30 13:42:05.581: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941 container agnhost-container: <nil>
STEP: delete the pod 03/30/23 13:42:05.584
Mar 30 13:42:05.588: INFO: Waiting for pod pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941 to disappear
Mar 30 13:42:05.589: INFO: Pod pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:42:05.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6252" for this suite. 03/30/23 13:42:05.592
------------------------------
â€¢ [SLOW TEST] [6.034 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:41:59.56
    Mar 30 13:41:59.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:41:59.561
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:41:59.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:41:59.567
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-66f64f6a-4609-4523-837a-bb303babcde0 03/30/23 13:41:59.569
    STEP: Creating a pod to test consume configMaps 03/30/23 13:41:59.571
    Mar 30 13:41:59.575: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941" in namespace "projected-6252" to be "Succeeded or Failed"
    Mar 30 13:41:59.577: INFO: Pod "pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941": Phase="Pending", Reason="", readiness=false. Elapsed: 1.536637ms
    Mar 30 13:42:01.579: INFO: Pod "pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003919874s
    Mar 30 13:42:03.579: INFO: Pod "pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0037664s
    Mar 30 13:42:05.579: INFO: Pod "pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004135326s
    STEP: Saw pod success 03/30/23 13:42:05.579
    Mar 30 13:42:05.579: INFO: Pod "pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941" satisfied condition "Succeeded or Failed"
    Mar 30 13:42:05.581: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941 container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 13:42:05.584
    Mar 30 13:42:05.588: INFO: Waiting for pod pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941 to disappear
    Mar 30 13:42:05.589: INFO: Pod pod-projected-configmaps-03028a79-5468-4875-bf14-bc6705092941 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:42:05.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6252" for this suite. 03/30/23 13:42:05.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:42:05.594
Mar 30 13:42:05.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename dns 03/30/23 13:42:05.595
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:05.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:05.602
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/30/23 13:42:05.603
Mar 30 13:42:05.607: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2544  29bcea37-ba0e-4e4d-9575-a23aba51d67a 74506 0 2023-03-30 13:42:05 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-30 13:42:05 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bmxcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bmxcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 13:42:05.607: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2544" to be "running and ready"
Mar 30 13:42:05.609: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.842219ms
Mar 30 13:42:05.609: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:42:07.611: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.004124627s
Mar 30 13:42:07.611: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Mar 30 13:42:07.611: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 03/30/23 13:42:07.611
Mar 30 13:42:07.611: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2544 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:42:07.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:42:07.612: INFO: ExecWithOptions: Clientset creation
Mar 30 13:42:07.612: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/dns-2544/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 03/30/23 13:42:07.666
Mar 30 13:42:07.666: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2544 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:42:07.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:42:07.667: INFO: ExecWithOptions: Clientset creation
Mar 30 13:42:07.667: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/dns-2544/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 30 13:42:07.721: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 30 13:42:07.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2544" for this suite. 03/30/23 13:42:07.729
------------------------------
â€¢ [2.137 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:42:05.594
    Mar 30 13:42:05.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename dns 03/30/23 13:42:05.595
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:05.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:05.602
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/30/23 13:42:05.603
    Mar 30 13:42:05.607: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2544  29bcea37-ba0e-4e4d-9575-a23aba51d67a 74506 0 2023-03-30 13:42:05 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-30 13:42:05 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bmxcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bmxcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 13:42:05.607: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2544" to be "running and ready"
    Mar 30 13:42:05.609: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.842219ms
    Mar 30 13:42:05.609: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:42:07.611: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.004124627s
    Mar 30 13:42:07.611: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Mar 30 13:42:07.611: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 03/30/23 13:42:07.611
    Mar 30 13:42:07.611: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2544 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:42:07.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:42:07.612: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:42:07.612: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/dns-2544/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 03/30/23 13:42:07.666
    Mar 30 13:42:07.666: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2544 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:42:07.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:42:07.667: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:42:07.667: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/dns-2544/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 30 13:42:07.721: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:42:07.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2544" for this suite. 03/30/23 13:42:07.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:42:07.732
Mar 30 13:42:07.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 13:42:07.733
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:07.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:07.74
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 13:42:07.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7114" for this suite. 03/30/23 13:42:07.745
------------------------------
â€¢ [0.015 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:42:07.732
    Mar 30 13:42:07.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 13:42:07.733
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:07.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:07.74
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:42:07.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7114" for this suite. 03/30/23 13:42:07.745
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:42:07.747
Mar 30 13:42:07.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:42:07.748
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:07.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:07.754
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 03/30/23 13:42:07.756
STEP: fetching the ConfigMap 03/30/23 13:42:07.758
STEP: patching the ConfigMap 03/30/23 13:42:07.759
STEP: listing all ConfigMaps in all namespaces with a label selector 03/30/23 13:42:07.761
STEP: deleting the ConfigMap by collection with a label selector 03/30/23 13:42:07.763
STEP: listing all ConfigMaps in test namespace 03/30/23 13:42:07.766
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:42:07.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8049" for this suite. 03/30/23 13:42:07.77
------------------------------
â€¢ [0.025 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:42:07.747
    Mar 30 13:42:07.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:42:07.748
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:07.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:07.754
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 03/30/23 13:42:07.756
    STEP: fetching the ConfigMap 03/30/23 13:42:07.758
    STEP: patching the ConfigMap 03/30/23 13:42:07.759
    STEP: listing all ConfigMaps in all namespaces with a label selector 03/30/23 13:42:07.761
    STEP: deleting the ConfigMap by collection with a label selector 03/30/23 13:42:07.763
    STEP: listing all ConfigMaps in test namespace 03/30/23 13:42:07.766
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:42:07.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8049" for this suite. 03/30/23 13:42:07.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:42:07.772
Mar 30 13:42:07.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename namespaces 03/30/23 13:42:07.773
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:07.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:07.779
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 03/30/23 13:42:07.78
Mar 30 13:42:07.782: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 03/30/23 13:42:07.782
Mar 30 13:42:07.784: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 03/30/23 13:42:07.784
Mar 30 13:42:07.788: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:42:07.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3162" for this suite. 03/30/23 13:42:07.79
------------------------------
â€¢ [0.020 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:42:07.772
    Mar 30 13:42:07.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename namespaces 03/30/23 13:42:07.773
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:07.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:07.779
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 03/30/23 13:42:07.78
    Mar 30 13:42:07.782: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 03/30/23 13:42:07.782
    Mar 30 13:42:07.784: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 03/30/23 13:42:07.784
    Mar 30 13:42:07.788: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:42:07.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3162" for this suite. 03/30/23 13:42:07.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:42:07.794
Mar 30 13:42:07.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename security-context-test 03/30/23 13:42:07.795
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:07.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:07.801
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Mar 30 13:42:07.805: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3f58bfc9-3d8c-4d5a-9e1d-9f599151454d" in namespace "security-context-test-1123" to be "Succeeded or Failed"
Mar 30 13:42:07.807: INFO: Pod "busybox-readonly-false-3f58bfc9-3d8c-4d5a-9e1d-9f599151454d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.420351ms
Mar 30 13:42:09.809: INFO: Pod "busybox-readonly-false-3f58bfc9-3d8c-4d5a-9e1d-9f599151454d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003766661s
Mar 30 13:42:11.809: INFO: Pod "busybox-readonly-false-3f58bfc9-3d8c-4d5a-9e1d-9f599151454d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003683001s
Mar 30 13:42:11.809: INFO: Pod "busybox-readonly-false-3f58bfc9-3d8c-4d5a-9e1d-9f599151454d" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 30 13:42:11.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1123" for this suite. 03/30/23 13:42:11.812
------------------------------
â€¢ [4.020 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:42:07.794
    Mar 30 13:42:07.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename security-context-test 03/30/23 13:42:07.795
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:07.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:07.801
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Mar 30 13:42:07.805: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3f58bfc9-3d8c-4d5a-9e1d-9f599151454d" in namespace "security-context-test-1123" to be "Succeeded or Failed"
    Mar 30 13:42:07.807: INFO: Pod "busybox-readonly-false-3f58bfc9-3d8c-4d5a-9e1d-9f599151454d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.420351ms
    Mar 30 13:42:09.809: INFO: Pod "busybox-readonly-false-3f58bfc9-3d8c-4d5a-9e1d-9f599151454d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003766661s
    Mar 30 13:42:11.809: INFO: Pod "busybox-readonly-false-3f58bfc9-3d8c-4d5a-9e1d-9f599151454d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003683001s
    Mar 30 13:42:11.809: INFO: Pod "busybox-readonly-false-3f58bfc9-3d8c-4d5a-9e1d-9f599151454d" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:42:11.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1123" for this suite. 03/30/23 13:42:11.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:42:11.815
Mar 30 13:42:11.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename disruption 03/30/23 13:42:11.815
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:11.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:11.822
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:42:11.823
Mar 30 13:42:11.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename disruption-2 03/30/23 13:42:11.824
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:11.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:11.83
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 03/30/23 13:42:11.834
STEP: Waiting for the pdb to be processed 03/30/23 13:42:13.84
STEP: Waiting for the pdb to be processed 03/30/23 13:42:15.846
STEP: listing a collection of PDBs across all namespaces 03/30/23 13:42:17.851
STEP: listing a collection of PDBs in namespace disruption-3470 03/30/23 13:42:17.852
STEP: deleting a collection of PDBs 03/30/23 13:42:17.854
STEP: Waiting for the PDB collection to be deleted 03/30/23 13:42:17.858
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Mar 30 13:42:17.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 30 13:42:17.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-5999" for this suite. 03/30/23 13:42:17.864
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3470" for this suite. 03/30/23 13:42:17.866
------------------------------
â€¢ [SLOW TEST] [6.055 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:42:11.815
    Mar 30 13:42:11.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename disruption 03/30/23 13:42:11.815
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:11.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:11.822
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:42:11.823
    Mar 30 13:42:11.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename disruption-2 03/30/23 13:42:11.824
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:11.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:11.83
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 03/30/23 13:42:11.834
    STEP: Waiting for the pdb to be processed 03/30/23 13:42:13.84
    STEP: Waiting for the pdb to be processed 03/30/23 13:42:15.846
    STEP: listing a collection of PDBs across all namespaces 03/30/23 13:42:17.851
    STEP: listing a collection of PDBs in namespace disruption-3470 03/30/23 13:42:17.852
    STEP: deleting a collection of PDBs 03/30/23 13:42:17.854
    STEP: Waiting for the PDB collection to be deleted 03/30/23 13:42:17.858
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:42:17.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:42:17.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-5999" for this suite. 03/30/23 13:42:17.864
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3470" for this suite. 03/30/23 13:42:17.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:42:17.87
Mar 30 13:42:17.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-runtime 03/30/23 13:42:17.87
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:17.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:17.876
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 03/30/23 13:42:17.878
STEP: wait for the container to reach Succeeded 03/30/23 13:42:17.881
STEP: get the container status 03/30/23 13:42:20.89
STEP: the container should be terminated 03/30/23 13:42:20.892
STEP: the termination message should be set 03/30/23 13:42:20.892
Mar 30 13:42:20.892: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 03/30/23 13:42:20.892
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 30 13:42:20.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6517" for this suite. 03/30/23 13:42:20.899
------------------------------
â€¢ [3.032 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:42:17.87
    Mar 30 13:42:17.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-runtime 03/30/23 13:42:17.87
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:17.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:17.876
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 03/30/23 13:42:17.878
    STEP: wait for the container to reach Succeeded 03/30/23 13:42:17.881
    STEP: get the container status 03/30/23 13:42:20.89
    STEP: the container should be terminated 03/30/23 13:42:20.892
    STEP: the termination message should be set 03/30/23 13:42:20.892
    Mar 30 13:42:20.892: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 03/30/23 13:42:20.892
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:42:20.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6517" for this suite. 03/30/23 13:42:20.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:42:20.902
Mar 30 13:42:20.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 13:42:20.902
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:20.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:20.909
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 03/30/23 13:42:20.91
Mar 30 13:42:20.910: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3343 proxy --unix-socket=/tmp/kubectl-proxy-unix2332714450/test'
STEP: retrieving proxy /api/ output 03/30/23 13:42:20.949
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 13:42:20.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3343" for this suite. 03/30/23 13:42:20.952
------------------------------
â€¢ [0.053 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:42:20.902
    Mar 30 13:42:20.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 13:42:20.902
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:20.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:20.909
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 03/30/23 13:42:20.91
    Mar 30 13:42:20.910: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3343 proxy --unix-socket=/tmp/kubectl-proxy-unix2332714450/test'
    STEP: retrieving proxy /api/ output 03/30/23 13:42:20.949
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:42:20.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3343" for this suite. 03/30/23 13:42:20.952
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:42:20.955
Mar 30 13:42:20.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:42:20.955
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:20.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:20.962
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-1467172a-d7da-441c-a66d-99ac14213550 03/30/23 13:42:20.963
STEP: Creating a pod to test consume configMaps 03/30/23 13:42:20.965
Mar 30 13:42:20.968: INFO: Waiting up to 5m0s for pod "pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb" in namespace "configmap-8459" to be "Succeeded or Failed"
Mar 30 13:42:20.970: INFO: Pod "pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.891393ms
Mar 30 13:42:22.973: INFO: Pod "pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb": Phase="Running", Reason="", readiness=false. Elapsed: 2.004163325s
Mar 30 13:42:24.973: INFO: Pod "pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004971622s
STEP: Saw pod success 03/30/23 13:42:24.973
Mar 30 13:42:24.973: INFO: Pod "pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb" satisfied condition "Succeeded or Failed"
Mar 30 13:42:24.975: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb container agnhost-container: <nil>
STEP: delete the pod 03/30/23 13:42:24.978
Mar 30 13:42:24.982: INFO: Waiting for pod pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb to disappear
Mar 30 13:42:24.984: INFO: Pod pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:42:24.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8459" for this suite. 03/30/23 13:42:24.986
------------------------------
â€¢ [4.034 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:42:20.955
    Mar 30 13:42:20.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:42:20.955
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:20.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:20.962
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-1467172a-d7da-441c-a66d-99ac14213550 03/30/23 13:42:20.963
    STEP: Creating a pod to test consume configMaps 03/30/23 13:42:20.965
    Mar 30 13:42:20.968: INFO: Waiting up to 5m0s for pod "pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb" in namespace "configmap-8459" to be "Succeeded or Failed"
    Mar 30 13:42:20.970: INFO: Pod "pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.891393ms
    Mar 30 13:42:22.973: INFO: Pod "pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb": Phase="Running", Reason="", readiness=false. Elapsed: 2.004163325s
    Mar 30 13:42:24.973: INFO: Pod "pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004971622s
    STEP: Saw pod success 03/30/23 13:42:24.973
    Mar 30 13:42:24.973: INFO: Pod "pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb" satisfied condition "Succeeded or Failed"
    Mar 30 13:42:24.975: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 13:42:24.978
    Mar 30 13:42:24.982: INFO: Waiting for pod pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb to disappear
    Mar 30 13:42:24.984: INFO: Pod pod-configmaps-05ddbe11-6a12-4d82-9418-6b8715ac9ccb no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:42:24.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8459" for this suite. 03/30/23 13:42:24.986
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:42:24.989
Mar 30 13:42:24.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename job 03/30/23 13:42:24.989
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:24.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:24.996
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 03/30/23 13:42:24.998
STEP: Ensuring job reaches completions 03/30/23 13:42:25
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 30 13:42:35.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1633" for this suite. 03/30/23 13:42:35.005
------------------------------
â€¢ [SLOW TEST] [10.019 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:42:24.989
    Mar 30 13:42:24.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename job 03/30/23 13:42:24.989
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:24.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:24.996
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 03/30/23 13:42:24.998
    STEP: Ensuring job reaches completions 03/30/23 13:42:25
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:42:35.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1633" for this suite. 03/30/23 13:42:35.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:42:35.009
Mar 30 13:42:35.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-probe 03/30/23 13:42:35.01
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:35.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:35.016
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9 in namespace container-probe-3248 03/30/23 13:42:35.018
Mar 30 13:42:35.021: INFO: Waiting up to 5m0s for pod "test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9" in namespace "container-probe-3248" to be "not pending"
Mar 30 13:42:35.023: INFO: Pod "test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.605007ms
Mar 30 13:42:37.026: INFO: Pod "test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.004745608s
Mar 30 13:42:37.026: INFO: Pod "test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9" satisfied condition "not pending"
Mar 30 13:42:37.026: INFO: Started pod test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9 in namespace container-probe-3248
STEP: checking the pod's current state and verifying that restartCount is present 03/30/23 13:42:37.026
Mar 30 13:42:37.028: INFO: Initial restart count of pod test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9 is 0
STEP: deleting the pod 03/30/23 13:46:37.337
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 30 13:46:37.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3248" for this suite. 03/30/23 13:46:37.346
------------------------------
â€¢ [SLOW TEST] [242.339 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:42:35.009
    Mar 30 13:42:35.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-probe 03/30/23 13:42:35.01
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:42:35.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:42:35.016
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9 in namespace container-probe-3248 03/30/23 13:42:35.018
    Mar 30 13:42:35.021: INFO: Waiting up to 5m0s for pod "test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9" in namespace "container-probe-3248" to be "not pending"
    Mar 30 13:42:35.023: INFO: Pod "test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.605007ms
    Mar 30 13:42:37.026: INFO: Pod "test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.004745608s
    Mar 30 13:42:37.026: INFO: Pod "test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9" satisfied condition "not pending"
    Mar 30 13:42:37.026: INFO: Started pod test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9 in namespace container-probe-3248
    STEP: checking the pod's current state and verifying that restartCount is present 03/30/23 13:42:37.026
    Mar 30 13:42:37.028: INFO: Initial restart count of pod test-webserver-3980912b-1d73-4051-98f4-1a9872dc08e9 is 0
    STEP: deleting the pod 03/30/23 13:46:37.337
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:46:37.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3248" for this suite. 03/30/23 13:46:37.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:46:37.349
Mar 30 13:46:37.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename dns 03/30/23 13:46:37.35
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:46:37.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:46:37.356
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/30/23 13:46:37.358
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/30/23 13:46:37.358
STEP: creating a pod to probe DNS 03/30/23 13:46:37.358
STEP: submitting the pod to kubernetes 03/30/23 13:46:37.358
Mar 30 13:46:37.361: INFO: Waiting up to 15m0s for pod "dns-test-762563ef-dfbe-4d7d-95cf-8b40fb58c3f9" in namespace "dns-6398" to be "running"
Mar 30 13:46:37.363: INFO: Pod "dns-test-762563ef-dfbe-4d7d-95cf-8b40fb58c3f9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.537286ms
Mar 30 13:46:39.365: INFO: Pod "dns-test-762563ef-dfbe-4d7d-95cf-8b40fb58c3f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.003658822s
Mar 30 13:46:39.365: INFO: Pod "dns-test-762563ef-dfbe-4d7d-95cf-8b40fb58c3f9" satisfied condition "running"
STEP: retrieving the pod 03/30/23 13:46:39.365
STEP: looking for the results for each expected name from probers 03/30/23 13:46:39.367
Mar 30 13:46:39.375: INFO: DNS probes using dns-6398/dns-test-762563ef-dfbe-4d7d-95cf-8b40fb58c3f9 succeeded

STEP: deleting the pod 03/30/23 13:46:39.375
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 30 13:46:39.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6398" for this suite. 03/30/23 13:46:39.381
------------------------------
â€¢ [2.035 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:46:37.349
    Mar 30 13:46:37.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename dns 03/30/23 13:46:37.35
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:46:37.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:46:37.356
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/30/23 13:46:37.358
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/30/23 13:46:37.358
    STEP: creating a pod to probe DNS 03/30/23 13:46:37.358
    STEP: submitting the pod to kubernetes 03/30/23 13:46:37.358
    Mar 30 13:46:37.361: INFO: Waiting up to 15m0s for pod "dns-test-762563ef-dfbe-4d7d-95cf-8b40fb58c3f9" in namespace "dns-6398" to be "running"
    Mar 30 13:46:37.363: INFO: Pod "dns-test-762563ef-dfbe-4d7d-95cf-8b40fb58c3f9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.537286ms
    Mar 30 13:46:39.365: INFO: Pod "dns-test-762563ef-dfbe-4d7d-95cf-8b40fb58c3f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.003658822s
    Mar 30 13:46:39.365: INFO: Pod "dns-test-762563ef-dfbe-4d7d-95cf-8b40fb58c3f9" satisfied condition "running"
    STEP: retrieving the pod 03/30/23 13:46:39.365
    STEP: looking for the results for each expected name from probers 03/30/23 13:46:39.367
    Mar 30 13:46:39.375: INFO: DNS probes using dns-6398/dns-test-762563ef-dfbe-4d7d-95cf-8b40fb58c3f9 succeeded

    STEP: deleting the pod 03/30/23 13:46:39.375
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:46:39.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6398" for this suite. 03/30/23 13:46:39.381
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:46:39.384
Mar 30 13:46:39.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename watch 03/30/23 13:46:39.385
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:46:39.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:46:39.392
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 03/30/23 13:46:39.393
STEP: modifying the configmap once 03/30/23 13:46:39.395
STEP: modifying the configmap a second time 03/30/23 13:46:39.398
STEP: deleting the configmap 03/30/23 13:46:39.401
STEP: creating a watch on configmaps from the resource version returned by the first update 03/30/23 13:46:39.404
STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/30/23 13:46:39.404
Mar 30 13:46:39.404: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8711  ef7ebf9b-a3b1-4f2e-b5c4-f6624b6ce2f9 76109 0 2023-03-30 13:46:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-30 13:46:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 13:46:39.405: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8711  ef7ebf9b-a3b1-4f2e-b5c4-f6624b6ce2f9 76110 0 2023-03-30 13:46:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-30 13:46:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 30 13:46:39.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8711" for this suite. 03/30/23 13:46:39.407
------------------------------
â€¢ [0.025 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:46:39.384
    Mar 30 13:46:39.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename watch 03/30/23 13:46:39.385
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:46:39.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:46:39.392
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 03/30/23 13:46:39.393
    STEP: modifying the configmap once 03/30/23 13:46:39.395
    STEP: modifying the configmap a second time 03/30/23 13:46:39.398
    STEP: deleting the configmap 03/30/23 13:46:39.401
    STEP: creating a watch on configmaps from the resource version returned by the first update 03/30/23 13:46:39.404
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/30/23 13:46:39.404
    Mar 30 13:46:39.404: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8711  ef7ebf9b-a3b1-4f2e-b5c4-f6624b6ce2f9 76109 0 2023-03-30 13:46:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-30 13:46:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 13:46:39.405: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8711  ef7ebf9b-a3b1-4f2e-b5c4-f6624b6ce2f9 76110 0 2023-03-30 13:46:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-30 13:46:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:46:39.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8711" for this suite. 03/30/23 13:46:39.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:46:39.41
Mar 30 13:46:39.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 13:46:39.41
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:46:39.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:46:39.417
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 13:46:39.423
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:46:39.753
STEP: Deploying the webhook pod 03/30/23 13:46:39.757
STEP: Wait for the deployment to be ready 03/30/23 13:46:39.761
Mar 30 13:46:39.764: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 03/30/23 13:46:41.77
STEP: Verifying the service has paired with the endpoint 03/30/23 13:46:41.775
Mar 30 13:46:42.775: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 03/30/23 13:46:42.777
STEP: Creating a configMap that does not comply to the validation webhook rules 03/30/23 13:46:42.786
STEP: Updating a validating webhook configuration's rules to not include the create operation 03/30/23 13:46:42.791
STEP: Creating a configMap that does not comply to the validation webhook rules 03/30/23 13:46:42.796
STEP: Patching a validating webhook configuration's rules to include the create operation 03/30/23 13:46:42.8
STEP: Creating a configMap that does not comply to the validation webhook rules 03/30/23 13:46:42.803
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:46:42.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2642" for this suite. 03/30/23 13:46:42.828
STEP: Destroying namespace "webhook-2642-markers" for this suite. 03/30/23 13:46:42.833
------------------------------
â€¢ [3.426 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:46:39.41
    Mar 30 13:46:39.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 13:46:39.41
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:46:39.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:46:39.417
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 13:46:39.423
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:46:39.753
    STEP: Deploying the webhook pod 03/30/23 13:46:39.757
    STEP: Wait for the deployment to be ready 03/30/23 13:46:39.761
    Mar 30 13:46:39.764: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 03/30/23 13:46:41.77
    STEP: Verifying the service has paired with the endpoint 03/30/23 13:46:41.775
    Mar 30 13:46:42.775: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 03/30/23 13:46:42.777
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/30/23 13:46:42.786
    STEP: Updating a validating webhook configuration's rules to not include the create operation 03/30/23 13:46:42.791
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/30/23 13:46:42.796
    STEP: Patching a validating webhook configuration's rules to include the create operation 03/30/23 13:46:42.8
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/30/23 13:46:42.803
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:46:42.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2642" for this suite. 03/30/23 13:46:42.828
    STEP: Destroying namespace "webhook-2642-markers" for this suite. 03/30/23 13:46:42.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:46:42.836
Mar 30 13:46:42.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:46:42.837
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:46:42.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:46:42.846
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:46:42.847
Mar 30 13:46:42.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384" in namespace "projected-4735" to be "Succeeded or Failed"
Mar 30 13:46:42.853: INFO: Pod "downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384": Phase="Pending", Reason="", readiness=false. Elapsed: 1.501743ms
Mar 30 13:46:44.856: INFO: Pod "downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004180234s
Mar 30 13:46:46.855: INFO: Pod "downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003589628s
STEP: Saw pod success 03/30/23 13:46:46.855
Mar 30 13:46:46.855: INFO: Pod "downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384" satisfied condition "Succeeded or Failed"
Mar 30 13:46:46.857: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384 container client-container: <nil>
STEP: delete the pod 03/30/23 13:46:46.867
Mar 30 13:46:46.872: INFO: Waiting for pod downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384 to disappear
Mar 30 13:46:46.873: INFO: Pod downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 30 13:46:46.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4735" for this suite. 03/30/23 13:46:46.876
------------------------------
â€¢ [4.042 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:46:42.836
    Mar 30 13:46:42.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:46:42.837
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:46:42.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:46:42.846
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:46:42.847
    Mar 30 13:46:42.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384" in namespace "projected-4735" to be "Succeeded or Failed"
    Mar 30 13:46:42.853: INFO: Pod "downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384": Phase="Pending", Reason="", readiness=false. Elapsed: 1.501743ms
    Mar 30 13:46:44.856: INFO: Pod "downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004180234s
    Mar 30 13:46:46.855: INFO: Pod "downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003589628s
    STEP: Saw pod success 03/30/23 13:46:46.855
    Mar 30 13:46:46.855: INFO: Pod "downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384" satisfied condition "Succeeded or Failed"
    Mar 30 13:46:46.857: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384 container client-container: <nil>
    STEP: delete the pod 03/30/23 13:46:46.867
    Mar 30 13:46:46.872: INFO: Waiting for pod downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384 to disappear
    Mar 30 13:46:46.873: INFO: Pod downwardapi-volume-7d056ed9-b324-4f2f-b012-a99650dd1384 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:46:46.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4735" for this suite. 03/30/23 13:46:46.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:46:46.879
Mar 30 13:46:46.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename deployment 03/30/23 13:46:46.88
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:46:46.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:46:46.886
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 03/30/23 13:46:46.89
Mar 30 13:46:46.890: INFO: Creating simple deployment test-deployment-dg8zs
Mar 30 13:46:46.895: INFO: deployment "test-deployment-dg8zs" doesn't have the required revision set
STEP: Getting /status 03/30/23 13:46:48.901
Mar 30 13:46:48.903: INFO: Deployment test-deployment-dg8zs has Conditions: [{Available True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dg8zs-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 03/30/23 13:46:48.903
Mar 30 13:46:48.908: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 46, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 46, 47, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 46, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 46, 46, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-dg8zs-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 03/30/23 13:46:48.908
Mar 30 13:46:48.909: INFO: Observed &Deployment event: ADDED
Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dg8zs-54bc444df"}
Mar 30 13:46:48.909: INFO: Observed &Deployment event: MODIFIED
Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dg8zs-54bc444df"}
Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 30 13:46:48.909: INFO: Observed &Deployment event: MODIFIED
Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dg8zs-54bc444df" is progressing.}
Mar 30 13:46:48.909: INFO: Observed &Deployment event: MODIFIED
Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dg8zs-54bc444df" has successfully progressed.}
Mar 30 13:46:48.909: INFO: Observed &Deployment event: MODIFIED
Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dg8zs-54bc444df" has successfully progressed.}
Mar 30 13:46:48.909: INFO: Found Deployment test-deployment-dg8zs in namespace deployment-6764 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 30 13:46:48.909: INFO: Deployment test-deployment-dg8zs has an updated status
STEP: patching the Statefulset Status 03/30/23 13:46:48.909
Mar 30 13:46:48.909: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 30 13:46:48.913: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 03/30/23 13:46:48.913
Mar 30 13:46:48.914: INFO: Observed &Deployment event: ADDED
Mar 30 13:46:48.914: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dg8zs-54bc444df"}
Mar 30 13:46:48.914: INFO: Observed &Deployment event: MODIFIED
Mar 30 13:46:48.914: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dg8zs-54bc444df"}
Mar 30 13:46:48.914: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 30 13:46:48.914: INFO: Observed &Deployment event: MODIFIED
Mar 30 13:46:48.914: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 30 13:46:48.914: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dg8zs-54bc444df" is progressing.}
Mar 30 13:46:48.915: INFO: Observed &Deployment event: MODIFIED
Mar 30 13:46:48.915: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 30 13:46:48.915: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dg8zs-54bc444df" has successfully progressed.}
Mar 30 13:46:48.915: INFO: Observed &Deployment event: MODIFIED
Mar 30 13:46:48.915: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 30 13:46:48.915: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dg8zs-54bc444df" has successfully progressed.}
Mar 30 13:46:48.915: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 30 13:46:48.915: INFO: Observed &Deployment event: MODIFIED
Mar 30 13:46:48.915: INFO: Found deployment test-deployment-dg8zs in namespace deployment-6764 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar 30 13:46:48.915: INFO: Deployment test-deployment-dg8zs has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 30 13:46:48.916: INFO: Deployment "test-deployment-dg8zs":
&Deployment{ObjectMeta:{test-deployment-dg8zs  deployment-6764  e95573cb-3aea-4753-8148-09f0f5b2b752 76286 1 2023-03-30 13:46:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-30 13:46:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-30 13:46:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-30 13:46:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004240a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-dg8zs-54bc444df",LastUpdateTime:2023-03-30 13:46:48 +0000 UTC,LastTransitionTime:2023-03-30 13:46:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 30 13:46:48.918: INFO: New ReplicaSet "test-deployment-dg8zs-54bc444df" of Deployment "test-deployment-dg8zs":
&ReplicaSet{ObjectMeta:{test-deployment-dg8zs-54bc444df  deployment-6764  5e437f19-26c4-4055-bbbe-4132afa2f0f4 76277 1 2023-03-30 13:46:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-dg8zs e95573cb-3aea-4753-8148-09f0f5b2b752 0xc004240e10 0xc004240e11}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:46:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e95573cb-3aea-4753-8148-09f0f5b2b752\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:46:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004240eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 30 13:46:48.920: INFO: Pod "test-deployment-dg8zs-54bc444df-d7qsw" is available:
&Pod{ObjectMeta:{test-deployment-dg8zs-54bc444df-d7qsw test-deployment-dg8zs-54bc444df- deployment-6764  ffe651ee-cfa4-49fc-a50f-a648277db9b5 76276 0 2023-03-30 13:46:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-dg8zs-54bc444df 5e437f19-26c4-4055-bbbe-4132afa2f0f4 0xc0055c0ab0 0xc0055c0ab1}] [] [{kube-controller-manager Update v1 2023-03-30 13:46:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e437f19-26c4-4055-bbbe-4132afa2f0f4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9zslh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zslh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:46:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:46:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.22,StartTime:2023-03-30 13:46:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:46:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://15bc0e5793eca42dac88282662753124462308bcaf57d4a4e1a82c8c10261958,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 30 13:46:48.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6764" for this suite. 03/30/23 13:46:48.922
------------------------------
â€¢ [2.046 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:46:46.879
    Mar 30 13:46:46.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename deployment 03/30/23 13:46:46.88
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:46:46.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:46:46.886
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 03/30/23 13:46:46.89
    Mar 30 13:46:46.890: INFO: Creating simple deployment test-deployment-dg8zs
    Mar 30 13:46:46.895: INFO: deployment "test-deployment-dg8zs" doesn't have the required revision set
    STEP: Getting /status 03/30/23 13:46:48.901
    Mar 30 13:46:48.903: INFO: Deployment test-deployment-dg8zs has Conditions: [{Available True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dg8zs-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 03/30/23 13:46:48.903
    Mar 30 13:46:48.908: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 46, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 46, 47, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 46, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 46, 46, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-dg8zs-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 03/30/23 13:46:48.908
    Mar 30 13:46:48.909: INFO: Observed &Deployment event: ADDED
    Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dg8zs-54bc444df"}
    Mar 30 13:46:48.909: INFO: Observed &Deployment event: MODIFIED
    Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dg8zs-54bc444df"}
    Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 30 13:46:48.909: INFO: Observed &Deployment event: MODIFIED
    Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dg8zs-54bc444df" is progressing.}
    Mar 30 13:46:48.909: INFO: Observed &Deployment event: MODIFIED
    Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dg8zs-54bc444df" has successfully progressed.}
    Mar 30 13:46:48.909: INFO: Observed &Deployment event: MODIFIED
    Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 30 13:46:48.909: INFO: Observed Deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dg8zs-54bc444df" has successfully progressed.}
    Mar 30 13:46:48.909: INFO: Found Deployment test-deployment-dg8zs in namespace deployment-6764 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 30 13:46:48.909: INFO: Deployment test-deployment-dg8zs has an updated status
    STEP: patching the Statefulset Status 03/30/23 13:46:48.909
    Mar 30 13:46:48.909: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 30 13:46:48.913: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 03/30/23 13:46:48.913
    Mar 30 13:46:48.914: INFO: Observed &Deployment event: ADDED
    Mar 30 13:46:48.914: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dg8zs-54bc444df"}
    Mar 30 13:46:48.914: INFO: Observed &Deployment event: MODIFIED
    Mar 30 13:46:48.914: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dg8zs-54bc444df"}
    Mar 30 13:46:48.914: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 30 13:46:48.914: INFO: Observed &Deployment event: MODIFIED
    Mar 30 13:46:48.914: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 30 13:46:48.914: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:46 +0000 UTC 2023-03-30 13:46:46 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dg8zs-54bc444df" is progressing.}
    Mar 30 13:46:48.915: INFO: Observed &Deployment event: MODIFIED
    Mar 30 13:46:48.915: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 30 13:46:48.915: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dg8zs-54bc444df" has successfully progressed.}
    Mar 30 13:46:48.915: INFO: Observed &Deployment event: MODIFIED
    Mar 30 13:46:48.915: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 30 13:46:48.915: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-30 13:46:47 +0000 UTC 2023-03-30 13:46:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dg8zs-54bc444df" has successfully progressed.}
    Mar 30 13:46:48.915: INFO: Observed deployment test-deployment-dg8zs in namespace deployment-6764 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 30 13:46:48.915: INFO: Observed &Deployment event: MODIFIED
    Mar 30 13:46:48.915: INFO: Found deployment test-deployment-dg8zs in namespace deployment-6764 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Mar 30 13:46:48.915: INFO: Deployment test-deployment-dg8zs has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 30 13:46:48.916: INFO: Deployment "test-deployment-dg8zs":
    &Deployment{ObjectMeta:{test-deployment-dg8zs  deployment-6764  e95573cb-3aea-4753-8148-09f0f5b2b752 76286 1 2023-03-30 13:46:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-30 13:46:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-30 13:46:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-30 13:46:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004240a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-dg8zs-54bc444df",LastUpdateTime:2023-03-30 13:46:48 +0000 UTC,LastTransitionTime:2023-03-30 13:46:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 30 13:46:48.918: INFO: New ReplicaSet "test-deployment-dg8zs-54bc444df" of Deployment "test-deployment-dg8zs":
    &ReplicaSet{ObjectMeta:{test-deployment-dg8zs-54bc444df  deployment-6764  5e437f19-26c4-4055-bbbe-4132afa2f0f4 76277 1 2023-03-30 13:46:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-dg8zs e95573cb-3aea-4753-8148-09f0f5b2b752 0xc004240e10 0xc004240e11}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:46:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e95573cb-3aea-4753-8148-09f0f5b2b752\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:46:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004240eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 13:46:48.920: INFO: Pod "test-deployment-dg8zs-54bc444df-d7qsw" is available:
    &Pod{ObjectMeta:{test-deployment-dg8zs-54bc444df-d7qsw test-deployment-dg8zs-54bc444df- deployment-6764  ffe651ee-cfa4-49fc-a50f-a648277db9b5 76276 0 2023-03-30 13:46:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-dg8zs-54bc444df 5e437f19-26c4-4055-bbbe-4132afa2f0f4 0xc0055c0ab0 0xc0055c0ab1}] [] [{kube-controller-manager Update v1 2023-03-30 13:46:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e437f19-26c4-4055-bbbe-4132afa2f0f4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:46:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9zslh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zslh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:46:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:46:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.22,StartTime:2023-03-30 13:46:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:46:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://15bc0e5793eca42dac88282662753124462308bcaf57d4a4e1a82c8c10261958,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:46:48.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6764" for this suite. 03/30/23 13:46:48.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:46:48.926
Mar 30 13:46:48.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 13:46:48.927
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:46:48.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:46:48.933
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 03/30/23 13:46:48.935
Mar 30 13:46:48.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 create -f -'
Mar 30 13:46:49.320: INFO: stderr: ""
Mar 30 13:46:49.320: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/30/23 13:46:49.32
Mar 30 13:46:49.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 30 13:46:49.375: INFO: stderr: ""
Mar 30 13:46:49.375: INFO: stdout: "update-demo-nautilus-cd2sv update-demo-nautilus-d2th9 "
Mar 30 13:46:49.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-cd2sv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 30 13:46:49.426: INFO: stderr: ""
Mar 30 13:46:49.426: INFO: stdout: ""
Mar 30 13:46:49.426: INFO: update-demo-nautilus-cd2sv is created but not running
Mar 30 13:46:54.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 30 13:46:54.481: INFO: stderr: ""
Mar 30 13:46:54.481: INFO: stdout: "update-demo-nautilus-cd2sv update-demo-nautilus-d2th9 "
Mar 30 13:46:54.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-cd2sv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 30 13:46:54.533: INFO: stderr: ""
Mar 30 13:46:54.533: INFO: stdout: "true"
Mar 30 13:46:54.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-cd2sv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 30 13:46:54.585: INFO: stderr: ""
Mar 30 13:46:54.585: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 30 13:46:54.585: INFO: validating pod update-demo-nautilus-cd2sv
Mar 30 13:46:54.588: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 13:46:54.588: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 13:46:54.588: INFO: update-demo-nautilus-cd2sv is verified up and running
Mar 30 13:46:54.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-d2th9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 30 13:46:54.639: INFO: stderr: ""
Mar 30 13:46:54.639: INFO: stdout: "true"
Mar 30 13:46:54.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-d2th9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 30 13:46:54.691: INFO: stderr: ""
Mar 30 13:46:54.691: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 30 13:46:54.691: INFO: validating pod update-demo-nautilus-d2th9
Mar 30 13:46:54.694: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 13:46:54.694: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 13:46:54.694: INFO: update-demo-nautilus-d2th9 is verified up and running
STEP: scaling down the replication controller 03/30/23 13:46:54.694
Mar 30 13:46:54.695: INFO: scanned /root for discovery docs: <nil>
Mar 30 13:46:54.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 30 13:46:55.756: INFO: stderr: ""
Mar 30 13:46:55.756: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/30/23 13:46:55.756
Mar 30 13:46:55.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 30 13:46:55.808: INFO: stderr: ""
Mar 30 13:46:55.808: INFO: stdout: "update-demo-nautilus-cd2sv update-demo-nautilus-d2th9 "
STEP: Replicas for name=update-demo: expected=1 actual=2 03/30/23 13:46:55.808
Mar 30 13:47:00.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 30 13:47:00.860: INFO: stderr: ""
Mar 30 13:47:00.860: INFO: stdout: "update-demo-nautilus-d2th9 "
Mar 30 13:47:00.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-d2th9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 30 13:47:00.911: INFO: stderr: ""
Mar 30 13:47:00.911: INFO: stdout: "true"
Mar 30 13:47:00.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-d2th9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 30 13:47:00.960: INFO: stderr: ""
Mar 30 13:47:00.960: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 30 13:47:00.960: INFO: validating pod update-demo-nautilus-d2th9
Mar 30 13:47:00.962: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 13:47:00.962: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 13:47:00.962: INFO: update-demo-nautilus-d2th9 is verified up and running
STEP: scaling up the replication controller 03/30/23 13:47:00.962
Mar 30 13:47:00.963: INFO: scanned /root for discovery docs: <nil>
Mar 30 13:47:00.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 30 13:47:02.023: INFO: stderr: ""
Mar 30 13:47:02.023: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/30/23 13:47:02.023
Mar 30 13:47:02.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 30 13:47:02.076: INFO: stderr: ""
Mar 30 13:47:02.076: INFO: stdout: "update-demo-nautilus-d2th9 update-demo-nautilus-dd892 "
Mar 30 13:47:02.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-d2th9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 30 13:47:02.125: INFO: stderr: ""
Mar 30 13:47:02.125: INFO: stdout: "true"
Mar 30 13:47:02.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-d2th9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 30 13:47:02.177: INFO: stderr: ""
Mar 30 13:47:02.177: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 30 13:47:02.177: INFO: validating pod update-demo-nautilus-d2th9
Mar 30 13:47:02.180: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 13:47:02.180: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 13:47:02.180: INFO: update-demo-nautilus-d2th9 is verified up and running
Mar 30 13:47:02.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-dd892 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 30 13:47:02.230: INFO: stderr: ""
Mar 30 13:47:02.230: INFO: stdout: "true"
Mar 30 13:47:02.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-dd892 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 30 13:47:02.280: INFO: stderr: ""
Mar 30 13:47:02.280: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 30 13:47:02.280: INFO: validating pod update-demo-nautilus-dd892
Mar 30 13:47:02.282: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 13:47:02.282: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 13:47:02.282: INFO: update-demo-nautilus-dd892 is verified up and running
STEP: using delete to clean up resources 03/30/23 13:47:02.282
Mar 30 13:47:02.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 delete --grace-period=0 --force -f -'
Mar 30 13:47:02.334: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 13:47:02.334: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 30 13:47:02.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get rc,svc -l name=update-demo --no-headers'
Mar 30 13:47:02.391: INFO: stderr: "No resources found in kubectl-8498 namespace.\n"
Mar 30 13:47:02.391: INFO: stdout: ""
Mar 30 13:47:02.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 30 13:47:02.446: INFO: stderr: ""
Mar 30 13:47:02.446: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:02.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8498" for this suite. 03/30/23 13:47:02.449
------------------------------
â€¢ [SLOW TEST] [13.526 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:46:48.926
    Mar 30 13:46:48.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 13:46:48.927
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:46:48.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:46:48.933
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 03/30/23 13:46:48.935
    Mar 30 13:46:48.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 create -f -'
    Mar 30 13:46:49.320: INFO: stderr: ""
    Mar 30 13:46:49.320: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/30/23 13:46:49.32
    Mar 30 13:46:49.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 30 13:46:49.375: INFO: stderr: ""
    Mar 30 13:46:49.375: INFO: stdout: "update-demo-nautilus-cd2sv update-demo-nautilus-d2th9 "
    Mar 30 13:46:49.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-cd2sv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 30 13:46:49.426: INFO: stderr: ""
    Mar 30 13:46:49.426: INFO: stdout: ""
    Mar 30 13:46:49.426: INFO: update-demo-nautilus-cd2sv is created but not running
    Mar 30 13:46:54.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 30 13:46:54.481: INFO: stderr: ""
    Mar 30 13:46:54.481: INFO: stdout: "update-demo-nautilus-cd2sv update-demo-nautilus-d2th9 "
    Mar 30 13:46:54.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-cd2sv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 30 13:46:54.533: INFO: stderr: ""
    Mar 30 13:46:54.533: INFO: stdout: "true"
    Mar 30 13:46:54.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-cd2sv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 30 13:46:54.585: INFO: stderr: ""
    Mar 30 13:46:54.585: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 30 13:46:54.585: INFO: validating pod update-demo-nautilus-cd2sv
    Mar 30 13:46:54.588: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 30 13:46:54.588: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 30 13:46:54.588: INFO: update-demo-nautilus-cd2sv is verified up and running
    Mar 30 13:46:54.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-d2th9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 30 13:46:54.639: INFO: stderr: ""
    Mar 30 13:46:54.639: INFO: stdout: "true"
    Mar 30 13:46:54.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-d2th9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 30 13:46:54.691: INFO: stderr: ""
    Mar 30 13:46:54.691: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 30 13:46:54.691: INFO: validating pod update-demo-nautilus-d2th9
    Mar 30 13:46:54.694: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 30 13:46:54.694: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 30 13:46:54.694: INFO: update-demo-nautilus-d2th9 is verified up and running
    STEP: scaling down the replication controller 03/30/23 13:46:54.694
    Mar 30 13:46:54.695: INFO: scanned /root for discovery docs: <nil>
    Mar 30 13:46:54.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Mar 30 13:46:55.756: INFO: stderr: ""
    Mar 30 13:46:55.756: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/30/23 13:46:55.756
    Mar 30 13:46:55.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 30 13:46:55.808: INFO: stderr: ""
    Mar 30 13:46:55.808: INFO: stdout: "update-demo-nautilus-cd2sv update-demo-nautilus-d2th9 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 03/30/23 13:46:55.808
    Mar 30 13:47:00.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 30 13:47:00.860: INFO: stderr: ""
    Mar 30 13:47:00.860: INFO: stdout: "update-demo-nautilus-d2th9 "
    Mar 30 13:47:00.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-d2th9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 30 13:47:00.911: INFO: stderr: ""
    Mar 30 13:47:00.911: INFO: stdout: "true"
    Mar 30 13:47:00.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-d2th9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 30 13:47:00.960: INFO: stderr: ""
    Mar 30 13:47:00.960: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 30 13:47:00.960: INFO: validating pod update-demo-nautilus-d2th9
    Mar 30 13:47:00.962: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 30 13:47:00.962: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 30 13:47:00.962: INFO: update-demo-nautilus-d2th9 is verified up and running
    STEP: scaling up the replication controller 03/30/23 13:47:00.962
    Mar 30 13:47:00.963: INFO: scanned /root for discovery docs: <nil>
    Mar 30 13:47:00.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Mar 30 13:47:02.023: INFO: stderr: ""
    Mar 30 13:47:02.023: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/30/23 13:47:02.023
    Mar 30 13:47:02.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 30 13:47:02.076: INFO: stderr: ""
    Mar 30 13:47:02.076: INFO: stdout: "update-demo-nautilus-d2th9 update-demo-nautilus-dd892 "
    Mar 30 13:47:02.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-d2th9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 30 13:47:02.125: INFO: stderr: ""
    Mar 30 13:47:02.125: INFO: stdout: "true"
    Mar 30 13:47:02.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-d2th9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 30 13:47:02.177: INFO: stderr: ""
    Mar 30 13:47:02.177: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 30 13:47:02.177: INFO: validating pod update-demo-nautilus-d2th9
    Mar 30 13:47:02.180: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 30 13:47:02.180: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 30 13:47:02.180: INFO: update-demo-nautilus-d2th9 is verified up and running
    Mar 30 13:47:02.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-dd892 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 30 13:47:02.230: INFO: stderr: ""
    Mar 30 13:47:02.230: INFO: stdout: "true"
    Mar 30 13:47:02.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods update-demo-nautilus-dd892 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 30 13:47:02.280: INFO: stderr: ""
    Mar 30 13:47:02.280: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 30 13:47:02.280: INFO: validating pod update-demo-nautilus-dd892
    Mar 30 13:47:02.282: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 30 13:47:02.282: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 30 13:47:02.282: INFO: update-demo-nautilus-dd892 is verified up and running
    STEP: using delete to clean up resources 03/30/23 13:47:02.282
    Mar 30 13:47:02.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 delete --grace-period=0 --force -f -'
    Mar 30 13:47:02.334: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 30 13:47:02.334: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 30 13:47:02.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get rc,svc -l name=update-demo --no-headers'
    Mar 30 13:47:02.391: INFO: stderr: "No resources found in kubectl-8498 namespace.\n"
    Mar 30 13:47:02.391: INFO: stdout: ""
    Mar 30 13:47:02.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8498 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 30 13:47:02.446: INFO: stderr: ""
    Mar 30 13:47:02.446: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:02.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8498" for this suite. 03/30/23 13:47:02.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:02.452
Mar 30 13:47:02.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 13:47:02.453
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:02.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:02.46
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 03/30/23 13:47:02.461
Mar 30 13:47:02.465: INFO: Waiting up to 5m0s for pod "downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32" in namespace "downward-api-2979" to be "Succeeded or Failed"
Mar 30 13:47:02.466: INFO: Pod "downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32": Phase="Pending", Reason="", readiness=false. Elapsed: 1.542641ms
Mar 30 13:47:04.469: INFO: Pod "downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004432193s
Mar 30 13:47:06.469: INFO: Pod "downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004476673s
STEP: Saw pod success 03/30/23 13:47:06.469
Mar 30 13:47:06.469: INFO: Pod "downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32" satisfied condition "Succeeded or Failed"
Mar 30 13:47:06.471: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32 container dapi-container: <nil>
STEP: delete the pod 03/30/23 13:47:06.474
Mar 30 13:47:06.479: INFO: Waiting for pod downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32 to disappear
Mar 30 13:47:06.480: INFO: Pod downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:06.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2979" for this suite. 03/30/23 13:47:06.482
------------------------------
â€¢ [4.032 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:02.452
    Mar 30 13:47:02.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 13:47:02.453
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:02.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:02.46
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 03/30/23 13:47:02.461
    Mar 30 13:47:02.465: INFO: Waiting up to 5m0s for pod "downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32" in namespace "downward-api-2979" to be "Succeeded or Failed"
    Mar 30 13:47:02.466: INFO: Pod "downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32": Phase="Pending", Reason="", readiness=false. Elapsed: 1.542641ms
    Mar 30 13:47:04.469: INFO: Pod "downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004432193s
    Mar 30 13:47:06.469: INFO: Pod "downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004476673s
    STEP: Saw pod success 03/30/23 13:47:06.469
    Mar 30 13:47:06.469: INFO: Pod "downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32" satisfied condition "Succeeded or Failed"
    Mar 30 13:47:06.471: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32 container dapi-container: <nil>
    STEP: delete the pod 03/30/23 13:47:06.474
    Mar 30 13:47:06.479: INFO: Waiting for pod downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32 to disappear
    Mar 30 13:47:06.480: INFO: Pod downward-api-0f2597d3-7cef-4ed0-ac3b-93c8037afd32 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:06.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2979" for this suite. 03/30/23 13:47:06.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:06.485
Mar 30 13:47:06.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename prestop 03/30/23 13:47:06.485
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:06.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:06.492
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-6997 03/30/23 13:47:06.494
STEP: Waiting for pods to come up. 03/30/23 13:47:06.496
Mar 30 13:47:06.497: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6997" to be "running"
Mar 30 13:47:06.498: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.448865ms
Mar 30 13:47:08.501: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.004783615s
Mar 30 13:47:08.501: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-6997 03/30/23 13:47:08.503
Mar 30 13:47:08.506: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6997" to be "running"
Mar 30 13:47:08.507: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.475297ms
Mar 30 13:47:10.510: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.004682271s
Mar 30 13:47:10.510: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 03/30/23 13:47:10.51
Mar 30 13:47:15.518: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 03/30/23 13:47:15.518
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:15.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-6997" for this suite. 03/30/23 13:47:15.525
------------------------------
â€¢ [SLOW TEST] [9.043 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:06.485
    Mar 30 13:47:06.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename prestop 03/30/23 13:47:06.485
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:06.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:06.492
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-6997 03/30/23 13:47:06.494
    STEP: Waiting for pods to come up. 03/30/23 13:47:06.496
    Mar 30 13:47:06.497: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6997" to be "running"
    Mar 30 13:47:06.498: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.448865ms
    Mar 30 13:47:08.501: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.004783615s
    Mar 30 13:47:08.501: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-6997 03/30/23 13:47:08.503
    Mar 30 13:47:08.506: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6997" to be "running"
    Mar 30 13:47:08.507: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.475297ms
    Mar 30 13:47:10.510: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.004682271s
    Mar 30 13:47:10.510: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 03/30/23 13:47:10.51
    Mar 30 13:47:15.518: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 03/30/23 13:47:15.518
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:15.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-6997" for this suite. 03/30/23 13:47:15.525
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:15.528
Mar 30 13:47:15.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename svcaccounts 03/30/23 13:47:15.528
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:15.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:15.536
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Mar 30 13:47:15.544: INFO: created pod pod-service-account-defaultsa
Mar 30 13:47:15.544: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 30 13:47:15.546: INFO: created pod pod-service-account-mountsa
Mar 30 13:47:15.546: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 30 13:47:15.549: INFO: created pod pod-service-account-nomountsa
Mar 30 13:47:15.549: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 30 13:47:15.551: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 30 13:47:15.551: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 30 13:47:15.554: INFO: created pod pod-service-account-mountsa-mountspec
Mar 30 13:47:15.554: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 30 13:47:15.556: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 30 13:47:15.556: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 30 13:47:15.559: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 30 13:47:15.559: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 30 13:47:15.562: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 30 13:47:15.562: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 30 13:47:15.565: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 30 13:47:15.565: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:15.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2401" for this suite. 03/30/23 13:47:15.568
------------------------------
â€¢ [0.043 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:15.528
    Mar 30 13:47:15.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename svcaccounts 03/30/23 13:47:15.528
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:15.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:15.536
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Mar 30 13:47:15.544: INFO: created pod pod-service-account-defaultsa
    Mar 30 13:47:15.544: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Mar 30 13:47:15.546: INFO: created pod pod-service-account-mountsa
    Mar 30 13:47:15.546: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Mar 30 13:47:15.549: INFO: created pod pod-service-account-nomountsa
    Mar 30 13:47:15.549: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Mar 30 13:47:15.551: INFO: created pod pod-service-account-defaultsa-mountspec
    Mar 30 13:47:15.551: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Mar 30 13:47:15.554: INFO: created pod pod-service-account-mountsa-mountspec
    Mar 30 13:47:15.554: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Mar 30 13:47:15.556: INFO: created pod pod-service-account-nomountsa-mountspec
    Mar 30 13:47:15.556: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Mar 30 13:47:15.559: INFO: created pod pod-service-account-defaultsa-nomountspec
    Mar 30 13:47:15.559: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Mar 30 13:47:15.562: INFO: created pod pod-service-account-mountsa-nomountspec
    Mar 30 13:47:15.562: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Mar 30 13:47:15.565: INFO: created pod pod-service-account-nomountsa-nomountspec
    Mar 30 13:47:15.565: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:15.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2401" for this suite. 03/30/23 13:47:15.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:15.573
Mar 30 13:47:15.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename security-context 03/30/23 13:47:15.573
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:15.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:15.581
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/30/23 13:47:15.582
Mar 30 13:47:15.586: INFO: Waiting up to 5m0s for pod "security-context-b9043678-ebaa-4fc7-8f17-de764b713104" in namespace "security-context-6089" to be "Succeeded or Failed"
Mar 30 13:47:15.588: INFO: Pod "security-context-b9043678-ebaa-4fc7-8f17-de764b713104": Phase="Pending", Reason="", readiness=false. Elapsed: 1.520186ms
Mar 30 13:47:17.590: INFO: Pod "security-context-b9043678-ebaa-4fc7-8f17-de764b713104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003790208s
Mar 30 13:47:19.590: INFO: Pod "security-context-b9043678-ebaa-4fc7-8f17-de764b713104": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004138965s
Mar 30 13:47:21.590: INFO: Pod "security-context-b9043678-ebaa-4fc7-8f17-de764b713104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004135567s
STEP: Saw pod success 03/30/23 13:47:21.59
Mar 30 13:47:21.590: INFO: Pod "security-context-b9043678-ebaa-4fc7-8f17-de764b713104" satisfied condition "Succeeded or Failed"
Mar 30 13:47:21.592: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod security-context-b9043678-ebaa-4fc7-8f17-de764b713104 container test-container: <nil>
STEP: delete the pod 03/30/23 13:47:21.595
Mar 30 13:47:21.600: INFO: Waiting for pod security-context-b9043678-ebaa-4fc7-8f17-de764b713104 to disappear
Mar 30 13:47:21.601: INFO: Pod security-context-b9043678-ebaa-4fc7-8f17-de764b713104 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:21.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-6089" for this suite. 03/30/23 13:47:21.604
------------------------------
â€¢ [SLOW TEST] [6.034 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:15.573
    Mar 30 13:47:15.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename security-context 03/30/23 13:47:15.573
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:15.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:15.581
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/30/23 13:47:15.582
    Mar 30 13:47:15.586: INFO: Waiting up to 5m0s for pod "security-context-b9043678-ebaa-4fc7-8f17-de764b713104" in namespace "security-context-6089" to be "Succeeded or Failed"
    Mar 30 13:47:15.588: INFO: Pod "security-context-b9043678-ebaa-4fc7-8f17-de764b713104": Phase="Pending", Reason="", readiness=false. Elapsed: 1.520186ms
    Mar 30 13:47:17.590: INFO: Pod "security-context-b9043678-ebaa-4fc7-8f17-de764b713104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003790208s
    Mar 30 13:47:19.590: INFO: Pod "security-context-b9043678-ebaa-4fc7-8f17-de764b713104": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004138965s
    Mar 30 13:47:21.590: INFO: Pod "security-context-b9043678-ebaa-4fc7-8f17-de764b713104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004135567s
    STEP: Saw pod success 03/30/23 13:47:21.59
    Mar 30 13:47:21.590: INFO: Pod "security-context-b9043678-ebaa-4fc7-8f17-de764b713104" satisfied condition "Succeeded or Failed"
    Mar 30 13:47:21.592: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod security-context-b9043678-ebaa-4fc7-8f17-de764b713104 container test-container: <nil>
    STEP: delete the pod 03/30/23 13:47:21.595
    Mar 30 13:47:21.600: INFO: Waiting for pod security-context-b9043678-ebaa-4fc7-8f17-de764b713104 to disappear
    Mar 30 13:47:21.601: INFO: Pod security-context-b9043678-ebaa-4fc7-8f17-de764b713104 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:21.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-6089" for this suite. 03/30/23 13:47:21.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:21.608
Mar 30 13:47:21.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:47:21.608
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:21.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:21.615
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-9afa4c0d-616d-418e-ac91-c9916ce547d9 03/30/23 13:47:21.617
STEP: Creating a pod to test consume secrets 03/30/23 13:47:21.619
Mar 30 13:47:21.624: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1" in namespace "projected-2383" to be "Succeeded or Failed"
Mar 30 13:47:21.625: INFO: Pod "pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.856373ms
Mar 30 13:47:23.628: INFO: Pod "pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004546012s
Mar 30 13:47:25.628: INFO: Pod "pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004821608s
STEP: Saw pod success 03/30/23 13:47:25.628
Mar 30 13:47:25.629: INFO: Pod "pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1" satisfied condition "Succeeded or Failed"
Mar 30 13:47:25.630: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1 container secret-volume-test: <nil>
STEP: delete the pod 03/30/23 13:47:25.64
Mar 30 13:47:25.644: INFO: Waiting for pod pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1 to disappear
Mar 30 13:47:25.646: INFO: Pod pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:25.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2383" for this suite. 03/30/23 13:47:25.648
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:21.608
    Mar 30 13:47:21.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:47:21.608
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:21.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:21.615
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-9afa4c0d-616d-418e-ac91-c9916ce547d9 03/30/23 13:47:21.617
    STEP: Creating a pod to test consume secrets 03/30/23 13:47:21.619
    Mar 30 13:47:21.624: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1" in namespace "projected-2383" to be "Succeeded or Failed"
    Mar 30 13:47:21.625: INFO: Pod "pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.856373ms
    Mar 30 13:47:23.628: INFO: Pod "pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004546012s
    Mar 30 13:47:25.628: INFO: Pod "pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004821608s
    STEP: Saw pod success 03/30/23 13:47:25.628
    Mar 30 13:47:25.629: INFO: Pod "pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1" satisfied condition "Succeeded or Failed"
    Mar 30 13:47:25.630: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1 container secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 13:47:25.64
    Mar 30 13:47:25.644: INFO: Waiting for pod pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1 to disappear
    Mar 30 13:47:25.646: INFO: Pod pod-projected-secrets-0b47afeb-be3f-4677-bf00-3d5d054033c1 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:25.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2383" for this suite. 03/30/23 13:47:25.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:25.651
Mar 30 13:47:25.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename replicaset 03/30/23 13:47:25.652
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:25.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:25.659
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 03/30/23 13:47:25.661
STEP: Verify that the required pods have come up 03/30/23 13:47:25.664
Mar 30 13:47:25.665: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar 30 13:47:30.669: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 03/30/23 13:47:30.669
Mar 30 13:47:30.671: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 03/30/23 13:47:30.671
STEP: DeleteCollection of the ReplicaSets 03/30/23 13:47:30.673
STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/30/23 13:47:30.677
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:30.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8959" for this suite. 03/30/23 13:47:30.681
------------------------------
â€¢ [SLOW TEST] [5.033 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:25.651
    Mar 30 13:47:25.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename replicaset 03/30/23 13:47:25.652
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:25.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:25.659
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 03/30/23 13:47:25.661
    STEP: Verify that the required pods have come up 03/30/23 13:47:25.664
    Mar 30 13:47:25.665: INFO: Pod name sample-pod: Found 0 pods out of 3
    Mar 30 13:47:30.669: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 03/30/23 13:47:30.669
    Mar 30 13:47:30.671: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 03/30/23 13:47:30.671
    STEP: DeleteCollection of the ReplicaSets 03/30/23 13:47:30.673
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/30/23 13:47:30.677
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:30.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8959" for this suite. 03/30/23 13:47:30.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:30.685
Mar 30 13:47:30.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 13:47:30.685
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:30.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:30.693
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/30/23 13:47:30.695
Mar 30 13:47:30.698: INFO: Waiting up to 5m0s for pod "pod-493683c3-3436-420a-8fb5-3cf8dae83879" in namespace "emptydir-7666" to be "Succeeded or Failed"
Mar 30 13:47:30.700: INFO: Pod "pod-493683c3-3436-420a-8fb5-3cf8dae83879": Phase="Pending", Reason="", readiness=false. Elapsed: 1.623398ms
Mar 30 13:47:32.702: INFO: Pod "pod-493683c3-3436-420a-8fb5-3cf8dae83879": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004138764s
Mar 30 13:47:34.703: INFO: Pod "pod-493683c3-3436-420a-8fb5-3cf8dae83879": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005249019s
STEP: Saw pod success 03/30/23 13:47:34.703
Mar 30 13:47:34.703: INFO: Pod "pod-493683c3-3436-420a-8fb5-3cf8dae83879" satisfied condition "Succeeded or Failed"
Mar 30 13:47:34.705: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-493683c3-3436-420a-8fb5-3cf8dae83879 container test-container: <nil>
STEP: delete the pod 03/30/23 13:47:34.708
Mar 30 13:47:34.712: INFO: Waiting for pod pod-493683c3-3436-420a-8fb5-3cf8dae83879 to disappear
Mar 30 13:47:34.714: INFO: Pod pod-493683c3-3436-420a-8fb5-3cf8dae83879 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:34.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7666" for this suite. 03/30/23 13:47:34.716
------------------------------
â€¢ [4.034 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:30.685
    Mar 30 13:47:30.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 13:47:30.685
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:30.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:30.693
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/30/23 13:47:30.695
    Mar 30 13:47:30.698: INFO: Waiting up to 5m0s for pod "pod-493683c3-3436-420a-8fb5-3cf8dae83879" in namespace "emptydir-7666" to be "Succeeded or Failed"
    Mar 30 13:47:30.700: INFO: Pod "pod-493683c3-3436-420a-8fb5-3cf8dae83879": Phase="Pending", Reason="", readiness=false. Elapsed: 1.623398ms
    Mar 30 13:47:32.702: INFO: Pod "pod-493683c3-3436-420a-8fb5-3cf8dae83879": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004138764s
    Mar 30 13:47:34.703: INFO: Pod "pod-493683c3-3436-420a-8fb5-3cf8dae83879": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005249019s
    STEP: Saw pod success 03/30/23 13:47:34.703
    Mar 30 13:47:34.703: INFO: Pod "pod-493683c3-3436-420a-8fb5-3cf8dae83879" satisfied condition "Succeeded or Failed"
    Mar 30 13:47:34.705: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-493683c3-3436-420a-8fb5-3cf8dae83879 container test-container: <nil>
    STEP: delete the pod 03/30/23 13:47:34.708
    Mar 30 13:47:34.712: INFO: Waiting for pod pod-493683c3-3436-420a-8fb5-3cf8dae83879 to disappear
    Mar 30 13:47:34.714: INFO: Pod pod-493683c3-3436-420a-8fb5-3cf8dae83879 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:34.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7666" for this suite. 03/30/23 13:47:34.716
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:34.719
Mar 30 13:47:34.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename endpointslice 03/30/23 13:47:34.719
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:34.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:34.726
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 03/30/23 13:47:34.728
STEP: getting /apis/discovery.k8s.io 03/30/23 13:47:34.729
STEP: getting /apis/discovery.k8s.iov1 03/30/23 13:47:34.73
STEP: creating 03/30/23 13:47:34.73
STEP: getting 03/30/23 13:47:34.736
STEP: listing 03/30/23 13:47:34.737
STEP: watching 03/30/23 13:47:34.739
Mar 30 13:47:34.739: INFO: starting watch
STEP: cluster-wide listing 03/30/23 13:47:34.739
STEP: cluster-wide watching 03/30/23 13:47:34.741
Mar 30 13:47:34.741: INFO: starting watch
STEP: patching 03/30/23 13:47:34.742
STEP: updating 03/30/23 13:47:34.744
Mar 30 13:47:34.747: INFO: waiting for watch events with expected annotations
Mar 30 13:47:34.747: INFO: saw patched and updated annotations
STEP: deleting 03/30/23 13:47:34.747
STEP: deleting a collection 03/30/23 13:47:34.752
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:34.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9846" for this suite. 03/30/23 13:47:34.76
------------------------------
â€¢ [0.044 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:34.719
    Mar 30 13:47:34.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename endpointslice 03/30/23 13:47:34.719
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:34.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:34.726
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 03/30/23 13:47:34.728
    STEP: getting /apis/discovery.k8s.io 03/30/23 13:47:34.729
    STEP: getting /apis/discovery.k8s.iov1 03/30/23 13:47:34.73
    STEP: creating 03/30/23 13:47:34.73
    STEP: getting 03/30/23 13:47:34.736
    STEP: listing 03/30/23 13:47:34.737
    STEP: watching 03/30/23 13:47:34.739
    Mar 30 13:47:34.739: INFO: starting watch
    STEP: cluster-wide listing 03/30/23 13:47:34.739
    STEP: cluster-wide watching 03/30/23 13:47:34.741
    Mar 30 13:47:34.741: INFO: starting watch
    STEP: patching 03/30/23 13:47:34.742
    STEP: updating 03/30/23 13:47:34.744
    Mar 30 13:47:34.747: INFO: waiting for watch events with expected annotations
    Mar 30 13:47:34.747: INFO: saw patched and updated annotations
    STEP: deleting 03/30/23 13:47:34.747
    STEP: deleting a collection 03/30/23 13:47:34.752
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:34.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9846" for this suite. 03/30/23 13:47:34.76
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:34.763
Mar 30 13:47:34.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir-wrapper 03/30/23 13:47:34.763
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:34.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:34.769
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Mar 30 13:47:34.777: INFO: Waiting up to 5m0s for pod "pod-secrets-d62ac98c-e44f-4a44-95df-3657cb45796c" in namespace "emptydir-wrapper-9731" to be "running and ready"
Mar 30 13:47:34.779: INFO: Pod "pod-secrets-d62ac98c-e44f-4a44-95df-3657cb45796c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.238057ms
Mar 30 13:47:34.779: INFO: The phase of Pod pod-secrets-d62ac98c-e44f-4a44-95df-3657cb45796c is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:47:36.782: INFO: Pod "pod-secrets-d62ac98c-e44f-4a44-95df-3657cb45796c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004242923s
Mar 30 13:47:36.782: INFO: The phase of Pod pod-secrets-d62ac98c-e44f-4a44-95df-3657cb45796c is Running (Ready = true)
Mar 30 13:47:36.782: INFO: Pod "pod-secrets-d62ac98c-e44f-4a44-95df-3657cb45796c" satisfied condition "running and ready"
STEP: Cleaning up the secret 03/30/23 13:47:36.783
STEP: Cleaning up the configmap 03/30/23 13:47:36.786
STEP: Cleaning up the pod 03/30/23 13:47:36.788
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:36.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-9731" for this suite. 03/30/23 13:47:36.795
------------------------------
â€¢ [2.034 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:34.763
    Mar 30 13:47:34.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir-wrapper 03/30/23 13:47:34.763
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:34.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:34.769
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Mar 30 13:47:34.777: INFO: Waiting up to 5m0s for pod "pod-secrets-d62ac98c-e44f-4a44-95df-3657cb45796c" in namespace "emptydir-wrapper-9731" to be "running and ready"
    Mar 30 13:47:34.779: INFO: Pod "pod-secrets-d62ac98c-e44f-4a44-95df-3657cb45796c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.238057ms
    Mar 30 13:47:34.779: INFO: The phase of Pod pod-secrets-d62ac98c-e44f-4a44-95df-3657cb45796c is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:47:36.782: INFO: Pod "pod-secrets-d62ac98c-e44f-4a44-95df-3657cb45796c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004242923s
    Mar 30 13:47:36.782: INFO: The phase of Pod pod-secrets-d62ac98c-e44f-4a44-95df-3657cb45796c is Running (Ready = true)
    Mar 30 13:47:36.782: INFO: Pod "pod-secrets-d62ac98c-e44f-4a44-95df-3657cb45796c" satisfied condition "running and ready"
    STEP: Cleaning up the secret 03/30/23 13:47:36.783
    STEP: Cleaning up the configmap 03/30/23 13:47:36.786
    STEP: Cleaning up the pod 03/30/23 13:47:36.788
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:36.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-9731" for this suite. 03/30/23 13:47:36.795
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:36.797
Mar 30 13:47:36.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pods 03/30/23 13:47:36.798
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:36.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:36.805
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 03/30/23 13:47:36.807
Mar 30 13:47:36.810: INFO: Waiting up to 5m0s for pod "pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96" in namespace "pods-5756" to be "running and ready"
Mar 30 13:47:36.812: INFO: Pod "pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.223358ms
Mar 30 13:47:36.812: INFO: The phase of Pod pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:47:38.815: INFO: Pod "pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96": Phase="Running", Reason="", readiness=true. Elapsed: 2.004834035s
Mar 30 13:47:38.815: INFO: The phase of Pod pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96 is Running (Ready = true)
Mar 30 13:47:38.815: INFO: Pod "pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96" satisfied condition "running and ready"
Mar 30 13:47:38.818: INFO: Pod pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96 has hostIP: 192.168.0.3
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:38.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5756" for this suite. 03/30/23 13:47:38.82
------------------------------
â€¢ [2.026 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:36.797
    Mar 30 13:47:36.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pods 03/30/23 13:47:36.798
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:36.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:36.805
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 03/30/23 13:47:36.807
    Mar 30 13:47:36.810: INFO: Waiting up to 5m0s for pod "pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96" in namespace "pods-5756" to be "running and ready"
    Mar 30 13:47:36.812: INFO: Pod "pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.223358ms
    Mar 30 13:47:36.812: INFO: The phase of Pod pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:47:38.815: INFO: Pod "pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96": Phase="Running", Reason="", readiness=true. Elapsed: 2.004834035s
    Mar 30 13:47:38.815: INFO: The phase of Pod pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96 is Running (Ready = true)
    Mar 30 13:47:38.815: INFO: Pod "pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96" satisfied condition "running and ready"
    Mar 30 13:47:38.818: INFO: Pod pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96 has hostIP: 192.168.0.3
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:38.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5756" for this suite. 03/30/23 13:47:38.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:38.824
Mar 30 13:47:38.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:47:38.824
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:38.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:38.831
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:47:38.833
Mar 30 13:47:38.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d" in namespace "projected-9573" to be "Succeeded or Failed"
Mar 30 13:47:38.839: INFO: Pod "downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134574ms
Mar 30 13:47:40.841: INFO: Pod "downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004853351s
Mar 30 13:47:42.841: INFO: Pod "downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004386928s
STEP: Saw pod success 03/30/23 13:47:42.841
Mar 30 13:47:42.841: INFO: Pod "downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d" satisfied condition "Succeeded or Failed"
Mar 30 13:47:42.842: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d container client-container: <nil>
STEP: delete the pod 03/30/23 13:47:42.846
Mar 30 13:47:42.850: INFO: Waiting for pod downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d to disappear
Mar 30 13:47:42.852: INFO: Pod downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:42.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9573" for this suite. 03/30/23 13:47:42.854
------------------------------
â€¢ [4.033 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:38.824
    Mar 30 13:47:38.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:47:38.824
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:38.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:38.831
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:47:38.833
    Mar 30 13:47:38.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d" in namespace "projected-9573" to be "Succeeded or Failed"
    Mar 30 13:47:38.839: INFO: Pod "downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134574ms
    Mar 30 13:47:40.841: INFO: Pod "downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004853351s
    Mar 30 13:47:42.841: INFO: Pod "downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004386928s
    STEP: Saw pod success 03/30/23 13:47:42.841
    Mar 30 13:47:42.841: INFO: Pod "downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d" satisfied condition "Succeeded or Failed"
    Mar 30 13:47:42.842: INFO: Trying to get logs from node cn-hongkong.192.168.0.3 pod downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d container client-container: <nil>
    STEP: delete the pod 03/30/23 13:47:42.846
    Mar 30 13:47:42.850: INFO: Waiting for pod downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d to disappear
    Mar 30 13:47:42.852: INFO: Pod downwardapi-volume-7bee3c43-0b90-40c6-8ccb-befc23db697d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:42.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9573" for this suite. 03/30/23 13:47:42.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:42.857
Mar 30 13:47:42.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename sched-pred 03/30/23 13:47:42.858
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:42.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:42.865
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 30 13:47:42.866: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 30 13:47:42.871: INFO: Waiting for terminating namespaces to be deleted...
Mar 30 13:47:42.872: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.3 before test
Mar 30 13:47:42.876: INFO: coredns-5ff46f8d6f-6r2mh from kube-system started at 2023-03-30 13:39:47 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.876: INFO: 	Container coredns ready: true, restart count 0
Mar 30 13:47:42.876: INFO: csi-plugin-2x6v8 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (4 container statuses recorded)
Mar 30 13:47:42.876: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 30 13:47:42.876: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 30 13:47:42.876: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 30 13:47:42.876: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 30 13:47:42.876: INFO: kube-flannel-ds-swjx9 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.876: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 30 13:47:42.876: INFO: kube-proxy-worker-rxqts from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.876: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 30 13:47:42.876: INFO: storage-cnfs-5bbdf677b6-s48rj from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.876: INFO: 	Container storage-cnfs ready: true, restart count 0
Mar 30 13:47:42.876: INFO: storage-snapshot-manager-55c574dc6c-ths98 from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.876: INFO: 	Container storage-snapshot-manager ready: true, restart count 0
Mar 30 13:47:42.877: INFO: pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96 from pods-5756 started at 2023-03-30 13:47:36 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.877: INFO: 	Container test ready: true, restart count 0
Mar 30 13:47:42.877: INFO: sonobuoy-e2e-job-5f38eb9e10894fe7 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:47:42.877: INFO: 	Container e2e ready: true, restart count 0
Mar 30 13:47:42.877: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:47:42.877: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-bn4wq from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:47:42.877: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:47:42.877: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 13:47:42.877: INFO: pod-service-account-mountsa-mountspec from svcaccounts-2401 started at 2023-03-30 13:47:15 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.877: INFO: 	Container token-test ready: false, restart count 0
Mar 30 13:47:42.877: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.4 before test
Mar 30 13:47:42.881: INFO: alicloud-monitor-controller-77f876c7d8-kqc5x from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.881: INFO: 	Container alicloud-monitor-controller ready: true, restart count 0
Mar 30 13:47:42.881: INFO: coredns-5ff46f8d6f-llbsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.881: INFO: 	Container coredns ready: true, restart count 0
Mar 30 13:47:42.881: INFO: csi-plugin-pgrk2 from kube-system started at 2023-03-30 10:07:49 +0000 UTC (4 container statuses recorded)
Mar 30 13:47:42.881: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 30 13:47:42.881: INFO: csi-provisioner-648c597bcb-67xzk from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
Mar 30 13:47:42.881: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-nas-resizer ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-oss-provisioner ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 30 13:47:42.881: INFO: csi-provisioner-648c597bcb-jwm9r from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
Mar 30 13:47:42.881: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-nas-resizer ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-oss-provisioner ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 30 13:47:42.881: INFO: kube-flannel-ds-dftjj from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.881: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 30 13:47:42.881: INFO: kube-proxy-worker-2mrsr from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.881: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 30 13:47:42.881: INFO: metrics-server-5c58794dd-cbrsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.881: INFO: 	Container metrics-server ready: true, restart count 0
Mar 30 13:47:42.881: INFO: storage-operator-5f775996-dphqv from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.881: INFO: 	Container storage-operator ready: true, restart count 0
Mar 30 13:47:42.881: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-prg6s from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:47:42.881: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 13:47:42.881: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.5 before test
Mar 30 13:47:42.886: INFO: csi-plugin-rgcxj from kube-system started at 2023-03-30 10:07:52 +0000 UTC (4 container statuses recorded)
Mar 30 13:47:42.886: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 30 13:47:42.886: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 30 13:47:42.886: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 30 13:47:42.886: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 30 13:47:42.886: INFO: kube-flannel-ds-dspdm from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.886: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 30 13:47:42.886: INFO: kube-proxy-worker-h688q from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.886: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 30 13:47:42.886: INFO: storage-auto-expander-7fd5f8f78-2cvxn from kube-system started at 2023-03-30 13:39:47 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.886: INFO: 	Container storage-auto-expander ready: true, restart count 0
Mar 30 13:47:42.886: INFO: storage-monitor-8554bcf4c7-25bct from kube-system started at 2023-03-30 13:39:47 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.886: INFO: 	Container storage-monitor ready: true, restart count 0
Mar 30 13:47:42.886: INFO: tester from prestop-6997 started at 2023-03-30 13:47:08 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.886: INFO: 	Container tester ready: true, restart count 0
Mar 30 13:47:42.886: INFO: sonobuoy from sonobuoy started at 2023-03-30 13:01:31 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.886: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 30 13:47:42.886: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-87ph2 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:47:42.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:47:42.886: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 13:47:42.886: INFO: pod-service-account-defaultsa from svcaccounts-2401 started at 2023-03-30 13:47:15 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.886: INFO: 	Container token-test ready: false, restart count 0
Mar 30 13:47:42.886: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-2401 started at 2023-03-30 13:47:15 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.886: INFO: 	Container token-test ready: false, restart count 0
Mar 30 13:47:42.886: INFO: pod-service-account-mountsa from svcaccounts-2401 started at 2023-03-30 13:47:15 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.886: INFO: 	Container token-test ready: false, restart count 0
Mar 30 13:47:42.886: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-2401 started at 2023-03-30 13:47:15 +0000 UTC (1 container statuses recorded)
Mar 30 13:47:42.886: INFO: 	Container token-test ready: false, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 03/30/23 13:47:42.886
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17513680d2832fd6], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] 03/30/23 13:47:42.902
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:43.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5577" for this suite. 03/30/23 13:47:43.906
------------------------------
â€¢ [1.051 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:42.857
    Mar 30 13:47:42.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename sched-pred 03/30/23 13:47:42.858
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:42.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:42.865
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 30 13:47:42.866: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 30 13:47:42.871: INFO: Waiting for terminating namespaces to be deleted...
    Mar 30 13:47:42.872: INFO: 
    Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.3 before test
    Mar 30 13:47:42.876: INFO: coredns-5ff46f8d6f-6r2mh from kube-system started at 2023-03-30 13:39:47 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.876: INFO: 	Container coredns ready: true, restart count 0
    Mar 30 13:47:42.876: INFO: csi-plugin-2x6v8 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (4 container statuses recorded)
    Mar 30 13:47:42.876: INFO: 	Container csi-plugin ready: true, restart count 0
    Mar 30 13:47:42.876: INFO: 	Container disk-driver-registrar ready: true, restart count 0
    Mar 30 13:47:42.876: INFO: 	Container nas-driver-registrar ready: true, restart count 0
    Mar 30 13:47:42.876: INFO: 	Container oss-driver-registrar ready: true, restart count 0
    Mar 30 13:47:42.876: INFO: kube-flannel-ds-swjx9 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.876: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 30 13:47:42.876: INFO: kube-proxy-worker-rxqts from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.876: INFO: 	Container kube-proxy-worker ready: true, restart count 0
    Mar 30 13:47:42.876: INFO: storage-cnfs-5bbdf677b6-s48rj from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.876: INFO: 	Container storage-cnfs ready: true, restart count 0
    Mar 30 13:47:42.876: INFO: storage-snapshot-manager-55c574dc6c-ths98 from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.876: INFO: 	Container storage-snapshot-manager ready: true, restart count 0
    Mar 30 13:47:42.877: INFO: pod-hostip-f618ee42-163f-4d58-8b0b-7d6c6e83df96 from pods-5756 started at 2023-03-30 13:47:36 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.877: INFO: 	Container test ready: true, restart count 0
    Mar 30 13:47:42.877: INFO: sonobuoy-e2e-job-5f38eb9e10894fe7 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:47:42.877: INFO: 	Container e2e ready: true, restart count 0
    Mar 30 13:47:42.877: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:47:42.877: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-bn4wq from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:47:42.877: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:47:42.877: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 30 13:47:42.877: INFO: pod-service-account-mountsa-mountspec from svcaccounts-2401 started at 2023-03-30 13:47:15 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.877: INFO: 	Container token-test ready: false, restart count 0
    Mar 30 13:47:42.877: INFO: 
    Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.4 before test
    Mar 30 13:47:42.881: INFO: alicloud-monitor-controller-77f876c7d8-kqc5x from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.881: INFO: 	Container alicloud-monitor-controller ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: coredns-5ff46f8d6f-llbsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.881: INFO: 	Container coredns ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: csi-plugin-pgrk2 from kube-system started at 2023-03-30 10:07:49 +0000 UTC (4 container statuses recorded)
    Mar 30 13:47:42.881: INFO: 	Container csi-plugin ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container disk-driver-registrar ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container nas-driver-registrar ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container oss-driver-registrar ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: csi-provisioner-648c597bcb-67xzk from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
    Mar 30 13:47:42.881: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-disk-attacher ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-disk-provisioner ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-disk-resizer ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-nas-provisioner ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-nas-resizer ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-oss-provisioner ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-snapshot-controller ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: csi-provisioner-648c597bcb-jwm9r from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
    Mar 30 13:47:42.881: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-disk-attacher ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-disk-provisioner ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-disk-resizer ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-nas-provisioner ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-nas-resizer ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-oss-provisioner ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container external-snapshot-controller ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: kube-flannel-ds-dftjj from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.881: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: kube-proxy-worker-2mrsr from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.881: INFO: 	Container kube-proxy-worker ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: metrics-server-5c58794dd-cbrsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.881: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: storage-operator-5f775996-dphqv from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.881: INFO: 	Container storage-operator ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-prg6s from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:47:42.881: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 30 13:47:42.881: INFO: 
    Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.5 before test
    Mar 30 13:47:42.886: INFO: csi-plugin-rgcxj from kube-system started at 2023-03-30 10:07:52 +0000 UTC (4 container statuses recorded)
    Mar 30 13:47:42.886: INFO: 	Container csi-plugin ready: true, restart count 0
    Mar 30 13:47:42.886: INFO: 	Container disk-driver-registrar ready: true, restart count 0
    Mar 30 13:47:42.886: INFO: 	Container nas-driver-registrar ready: true, restart count 0
    Mar 30 13:47:42.886: INFO: 	Container oss-driver-registrar ready: true, restart count 0
    Mar 30 13:47:42.886: INFO: kube-flannel-ds-dspdm from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.886: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 30 13:47:42.886: INFO: kube-proxy-worker-h688q from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.886: INFO: 	Container kube-proxy-worker ready: true, restart count 0
    Mar 30 13:47:42.886: INFO: storage-auto-expander-7fd5f8f78-2cvxn from kube-system started at 2023-03-30 13:39:47 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.886: INFO: 	Container storage-auto-expander ready: true, restart count 0
    Mar 30 13:47:42.886: INFO: storage-monitor-8554bcf4c7-25bct from kube-system started at 2023-03-30 13:39:47 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.886: INFO: 	Container storage-monitor ready: true, restart count 0
    Mar 30 13:47:42.886: INFO: tester from prestop-6997 started at 2023-03-30 13:47:08 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.886: INFO: 	Container tester ready: true, restart count 0
    Mar 30 13:47:42.886: INFO: sonobuoy from sonobuoy started at 2023-03-30 13:01:31 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.886: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 30 13:47:42.886: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-87ph2 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:47:42.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:47:42.886: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 30 13:47:42.886: INFO: pod-service-account-defaultsa from svcaccounts-2401 started at 2023-03-30 13:47:15 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.886: INFO: 	Container token-test ready: false, restart count 0
    Mar 30 13:47:42.886: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-2401 started at 2023-03-30 13:47:15 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.886: INFO: 	Container token-test ready: false, restart count 0
    Mar 30 13:47:42.886: INFO: pod-service-account-mountsa from svcaccounts-2401 started at 2023-03-30 13:47:15 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.886: INFO: 	Container token-test ready: false, restart count 0
    Mar 30 13:47:42.886: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-2401 started at 2023-03-30 13:47:15 +0000 UTC (1 container statuses recorded)
    Mar 30 13:47:42.886: INFO: 	Container token-test ready: false, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 03/30/23 13:47:42.886
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17513680d2832fd6], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] 03/30/23 13:47:42.902
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:43.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5577" for this suite. 03/30/23 13:47:43.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:43.91
Mar 30 13:47:43.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 13:47:43.91
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:43.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:43.917
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 13:47:43.924
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:47:44.157
STEP: Deploying the webhook pod 03/30/23 13:47:44.16
STEP: Wait for the deployment to be ready 03/30/23 13:47:44.165
Mar 30 13:47:44.168: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/30/23 13:47:46.174
STEP: Verifying the service has paired with the endpoint 03/30/23 13:47:46.178
Mar 30 13:47:47.179: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/30/23 13:47:47.181
STEP: Registering slow webhook via the AdmissionRegistration API 03/30/23 13:47:47.181
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/30/23 13:47:47.189
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/30/23 13:47:48.193
STEP: Registering slow webhook via the AdmissionRegistration API 03/30/23 13:47:48.193
STEP: Having no error when timeout is longer than webhook latency 03/30/23 13:47:49.205
STEP: Registering slow webhook via the AdmissionRegistration API 03/30/23 13:47:49.205
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/30/23 13:47:54.222
STEP: Registering slow webhook via the AdmissionRegistration API 03/30/23 13:47:54.222
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:47:59.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5188" for this suite. 03/30/23 13:47:59.252
STEP: Destroying namespace "webhook-5188-markers" for this suite. 03/30/23 13:47:59.256
------------------------------
â€¢ [SLOW TEST] [15.349 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:43.91
    Mar 30 13:47:43.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 13:47:43.91
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:43.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:43.917
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 13:47:43.924
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:47:44.157
    STEP: Deploying the webhook pod 03/30/23 13:47:44.16
    STEP: Wait for the deployment to be ready 03/30/23 13:47:44.165
    Mar 30 13:47:44.168: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/30/23 13:47:46.174
    STEP: Verifying the service has paired with the endpoint 03/30/23 13:47:46.178
    Mar 30 13:47:47.179: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/30/23 13:47:47.181
    STEP: Registering slow webhook via the AdmissionRegistration API 03/30/23 13:47:47.181
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/30/23 13:47:47.189
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/30/23 13:47:48.193
    STEP: Registering slow webhook via the AdmissionRegistration API 03/30/23 13:47:48.193
    STEP: Having no error when timeout is longer than webhook latency 03/30/23 13:47:49.205
    STEP: Registering slow webhook via the AdmissionRegistration API 03/30/23 13:47:49.205
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/30/23 13:47:54.222
    STEP: Registering slow webhook via the AdmissionRegistration API 03/30/23 13:47:54.222
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:47:59.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5188" for this suite. 03/30/23 13:47:59.252
    STEP: Destroying namespace "webhook-5188-markers" for this suite. 03/30/23 13:47:59.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:47:59.261
Mar 30 13:47:59.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename var-expansion 03/30/23 13:47:59.261
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:59.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:59.269
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 03/30/23 13:47:59.27
Mar 30 13:47:59.274: INFO: Waiting up to 5m0s for pod "var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe" in namespace "var-expansion-827" to be "Succeeded or Failed"
Mar 30 13:47:59.277: INFO: Pod "var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.473933ms
Mar 30 13:48:01.279: INFO: Pod "var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004576374s
Mar 30 13:48:03.279: INFO: Pod "var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00500736s
STEP: Saw pod success 03/30/23 13:48:03.279
Mar 30 13:48:03.279: INFO: Pod "var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe" satisfied condition "Succeeded or Failed"
Mar 30 13:48:03.281: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe container dapi-container: <nil>
STEP: delete the pod 03/30/23 13:48:03.285
Mar 30 13:48:03.289: INFO: Waiting for pod var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe to disappear
Mar 30 13:48:03.290: INFO: Pod var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 30 13:48:03.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-827" for this suite. 03/30/23 13:48:03.293
------------------------------
â€¢ [4.035 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:47:59.261
    Mar 30 13:47:59.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename var-expansion 03/30/23 13:47:59.261
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:47:59.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:47:59.269
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 03/30/23 13:47:59.27
    Mar 30 13:47:59.274: INFO: Waiting up to 5m0s for pod "var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe" in namespace "var-expansion-827" to be "Succeeded or Failed"
    Mar 30 13:47:59.277: INFO: Pod "var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.473933ms
    Mar 30 13:48:01.279: INFO: Pod "var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004576374s
    Mar 30 13:48:03.279: INFO: Pod "var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00500736s
    STEP: Saw pod success 03/30/23 13:48:03.279
    Mar 30 13:48:03.279: INFO: Pod "var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe" satisfied condition "Succeeded or Failed"
    Mar 30 13:48:03.281: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe container dapi-container: <nil>
    STEP: delete the pod 03/30/23 13:48:03.285
    Mar 30 13:48:03.289: INFO: Waiting for pod var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe to disappear
    Mar 30 13:48:03.290: INFO: Pod var-expansion-ef3293df-46f0-425e-a6ca-8f307c11ddbe no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:48:03.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-827" for this suite. 03/30/23 13:48:03.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:48:03.296
Mar 30 13:48:03.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubelet-test 03/30/23 13:48:03.296
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:03.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:03.303
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 03/30/23 13:48:03.309
Mar 30 13:48:03.309: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases728ee26a-75c5-4014-9d66-5b3d22b1fb36" in namespace "kubelet-test-1903" to be "completed"
Mar 30 13:48:03.310: INFO: Pod "agnhost-host-aliases728ee26a-75c5-4014-9d66-5b3d22b1fb36": Phase="Pending", Reason="", readiness=false. Elapsed: 1.862185ms
Mar 30 13:48:05.313: INFO: Pod "agnhost-host-aliases728ee26a-75c5-4014-9d66-5b3d22b1fb36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004519197s
Mar 30 13:48:07.313: INFO: Pod "agnhost-host-aliases728ee26a-75c5-4014-9d66-5b3d22b1fb36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004274301s
Mar 30 13:48:07.313: INFO: Pod "agnhost-host-aliases728ee26a-75c5-4014-9d66-5b3d22b1fb36" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 30 13:48:07.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1903" for this suite. 03/30/23 13:48:07.319
------------------------------
â€¢ [4.026 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:48:03.296
    Mar 30 13:48:03.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubelet-test 03/30/23 13:48:03.296
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:03.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:03.303
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 03/30/23 13:48:03.309
    Mar 30 13:48:03.309: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases728ee26a-75c5-4014-9d66-5b3d22b1fb36" in namespace "kubelet-test-1903" to be "completed"
    Mar 30 13:48:03.310: INFO: Pod "agnhost-host-aliases728ee26a-75c5-4014-9d66-5b3d22b1fb36": Phase="Pending", Reason="", readiness=false. Elapsed: 1.862185ms
    Mar 30 13:48:05.313: INFO: Pod "agnhost-host-aliases728ee26a-75c5-4014-9d66-5b3d22b1fb36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004519197s
    Mar 30 13:48:07.313: INFO: Pod "agnhost-host-aliases728ee26a-75c5-4014-9d66-5b3d22b1fb36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004274301s
    Mar 30 13:48:07.313: INFO: Pod "agnhost-host-aliases728ee26a-75c5-4014-9d66-5b3d22b1fb36" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:48:07.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1903" for this suite. 03/30/23 13:48:07.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:48:07.322
Mar 30 13:48:07.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename disruption 03/30/23 13:48:07.323
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:07.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:07.33
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 03/30/23 13:48:07.332
STEP: Waiting for the pdb to be processed 03/30/23 13:48:07.333
STEP: updating the pdb 03/30/23 13:48:09.337
STEP: Waiting for the pdb to be processed 03/30/23 13:48:09.342
STEP: patching the pdb 03/30/23 13:48:11.345
STEP: Waiting for the pdb to be processed 03/30/23 13:48:11.35
STEP: Waiting for the pdb to be deleted 03/30/23 13:48:13.356
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 30 13:48:13.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2312" for this suite. 03/30/23 13:48:13.36
------------------------------
â€¢ [SLOW TEST] [6.041 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:48:07.322
    Mar 30 13:48:07.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename disruption 03/30/23 13:48:07.323
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:07.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:07.33
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 03/30/23 13:48:07.332
    STEP: Waiting for the pdb to be processed 03/30/23 13:48:07.333
    STEP: updating the pdb 03/30/23 13:48:09.337
    STEP: Waiting for the pdb to be processed 03/30/23 13:48:09.342
    STEP: patching the pdb 03/30/23 13:48:11.345
    STEP: Waiting for the pdb to be processed 03/30/23 13:48:11.35
    STEP: Waiting for the pdb to be deleted 03/30/23 13:48:13.356
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:48:13.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2312" for this suite. 03/30/23 13:48:13.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:48:13.363
Mar 30 13:48:13.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename svcaccounts 03/30/23 13:48:13.363
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:13.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:13.371
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-qtbb9"  03/30/23 13:48:13.372
Mar 30 13:48:13.374: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-qtbb9"  03/30/23 13:48:13.374
Mar 30 13:48:13.377: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 30 13:48:13.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4772" for this suite. 03/30/23 13:48:13.379
------------------------------
â€¢ [0.019 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:48:13.363
    Mar 30 13:48:13.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename svcaccounts 03/30/23 13:48:13.363
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:13.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:13.371
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-qtbb9"  03/30/23 13:48:13.372
    Mar 30 13:48:13.374: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-qtbb9"  03/30/23 13:48:13.374
    Mar 30 13:48:13.377: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:48:13.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4772" for this suite. 03/30/23 13:48:13.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:48:13.382
Mar 30 13:48:13.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:48:13.383
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:13.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:13.389
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-2c0ab6ce-246c-4164-9cb3-66045840db95 03/30/23 13:48:13.391
STEP: Creating a pod to test consume configMaps 03/30/23 13:48:13.392
Mar 30 13:48:13.396: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9" in namespace "projected-2932" to be "Succeeded or Failed"
Mar 30 13:48:13.398: INFO: Pod "pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.419321ms
Mar 30 13:48:15.401: INFO: Pod "pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004579652s
Mar 30 13:48:17.401: INFO: Pod "pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004842236s
STEP: Saw pod success 03/30/23 13:48:17.401
Mar 30 13:48:17.401: INFO: Pod "pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9" satisfied condition "Succeeded or Failed"
Mar 30 13:48:17.402: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9 container agnhost-container: <nil>
STEP: delete the pod 03/30/23 13:48:17.406
Mar 30 13:48:17.410: INFO: Waiting for pod pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9 to disappear
Mar 30 13:48:17.411: INFO: Pod pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:48:17.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2932" for this suite. 03/30/23 13:48:17.414
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:48:13.382
    Mar 30 13:48:13.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:48:13.383
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:13.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:13.389
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-2c0ab6ce-246c-4164-9cb3-66045840db95 03/30/23 13:48:13.391
    STEP: Creating a pod to test consume configMaps 03/30/23 13:48:13.392
    Mar 30 13:48:13.396: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9" in namespace "projected-2932" to be "Succeeded or Failed"
    Mar 30 13:48:13.398: INFO: Pod "pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.419321ms
    Mar 30 13:48:15.401: INFO: Pod "pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004579652s
    Mar 30 13:48:17.401: INFO: Pod "pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004842236s
    STEP: Saw pod success 03/30/23 13:48:17.401
    Mar 30 13:48:17.401: INFO: Pod "pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9" satisfied condition "Succeeded or Failed"
    Mar 30 13:48:17.402: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9 container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 13:48:17.406
    Mar 30 13:48:17.410: INFO: Waiting for pod pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9 to disappear
    Mar 30 13:48:17.411: INFO: Pod pod-projected-configmaps-06acb0f3-24b5-4683-85af-b21d81f72ac9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:48:17.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2932" for this suite. 03/30/23 13:48:17.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:48:17.417
Mar 30 13:48:17.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 13:48:17.418
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:17.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:17.425
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 03/30/23 13:48:17.426
Mar 30 13:48:17.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 30 13:48:17.484: INFO: stderr: ""
Mar 30 13:48:17.484: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 03/30/23 13:48:17.484
Mar 30 13:48:17.484: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 30 13:48:17.484: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8345" to be "running and ready, or succeeded"
Mar 30 13:48:17.486: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.634799ms
Mar 30 13:48:17.486: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'cn-hongkong.192.168.0.5' to be 'Running' but was 'Pending'
Mar 30 13:48:19.489: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.004097487s
Mar 30 13:48:19.489: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 30 13:48:19.489: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 03/30/23 13:48:19.489
Mar 30 13:48:19.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 logs logs-generator logs-generator'
Mar 30 13:48:19.550: INFO: stderr: ""
Mar 30 13:48:19.550: INFO: stdout: "I0330 13:48:18.022854       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/df7 447\nI0330 13:48:18.222932       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/z5mb 396\nI0330 13:48:18.423462       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/gfl6 246\nI0330 13:48:18.623754       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/kbvw 488\nI0330 13:48:18.822912       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/blr 487\nI0330 13:48:19.023213       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/s2hv 278\nI0330 13:48:19.223508       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/dbw 592\nI0330 13:48:19.423617       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/4v9n 293\n"
STEP: limiting log lines 03/30/23 13:48:19.55
Mar 30 13:48:19.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 logs logs-generator logs-generator --tail=1'
Mar 30 13:48:19.605: INFO: stderr: ""
Mar 30 13:48:19.605: INFO: stdout: "I0330 13:48:19.423617       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/4v9n 293\n"
Mar 30 13:48:19.605: INFO: got output "I0330 13:48:19.423617       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/4v9n 293\n"
STEP: limiting log bytes 03/30/23 13:48:19.605
Mar 30 13:48:19.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 logs logs-generator logs-generator --limit-bytes=1'
Mar 30 13:48:19.666: INFO: stderr: ""
Mar 30 13:48:19.666: INFO: stdout: "I"
Mar 30 13:48:19.666: INFO: got output "I"
STEP: exposing timestamps 03/30/23 13:48:19.666
Mar 30 13:48:19.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 30 13:48:19.720: INFO: stderr: ""
Mar 30 13:48:19.720: INFO: stdout: "2023-03-30T21:48:19.623998350+08:00 I0330 13:48:19.623913       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/n6k 575\n"
Mar 30 13:48:19.720: INFO: got output "2023-03-30T21:48:19.623998350+08:00 I0330 13:48:19.623913       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/n6k 575\n"
STEP: restricting to a time range 03/30/23 13:48:19.72
Mar 30 13:48:22.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 logs logs-generator logs-generator --since=1s'
Mar 30 13:48:22.276: INFO: stderr: ""
Mar 30 13:48:22.276: INFO: stdout: "I0330 13:48:21.423219       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/6gm 265\nI0330 13:48:21.623476       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/xpfz 357\nI0330 13:48:21.823778       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/zjqf 405\nI0330 13:48:22.022937       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/9mhj 375\nI0330 13:48:22.223227       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/pjd 371\n"
Mar 30 13:48:22.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 logs logs-generator logs-generator --since=24h'
Mar 30 13:48:22.330: INFO: stderr: ""
Mar 30 13:48:22.330: INFO: stdout: "I0330 13:48:18.022854       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/df7 447\nI0330 13:48:18.222932       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/z5mb 396\nI0330 13:48:18.423462       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/gfl6 246\nI0330 13:48:18.623754       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/kbvw 488\nI0330 13:48:18.822912       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/blr 487\nI0330 13:48:19.023213       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/s2hv 278\nI0330 13:48:19.223508       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/dbw 592\nI0330 13:48:19.423617       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/4v9n 293\nI0330 13:48:19.623913       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/n6k 575\nI0330 13:48:19.823210       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/tz9 286\nI0330 13:48:20.023501       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/m26 582\nI0330 13:48:20.223800       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/scln 377\nI0330 13:48:20.422914       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/fsfc 442\nI0330 13:48:20.623213       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/lrr 208\nI0330 13:48:20.823400       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/mr98 449\nI0330 13:48:21.023695       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/68ks 345\nI0330 13:48:21.222919       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/x7s 404\nI0330 13:48:21.423219       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/6gm 265\nI0330 13:48:21.623476       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/xpfz 357\nI0330 13:48:21.823778       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/zjqf 405\nI0330 13:48:22.022937       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/9mhj 375\nI0330 13:48:22.223227       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/pjd 371\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Mar 30 13:48:22.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 delete pod logs-generator'
Mar 30 13:48:23.093: INFO: stderr: ""
Mar 30 13:48:23.093: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 13:48:23.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8345" for this suite. 03/30/23 13:48:23.096
------------------------------
â€¢ [SLOW TEST] [5.681 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:48:17.417
    Mar 30 13:48:17.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 13:48:17.418
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:17.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:17.425
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 03/30/23 13:48:17.426
    Mar 30 13:48:17.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Mar 30 13:48:17.484: INFO: stderr: ""
    Mar 30 13:48:17.484: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 03/30/23 13:48:17.484
    Mar 30 13:48:17.484: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Mar 30 13:48:17.484: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8345" to be "running and ready, or succeeded"
    Mar 30 13:48:17.486: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.634799ms
    Mar 30 13:48:17.486: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'cn-hongkong.192.168.0.5' to be 'Running' but was 'Pending'
    Mar 30 13:48:19.489: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.004097487s
    Mar 30 13:48:19.489: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Mar 30 13:48:19.489: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 03/30/23 13:48:19.489
    Mar 30 13:48:19.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 logs logs-generator logs-generator'
    Mar 30 13:48:19.550: INFO: stderr: ""
    Mar 30 13:48:19.550: INFO: stdout: "I0330 13:48:18.022854       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/df7 447\nI0330 13:48:18.222932       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/z5mb 396\nI0330 13:48:18.423462       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/gfl6 246\nI0330 13:48:18.623754       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/kbvw 488\nI0330 13:48:18.822912       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/blr 487\nI0330 13:48:19.023213       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/s2hv 278\nI0330 13:48:19.223508       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/dbw 592\nI0330 13:48:19.423617       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/4v9n 293\n"
    STEP: limiting log lines 03/30/23 13:48:19.55
    Mar 30 13:48:19.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 logs logs-generator logs-generator --tail=1'
    Mar 30 13:48:19.605: INFO: stderr: ""
    Mar 30 13:48:19.605: INFO: stdout: "I0330 13:48:19.423617       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/4v9n 293\n"
    Mar 30 13:48:19.605: INFO: got output "I0330 13:48:19.423617       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/4v9n 293\n"
    STEP: limiting log bytes 03/30/23 13:48:19.605
    Mar 30 13:48:19.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 logs logs-generator logs-generator --limit-bytes=1'
    Mar 30 13:48:19.666: INFO: stderr: ""
    Mar 30 13:48:19.666: INFO: stdout: "I"
    Mar 30 13:48:19.666: INFO: got output "I"
    STEP: exposing timestamps 03/30/23 13:48:19.666
    Mar 30 13:48:19.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 logs logs-generator logs-generator --tail=1 --timestamps'
    Mar 30 13:48:19.720: INFO: stderr: ""
    Mar 30 13:48:19.720: INFO: stdout: "2023-03-30T21:48:19.623998350+08:00 I0330 13:48:19.623913       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/n6k 575\n"
    Mar 30 13:48:19.720: INFO: got output "2023-03-30T21:48:19.623998350+08:00 I0330 13:48:19.623913       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/n6k 575\n"
    STEP: restricting to a time range 03/30/23 13:48:19.72
    Mar 30 13:48:22.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 logs logs-generator logs-generator --since=1s'
    Mar 30 13:48:22.276: INFO: stderr: ""
    Mar 30 13:48:22.276: INFO: stdout: "I0330 13:48:21.423219       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/6gm 265\nI0330 13:48:21.623476       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/xpfz 357\nI0330 13:48:21.823778       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/zjqf 405\nI0330 13:48:22.022937       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/9mhj 375\nI0330 13:48:22.223227       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/pjd 371\n"
    Mar 30 13:48:22.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 logs logs-generator logs-generator --since=24h'
    Mar 30 13:48:22.330: INFO: stderr: ""
    Mar 30 13:48:22.330: INFO: stdout: "I0330 13:48:18.022854       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/df7 447\nI0330 13:48:18.222932       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/z5mb 396\nI0330 13:48:18.423462       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/gfl6 246\nI0330 13:48:18.623754       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/kbvw 488\nI0330 13:48:18.822912       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/blr 487\nI0330 13:48:19.023213       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/s2hv 278\nI0330 13:48:19.223508       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/dbw 592\nI0330 13:48:19.423617       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/4v9n 293\nI0330 13:48:19.623913       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/n6k 575\nI0330 13:48:19.823210       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/tz9 286\nI0330 13:48:20.023501       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/m26 582\nI0330 13:48:20.223800       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/scln 377\nI0330 13:48:20.422914       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/fsfc 442\nI0330 13:48:20.623213       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/lrr 208\nI0330 13:48:20.823400       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/mr98 449\nI0330 13:48:21.023695       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/68ks 345\nI0330 13:48:21.222919       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/x7s 404\nI0330 13:48:21.423219       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/6gm 265\nI0330 13:48:21.623476       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/xpfz 357\nI0330 13:48:21.823778       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/zjqf 405\nI0330 13:48:22.022937       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/9mhj 375\nI0330 13:48:22.223227       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/pjd 371\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Mar 30 13:48:22.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8345 delete pod logs-generator'
    Mar 30 13:48:23.093: INFO: stderr: ""
    Mar 30 13:48:23.093: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:48:23.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8345" for this suite. 03/30/23 13:48:23.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:48:23.099
Mar 30 13:48:23.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:48:23.1
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:23.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:23.106
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-d168dfae-741a-4759-949a-2b66060851ba 03/30/23 13:48:23.108
STEP: Creating secret with name secret-projected-all-test-volume-92efafad-6e96-4912-83d1-ba54e3c37693 03/30/23 13:48:23.11
STEP: Creating a pod to test Check all projections for projected volume plugin 03/30/23 13:48:23.111
Mar 30 13:48:23.115: INFO: Waiting up to 5m0s for pod "projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d" in namespace "projected-1154" to be "Succeeded or Failed"
Mar 30 13:48:23.116: INFO: Pod "projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.366156ms
Mar 30 13:48:25.119: INFO: Pod "projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004559829s
Mar 30 13:48:27.119: INFO: Pod "projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003786722s
STEP: Saw pod success 03/30/23 13:48:27.119
Mar 30 13:48:27.119: INFO: Pod "projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d" satisfied condition "Succeeded or Failed"
Mar 30 13:48:27.121: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d container projected-all-volume-test: <nil>
STEP: delete the pod 03/30/23 13:48:27.124
Mar 30 13:48:27.128: INFO: Waiting for pod projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d to disappear
Mar 30 13:48:27.129: INFO: Pod projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Mar 30 13:48:27.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1154" for this suite. 03/30/23 13:48:27.132
------------------------------
â€¢ [4.035 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:48:23.099
    Mar 30 13:48:23.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:48:23.1
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:23.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:23.106
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-d168dfae-741a-4759-949a-2b66060851ba 03/30/23 13:48:23.108
    STEP: Creating secret with name secret-projected-all-test-volume-92efafad-6e96-4912-83d1-ba54e3c37693 03/30/23 13:48:23.11
    STEP: Creating a pod to test Check all projections for projected volume plugin 03/30/23 13:48:23.111
    Mar 30 13:48:23.115: INFO: Waiting up to 5m0s for pod "projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d" in namespace "projected-1154" to be "Succeeded or Failed"
    Mar 30 13:48:23.116: INFO: Pod "projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.366156ms
    Mar 30 13:48:25.119: INFO: Pod "projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004559829s
    Mar 30 13:48:27.119: INFO: Pod "projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003786722s
    STEP: Saw pod success 03/30/23 13:48:27.119
    Mar 30 13:48:27.119: INFO: Pod "projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d" satisfied condition "Succeeded or Failed"
    Mar 30 13:48:27.121: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d container projected-all-volume-test: <nil>
    STEP: delete the pod 03/30/23 13:48:27.124
    Mar 30 13:48:27.128: INFO: Waiting for pod projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d to disappear
    Mar 30 13:48:27.129: INFO: Pod projected-volume-efedec87-ef47-40a0-a135-6892e5dd056d no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:48:27.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1154" for this suite. 03/30/23 13:48:27.132
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:48:27.134
Mar 30 13:48:27.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename statefulset 03/30/23 13:48:27.135
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:27.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:27.142
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4245 03/30/23 13:48:27.143
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Mar 30 13:48:27.150: INFO: Found 0 stateful pods, waiting for 1
Mar 30 13:48:37.152: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 03/30/23 13:48:37.155
W0330 13:48:37.160700      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 30 13:48:37.163: INFO: Found 1 stateful pods, waiting for 2
Mar 30 13:48:47.165: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 13:48:47.165: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 03/30/23 13:48:47.168
STEP: Delete all of the StatefulSets 03/30/23 13:48:47.17
STEP: Verify that StatefulSets have been deleted 03/30/23 13:48:47.173
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 30 13:48:47.175: INFO: Deleting all statefulset in ns statefulset-4245
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 30 13:48:47.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4245" for this suite. 03/30/23 13:48:47.183
------------------------------
â€¢ [SLOW TEST] [20.051 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:48:27.134
    Mar 30 13:48:27.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename statefulset 03/30/23 13:48:27.135
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:27.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:27.142
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4245 03/30/23 13:48:27.143
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Mar 30 13:48:27.150: INFO: Found 0 stateful pods, waiting for 1
    Mar 30 13:48:37.152: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 03/30/23 13:48:37.155
    W0330 13:48:37.160700      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 30 13:48:37.163: INFO: Found 1 stateful pods, waiting for 2
    Mar 30 13:48:47.165: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 30 13:48:47.165: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 03/30/23 13:48:47.168
    STEP: Delete all of the StatefulSets 03/30/23 13:48:47.17
    STEP: Verify that StatefulSets have been deleted 03/30/23 13:48:47.173
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 30 13:48:47.175: INFO: Deleting all statefulset in ns statefulset-4245
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:48:47.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4245" for this suite. 03/30/23 13:48:47.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:48:47.186
Mar 30 13:48:47.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename conformance-tests 03/30/23 13:48:47.187
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:47.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:47.193
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 03/30/23 13:48:47.195
Mar 30 13:48:47.195: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Mar 30 13:48:47.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-1672" for this suite. 03/30/23 13:48:47.2
------------------------------
â€¢ [0.017 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:48:47.186
    Mar 30 13:48:47.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename conformance-tests 03/30/23 13:48:47.187
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:47.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:47.193
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 03/30/23 13:48:47.195
    Mar 30 13:48:47.195: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:48:47.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-1672" for this suite. 03/30/23 13:48:47.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:48:47.203
Mar 30 13:48:47.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename csiinlinevolumes 03/30/23 13:48:47.204
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:47.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:47.21
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 03/30/23 13:48:47.212
STEP: getting 03/30/23 13:48:47.218
STEP: listing 03/30/23 13:48:47.221
STEP: deleting 03/30/23 13:48:47.222
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Mar 30 13:48:47.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-1426" for this suite. 03/30/23 13:48:47.232
------------------------------
â€¢ [0.031 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:48:47.203
    Mar 30 13:48:47.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename csiinlinevolumes 03/30/23 13:48:47.204
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:47.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:47.21
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 03/30/23 13:48:47.212
    STEP: getting 03/30/23 13:48:47.218
    STEP: listing 03/30/23 13:48:47.221
    STEP: deleting 03/30/23 13:48:47.222
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:48:47.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-1426" for this suite. 03/30/23 13:48:47.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:48:47.235
Mar 30 13:48:47.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-runtime 03/30/23 13:48:47.235
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:47.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:47.242
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/30/23 13:48:47.247
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/30/23 13:49:05.294
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/30/23 13:49:05.295
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/30/23 13:49:05.298
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/30/23 13:49:05.298
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/30/23 13:49:05.306
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/30/23 13:49:07.313
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/30/23 13:49:09.32
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/30/23 13:49:09.323
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/30/23 13:49:09.323
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/30/23 13:49:09.331
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/30/23 13:49:10.336
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/30/23 13:49:12.342
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/30/23 13:49:12.345
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/30/23 13:49:12.345
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 30 13:49:12.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-975" for this suite. 03/30/23 13:49:12.358
------------------------------
â€¢ [SLOW TEST] [25.126 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:48:47.235
    Mar 30 13:48:47.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-runtime 03/30/23 13:48:47.235
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:48:47.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:48:47.242
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/30/23 13:48:47.247
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/30/23 13:49:05.294
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/30/23 13:49:05.295
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/30/23 13:49:05.298
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/30/23 13:49:05.298
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/30/23 13:49:05.306
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/30/23 13:49:07.313
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/30/23 13:49:09.32
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/30/23 13:49:09.323
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/30/23 13:49:09.323
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/30/23 13:49:09.331
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/30/23 13:49:10.336
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/30/23 13:49:12.342
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/30/23 13:49:12.345
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/30/23 13:49:12.345
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:49:12.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-975" for this suite. 03/30/23 13:49:12.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:49:12.361
Mar 30 13:49:12.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pod-network-test 03/30/23 13:49:12.362
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:49:12.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:49:12.371
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-9923 03/30/23 13:49:12.372
STEP: creating a selector 03/30/23 13:49:12.372
STEP: Creating the service pods in kubernetes 03/30/23 13:49:12.372
Mar 30 13:49:12.372: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 30 13:49:12.385: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9923" to be "running and ready"
Mar 30 13:49:12.387: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012374ms
Mar 30 13:49:12.387: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:49:14.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004047843s
Mar 30 13:49:14.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:49:16.390: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004799748s
Mar 30 13:49:16.390: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:49:18.390: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005008997s
Mar 30 13:49:18.390: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:49:20.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.003956897s
Mar 30 13:49:20.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:49:22.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004136565s
Mar 30 13:49:22.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:49:24.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.003751993s
Mar 30 13:49:24.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:49:26.390: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005069308s
Mar 30 13:49:26.390: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:49:28.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.004079386s
Mar 30 13:49:28.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:49:30.390: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004966342s
Mar 30 13:49:30.390: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:49:32.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004124419s
Mar 30 13:49:32.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:49:34.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.003756166s
Mar 30 13:49:34.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:49:36.390: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 24.004876775s
Mar 30 13:49:36.390: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 13:49:38.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 26.004080909s
Mar 30 13:49:38.389: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 30 13:49:38.389: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 30 13:49:38.391: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9923" to be "running and ready"
Mar 30 13:49:38.392: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.407473ms
Mar 30 13:49:38.392: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 30 13:49:38.392: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 30 13:49:38.394: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9923" to be "running and ready"
Mar 30 13:49:38.395: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.31214ms
Mar 30 13:49:38.395: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 30 13:49:38.395: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/30/23 13:49:38.396
Mar 30 13:49:38.402: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9923" to be "running"
Mar 30 13:49:38.404: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.287688ms
Mar 30 13:49:40.406: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004399021s
Mar 30 13:49:40.406: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 30 13:49:40.408: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9923" to be "running"
Mar 30 13:49:40.409: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.405472ms
Mar 30 13:49:40.409: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 30 13:49:40.410: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 30 13:49:40.410: INFO: Going to poll 10.29.1.121 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 30 13:49:40.412: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.29.1.121:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9923 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:49:40.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:49:40.412: INFO: ExecWithOptions: Clientset creation
Mar 30 13:49:40.412: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-9923/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.29.1.121%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 30 13:49:40.456: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 30 13:49:40.456: INFO: Going to poll 10.29.0.203 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 30 13:49:40.457: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.29.0.203:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9923 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:49:40.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:49:40.458: INFO: ExecWithOptions: Clientset creation
Mar 30 13:49:40.458: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-9923/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.29.0.203%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 30 13:49:40.501: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 30 13:49:40.501: INFO: Going to poll 10.29.1.44 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 30 13:49:40.502: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.29.1.44:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9923 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:49:40.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:49:40.503: INFO: ExecWithOptions: Clientset creation
Mar 30 13:49:40.503: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-9923/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.29.1.44%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 30 13:49:40.548: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 30 13:49:40.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9923" for this suite. 03/30/23 13:49:40.55
------------------------------
â€¢ [SLOW TEST] [28.191 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:49:12.361
    Mar 30 13:49:12.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pod-network-test 03/30/23 13:49:12.362
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:49:12.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:49:12.371
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-9923 03/30/23 13:49:12.372
    STEP: creating a selector 03/30/23 13:49:12.372
    STEP: Creating the service pods in kubernetes 03/30/23 13:49:12.372
    Mar 30 13:49:12.372: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 30 13:49:12.385: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9923" to be "running and ready"
    Mar 30 13:49:12.387: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012374ms
    Mar 30 13:49:12.387: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:49:14.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004047843s
    Mar 30 13:49:14.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:49:16.390: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004799748s
    Mar 30 13:49:16.390: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:49:18.390: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005008997s
    Mar 30 13:49:18.390: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:49:20.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.003956897s
    Mar 30 13:49:20.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:49:22.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004136565s
    Mar 30 13:49:22.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:49:24.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.003751993s
    Mar 30 13:49:24.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:49:26.390: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005069308s
    Mar 30 13:49:26.390: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:49:28.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.004079386s
    Mar 30 13:49:28.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:49:30.390: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004966342s
    Mar 30 13:49:30.390: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:49:32.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004124419s
    Mar 30 13:49:32.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:49:34.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.003756166s
    Mar 30 13:49:34.389: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:49:36.390: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 24.004876775s
    Mar 30 13:49:36.390: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 13:49:38.389: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 26.004080909s
    Mar 30 13:49:38.389: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 30 13:49:38.389: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 30 13:49:38.391: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9923" to be "running and ready"
    Mar 30 13:49:38.392: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.407473ms
    Mar 30 13:49:38.392: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 30 13:49:38.392: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 30 13:49:38.394: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9923" to be "running and ready"
    Mar 30 13:49:38.395: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.31214ms
    Mar 30 13:49:38.395: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 30 13:49:38.395: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/30/23 13:49:38.396
    Mar 30 13:49:38.402: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9923" to be "running"
    Mar 30 13:49:38.404: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.287688ms
    Mar 30 13:49:40.406: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004399021s
    Mar 30 13:49:40.406: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 30 13:49:40.408: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9923" to be "running"
    Mar 30 13:49:40.409: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.405472ms
    Mar 30 13:49:40.409: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 30 13:49:40.410: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 30 13:49:40.410: INFO: Going to poll 10.29.1.121 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 30 13:49:40.412: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.29.1.121:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9923 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:49:40.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:49:40.412: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:49:40.412: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-9923/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.29.1.121%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 30 13:49:40.456: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 30 13:49:40.456: INFO: Going to poll 10.29.0.203 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 30 13:49:40.457: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.29.0.203:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9923 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:49:40.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:49:40.458: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:49:40.458: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-9923/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.29.0.203%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 30 13:49:40.501: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 30 13:49:40.501: INFO: Going to poll 10.29.1.44 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 30 13:49:40.502: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.29.1.44:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9923 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:49:40.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:49:40.503: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:49:40.503: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-9923/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.29.1.44%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 30 13:49:40.548: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:49:40.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9923" for this suite. 03/30/23 13:49:40.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:49:40.553
Mar 30 13:49:40.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename var-expansion 03/30/23 13:49:40.554
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:49:40.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:49:40.566
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 03/30/23 13:49:40.568
STEP: waiting for pod running 03/30/23 13:49:40.571
Mar 30 13:49:40.571: INFO: Waiting up to 2m0s for pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5" in namespace "var-expansion-2381" to be "running"
Mar 30 13:49:40.573: INFO: Pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.470074ms
Mar 30 13:49:42.576: INFO: Pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004309191s
Mar 30 13:49:42.576: INFO: Pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5" satisfied condition "running"
STEP: creating a file in subpath 03/30/23 13:49:42.576
Mar 30 13:49:42.577: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2381 PodName:var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:49:42.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:49:42.578: INFO: ExecWithOptions: Clientset creation
Mar 30 13:49:42.578: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/var-expansion-2381/pods/var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 03/30/23 13:49:42.616
Mar 30 13:49:42.618: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2381 PodName:var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:49:42.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:49:42.619: INFO: ExecWithOptions: Clientset creation
Mar 30 13:49:42.619: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/var-expansion-2381/pods/var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 03/30/23 13:49:42.66
Mar 30 13:49:43.166: INFO: Successfully updated pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5"
STEP: waiting for annotated pod running 03/30/23 13:49:43.166
Mar 30 13:49:43.166: INFO: Waiting up to 2m0s for pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5" in namespace "var-expansion-2381" to be "running"
Mar 30 13:49:43.168: INFO: Pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5": Phase="Running", Reason="", readiness=true. Elapsed: 1.573802ms
Mar 30 13:49:43.168: INFO: Pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5" satisfied condition "running"
STEP: deleting the pod gracefully 03/30/23 13:49:43.168
Mar 30 13:49:43.168: INFO: Deleting pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5" in namespace "var-expansion-2381"
Mar 30 13:49:43.171: INFO: Wait up to 5m0s for pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 30 13:50:17.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2381" for this suite. 03/30/23 13:50:17.177
------------------------------
â€¢ [SLOW TEST] [36.626 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:49:40.553
    Mar 30 13:49:40.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename var-expansion 03/30/23 13:49:40.554
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:49:40.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:49:40.566
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 03/30/23 13:49:40.568
    STEP: waiting for pod running 03/30/23 13:49:40.571
    Mar 30 13:49:40.571: INFO: Waiting up to 2m0s for pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5" in namespace "var-expansion-2381" to be "running"
    Mar 30 13:49:40.573: INFO: Pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.470074ms
    Mar 30 13:49:42.576: INFO: Pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004309191s
    Mar 30 13:49:42.576: INFO: Pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5" satisfied condition "running"
    STEP: creating a file in subpath 03/30/23 13:49:42.576
    Mar 30 13:49:42.577: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2381 PodName:var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:49:42.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:49:42.578: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:49:42.578: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/var-expansion-2381/pods/var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 03/30/23 13:49:42.616
    Mar 30 13:49:42.618: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2381 PodName:var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:49:42.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:49:42.619: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:49:42.619: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/var-expansion-2381/pods/var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 03/30/23 13:49:42.66
    Mar 30 13:49:43.166: INFO: Successfully updated pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5"
    STEP: waiting for annotated pod running 03/30/23 13:49:43.166
    Mar 30 13:49:43.166: INFO: Waiting up to 2m0s for pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5" in namespace "var-expansion-2381" to be "running"
    Mar 30 13:49:43.168: INFO: Pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5": Phase="Running", Reason="", readiness=true. Elapsed: 1.573802ms
    Mar 30 13:49:43.168: INFO: Pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5" satisfied condition "running"
    STEP: deleting the pod gracefully 03/30/23 13:49:43.168
    Mar 30 13:49:43.168: INFO: Deleting pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5" in namespace "var-expansion-2381"
    Mar 30 13:49:43.171: INFO: Wait up to 5m0s for pod "var-expansion-3fd469c4-3e1f-4361-b711-bb766c1a48d5" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:50:17.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2381" for this suite. 03/30/23 13:50:17.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:50:17.18
Mar 30 13:50:17.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename sysctl 03/30/23 13:50:17.181
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:50:17.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:50:17.188
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 03/30/23 13:50:17.19
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:50:17.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-9172" for this suite. 03/30/23 13:50:17.194
------------------------------
â€¢ [0.016 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:50:17.18
    Mar 30 13:50:17.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename sysctl 03/30/23 13:50:17.181
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:50:17.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:50:17.188
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 03/30/23 13:50:17.19
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:50:17.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-9172" for this suite. 03/30/23 13:50:17.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:50:17.197
Mar 30 13:50:17.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/30/23 13:50:17.198
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:50:17.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:50:17.204
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 03/30/23 13:50:17.205
STEP: Creating hostNetwork=false pod 03/30/23 13:50:17.206
Mar 30 13:50:17.209: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6130" to be "running and ready"
Mar 30 13:50:17.211: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.891518ms
Mar 30 13:50:17.211: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:50:19.214: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004870646s
Mar 30 13:50:19.214: INFO: The phase of Pod test-pod is Running (Ready = true)
Mar 30 13:50:19.214: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 03/30/23 13:50:19.216
Mar 30 13:50:19.218: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6130" to be "running and ready"
Mar 30 13:50:19.219: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.482958ms
Mar 30 13:50:19.220: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:50:21.222: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00386683s
Mar 30 13:50:21.222: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Mar 30 13:50:21.222: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 03/30/23 13:50:21.224
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/30/23 13:50:21.224
Mar 30 13:50:21.224: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:50:21.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:50:21.224: INFO: ExecWithOptions: Clientset creation
Mar 30 13:50:21.224: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 30 13:50:21.265: INFO: Exec stderr: ""
Mar 30 13:50:21.265: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:50:21.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:50:21.266: INFO: ExecWithOptions: Clientset creation
Mar 30 13:50:21.266: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 30 13:50:21.303: INFO: Exec stderr: ""
Mar 30 13:50:21.303: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:50:21.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:50:21.304: INFO: ExecWithOptions: Clientset creation
Mar 30 13:50:21.304: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 30 13:50:21.346: INFO: Exec stderr: ""
Mar 30 13:50:21.346: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:50:21.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:50:21.346: INFO: ExecWithOptions: Clientset creation
Mar 30 13:50:21.346: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 30 13:50:21.385: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/30/23 13:50:21.385
Mar 30 13:50:21.385: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:50:21.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:50:21.386: INFO: ExecWithOptions: Clientset creation
Mar 30 13:50:21.386: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 30 13:50:21.428: INFO: Exec stderr: ""
Mar 30 13:50:21.428: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:50:21.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:50:21.429: INFO: ExecWithOptions: Clientset creation
Mar 30 13:50:21.429: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 30 13:50:21.471: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/30/23 13:50:21.471
Mar 30 13:50:21.471: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:50:21.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:50:21.471: INFO: ExecWithOptions: Clientset creation
Mar 30 13:50:21.471: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 30 13:50:21.511: INFO: Exec stderr: ""
Mar 30 13:50:21.511: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:50:21.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:50:21.511: INFO: ExecWithOptions: Clientset creation
Mar 30 13:50:21.511: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 30 13:50:21.550: INFO: Exec stderr: ""
Mar 30 13:50:21.550: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:50:21.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:50:21.551: INFO: ExecWithOptions: Clientset creation
Mar 30 13:50:21.551: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 30 13:50:21.588: INFO: Exec stderr: ""
Mar 30 13:50:21.588: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:50:21.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:50:21.589: INFO: ExecWithOptions: Clientset creation
Mar 30 13:50:21.589: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 30 13:50:21.626: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Mar 30 13:50:21.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6130" for this suite. 03/30/23 13:50:21.629
------------------------------
â€¢ [4.435 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:50:17.197
    Mar 30 13:50:17.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/30/23 13:50:17.198
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:50:17.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:50:17.204
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 03/30/23 13:50:17.205
    STEP: Creating hostNetwork=false pod 03/30/23 13:50:17.206
    Mar 30 13:50:17.209: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6130" to be "running and ready"
    Mar 30 13:50:17.211: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.891518ms
    Mar 30 13:50:17.211: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:50:19.214: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004870646s
    Mar 30 13:50:19.214: INFO: The phase of Pod test-pod is Running (Ready = true)
    Mar 30 13:50:19.214: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 03/30/23 13:50:19.216
    Mar 30 13:50:19.218: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6130" to be "running and ready"
    Mar 30 13:50:19.219: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.482958ms
    Mar 30 13:50:19.220: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:50:21.222: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00386683s
    Mar 30 13:50:21.222: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Mar 30 13:50:21.222: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 03/30/23 13:50:21.224
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/30/23 13:50:21.224
    Mar 30 13:50:21.224: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:50:21.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:50:21.224: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:50:21.224: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 30 13:50:21.265: INFO: Exec stderr: ""
    Mar 30 13:50:21.265: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:50:21.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:50:21.266: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:50:21.266: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 30 13:50:21.303: INFO: Exec stderr: ""
    Mar 30 13:50:21.303: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:50:21.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:50:21.304: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:50:21.304: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 30 13:50:21.346: INFO: Exec stderr: ""
    Mar 30 13:50:21.346: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:50:21.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:50:21.346: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:50:21.346: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 30 13:50:21.385: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/30/23 13:50:21.385
    Mar 30 13:50:21.385: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:50:21.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:50:21.386: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:50:21.386: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 30 13:50:21.428: INFO: Exec stderr: ""
    Mar 30 13:50:21.428: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:50:21.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:50:21.429: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:50:21.429: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 30 13:50:21.471: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/30/23 13:50:21.471
    Mar 30 13:50:21.471: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:50:21.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:50:21.471: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:50:21.471: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 30 13:50:21.511: INFO: Exec stderr: ""
    Mar 30 13:50:21.511: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:50:21.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:50:21.511: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:50:21.511: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 30 13:50:21.550: INFO: Exec stderr: ""
    Mar 30 13:50:21.550: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:50:21.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:50:21.551: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:50:21.551: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 30 13:50:21.588: INFO: Exec stderr: ""
    Mar 30 13:50:21.588: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6130 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:50:21.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:50:21.589: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:50:21.589: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6130/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 30 13:50:21.626: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:50:21.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-6130" for this suite. 03/30/23 13:50:21.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:50:21.632
Mar 30 13:50:21.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename taint-multiple-pods 03/30/23 13:50:21.633
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:50:21.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:50:21.64
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Mar 30 13:50:21.642: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 30 13:51:21.664: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Mar 30 13:51:21.666: INFO: Starting informer...
STEP: Starting pods... 03/30/23 13:51:21.666
Mar 30 13:51:21.875: INFO: Pod1 is running on cn-hongkong.192.168.0.5. Tainting Node
Mar 30 13:51:22.079: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-4567" to be "running"
Mar 30 13:51:22.081: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.745139ms
Mar 30 13:51:24.083: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004312155s
Mar 30 13:51:24.083: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Mar 30 13:51:24.083: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-4567" to be "running"
Mar 30 13:51:24.085: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.707675ms
Mar 30 13:51:24.085: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Mar 30 13:51:24.085: INFO: Pod2 is running on cn-hongkong.192.168.0.5. Tainting Node
STEP: Trying to apply a taint on the Node 03/30/23 13:51:24.085
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/30/23 13:51:24.091
STEP: Waiting for Pod1 and Pod2 to be deleted 03/30/23 13:51:24.093
Mar 30 13:51:29.695: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 30 13:51:49.724: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/30/23 13:51:49.731
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:51:49.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-4567" for this suite. 03/30/23 13:51:49.735
------------------------------
â€¢ [SLOW TEST] [88.105 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:50:21.632
    Mar 30 13:50:21.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename taint-multiple-pods 03/30/23 13:50:21.633
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:50:21.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:50:21.64
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Mar 30 13:50:21.642: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 30 13:51:21.664: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Mar 30 13:51:21.666: INFO: Starting informer...
    STEP: Starting pods... 03/30/23 13:51:21.666
    Mar 30 13:51:21.875: INFO: Pod1 is running on cn-hongkong.192.168.0.5. Tainting Node
    Mar 30 13:51:22.079: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-4567" to be "running"
    Mar 30 13:51:22.081: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.745139ms
    Mar 30 13:51:24.083: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004312155s
    Mar 30 13:51:24.083: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Mar 30 13:51:24.083: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-4567" to be "running"
    Mar 30 13:51:24.085: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.707675ms
    Mar 30 13:51:24.085: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Mar 30 13:51:24.085: INFO: Pod2 is running on cn-hongkong.192.168.0.5. Tainting Node
    STEP: Trying to apply a taint on the Node 03/30/23 13:51:24.085
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/30/23 13:51:24.091
    STEP: Waiting for Pod1 and Pod2 to be deleted 03/30/23 13:51:24.093
    Mar 30 13:51:29.695: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Mar 30 13:51:49.724: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/30/23 13:51:49.731
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:51:49.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-4567" for this suite. 03/30/23 13:51:49.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:51:49.742
Mar 30 13:51:49.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-lifecycle-hook 03/30/23 13:51:49.743
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:51:49.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:51:49.749
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/30/23 13:51:49.753
Mar 30 13:51:49.756: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8858" to be "running and ready"
Mar 30 13:51:49.758: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.996802ms
Mar 30 13:51:49.758: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:51:51.761: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004405364s
Mar 30 13:51:51.761: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 30 13:51:51.761: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 03/30/23 13:51:51.762
Mar 30 13:51:51.765: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8858" to be "running and ready"
Mar 30 13:51:51.767: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.994437ms
Mar 30 13:51:51.767: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 30 13:51:53.769: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004331996s
Mar 30 13:51:53.769: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Mar 30 13:51:53.769: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/30/23 13:51:53.771
Mar 30 13:51:53.774: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 30 13:51:53.776: INFO: Pod pod-with-prestop-http-hook still exists
Mar 30 13:51:55.777: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 30 13:51:55.779: INFO: Pod pod-with-prestop-http-hook still exists
Mar 30 13:51:57.777: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 30 13:51:57.779: INFO: Pod pod-with-prestop-http-hook still exists
Mar 30 13:51:59.777: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 30 13:51:59.779: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 03/30/23 13:51:59.779
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 30 13:51:59.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-8858" for this suite. 03/30/23 13:51:59.79
------------------------------
â€¢ [SLOW TEST] [10.050 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:51:49.742
    Mar 30 13:51:49.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/30/23 13:51:49.743
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:51:49.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:51:49.749
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/30/23 13:51:49.753
    Mar 30 13:51:49.756: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8858" to be "running and ready"
    Mar 30 13:51:49.758: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.996802ms
    Mar 30 13:51:49.758: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:51:51.761: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004405364s
    Mar 30 13:51:51.761: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 30 13:51:51.761: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 03/30/23 13:51:51.762
    Mar 30 13:51:51.765: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8858" to be "running and ready"
    Mar 30 13:51:51.767: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.994437ms
    Mar 30 13:51:51.767: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 13:51:53.769: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004331996s
    Mar 30 13:51:53.769: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Mar 30 13:51:53.769: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/30/23 13:51:53.771
    Mar 30 13:51:53.774: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 30 13:51:53.776: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 30 13:51:55.777: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 30 13:51:55.779: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 30 13:51:57.777: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 30 13:51:57.779: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 30 13:51:59.777: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 30 13:51:59.779: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 03/30/23 13:51:59.779
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:51:59.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-8858" for this suite. 03/30/23 13:51:59.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:51:59.793
Mar 30 13:51:59.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 13:51:59.793
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:51:59.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:51:59.8
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:51:59.802
Mar 30 13:51:59.805: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5" in namespace "downward-api-5548" to be "Succeeded or Failed"
Mar 30 13:51:59.807: INFO: Pod "downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.282145ms
Mar 30 13:52:01.809: INFO: Pod "downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00368692s
Mar 30 13:52:03.810: INFO: Pod "downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004719914s
STEP: Saw pod success 03/30/23 13:52:03.81
Mar 30 13:52:03.810: INFO: Pod "downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5" satisfied condition "Succeeded or Failed"
Mar 30 13:52:03.812: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5 container client-container: <nil>
STEP: delete the pod 03/30/23 13:52:03.823
Mar 30 13:52:03.828: INFO: Waiting for pod downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5 to disappear
Mar 30 13:52:03.829: INFO: Pod downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 30 13:52:03.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5548" for this suite. 03/30/23 13:52:03.831
------------------------------
â€¢ [4.041 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:51:59.793
    Mar 30 13:51:59.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 13:51:59.793
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:51:59.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:51:59.8
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:51:59.802
    Mar 30 13:51:59.805: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5" in namespace "downward-api-5548" to be "Succeeded or Failed"
    Mar 30 13:51:59.807: INFO: Pod "downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.282145ms
    Mar 30 13:52:01.809: INFO: Pod "downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00368692s
    Mar 30 13:52:03.810: INFO: Pod "downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004719914s
    STEP: Saw pod success 03/30/23 13:52:03.81
    Mar 30 13:52:03.810: INFO: Pod "downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5" satisfied condition "Succeeded or Failed"
    Mar 30 13:52:03.812: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5 container client-container: <nil>
    STEP: delete the pod 03/30/23 13:52:03.823
    Mar 30 13:52:03.828: INFO: Waiting for pod downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5 to disappear
    Mar 30 13:52:03.829: INFO: Pod downwardapi-volume-76e45b64-eaee-4235-9090-5fc22c39a4d5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:52:03.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5548" for this suite. 03/30/23 13:52:03.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:52:03.835
Mar 30 13:52:03.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:52:03.835
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:03.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:03.842
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-90ce3edb-ca02-46f9-8f81-c898567fd5c1 03/30/23 13:52:03.843
STEP: Creating a pod to test consume configMaps 03/30/23 13:52:03.845
Mar 30 13:52:03.849: INFO: Waiting up to 5m0s for pod "pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f" in namespace "configmap-4152" to be "Succeeded or Failed"
Mar 30 13:52:03.851: INFO: Pod "pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.929474ms
Mar 30 13:52:05.853: INFO: Pod "pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f": Phase="Running", Reason="", readiness=false. Elapsed: 2.00391004s
Mar 30 13:52:07.853: INFO: Pod "pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004492512s
STEP: Saw pod success 03/30/23 13:52:07.853
Mar 30 13:52:07.853: INFO: Pod "pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f" satisfied condition "Succeeded or Failed"
Mar 30 13:52:07.855: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f container agnhost-container: <nil>
STEP: delete the pod 03/30/23 13:52:07.858
Mar 30 13:52:07.863: INFO: Waiting for pod pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f to disappear
Mar 30 13:52:07.864: INFO: Pod pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:52:07.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4152" for this suite. 03/30/23 13:52:07.866
------------------------------
â€¢ [4.034 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:52:03.835
    Mar 30 13:52:03.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:52:03.835
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:03.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:03.842
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-90ce3edb-ca02-46f9-8f81-c898567fd5c1 03/30/23 13:52:03.843
    STEP: Creating a pod to test consume configMaps 03/30/23 13:52:03.845
    Mar 30 13:52:03.849: INFO: Waiting up to 5m0s for pod "pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f" in namespace "configmap-4152" to be "Succeeded or Failed"
    Mar 30 13:52:03.851: INFO: Pod "pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.929474ms
    Mar 30 13:52:05.853: INFO: Pod "pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f": Phase="Running", Reason="", readiness=false. Elapsed: 2.00391004s
    Mar 30 13:52:07.853: INFO: Pod "pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004492512s
    STEP: Saw pod success 03/30/23 13:52:07.853
    Mar 30 13:52:07.853: INFO: Pod "pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f" satisfied condition "Succeeded or Failed"
    Mar 30 13:52:07.855: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 13:52:07.858
    Mar 30 13:52:07.863: INFO: Waiting for pod pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f to disappear
    Mar 30 13:52:07.864: INFO: Pod pod-configmaps-de3daecb-2e1b-4b17-b53b-493d388b2f4f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:52:07.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4152" for this suite. 03/30/23 13:52:07.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:52:07.87
Mar 30 13:52:07.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 13:52:07.871
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:07.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:07.877
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 03/30/23 13:52:07.879
Mar 30 13:52:07.882: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-5409cf40-5aad-4b53-9925-0fe9eabc82b0" in namespace "emptydir-7448" to be "running"
Mar 30 13:52:07.883: INFO: Pod "pod-sharedvolume-5409cf40-5aad-4b53-9925-0fe9eabc82b0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.490047ms
Mar 30 13:52:09.886: INFO: Pod "pod-sharedvolume-5409cf40-5aad-4b53-9925-0fe9eabc82b0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004121675s
Mar 30 13:52:09.886: INFO: Pod "pod-sharedvolume-5409cf40-5aad-4b53-9925-0fe9eabc82b0" satisfied condition "running"
STEP: Reading file content from the nginx-container 03/30/23 13:52:09.886
Mar 30 13:52:09.886: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7448 PodName:pod-sharedvolume-5409cf40-5aad-4b53-9925-0fe9eabc82b0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 13:52:09.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 13:52:09.886: INFO: ExecWithOptions: Clientset creation
Mar 30 13:52:09.886: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/emptydir-7448/pods/pod-sharedvolume-5409cf40-5aad-4b53-9925-0fe9eabc82b0/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar 30 13:52:09.925: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 13:52:09.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7448" for this suite. 03/30/23 13:52:09.927
------------------------------
â€¢ [2.059 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:52:07.87
    Mar 30 13:52:07.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 13:52:07.871
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:07.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:07.877
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 03/30/23 13:52:07.879
    Mar 30 13:52:07.882: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-5409cf40-5aad-4b53-9925-0fe9eabc82b0" in namespace "emptydir-7448" to be "running"
    Mar 30 13:52:07.883: INFO: Pod "pod-sharedvolume-5409cf40-5aad-4b53-9925-0fe9eabc82b0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.490047ms
    Mar 30 13:52:09.886: INFO: Pod "pod-sharedvolume-5409cf40-5aad-4b53-9925-0fe9eabc82b0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004121675s
    Mar 30 13:52:09.886: INFO: Pod "pod-sharedvolume-5409cf40-5aad-4b53-9925-0fe9eabc82b0" satisfied condition "running"
    STEP: Reading file content from the nginx-container 03/30/23 13:52:09.886
    Mar 30 13:52:09.886: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7448 PodName:pod-sharedvolume-5409cf40-5aad-4b53-9925-0fe9eabc82b0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 13:52:09.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 13:52:09.886: INFO: ExecWithOptions: Clientset creation
    Mar 30 13:52:09.886: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/emptydir-7448/pods/pod-sharedvolume-5409cf40-5aad-4b53-9925-0fe9eabc82b0/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Mar 30 13:52:09.925: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:52:09.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7448" for this suite. 03/30/23 13:52:09.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:52:09.93
Mar 30 13:52:09.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename deployment 03/30/23 13:52:09.931
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:09.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:09.937
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Mar 30 13:52:09.939: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 30 13:52:09.943: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 30 13:52:14.945: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/30/23 13:52:14.945
Mar 30 13:52:14.945: INFO: Creating deployment "test-rolling-update-deployment"
Mar 30 13:52:14.948: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 30 13:52:14.951: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 30 13:52:16.955: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 30 13:52:16.957: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 30 13:52:16.961: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1818  77750712-477f-4d21-9712-c6fceb2f7c7e 79232 1 2023-03-30 13:52:14 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-30 13:52:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:52:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006bd2a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-30 13:52:14 +0000 UTC,LastTransitionTime:2023-03-30 13:52:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-03-30 13:52:16 +0000 UTC,LastTransitionTime:2023-03-30 13:52:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 30 13:52:16.962: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1818  cb4ab664-b9d1-4967-8724-b18e2d4a492d 79222 1 2023-03-30 13:52:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 77750712-477f-4d21-9712-c6fceb2f7c7e 0xc003d914a7 0xc003d914a8}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:52:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77750712-477f-4d21-9712-c6fceb2f7c7e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:52:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d91558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 30 13:52:16.962: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 30 13:52:16.963: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1818  aec3ae10-210c-486a-9af8-489dee290ab4 79231 2 2023-03-30 13:52:09 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 77750712-477f-4d21-9712-c6fceb2f7c7e 0xc003d91377 0xc003d91378}] [] [{e2e.test Update apps/v1 2023-03-30 13:52:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:52:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77750712-477f-4d21-9712-c6fceb2f7c7e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:52:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d91438 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 13:52:16.964: INFO: Pod "test-rolling-update-deployment-7549d9f46d-8tc8d" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-8tc8d test-rolling-update-deployment-7549d9f46d- deployment-1818  d0d2c8a6-6306-4bf2-ab2e-a75c59fd1652 79221 0 2023-03-30 13:52:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d cb4ab664-b9d1-4967-8724-b18e2d4a492d 0xc003778c17 0xc003778c18}] [] [{kube-controller-manager Update v1 2023-03-30 13:52:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb4ab664-b9d1-4967-8724-b18e2d4a492d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:52:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4ccsg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4ccsg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:52:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:52:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:52:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:52:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.54,StartTime:2023-03-30 13:52:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:52:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e89546beb1ddc9edc29ed1dd50d9932725641c0b65eeb78db6d980ed3c31dee7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 30 13:52:16.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1818" for this suite. 03/30/23 13:52:16.966
------------------------------
â€¢ [SLOW TEST] [7.039 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:52:09.93
    Mar 30 13:52:09.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename deployment 03/30/23 13:52:09.931
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:09.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:09.937
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Mar 30 13:52:09.939: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Mar 30 13:52:09.943: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 30 13:52:14.945: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/30/23 13:52:14.945
    Mar 30 13:52:14.945: INFO: Creating deployment "test-rolling-update-deployment"
    Mar 30 13:52:14.948: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Mar 30 13:52:14.951: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Mar 30 13:52:16.955: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Mar 30 13:52:16.957: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 30 13:52:16.961: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1818  77750712-477f-4d21-9712-c6fceb2f7c7e 79232 1 2023-03-30 13:52:14 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-30 13:52:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:52:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006bd2a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-30 13:52:14 +0000 UTC,LastTransitionTime:2023-03-30 13:52:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-03-30 13:52:16 +0000 UTC,LastTransitionTime:2023-03-30 13:52:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 30 13:52:16.962: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1818  cb4ab664-b9d1-4967-8724-b18e2d4a492d 79222 1 2023-03-30 13:52:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 77750712-477f-4d21-9712-c6fceb2f7c7e 0xc003d914a7 0xc003d914a8}] [] [{kube-controller-manager Update apps/v1 2023-03-30 13:52:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77750712-477f-4d21-9712-c6fceb2f7c7e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:52:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d91558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 13:52:16.962: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Mar 30 13:52:16.963: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1818  aec3ae10-210c-486a-9af8-489dee290ab4 79231 2 2023-03-30 13:52:09 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 77750712-477f-4d21-9712-c6fceb2f7c7e 0xc003d91377 0xc003d91378}] [] [{e2e.test Update apps/v1 2023-03-30 13:52:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:52:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77750712-477f-4d21-9712-c6fceb2f7c7e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-30 13:52:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d91438 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 13:52:16.964: INFO: Pod "test-rolling-update-deployment-7549d9f46d-8tc8d" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-8tc8d test-rolling-update-deployment-7549d9f46d- deployment-1818  d0d2c8a6-6306-4bf2-ab2e-a75c59fd1652 79221 0 2023-03-30 13:52:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d cb4ab664-b9d1-4967-8724-b18e2d4a492d 0xc003778c17 0xc003778c18}] [] [{kube-controller-manager Update v1 2023-03-30 13:52:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb4ab664-b9d1-4967-8724-b18e2d4a492d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 13:52:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4ccsg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4ccsg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:52:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:52:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:52:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 13:52:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.54,StartTime:2023-03-30 13:52:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 13:52:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e89546beb1ddc9edc29ed1dd50d9932725641c0b65eeb78db6d980ed3c31dee7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:52:16.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1818" for this suite. 03/30/23 13:52:16.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:52:16.969
Mar 30 13:52:16.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:52:16.97
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:16.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:16.976
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-e514a20a-93be-49fc-8e64-3ecd4f5e8af4 03/30/23 13:52:16.978
STEP: Creating a pod to test consume configMaps 03/30/23 13:52:16.98
Mar 30 13:52:16.983: INFO: Waiting up to 5m0s for pod "pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd" in namespace "configmap-2029" to be "Succeeded or Failed"
Mar 30 13:52:16.985: INFO: Pod "pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.82343ms
Mar 30 13:52:18.988: INFO: Pod "pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00489057s
Mar 30 13:52:20.988: INFO: Pod "pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004675022s
STEP: Saw pod success 03/30/23 13:52:20.988
Mar 30 13:52:20.988: INFO: Pod "pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd" satisfied condition "Succeeded or Failed"
Mar 30 13:52:20.990: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd container agnhost-container: <nil>
STEP: delete the pod 03/30/23 13:52:20.993
Mar 30 13:52:20.997: INFO: Waiting for pod pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd to disappear
Mar 30 13:52:20.999: INFO: Pod pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:52:20.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2029" for this suite. 03/30/23 13:52:21.001
------------------------------
â€¢ [4.034 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:52:16.969
    Mar 30 13:52:16.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:52:16.97
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:16.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:16.976
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-e514a20a-93be-49fc-8e64-3ecd4f5e8af4 03/30/23 13:52:16.978
    STEP: Creating a pod to test consume configMaps 03/30/23 13:52:16.98
    Mar 30 13:52:16.983: INFO: Waiting up to 5m0s for pod "pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd" in namespace "configmap-2029" to be "Succeeded or Failed"
    Mar 30 13:52:16.985: INFO: Pod "pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.82343ms
    Mar 30 13:52:18.988: INFO: Pod "pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00489057s
    Mar 30 13:52:20.988: INFO: Pod "pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004675022s
    STEP: Saw pod success 03/30/23 13:52:20.988
    Mar 30 13:52:20.988: INFO: Pod "pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd" satisfied condition "Succeeded or Failed"
    Mar 30 13:52:20.990: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 13:52:20.993
    Mar 30 13:52:20.997: INFO: Waiting for pod pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd to disappear
    Mar 30 13:52:20.999: INFO: Pod pod-configmaps-d2989a15-dfaf-459c-8948-7f697a22edfd no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:52:20.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2029" for this suite. 03/30/23 13:52:21.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:52:21.004
Mar 30 13:52:21.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 13:52:21.005
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:21.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:21.011
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-9bcafffe-70dc-472f-8f70-944f983ae2a6 03/30/23 13:52:21.013
STEP: Creating a pod to test consume secrets 03/30/23 13:52:21.015
Mar 30 13:52:21.019: INFO: Waiting up to 5m0s for pod "pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8" in namespace "secrets-2870" to be "Succeeded or Failed"
Mar 30 13:52:21.020: INFO: Pod "pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.705726ms
Mar 30 13:52:23.023: INFO: Pod "pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004065695s
Mar 30 13:52:25.023: INFO: Pod "pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004421981s
STEP: Saw pod success 03/30/23 13:52:25.023
Mar 30 13:52:25.023: INFO: Pod "pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8" satisfied condition "Succeeded or Failed"
Mar 30 13:52:25.025: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8 container secret-volume-test: <nil>
STEP: delete the pod 03/30/23 13:52:25.028
Mar 30 13:52:25.032: INFO: Waiting for pod pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8 to disappear
Mar 30 13:52:25.034: INFO: Pod pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 13:52:25.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2870" for this suite. 03/30/23 13:52:25.036
------------------------------
â€¢ [4.035 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:52:21.004
    Mar 30 13:52:21.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 13:52:21.005
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:21.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:21.011
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-9bcafffe-70dc-472f-8f70-944f983ae2a6 03/30/23 13:52:21.013
    STEP: Creating a pod to test consume secrets 03/30/23 13:52:21.015
    Mar 30 13:52:21.019: INFO: Waiting up to 5m0s for pod "pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8" in namespace "secrets-2870" to be "Succeeded or Failed"
    Mar 30 13:52:21.020: INFO: Pod "pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.705726ms
    Mar 30 13:52:23.023: INFO: Pod "pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004065695s
    Mar 30 13:52:25.023: INFO: Pod "pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004421981s
    STEP: Saw pod success 03/30/23 13:52:25.023
    Mar 30 13:52:25.023: INFO: Pod "pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8" satisfied condition "Succeeded or Failed"
    Mar 30 13:52:25.025: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8 container secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 13:52:25.028
    Mar 30 13:52:25.032: INFO: Waiting for pod pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8 to disappear
    Mar 30 13:52:25.034: INFO: Pod pod-secrets-db1c24be-ea79-46cf-8f16-c321dd202ad8 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:52:25.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2870" for this suite. 03/30/23 13:52:25.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:52:25.039
Mar 30 13:52:25.039: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename dns 03/30/23 13:52:25.04
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:25.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:25.046
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 03/30/23 13:52:25.048
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9144.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9144.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9144.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9144.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 33.200.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.200.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.200.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.200.33_tcp@PTR;sleep 1; done
 03/30/23 13:52:25.054
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9144.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9144.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9144.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9144.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 33.200.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.200.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.200.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.200.33_tcp@PTR;sleep 1; done
 03/30/23 13:52:25.054
STEP: creating a pod to probe DNS 03/30/23 13:52:25.054
STEP: submitting the pod to kubernetes 03/30/23 13:52:25.054
Mar 30 13:52:25.058: INFO: Waiting up to 15m0s for pod "dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4" in namespace "dns-9144" to be "running"
Mar 30 13:52:25.060: INFO: Pod "dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.79675ms
Mar 30 13:52:27.063: INFO: Pod "dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004207502s
Mar 30 13:52:27.063: INFO: Pod "dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4" satisfied condition "running"
STEP: retrieving the pod 03/30/23 13:52:27.063
STEP: looking for the results for each expected name from probers 03/30/23 13:52:27.064
Mar 30 13:52:27.067: INFO: Unable to read wheezy_udp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:27.069: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:27.070: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:27.072: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:27.080: INFO: Unable to read jessie_udp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:27.081: INFO: Unable to read jessie_tcp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:27.083: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:27.085: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:27.091: INFO: Lookups using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 failed for: [wheezy_udp@dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@dns-test-service.dns-9144.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_udp@dns-test-service.dns-9144.svc.cluster.local jessie_tcp@dns-test-service.dns-9144.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local]

Mar 30 13:52:32.097: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:32.099: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:32.107: INFO: Unable to read jessie_udp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:32.108: INFO: Unable to read jessie_tcp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:32.110: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:32.111: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:32.117: INFO: Lookups using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_udp@dns-test-service.dns-9144.svc.cluster.local jessie_tcp@dns-test-service.dns-9144.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local]

Mar 30 13:52:37.097: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:37.099: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:37.107: INFO: Unable to read jessie_udp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:37.108: INFO: Unable to read jessie_tcp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:37.110: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:37.111: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:37.117: INFO: Lookups using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_udp@dns-test-service.dns-9144.svc.cluster.local jessie_tcp@dns-test-service.dns-9144.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local]

Mar 30 13:52:42.097: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:42.099: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:42.106: INFO: Unable to read jessie_udp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:42.109: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:42.111: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:42.117: INFO: Lookups using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_udp@dns-test-service.dns-9144.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local]

Mar 30 13:52:47.098: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:47.100: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:47.111: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:47.113: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:47.119: INFO: Lookups using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local]

Mar 30 13:52:52.098: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:52.100: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:52.111: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:52.112: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
Mar 30 13:52:52.118: INFO: Lookups using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local]

Mar 30 13:52:57.117: INFO: DNS probes using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 succeeded

STEP: deleting the pod 03/30/23 13:52:57.117
STEP: deleting the test service 03/30/23 13:52:57.122
STEP: deleting the test headless service 03/30/23 13:52:57.156
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 30 13:52:57.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9144" for this suite. 03/30/23 13:52:57.164
------------------------------
â€¢ [SLOW TEST] [32.129 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:52:25.039
    Mar 30 13:52:25.039: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename dns 03/30/23 13:52:25.04
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:25.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:25.046
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 03/30/23 13:52:25.048
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9144.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9144.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9144.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9144.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 33.200.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.200.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.200.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.200.33_tcp@PTR;sleep 1; done
     03/30/23 13:52:25.054
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9144.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9144.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9144.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9144.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9144.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 33.200.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.200.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.200.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.200.33_tcp@PTR;sleep 1; done
     03/30/23 13:52:25.054
    STEP: creating a pod to probe DNS 03/30/23 13:52:25.054
    STEP: submitting the pod to kubernetes 03/30/23 13:52:25.054
    Mar 30 13:52:25.058: INFO: Waiting up to 15m0s for pod "dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4" in namespace "dns-9144" to be "running"
    Mar 30 13:52:25.060: INFO: Pod "dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.79675ms
    Mar 30 13:52:27.063: INFO: Pod "dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004207502s
    Mar 30 13:52:27.063: INFO: Pod "dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4" satisfied condition "running"
    STEP: retrieving the pod 03/30/23 13:52:27.063
    STEP: looking for the results for each expected name from probers 03/30/23 13:52:27.064
    Mar 30 13:52:27.067: INFO: Unable to read wheezy_udp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:27.069: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:27.070: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:27.072: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:27.080: INFO: Unable to read jessie_udp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:27.081: INFO: Unable to read jessie_tcp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:27.083: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:27.085: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:27.091: INFO: Lookups using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 failed for: [wheezy_udp@dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@dns-test-service.dns-9144.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_udp@dns-test-service.dns-9144.svc.cluster.local jessie_tcp@dns-test-service.dns-9144.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local]

    Mar 30 13:52:32.097: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:32.099: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:32.107: INFO: Unable to read jessie_udp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:32.108: INFO: Unable to read jessie_tcp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:32.110: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:32.111: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:32.117: INFO: Lookups using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_udp@dns-test-service.dns-9144.svc.cluster.local jessie_tcp@dns-test-service.dns-9144.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local]

    Mar 30 13:52:37.097: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:37.099: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:37.107: INFO: Unable to read jessie_udp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:37.108: INFO: Unable to read jessie_tcp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:37.110: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:37.111: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:37.117: INFO: Lookups using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_udp@dns-test-service.dns-9144.svc.cluster.local jessie_tcp@dns-test-service.dns-9144.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local]

    Mar 30 13:52:42.097: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:42.099: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:42.106: INFO: Unable to read jessie_udp@dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:42.109: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:42.111: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:42.117: INFO: Lookups using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_udp@dns-test-service.dns-9144.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local]

    Mar 30 13:52:47.098: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:47.100: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:47.111: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:47.113: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:47.119: INFO: Lookups using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local]

    Mar 30 13:52:52.098: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:52.100: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:52.111: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:52.112: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local from pod dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4: the server could not find the requested resource (get pods dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4)
    Mar 30 13:52:52.118: INFO: Lookups using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9144.svc.cluster.local]

    Mar 30 13:52:57.117: INFO: DNS probes using dns-9144/dns-test-9a2992f7-136c-4c4c-9810-4fede0a57ab4 succeeded

    STEP: deleting the pod 03/30/23 13:52:57.117
    STEP: deleting the test service 03/30/23 13:52:57.122
    STEP: deleting the test headless service 03/30/23 13:52:57.156
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:52:57.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9144" for this suite. 03/30/23 13:52:57.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:52:57.17
Mar 30 13:52:57.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 13:52:57.171
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:57.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:57.179
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 03/30/23 13:52:57.181
Mar 30 13:52:57.185: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134" in namespace "downward-api-474" to be "Succeeded or Failed"
Mar 30 13:52:57.188: INFO: Pod "downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297271ms
Mar 30 13:52:59.191: INFO: Pod "downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005953251s
Mar 30 13:53:01.192: INFO: Pod "downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007019004s
STEP: Saw pod success 03/30/23 13:53:01.192
Mar 30 13:53:01.192: INFO: Pod "downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134" satisfied condition "Succeeded or Failed"
Mar 30 13:53:01.194: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134 container client-container: <nil>
STEP: delete the pod 03/30/23 13:53:01.197
Mar 30 13:53:01.202: INFO: Waiting for pod downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134 to disappear
Mar 30 13:53:01.203: INFO: Pod downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 30 13:53:01.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-474" for this suite. 03/30/23 13:53:01.205
------------------------------
â€¢ [4.038 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:52:57.17
    Mar 30 13:52:57.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 13:52:57.171
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:52:57.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:52:57.179
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 03/30/23 13:52:57.181
    Mar 30 13:52:57.185: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134" in namespace "downward-api-474" to be "Succeeded or Failed"
    Mar 30 13:52:57.188: INFO: Pod "downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297271ms
    Mar 30 13:52:59.191: INFO: Pod "downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005953251s
    Mar 30 13:53:01.192: INFO: Pod "downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007019004s
    STEP: Saw pod success 03/30/23 13:53:01.192
    Mar 30 13:53:01.192: INFO: Pod "downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134" satisfied condition "Succeeded or Failed"
    Mar 30 13:53:01.194: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134 container client-container: <nil>
    STEP: delete the pod 03/30/23 13:53:01.197
    Mar 30 13:53:01.202: INFO: Waiting for pod downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134 to disappear
    Mar 30 13:53:01.203: INFO: Pod downwardapi-volume-a46d3f12-13f1-4517-ae15-2820a7d00134 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:53:01.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-474" for this suite. 03/30/23 13:53:01.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:53:01.208
Mar 30 13:53:01.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename resourcequota 03/30/23 13:53:01.209
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:53:01.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:53:01.216
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 03/30/23 13:53:01.217
STEP: Creating a ResourceQuota 03/30/23 13:53:06.22
STEP: Ensuring resource quota status is calculated 03/30/23 13:53:06.222
STEP: Creating a ReplicaSet 03/30/23 13:53:08.225
STEP: Ensuring resource quota status captures replicaset creation 03/30/23 13:53:08.23
STEP: Deleting a ReplicaSet 03/30/23 13:53:10.233
STEP: Ensuring resource quota status released usage 03/30/23 13:53:10.236
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 30 13:53:12.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8701" for this suite. 03/30/23 13:53:12.241
------------------------------
â€¢ [SLOW TEST] [11.035 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:53:01.208
    Mar 30 13:53:01.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename resourcequota 03/30/23 13:53:01.209
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:53:01.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:53:01.216
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 03/30/23 13:53:01.217
    STEP: Creating a ResourceQuota 03/30/23 13:53:06.22
    STEP: Ensuring resource quota status is calculated 03/30/23 13:53:06.222
    STEP: Creating a ReplicaSet 03/30/23 13:53:08.225
    STEP: Ensuring resource quota status captures replicaset creation 03/30/23 13:53:08.23
    STEP: Deleting a ReplicaSet 03/30/23 13:53:10.233
    STEP: Ensuring resource quota status released usage 03/30/23 13:53:10.236
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:53:12.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8701" for this suite. 03/30/23 13:53:12.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:53:12.244
Mar 30 13:53:12.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 13:53:12.245
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:53:12.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:53:12.252
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-71aeeaa2-8309-49de-b057-fa666363f045 03/30/23 13:53:12.253
STEP: Creating a pod to test consume configMaps 03/30/23 13:53:12.255
Mar 30 13:53:12.259: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63" in namespace "projected-8772" to be "Succeeded or Failed"
Mar 30 13:53:12.261: INFO: Pod "pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63": Phase="Pending", Reason="", readiness=false. Elapsed: 1.800008ms
Mar 30 13:53:14.263: INFO: Pod "pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004592825s
Mar 30 13:53:16.264: INFO: Pod "pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005345257s
STEP: Saw pod success 03/30/23 13:53:16.264
Mar 30 13:53:16.264: INFO: Pod "pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63" satisfied condition "Succeeded or Failed"
Mar 30 13:53:16.266: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63 container agnhost-container: <nil>
STEP: delete the pod 03/30/23 13:53:16.269
Mar 30 13:53:16.274: INFO: Waiting for pod pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63 to disappear
Mar 30 13:53:16.275: INFO: Pod pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:53:16.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8772" for this suite. 03/30/23 13:53:16.278
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:53:12.244
    Mar 30 13:53:12.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 13:53:12.245
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:53:12.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:53:12.252
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-71aeeaa2-8309-49de-b057-fa666363f045 03/30/23 13:53:12.253
    STEP: Creating a pod to test consume configMaps 03/30/23 13:53:12.255
    Mar 30 13:53:12.259: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63" in namespace "projected-8772" to be "Succeeded or Failed"
    Mar 30 13:53:12.261: INFO: Pod "pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63": Phase="Pending", Reason="", readiness=false. Elapsed: 1.800008ms
    Mar 30 13:53:14.263: INFO: Pod "pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004592825s
    Mar 30 13:53:16.264: INFO: Pod "pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005345257s
    STEP: Saw pod success 03/30/23 13:53:16.264
    Mar 30 13:53:16.264: INFO: Pod "pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63" satisfied condition "Succeeded or Failed"
    Mar 30 13:53:16.266: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63 container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 13:53:16.269
    Mar 30 13:53:16.274: INFO: Waiting for pod pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63 to disappear
    Mar 30 13:53:16.275: INFO: Pod pod-projected-configmaps-c9ce148c-668e-4d67-a36e-55dce1d68a63 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:53:16.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8772" for this suite. 03/30/23 13:53:16.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:53:16.281
Mar 30 13:53:16.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename endpointslice 03/30/23 13:53:16.282
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:53:16.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:53:16.288
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 03/30/23 13:53:21.314
STEP: referencing matching pods with named port 03/30/23 13:53:26.318
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/30/23 13:53:31.322
STEP: recreating EndpointSlices after they've been deleted 03/30/23 13:53:36.329
Mar 30 13:53:36.338: INFO: EndpointSlice for Service endpointslice-5161/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 30 13:53:46.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5161" for this suite. 03/30/23 13:53:46.353
------------------------------
â€¢ [SLOW TEST] [30.074 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:53:16.281
    Mar 30 13:53:16.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename endpointslice 03/30/23 13:53:16.282
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:53:16.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:53:16.288
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 03/30/23 13:53:21.314
    STEP: referencing matching pods with named port 03/30/23 13:53:26.318
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/30/23 13:53:31.322
    STEP: recreating EndpointSlices after they've been deleted 03/30/23 13:53:36.329
    Mar 30 13:53:36.338: INFO: EndpointSlice for Service endpointslice-5161/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:53:46.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5161" for this suite. 03/30/23 13:53:46.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:53:46.356
Mar 30 13:53:46.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename init-container 03/30/23 13:53:46.356
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:53:46.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:53:46.363
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 03/30/23 13:53:46.366
Mar 30 13:53:46.366: INFO: PodSpec: initContainers in spec.initContainers
Mar 30 13:54:28.682: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ab306bab-cf5f-4f64-9d9b-fa8917a9323b", GenerateName:"", Namespace:"init-container-1066", SelfLink:"", UID:"8ace9a39-9e47-4f42-b576-3a357b014209", ResourceVersion:"80159", Generation:0, CreationTimestamp:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"366716454"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013c15a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 30, 13, 54, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013c15f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-cbjzk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005ce2f40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-cbjzk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-cbjzk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-cbjzk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004475b68), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cn-hongkong.192.168.0.5", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002ec6e00), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004475be0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004475c00)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004475c08), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004475c0c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001318bb0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.5", PodIP:"10.29.1.62", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.29.1.62"}}, StartTime:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002ec6ee0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002ec6f50)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://684078bc79fbbba60942a30b629f4862f21a0714b5e3746b98364d5a2d1aaecd", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005ce2fc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005ce2fa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004475c8f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:54:28.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1066" for this suite. 03/30/23 13:54:28.684
------------------------------
â€¢ [SLOW TEST] [42.332 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:53:46.356
    Mar 30 13:53:46.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename init-container 03/30/23 13:53:46.356
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:53:46.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:53:46.363
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 03/30/23 13:53:46.366
    Mar 30 13:53:46.366: INFO: PodSpec: initContainers in spec.initContainers
    Mar 30 13:54:28.682: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ab306bab-cf5f-4f64-9d9b-fa8917a9323b", GenerateName:"", Namespace:"init-container-1066", SelfLink:"", UID:"8ace9a39-9e47-4f42-b576-3a357b014209", ResourceVersion:"80159", Generation:0, CreationTimestamp:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"366716454"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013c15a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 30, 13, 54, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013c15f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-cbjzk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005ce2f40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-cbjzk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-cbjzk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-cbjzk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004475b68), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cn-hongkong.192.168.0.5", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002ec6e00), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004475be0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004475c00)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004475c08), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004475c0c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001318bb0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.5", PodIP:"10.29.1.62", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.29.1.62"}}, StartTime:time.Date(2023, time.March, 30, 13, 53, 46, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002ec6ee0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002ec6f50)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://684078bc79fbbba60942a30b629f4862f21a0714b5e3746b98364d5a2d1aaecd", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005ce2fc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005ce2fa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004475c8f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:54:28.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1066" for this suite. 03/30/23 13:54:28.684
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:54:28.688
Mar 30 13:54:28.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename sched-pred 03/30/23 13:54:28.688
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:54:28.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:54:28.695
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 30 13:54:28.696: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 30 13:54:28.700: INFO: Waiting for terminating namespaces to be deleted...
Mar 30 13:54:28.702: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.3 before test
Mar 30 13:54:28.706: INFO: coredns-5ff46f8d6f-6r2mh from kube-system started at 2023-03-30 13:39:47 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.706: INFO: 	Container coredns ready: true, restart count 0
Mar 30 13:54:28.706: INFO: csi-plugin-2x6v8 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (4 container statuses recorded)
Mar 30 13:54:28.706: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 30 13:54:28.706: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 30 13:54:28.706: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 30 13:54:28.706: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 30 13:54:28.706: INFO: kube-flannel-ds-swjx9 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.706: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 30 13:54:28.706: INFO: kube-proxy-worker-rxqts from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.706: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 30 13:54:28.706: INFO: storage-auto-expander-7fd5f8f78-xcqss from kube-system started at 2023-03-30 13:51:24 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.706: INFO: 	Container storage-auto-expander ready: true, restart count 0
Mar 30 13:54:28.706: INFO: storage-cnfs-5bbdf677b6-s48rj from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.706: INFO: 	Container storage-cnfs ready: true, restart count 0
Mar 30 13:54:28.706: INFO: storage-monitor-8554bcf4c7-76bcg from kube-system started at 2023-03-30 13:51:24 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.706: INFO: 	Container storage-monitor ready: true, restart count 0
Mar 30 13:54:28.706: INFO: storage-snapshot-manager-55c574dc6c-ths98 from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.706: INFO: 	Container storage-snapshot-manager ready: true, restart count 0
Mar 30 13:54:28.706: INFO: sonobuoy-e2e-job-5f38eb9e10894fe7 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:54:28.706: INFO: 	Container e2e ready: true, restart count 0
Mar 30 13:54:28.706: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:54:28.706: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-bn4wq from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:54:28.706: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:54:28.706: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 13:54:28.706: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.4 before test
Mar 30 13:54:28.710: INFO: alicloud-monitor-controller-77f876c7d8-kqc5x from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.710: INFO: 	Container alicloud-monitor-controller ready: true, restart count 0
Mar 30 13:54:28.710: INFO: coredns-5ff46f8d6f-llbsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.710: INFO: 	Container coredns ready: true, restart count 0
Mar 30 13:54:28.710: INFO: csi-plugin-pgrk2 from kube-system started at 2023-03-30 10:07:49 +0000 UTC (4 container statuses recorded)
Mar 30 13:54:28.710: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 30 13:54:28.710: INFO: csi-provisioner-648c597bcb-67xzk from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
Mar 30 13:54:28.710: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-nas-resizer ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-oss-provisioner ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 30 13:54:28.710: INFO: csi-provisioner-648c597bcb-jwm9r from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
Mar 30 13:54:28.710: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-nas-resizer ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-oss-provisioner ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 30 13:54:28.710: INFO: kube-flannel-ds-dftjj from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.710: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 30 13:54:28.710: INFO: kube-proxy-worker-2mrsr from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.710: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 30 13:54:28.710: INFO: metrics-server-5c58794dd-cbrsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.710: INFO: 	Container metrics-server ready: true, restart count 0
Mar 30 13:54:28.710: INFO: storage-operator-5f775996-dphqv from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.710: INFO: 	Container storage-operator ready: true, restart count 0
Mar 30 13:54:28.710: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-prg6s from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:54:28.710: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 30 13:54:28.710: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.5 before test
Mar 30 13:54:28.714: INFO: pod-init-ab306bab-cf5f-4f64-9d9b-fa8917a9323b from init-container-1066 started at 2023-03-30 13:53:46 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.714: INFO: 	Container run1 ready: false, restart count 0
Mar 30 13:54:28.714: INFO: csi-plugin-rgcxj from kube-system started at 2023-03-30 10:07:52 +0000 UTC (4 container statuses recorded)
Mar 30 13:54:28.714: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 30 13:54:28.714: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 30 13:54:28.714: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 30 13:54:28.714: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 30 13:54:28.714: INFO: kube-flannel-ds-dspdm from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.714: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 30 13:54:28.714: INFO: kube-proxy-worker-h688q from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.714: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 30 13:54:28.714: INFO: sonobuoy from sonobuoy started at 2023-03-30 13:01:31 +0000 UTC (1 container statuses recorded)
Mar 30 13:54:28.714: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 30 13:54:28.714: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-87ph2 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
Mar 30 13:54:28.714: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 30 13:54:28.714: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/30/23 13:54:28.715
Mar 30 13:54:28.718: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7290" to be "running"
Mar 30 13:54:28.719: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.383561ms
Mar 30 13:54:30.722: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004087364s
Mar 30 13:54:30.722: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/30/23 13:54:30.723
STEP: Trying to apply a random label on the found node. 03/30/23 13:54:30.729
STEP: verifying the node has the label kubernetes.io/e2e-e529dfb1-c436-4d55-9d42-50b0b0075044 95 03/30/23 13:54:30.751
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/30/23 13:54:30.752
Mar 30 13:54:30.755: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7290" to be "not pending"
Mar 30 13:54:30.757: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.218734ms
Mar 30 13:54:32.760: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.005117094s
Mar 30 13:54:32.760: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.0.5 on the node which pod4 resides and expect not scheduled 03/30/23 13:54:32.76
Mar 30 13:54:32.763: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7290" to be "not pending"
Mar 30 13:54:32.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556712ms
Mar 30 13:54:34.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004239117s
Mar 30 13:54:36.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003803743s
Mar 30 13:54:38.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004896691s
Mar 30 13:54:40.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004426695s
Mar 30 13:54:42.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00447798s
Mar 30 13:54:44.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005067877s
Mar 30 13:54:46.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.003461253s
Mar 30 13:54:48.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004228906s
Mar 30 13:54:50.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004460163s
Mar 30 13:54:52.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004621591s
Mar 30 13:54:54.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005378449s
Mar 30 13:54:56.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.003544963s
Mar 30 13:54:58.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.00372283s
Mar 30 13:55:00.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.003682792s
Mar 30 13:55:02.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004285443s
Mar 30 13:55:04.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004338812s
Mar 30 13:55:06.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.003497087s
Mar 30 13:55:08.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005348364s
Mar 30 13:55:10.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004581298s
Mar 30 13:55:12.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004653215s
Mar 30 13:55:14.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004140149s
Mar 30 13:55:16.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.003742101s
Mar 30 13:55:18.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.00430045s
Mar 30 13:55:20.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.00404697s
Mar 30 13:55:22.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.003731118s
Mar 30 13:55:24.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004698735s
Mar 30 13:55:26.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.003566551s
Mar 30 13:55:28.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005613222s
Mar 30 13:55:30.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005349264s
Mar 30 13:55:32.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004383631s
Mar 30 13:55:34.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004465668s
Mar 30 13:55:36.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.003699483s
Mar 30 13:55:38.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004229253s
Mar 30 13:55:40.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004389602s
Mar 30 13:55:42.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004329148s
Mar 30 13:55:44.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004538249s
Mar 30 13:55:46.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004811632s
Mar 30 13:55:48.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004321936s
Mar 30 13:55:50.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004240531s
Mar 30 13:55:52.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004376791s
Mar 30 13:55:54.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004409256s
Mar 30 13:55:56.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.003599727s
Mar 30 13:55:58.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.003678899s
Mar 30 13:56:00.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00367936s
Mar 30 13:56:02.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.003560257s
Mar 30 13:56:04.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.0057709s
Mar 30 13:56:06.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.003512676s
Mar 30 13:56:08.769: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005906217s
Mar 30 13:56:10.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.004375706s
Mar 30 13:56:12.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.00448891s
Mar 30 13:56:14.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005258319s
Mar 30 13:56:16.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.003424571s
Mar 30 13:56:18.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005394196s
Mar 30 13:56:20.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.004514778s
Mar 30 13:56:22.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004525149s
Mar 30 13:56:24.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004433804s
Mar 30 13:56:26.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.0032361s
Mar 30 13:56:28.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004627498s
Mar 30 13:56:30.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005668049s
Mar 30 13:56:32.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004577779s
Mar 30 13:56:34.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005335469s
Mar 30 13:56:36.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.003887402s
Mar 30 13:56:38.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.0045264s
Mar 30 13:56:40.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005291334s
Mar 30 13:56:42.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004336406s
Mar 30 13:56:44.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.004446657s
Mar 30 13:56:46.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.00322652s
Mar 30 13:56:48.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005170015s
Mar 30 13:56:50.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.004264613s
Mar 30 13:56:52.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.004698212s
Mar 30 13:56:54.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.00446596s
Mar 30 13:56:56.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.003452746s
Mar 30 13:56:58.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.004142124s
Mar 30 13:57:00.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.003996355s
Mar 30 13:57:02.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.004479993s
Mar 30 13:57:04.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.004353636s
Mar 30 13:57:06.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.003354116s
Mar 30 13:57:08.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.004558785s
Mar 30 13:57:10.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.005433312s
Mar 30 13:57:12.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.004272392s
Mar 30 13:57:14.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.004581721s
Mar 30 13:57:16.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.003463621s
Mar 30 13:57:18.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005471803s
Mar 30 13:57:20.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.004558198s
Mar 30 13:57:22.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.004612487s
Mar 30 13:57:24.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.005177082s
Mar 30 13:57:26.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.003344381s
Mar 30 13:57:28.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.004467447s
Mar 30 13:57:30.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.005368338s
Mar 30 13:57:32.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.004566913s
Mar 30 13:57:34.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005222041s
Mar 30 13:57:36.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.00353522s
Mar 30 13:57:38.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005202083s
Mar 30 13:57:40.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.004478058s
Mar 30 13:57:42.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.004306069s
Mar 30 13:57:44.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.004291352s
Mar 30 13:57:46.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.003582922s
Mar 30 13:57:48.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.005377533s
Mar 30 13:57:50.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.004464495s
Mar 30 13:57:52.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.004219288s
Mar 30 13:57:54.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.004572413s
Mar 30 13:57:56.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.003967412s
Mar 30 13:57:58.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.003958437s
Mar 30 13:58:00.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.004252654s
Mar 30 13:58:02.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.004376202s
Mar 30 13:58:04.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005036711s
Mar 30 13:58:06.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.003333974s
Mar 30 13:58:08.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.004212096s
Mar 30 13:58:10.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.004317145s
Mar 30 13:58:12.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004347161s
Mar 30 13:58:14.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.00433224s
Mar 30 13:58:16.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.003312131s
Mar 30 13:58:18.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.00468864s
Mar 30 13:58:20.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.00531028s
Mar 30 13:58:22.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.003945887s
Mar 30 13:58:24.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.004266251s
Mar 30 13:58:26.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.003487923s
Mar 30 13:58:28.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004511924s
Mar 30 13:58:30.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.005337287s
Mar 30 13:58:32.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.004259907s
Mar 30 13:58:34.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.004840605s
Mar 30 13:58:36.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.003438958s
Mar 30 13:58:38.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.004180412s
Mar 30 13:58:40.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.00424261s
Mar 30 13:58:42.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.004112126s
Mar 30 13:58:44.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.004137486s
Mar 30 13:58:46.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.003207789s
Mar 30 13:58:48.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.004155373s
Mar 30 13:58:50.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.004383139s
Mar 30 13:58:52.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.004750055s
Mar 30 13:58:54.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.00419695s
Mar 30 13:58:56.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004119522s
Mar 30 13:58:58.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.004620086s
Mar 30 13:59:00.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005703482s
Mar 30 13:59:02.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.003649495s
Mar 30 13:59:04.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.005380192s
Mar 30 13:59:06.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.003723803s
Mar 30 13:59:08.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004428735s
Mar 30 13:59:10.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.005070104s
Mar 30 13:59:12.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004326959s
Mar 30 13:59:14.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.004455018s
Mar 30 13:59:16.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.003756969s
Mar 30 13:59:18.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004263684s
Mar 30 13:59:20.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.004236051s
Mar 30 13:59:22.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.00421846s
Mar 30 13:59:24.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.004207237s
Mar 30 13:59:26.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.003596889s
Mar 30 13:59:28.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.005426763s
Mar 30 13:59:30.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.003950153s
Mar 30 13:59:32.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.004344683s
Mar 30 13:59:32.769: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.0059247s
STEP: removing the label kubernetes.io/e2e-e529dfb1-c436-4d55-9d42-50b0b0075044 off the node cn-hongkong.192.168.0.5 03/30/23 13:59:32.769
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e529dfb1-c436-4d55-9d42-50b0b0075044 03/30/23 13:59:32.775
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 13:59:32.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7290" for this suite. 03/30/23 13:59:32.779
------------------------------
â€¢ [SLOW TEST] [304.094 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:54:28.688
    Mar 30 13:54:28.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename sched-pred 03/30/23 13:54:28.688
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:54:28.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:54:28.695
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 30 13:54:28.696: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 30 13:54:28.700: INFO: Waiting for terminating namespaces to be deleted...
    Mar 30 13:54:28.702: INFO: 
    Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.3 before test
    Mar 30 13:54:28.706: INFO: coredns-5ff46f8d6f-6r2mh from kube-system started at 2023-03-30 13:39:47 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.706: INFO: 	Container coredns ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: csi-plugin-2x6v8 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (4 container statuses recorded)
    Mar 30 13:54:28.706: INFO: 	Container csi-plugin ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: 	Container disk-driver-registrar ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: 	Container nas-driver-registrar ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: 	Container oss-driver-registrar ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: kube-flannel-ds-swjx9 from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.706: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: kube-proxy-worker-rxqts from kube-system started at 2023-03-30 10:07:53 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.706: INFO: 	Container kube-proxy-worker ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: storage-auto-expander-7fd5f8f78-xcqss from kube-system started at 2023-03-30 13:51:24 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.706: INFO: 	Container storage-auto-expander ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: storage-cnfs-5bbdf677b6-s48rj from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.706: INFO: 	Container storage-cnfs ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: storage-monitor-8554bcf4c7-76bcg from kube-system started at 2023-03-30 13:51:24 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.706: INFO: 	Container storage-monitor ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: storage-snapshot-manager-55c574dc6c-ths98 from kube-system started at 2023-03-30 10:09:05 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.706: INFO: 	Container storage-snapshot-manager ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: sonobuoy-e2e-job-5f38eb9e10894fe7 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:54:28.706: INFO: 	Container e2e ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-bn4wq from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:54:28.706: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 30 13:54:28.706: INFO: 
    Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.4 before test
    Mar 30 13:54:28.710: INFO: alicloud-monitor-controller-77f876c7d8-kqc5x from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.710: INFO: 	Container alicloud-monitor-controller ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: coredns-5ff46f8d6f-llbsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.710: INFO: 	Container coredns ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: csi-plugin-pgrk2 from kube-system started at 2023-03-30 10:07:49 +0000 UTC (4 container statuses recorded)
    Mar 30 13:54:28.710: INFO: 	Container csi-plugin ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container disk-driver-registrar ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container nas-driver-registrar ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container oss-driver-registrar ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: csi-provisioner-648c597bcb-67xzk from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
    Mar 30 13:54:28.710: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-disk-attacher ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-disk-provisioner ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-disk-resizer ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-nas-provisioner ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-nas-resizer ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-oss-provisioner ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-snapshot-controller ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: csi-provisioner-648c597bcb-jwm9r from kube-system started at 2023-03-30 10:07:55 +0000 UTC (9 container statuses recorded)
    Mar 30 13:54:28.710: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-disk-attacher ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-disk-provisioner ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-disk-resizer ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-nas-provisioner ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-nas-resizer ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-oss-provisioner ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container external-snapshot-controller ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: kube-flannel-ds-dftjj from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.710: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: kube-proxy-worker-2mrsr from kube-system started at 2023-03-30 10:07:49 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.710: INFO: 	Container kube-proxy-worker ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: metrics-server-5c58794dd-cbrsl from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.710: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: storage-operator-5f775996-dphqv from kube-system started at 2023-03-30 10:07:55 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.710: INFO: 	Container storage-operator ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-prg6s from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:54:28.710: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 30 13:54:28.710: INFO: 
    Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.5 before test
    Mar 30 13:54:28.714: INFO: pod-init-ab306bab-cf5f-4f64-9d9b-fa8917a9323b from init-container-1066 started at 2023-03-30 13:53:46 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.714: INFO: 	Container run1 ready: false, restart count 0
    Mar 30 13:54:28.714: INFO: csi-plugin-rgcxj from kube-system started at 2023-03-30 10:07:52 +0000 UTC (4 container statuses recorded)
    Mar 30 13:54:28.714: INFO: 	Container csi-plugin ready: true, restart count 0
    Mar 30 13:54:28.714: INFO: 	Container disk-driver-registrar ready: true, restart count 0
    Mar 30 13:54:28.714: INFO: 	Container nas-driver-registrar ready: true, restart count 0
    Mar 30 13:54:28.714: INFO: 	Container oss-driver-registrar ready: true, restart count 0
    Mar 30 13:54:28.714: INFO: kube-flannel-ds-dspdm from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.714: INFO: 	Container kube-flannel ready: true, restart count 0
    Mar 30 13:54:28.714: INFO: kube-proxy-worker-h688q from kube-system started at 2023-03-30 10:07:52 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.714: INFO: 	Container kube-proxy-worker ready: true, restart count 0
    Mar 30 13:54:28.714: INFO: sonobuoy from sonobuoy started at 2023-03-30 13:01:31 +0000 UTC (1 container statuses recorded)
    Mar 30 13:54:28.714: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 30 13:54:28.714: INFO: sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-87ph2 from sonobuoy started at 2023-03-30 13:08:30 +0000 UTC (2 container statuses recorded)
    Mar 30 13:54:28.714: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 30 13:54:28.714: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/30/23 13:54:28.715
    Mar 30 13:54:28.718: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7290" to be "running"
    Mar 30 13:54:28.719: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.383561ms
    Mar 30 13:54:30.722: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004087364s
    Mar 30 13:54:30.722: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/30/23 13:54:30.723
    STEP: Trying to apply a random label on the found node. 03/30/23 13:54:30.729
    STEP: verifying the node has the label kubernetes.io/e2e-e529dfb1-c436-4d55-9d42-50b0b0075044 95 03/30/23 13:54:30.751
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/30/23 13:54:30.752
    Mar 30 13:54:30.755: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7290" to be "not pending"
    Mar 30 13:54:30.757: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.218734ms
    Mar 30 13:54:32.760: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.005117094s
    Mar 30 13:54:32.760: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.0.5 on the node which pod4 resides and expect not scheduled 03/30/23 13:54:32.76
    Mar 30 13:54:32.763: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7290" to be "not pending"
    Mar 30 13:54:32.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556712ms
    Mar 30 13:54:34.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004239117s
    Mar 30 13:54:36.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003803743s
    Mar 30 13:54:38.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004896691s
    Mar 30 13:54:40.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004426695s
    Mar 30 13:54:42.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00447798s
    Mar 30 13:54:44.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005067877s
    Mar 30 13:54:46.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.003461253s
    Mar 30 13:54:48.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004228906s
    Mar 30 13:54:50.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004460163s
    Mar 30 13:54:52.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004621591s
    Mar 30 13:54:54.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005378449s
    Mar 30 13:54:56.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.003544963s
    Mar 30 13:54:58.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.00372283s
    Mar 30 13:55:00.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.003682792s
    Mar 30 13:55:02.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004285443s
    Mar 30 13:55:04.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004338812s
    Mar 30 13:55:06.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.003497087s
    Mar 30 13:55:08.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005348364s
    Mar 30 13:55:10.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004581298s
    Mar 30 13:55:12.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004653215s
    Mar 30 13:55:14.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004140149s
    Mar 30 13:55:16.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.003742101s
    Mar 30 13:55:18.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.00430045s
    Mar 30 13:55:20.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.00404697s
    Mar 30 13:55:22.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.003731118s
    Mar 30 13:55:24.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004698735s
    Mar 30 13:55:26.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.003566551s
    Mar 30 13:55:28.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005613222s
    Mar 30 13:55:30.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005349264s
    Mar 30 13:55:32.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004383631s
    Mar 30 13:55:34.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004465668s
    Mar 30 13:55:36.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.003699483s
    Mar 30 13:55:38.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004229253s
    Mar 30 13:55:40.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004389602s
    Mar 30 13:55:42.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004329148s
    Mar 30 13:55:44.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004538249s
    Mar 30 13:55:46.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004811632s
    Mar 30 13:55:48.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004321936s
    Mar 30 13:55:50.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004240531s
    Mar 30 13:55:52.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004376791s
    Mar 30 13:55:54.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004409256s
    Mar 30 13:55:56.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.003599727s
    Mar 30 13:55:58.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.003678899s
    Mar 30 13:56:00.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00367936s
    Mar 30 13:56:02.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.003560257s
    Mar 30 13:56:04.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.0057709s
    Mar 30 13:56:06.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.003512676s
    Mar 30 13:56:08.769: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005906217s
    Mar 30 13:56:10.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.004375706s
    Mar 30 13:56:12.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.00448891s
    Mar 30 13:56:14.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005258319s
    Mar 30 13:56:16.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.003424571s
    Mar 30 13:56:18.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005394196s
    Mar 30 13:56:20.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.004514778s
    Mar 30 13:56:22.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004525149s
    Mar 30 13:56:24.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004433804s
    Mar 30 13:56:26.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.0032361s
    Mar 30 13:56:28.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004627498s
    Mar 30 13:56:30.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005668049s
    Mar 30 13:56:32.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004577779s
    Mar 30 13:56:34.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005335469s
    Mar 30 13:56:36.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.003887402s
    Mar 30 13:56:38.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.0045264s
    Mar 30 13:56:40.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005291334s
    Mar 30 13:56:42.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004336406s
    Mar 30 13:56:44.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.004446657s
    Mar 30 13:56:46.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.00322652s
    Mar 30 13:56:48.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005170015s
    Mar 30 13:56:50.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.004264613s
    Mar 30 13:56:52.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.004698212s
    Mar 30 13:56:54.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.00446596s
    Mar 30 13:56:56.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.003452746s
    Mar 30 13:56:58.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.004142124s
    Mar 30 13:57:00.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.003996355s
    Mar 30 13:57:02.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.004479993s
    Mar 30 13:57:04.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.004353636s
    Mar 30 13:57:06.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.003354116s
    Mar 30 13:57:08.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.004558785s
    Mar 30 13:57:10.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.005433312s
    Mar 30 13:57:12.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.004272392s
    Mar 30 13:57:14.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.004581721s
    Mar 30 13:57:16.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.003463621s
    Mar 30 13:57:18.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005471803s
    Mar 30 13:57:20.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.004558198s
    Mar 30 13:57:22.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.004612487s
    Mar 30 13:57:24.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.005177082s
    Mar 30 13:57:26.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.003344381s
    Mar 30 13:57:28.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.004467447s
    Mar 30 13:57:30.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.005368338s
    Mar 30 13:57:32.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.004566913s
    Mar 30 13:57:34.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005222041s
    Mar 30 13:57:36.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.00353522s
    Mar 30 13:57:38.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005202083s
    Mar 30 13:57:40.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.004478058s
    Mar 30 13:57:42.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.004306069s
    Mar 30 13:57:44.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.004291352s
    Mar 30 13:57:46.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.003582922s
    Mar 30 13:57:48.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.005377533s
    Mar 30 13:57:50.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.004464495s
    Mar 30 13:57:52.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.004219288s
    Mar 30 13:57:54.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.004572413s
    Mar 30 13:57:56.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.003967412s
    Mar 30 13:57:58.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.003958437s
    Mar 30 13:58:00.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.004252654s
    Mar 30 13:58:02.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.004376202s
    Mar 30 13:58:04.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005036711s
    Mar 30 13:58:06.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.003333974s
    Mar 30 13:58:08.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.004212096s
    Mar 30 13:58:10.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.004317145s
    Mar 30 13:58:12.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004347161s
    Mar 30 13:58:14.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.00433224s
    Mar 30 13:58:16.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.003312131s
    Mar 30 13:58:18.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.00468864s
    Mar 30 13:58:20.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.00531028s
    Mar 30 13:58:22.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.003945887s
    Mar 30 13:58:24.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.004266251s
    Mar 30 13:58:26.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.003487923s
    Mar 30 13:58:28.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004511924s
    Mar 30 13:58:30.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.005337287s
    Mar 30 13:58:32.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.004259907s
    Mar 30 13:58:34.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.004840605s
    Mar 30 13:58:36.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.003438958s
    Mar 30 13:58:38.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.004180412s
    Mar 30 13:58:40.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.00424261s
    Mar 30 13:58:42.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.004112126s
    Mar 30 13:58:44.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.004137486s
    Mar 30 13:58:46.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.003207789s
    Mar 30 13:58:48.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.004155373s
    Mar 30 13:58:50.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.004383139s
    Mar 30 13:58:52.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.004750055s
    Mar 30 13:58:54.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.00419695s
    Mar 30 13:58:56.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004119522s
    Mar 30 13:58:58.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.004620086s
    Mar 30 13:59:00.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005703482s
    Mar 30 13:59:02.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.003649495s
    Mar 30 13:59:04.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.005380192s
    Mar 30 13:59:06.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.003723803s
    Mar 30 13:59:08.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004428735s
    Mar 30 13:59:10.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.005070104s
    Mar 30 13:59:12.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004326959s
    Mar 30 13:59:14.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.004455018s
    Mar 30 13:59:16.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.003756969s
    Mar 30 13:59:18.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004263684s
    Mar 30 13:59:20.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.004236051s
    Mar 30 13:59:22.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.00421846s
    Mar 30 13:59:24.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.004207237s
    Mar 30 13:59:26.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.003596889s
    Mar 30 13:59:28.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.005426763s
    Mar 30 13:59:30.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.003950153s
    Mar 30 13:59:32.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.004344683s
    Mar 30 13:59:32.769: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.0059247s
    STEP: removing the label kubernetes.io/e2e-e529dfb1-c436-4d55-9d42-50b0b0075044 off the node cn-hongkong.192.168.0.5 03/30/23 13:59:32.769
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-e529dfb1-c436-4d55-9d42-50b0b0075044 03/30/23 13:59:32.775
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:59:32.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7290" for this suite. 03/30/23 13:59:32.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:59:32.782
Mar 30 13:59:32.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 13:59:32.783
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:59:32.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:59:32.79
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 13:59:32.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5878" for this suite. 03/30/23 13:59:32.808
------------------------------
â€¢ [0.029 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:59:32.782
    Mar 30 13:59:32.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 13:59:32.783
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:59:32.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:59:32.79
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:59:32.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5878" for this suite. 03/30/23 13:59:32.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:59:32.811
Mar 30 13:59:32.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename statefulset 03/30/23 13:59:32.812
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:59:32.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:59:32.818
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7765 03/30/23 13:59:32.819
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 03/30/23 13:59:32.822
STEP: Creating pod with conflicting port in namespace statefulset-7765 03/30/23 13:59:32.825
STEP: Waiting until pod test-pod will start running in namespace statefulset-7765 03/30/23 13:59:32.828
Mar 30 13:59:32.828: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7765" to be "running"
Mar 30 13:59:32.829: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.230808ms
Mar 30 13:59:34.832: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003772301s
Mar 30 13:59:34.832: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-7765 03/30/23 13:59:34.832
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7765 03/30/23 13:59:34.834
Mar 30 13:59:34.840: INFO: Observed stateful pod in namespace: statefulset-7765, name: ss-0, uid: 205c338b-2fce-43e8-b761-1ff0577af7bf, status phase: Pending. Waiting for statefulset controller to delete.
Mar 30 13:59:34.848: INFO: Observed stateful pod in namespace: statefulset-7765, name: ss-0, uid: 205c338b-2fce-43e8-b761-1ff0577af7bf, status phase: Failed. Waiting for statefulset controller to delete.
Mar 30 13:59:34.851: INFO: Observed stateful pod in namespace: statefulset-7765, name: ss-0, uid: 205c338b-2fce-43e8-b761-1ff0577af7bf, status phase: Failed. Waiting for statefulset controller to delete.
Mar 30 13:59:34.853: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7765
STEP: Removing pod with conflicting port in namespace statefulset-7765 03/30/23 13:59:34.853
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7765 and will be in running state 03/30/23 13:59:34.858
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 30 13:59:36.862: INFO: Deleting all statefulset in ns statefulset-7765
Mar 30 13:59:36.863: INFO: Scaling statefulset ss to 0
Mar 30 13:59:46.870: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 13:59:46.871: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 30 13:59:46.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7765" for this suite. 03/30/23 13:59:46.881
------------------------------
â€¢ [SLOW TEST] [14.072 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:59:32.811
    Mar 30 13:59:32.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename statefulset 03/30/23 13:59:32.812
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:59:32.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:59:32.818
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7765 03/30/23 13:59:32.819
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 03/30/23 13:59:32.822
    STEP: Creating pod with conflicting port in namespace statefulset-7765 03/30/23 13:59:32.825
    STEP: Waiting until pod test-pod will start running in namespace statefulset-7765 03/30/23 13:59:32.828
    Mar 30 13:59:32.828: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7765" to be "running"
    Mar 30 13:59:32.829: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.230808ms
    Mar 30 13:59:34.832: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003772301s
    Mar 30 13:59:34.832: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-7765 03/30/23 13:59:34.832
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7765 03/30/23 13:59:34.834
    Mar 30 13:59:34.840: INFO: Observed stateful pod in namespace: statefulset-7765, name: ss-0, uid: 205c338b-2fce-43e8-b761-1ff0577af7bf, status phase: Pending. Waiting for statefulset controller to delete.
    Mar 30 13:59:34.848: INFO: Observed stateful pod in namespace: statefulset-7765, name: ss-0, uid: 205c338b-2fce-43e8-b761-1ff0577af7bf, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 30 13:59:34.851: INFO: Observed stateful pod in namespace: statefulset-7765, name: ss-0, uid: 205c338b-2fce-43e8-b761-1ff0577af7bf, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 30 13:59:34.853: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7765
    STEP: Removing pod with conflicting port in namespace statefulset-7765 03/30/23 13:59:34.853
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7765 and will be in running state 03/30/23 13:59:34.858
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 30 13:59:36.862: INFO: Deleting all statefulset in ns statefulset-7765
    Mar 30 13:59:36.863: INFO: Scaling statefulset ss to 0
    Mar 30 13:59:46.870: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 30 13:59:46.871: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 13:59:46.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7765" for this suite. 03/30/23 13:59:46.881
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 13:59:46.883
Mar 30 13:59:46.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 13:59:46.884
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:59:46.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:59:46.89
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 13:59:46.896
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:59:47.168
STEP: Deploying the webhook pod 03/30/23 13:59:47.172
STEP: Wait for the deployment to be ready 03/30/23 13:59:47.177
Mar 30 13:59:47.179: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 30 13:59:49.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:59:51.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:59:53.188: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 13:59:55.188: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/30/23 13:59:57.188
STEP: Verifying the service has paired with the endpoint 03/30/23 13:59:57.192
Mar 30 13:59:58.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Mar 30 13:59:58.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/30/23 14:00:03.699
STEP: Creating a custom resource that should be denied by the webhook 03/30/23 14:00:03.708
STEP: Creating a custom resource whose deletion would be denied by the webhook 03/30/23 14:00:05.73
STEP: Updating the custom resource with disallowed data should be denied 03/30/23 14:00:05.733
STEP: Deleting the custom resource should be denied 03/30/23 14:00:05.737
STEP: Remove the offending key and value from the custom resource data 03/30/23 14:00:05.74
STEP: Deleting the updated custom resource should be successful 03/30/23 14:00:05.744
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:00:06.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9003" for this suite. 03/30/23 14:00:06.275
STEP: Destroying namespace "webhook-9003-markers" for this suite. 03/30/23 14:00:06.279
------------------------------
â€¢ [SLOW TEST] [19.398 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 13:59:46.883
    Mar 30 13:59:46.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 13:59:46.884
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 13:59:46.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 13:59:46.89
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 13:59:46.896
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 13:59:47.168
    STEP: Deploying the webhook pod 03/30/23 13:59:47.172
    STEP: Wait for the deployment to be ready 03/30/23 13:59:47.177
    Mar 30 13:59:47.179: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Mar 30 13:59:49.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:59:51.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:59:53.188: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 13:59:55.188: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 13, 59, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/30/23 13:59:57.188
    STEP: Verifying the service has paired with the endpoint 03/30/23 13:59:57.192
    Mar 30 13:59:58.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Mar 30 13:59:58.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/30/23 14:00:03.699
    STEP: Creating a custom resource that should be denied by the webhook 03/30/23 14:00:03.708
    STEP: Creating a custom resource whose deletion would be denied by the webhook 03/30/23 14:00:05.73
    STEP: Updating the custom resource with disallowed data should be denied 03/30/23 14:00:05.733
    STEP: Deleting the custom resource should be denied 03/30/23 14:00:05.737
    STEP: Remove the offending key and value from the custom resource data 03/30/23 14:00:05.74
    STEP: Deleting the updated custom resource should be successful 03/30/23 14:00:05.744
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:00:06.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9003" for this suite. 03/30/23 14:00:06.275
    STEP: Destroying namespace "webhook-9003-markers" for this suite. 03/30/23 14:00:06.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:00:06.282
Mar 30 14:00:06.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename statefulset 03/30/23 14:00:06.283
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:00:06.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:00:06.291
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1018 03/30/23 14:00:06.293
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-1018 03/30/23 14:00:06.295
Mar 30 14:00:06.300: INFO: Found 0 stateful pods, waiting for 1
Mar 30 14:00:16.302: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 03/30/23 14:00:16.305
STEP: updating a scale subresource 03/30/23 14:00:16.307
STEP: verifying the statefulset Spec.Replicas was modified 03/30/23 14:00:16.31
STEP: Patch a scale subresource 03/30/23 14:00:16.311
STEP: verifying the statefulset Spec.Replicas was modified 03/30/23 14:00:16.315
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 30 14:00:16.317: INFO: Deleting all statefulset in ns statefulset-1018
Mar 30 14:00:16.318: INFO: Scaling statefulset ss to 0
Mar 30 14:00:26.329: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 14:00:26.331: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 30 14:00:26.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1018" for this suite. 03/30/23 14:00:26.338
------------------------------
â€¢ [SLOW TEST] [20.058 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:00:06.282
    Mar 30 14:00:06.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename statefulset 03/30/23 14:00:06.283
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:00:06.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:00:06.291
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1018 03/30/23 14:00:06.293
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-1018 03/30/23 14:00:06.295
    Mar 30 14:00:06.300: INFO: Found 0 stateful pods, waiting for 1
    Mar 30 14:00:16.302: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 03/30/23 14:00:16.305
    STEP: updating a scale subresource 03/30/23 14:00:16.307
    STEP: verifying the statefulset Spec.Replicas was modified 03/30/23 14:00:16.31
    STEP: Patch a scale subresource 03/30/23 14:00:16.311
    STEP: verifying the statefulset Spec.Replicas was modified 03/30/23 14:00:16.315
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 30 14:00:16.317: INFO: Deleting all statefulset in ns statefulset-1018
    Mar 30 14:00:16.318: INFO: Scaling statefulset ss to 0
    Mar 30 14:00:26.329: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 30 14:00:26.331: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:00:26.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1018" for this suite. 03/30/23 14:00:26.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:00:26.342
Mar 30 14:00:26.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 14:00:26.342
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:00:26.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:00:26.349
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 03/30/23 14:00:26.351
Mar 30 14:00:26.354: INFO: Waiting up to 5m0s for pod "pod-49785186-92af-4682-af42-5792d421710d" in namespace "emptydir-1970" to be "Succeeded or Failed"
Mar 30 14:00:26.356: INFO: Pod "pod-49785186-92af-4682-af42-5792d421710d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.45395ms
Mar 30 14:00:28.358: INFO: Pod "pod-49785186-92af-4682-af42-5792d421710d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003838611s
Mar 30 14:00:30.359: INFO: Pod "pod-49785186-92af-4682-af42-5792d421710d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00466998s
STEP: Saw pod success 03/30/23 14:00:30.359
Mar 30 14:00:30.359: INFO: Pod "pod-49785186-92af-4682-af42-5792d421710d" satisfied condition "Succeeded or Failed"
Mar 30 14:00:30.361: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-49785186-92af-4682-af42-5792d421710d container test-container: <nil>
STEP: delete the pod 03/30/23 14:00:30.372
Mar 30 14:00:30.377: INFO: Waiting for pod pod-49785186-92af-4682-af42-5792d421710d to disappear
Mar 30 14:00:30.378: INFO: Pod pod-49785186-92af-4682-af42-5792d421710d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 14:00:30.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1970" for this suite. 03/30/23 14:00:30.38
------------------------------
â€¢ [4.041 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:00:26.342
    Mar 30 14:00:26.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 14:00:26.342
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:00:26.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:00:26.349
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 03/30/23 14:00:26.351
    Mar 30 14:00:26.354: INFO: Waiting up to 5m0s for pod "pod-49785186-92af-4682-af42-5792d421710d" in namespace "emptydir-1970" to be "Succeeded or Failed"
    Mar 30 14:00:26.356: INFO: Pod "pod-49785186-92af-4682-af42-5792d421710d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.45395ms
    Mar 30 14:00:28.358: INFO: Pod "pod-49785186-92af-4682-af42-5792d421710d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003838611s
    Mar 30 14:00:30.359: INFO: Pod "pod-49785186-92af-4682-af42-5792d421710d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00466998s
    STEP: Saw pod success 03/30/23 14:00:30.359
    Mar 30 14:00:30.359: INFO: Pod "pod-49785186-92af-4682-af42-5792d421710d" satisfied condition "Succeeded or Failed"
    Mar 30 14:00:30.361: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-49785186-92af-4682-af42-5792d421710d container test-container: <nil>
    STEP: delete the pod 03/30/23 14:00:30.372
    Mar 30 14:00:30.377: INFO: Waiting for pod pod-49785186-92af-4682-af42-5792d421710d to disappear
    Mar 30 14:00:30.378: INFO: Pod pod-49785186-92af-4682-af42-5792d421710d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:00:30.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1970" for this suite. 03/30/23 14:00:30.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:00:30.383
Mar 30 14:00:30.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename endpointslicemirroring 03/30/23 14:00:30.384
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:00:30.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:00:30.39
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 03/30/23 14:00:30.396
Mar 30 14:00:30.400: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 03/30/23 14:00:32.402
Mar 30 14:00:32.406: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 03/30/23 14:00:34.408
Mar 30 14:00:34.413: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Mar 30 14:00:36.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-5188" for this suite. 03/30/23 14:00:36.418
------------------------------
â€¢ [SLOW TEST] [6.037 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:00:30.383
    Mar 30 14:00:30.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename endpointslicemirroring 03/30/23 14:00:30.384
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:00:30.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:00:30.39
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 03/30/23 14:00:30.396
    Mar 30 14:00:30.400: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 03/30/23 14:00:32.402
    Mar 30 14:00:32.406: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 03/30/23 14:00:34.408
    Mar 30 14:00:34.413: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:00:36.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-5188" for this suite. 03/30/23 14:00:36.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:00:36.422
Mar 30 14:00:36.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 14:00:36.422
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:00:36.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:00:36.429
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-6056 03/30/23 14:00:36.431
STEP: creating service affinity-nodeport-transition in namespace services-6056 03/30/23 14:00:36.431
STEP: creating replication controller affinity-nodeport-transition in namespace services-6056 03/30/23 14:00:36.436
I0330 14:00:36.440058      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6056, replica count: 3
I0330 14:00:39.490550      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 14:00:39.496: INFO: Creating new exec pod
Mar 30 14:00:39.499: INFO: Waiting up to 5m0s for pod "execpod-affinity9j6tr" in namespace "services-6056" to be "running"
Mar 30 14:00:39.500: INFO: Pod "execpod-affinity9j6tr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.555243ms
Mar 30 14:00:41.503: INFO: Pod "execpod-affinity9j6tr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004237915s
Mar 30 14:00:41.503: INFO: Pod "execpod-affinity9j6tr" satisfied condition "running"
Mar 30 14:00:42.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Mar 30 14:00:42.604: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 30 14:00:42.604: INFO: stdout: ""
Mar 30 14:00:42.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c nc -v -z -w 2 172.16.32.105 80'
Mar 30 14:00:42.694: INFO: stderr: "+ nc -v -z -w 2 172.16.32.105 80\nConnection to 172.16.32.105 80 port [tcp/http] succeeded!\n"
Mar 30 14:00:42.694: INFO: stdout: ""
Mar 30 14:00:42.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c nc -v -z -w 2 192.168.0.5 31264'
Mar 30 14:00:42.785: INFO: stderr: "+ nc -v -z -w 2 192.168.0.5 31264\nConnection to 192.168.0.5 31264 port [tcp/*] succeeded!\n"
Mar 30 14:00:42.785: INFO: stdout: ""
Mar 30 14:00:42.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c nc -v -z -w 2 192.168.0.3 31264'
Mar 30 14:00:42.877: INFO: stderr: "+ nc -v -z -w 2 192.168.0.3 31264\nConnection to 192.168.0.3 31264 port [tcp/*] succeeded!\n"
Mar 30 14:00:42.877: INFO: stdout: ""
Mar 30 14:00:42.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.3:31264/ ; done'
Mar 30 14:00:54.067: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n"
Mar 30 14:00:54.067: INFO: stdout: "\n\n\naffinity-nodeport-transition-hpvgd\n\n\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\n\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn"
Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-2cntn
Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-2cntn
Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-j2q7x
Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-2cntn
Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-j2q7x
Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-2cntn
Mar 30 14:01:24.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.3:31264/ ; done'
Mar 30 14:01:24.218: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n"
Mar 30 14:01:24.218: INFO: stdout: "\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x"
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-j2q7x
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-2cntn
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-j2q7x
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-2cntn
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-j2q7x
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-2cntn
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-j2q7x
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-2cntn
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-j2q7x
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-2cntn
Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-j2q7x
Mar 30 14:01:24.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.3:31264/ ; done'
Mar 30 14:01:24.379: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n"
Mar 30 14:01:24.379: INFO: stdout: "\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd"
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
Mar 30 14:01:24.379: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6056, will wait for the garbage collector to delete the pods 03/30/23 14:01:24.384
Mar 30 14:01:24.439: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.712354ms
Mar 30 14:01:24.539: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.518962ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 14:01:26.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6056" for this suite. 03/30/23 14:01:26.352
------------------------------
â€¢ [SLOW TEST] [49.933 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:00:36.422
    Mar 30 14:00:36.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 14:00:36.422
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:00:36.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:00:36.429
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-6056 03/30/23 14:00:36.431
    STEP: creating service affinity-nodeport-transition in namespace services-6056 03/30/23 14:00:36.431
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6056 03/30/23 14:00:36.436
    I0330 14:00:36.440058      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6056, replica count: 3
    I0330 14:00:39.490550      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 30 14:00:39.496: INFO: Creating new exec pod
    Mar 30 14:00:39.499: INFO: Waiting up to 5m0s for pod "execpod-affinity9j6tr" in namespace "services-6056" to be "running"
    Mar 30 14:00:39.500: INFO: Pod "execpod-affinity9j6tr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.555243ms
    Mar 30 14:00:41.503: INFO: Pod "execpod-affinity9j6tr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004237915s
    Mar 30 14:00:41.503: INFO: Pod "execpod-affinity9j6tr" satisfied condition "running"
    Mar 30 14:00:42.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Mar 30 14:00:42.604: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Mar 30 14:00:42.604: INFO: stdout: ""
    Mar 30 14:00:42.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c nc -v -z -w 2 172.16.32.105 80'
    Mar 30 14:00:42.694: INFO: stderr: "+ nc -v -z -w 2 172.16.32.105 80\nConnection to 172.16.32.105 80 port [tcp/http] succeeded!\n"
    Mar 30 14:00:42.694: INFO: stdout: ""
    Mar 30 14:00:42.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c nc -v -z -w 2 192.168.0.5 31264'
    Mar 30 14:00:42.785: INFO: stderr: "+ nc -v -z -w 2 192.168.0.5 31264\nConnection to 192.168.0.5 31264 port [tcp/*] succeeded!\n"
    Mar 30 14:00:42.785: INFO: stdout: ""
    Mar 30 14:00:42.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c nc -v -z -w 2 192.168.0.3 31264'
    Mar 30 14:00:42.877: INFO: stderr: "+ nc -v -z -w 2 192.168.0.3 31264\nConnection to 192.168.0.3 31264 port [tcp/*] succeeded!\n"
    Mar 30 14:00:42.877: INFO: stdout: ""
    Mar 30 14:00:42.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.3:31264/ ; done'
    Mar 30 14:00:54.067: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n"
    Mar 30 14:00:54.067: INFO: stdout: "\n\n\naffinity-nodeport-transition-hpvgd\n\n\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\n\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn"
    Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-2cntn
    Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-2cntn
    Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-j2q7x
    Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-2cntn
    Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-j2q7x
    Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:00:54.067: INFO: Received response from host: affinity-nodeport-transition-2cntn
    Mar 30 14:01:24.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.3:31264/ ; done'
    Mar 30 14:01:24.218: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n"
    Mar 30 14:01:24.218: INFO: stdout: "\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-2cntn\naffinity-nodeport-transition-j2q7x"
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-j2q7x
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-2cntn
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-j2q7x
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-2cntn
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-j2q7x
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-2cntn
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-j2q7x
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-2cntn
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-j2q7x
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-2cntn
    Mar 30 14:01:24.218: INFO: Received response from host: affinity-nodeport-transition-j2q7x
    Mar 30 14:01:24.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-6056 exec execpod-affinity9j6tr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.3:31264/ ; done'
    Mar 30 14:01:24.379: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.3:31264/\n"
    Mar 30 14:01:24.379: INFO: stdout: "\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd\naffinity-nodeport-transition-hpvgd"
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Received response from host: affinity-nodeport-transition-hpvgd
    Mar 30 14:01:24.379: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6056, will wait for the garbage collector to delete the pods 03/30/23 14:01:24.384
    Mar 30 14:01:24.439: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.712354ms
    Mar 30 14:01:24.539: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.518962ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:01:26.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6056" for this suite. 03/30/23 14:01:26.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:01:26.356
Mar 30 14:01:26.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename resourcequota 03/30/23 14:01:26.356
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:01:26.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:01:26.364
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 03/30/23 14:01:26.365
STEP: Getting a ResourceQuota 03/30/23 14:01:26.367
STEP: Listing all ResourceQuotas with LabelSelector 03/30/23 14:01:26.369
STEP: Patching the ResourceQuota 03/30/23 14:01:26.371
STEP: Deleting a Collection of ResourceQuotas 03/30/23 14:01:26.374
STEP: Verifying the deleted ResourceQuota 03/30/23 14:01:26.378
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 30 14:01:26.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9134" for this suite. 03/30/23 14:01:26.382
------------------------------
â€¢ [0.028 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:01:26.356
    Mar 30 14:01:26.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename resourcequota 03/30/23 14:01:26.356
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:01:26.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:01:26.364
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 03/30/23 14:01:26.365
    STEP: Getting a ResourceQuota 03/30/23 14:01:26.367
    STEP: Listing all ResourceQuotas with LabelSelector 03/30/23 14:01:26.369
    STEP: Patching the ResourceQuota 03/30/23 14:01:26.371
    STEP: Deleting a Collection of ResourceQuotas 03/30/23 14:01:26.374
    STEP: Verifying the deleted ResourceQuota 03/30/23 14:01:26.378
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:01:26.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9134" for this suite. 03/30/23 14:01:26.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:01:26.384
Mar 30 14:01:26.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 14:01:26.385
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:01:26.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:01:26.392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 03/30/23 14:01:26.393
Mar 30 14:01:26.394: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8765 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 03/30/23 14:01:26.436
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 14:01:26.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8765" for this suite. 03/30/23 14:01:26.443
------------------------------
â€¢ [0.061 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:01:26.384
    Mar 30 14:01:26.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 14:01:26.385
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:01:26.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:01:26.392
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 03/30/23 14:01:26.393
    Mar 30 14:01:26.394: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-8765 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 03/30/23 14:01:26.436
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:01:26.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8765" for this suite. 03/30/23 14:01:26.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:01:26.447
Mar 30 14:01:26.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-probe 03/30/23 14:01:26.447
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:01:26.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:01:26.454
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-23d51d90-c591-433b-af4d-3d32e6886ba1 in namespace container-probe-9880 03/30/23 14:01:26.455
Mar 30 14:01:26.459: INFO: Waiting up to 5m0s for pod "busybox-23d51d90-c591-433b-af4d-3d32e6886ba1" in namespace "container-probe-9880" to be "not pending"
Mar 30 14:01:26.461: INFO: Pod "busybox-23d51d90-c591-433b-af4d-3d32e6886ba1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.627506ms
Mar 30 14:01:28.463: INFO: Pod "busybox-23d51d90-c591-433b-af4d-3d32e6886ba1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004207939s
Mar 30 14:01:28.463: INFO: Pod "busybox-23d51d90-c591-433b-af4d-3d32e6886ba1" satisfied condition "not pending"
Mar 30 14:01:28.463: INFO: Started pod busybox-23d51d90-c591-433b-af4d-3d32e6886ba1 in namespace container-probe-9880
STEP: checking the pod's current state and verifying that restartCount is present 03/30/23 14:01:28.463
Mar 30 14:01:28.465: INFO: Initial restart count of pod busybox-23d51d90-c591-433b-af4d-3d32e6886ba1 is 0
STEP: deleting the pod 03/30/23 14:05:28.778
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 30 14:05:28.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9880" for this suite. 03/30/23 14:05:28.786
------------------------------
â€¢ [SLOW TEST] [242.342 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:01:26.447
    Mar 30 14:01:26.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-probe 03/30/23 14:01:26.447
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:01:26.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:01:26.454
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-23d51d90-c591-433b-af4d-3d32e6886ba1 in namespace container-probe-9880 03/30/23 14:01:26.455
    Mar 30 14:01:26.459: INFO: Waiting up to 5m0s for pod "busybox-23d51d90-c591-433b-af4d-3d32e6886ba1" in namespace "container-probe-9880" to be "not pending"
    Mar 30 14:01:26.461: INFO: Pod "busybox-23d51d90-c591-433b-af4d-3d32e6886ba1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.627506ms
    Mar 30 14:01:28.463: INFO: Pod "busybox-23d51d90-c591-433b-af4d-3d32e6886ba1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004207939s
    Mar 30 14:01:28.463: INFO: Pod "busybox-23d51d90-c591-433b-af4d-3d32e6886ba1" satisfied condition "not pending"
    Mar 30 14:01:28.463: INFO: Started pod busybox-23d51d90-c591-433b-af4d-3d32e6886ba1 in namespace container-probe-9880
    STEP: checking the pod's current state and verifying that restartCount is present 03/30/23 14:01:28.463
    Mar 30 14:01:28.465: INFO: Initial restart count of pod busybox-23d51d90-c591-433b-af4d-3d32e6886ba1 is 0
    STEP: deleting the pod 03/30/23 14:05:28.778
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:05:28.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9880" for this suite. 03/30/23 14:05:28.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:05:28.789
Mar 30 14:05:28.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-probe 03/30/23 14:05:28.79
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:05:28.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:05:28.796
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Mar 30 14:05:28.801: INFO: Waiting up to 5m0s for pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429" in namespace "container-probe-4171" to be "running and ready"
Mar 30 14:05:28.803: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Pending", Reason="", readiness=false. Elapsed: 1.518147ms
Mar 30 14:05:28.803: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 14:05:30.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 2.003653586s
Mar 30 14:05:30.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:32.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 4.003577163s
Mar 30 14:05:32.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:34.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 6.004388718s
Mar 30 14:05:34.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:36.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 8.003867076s
Mar 30 14:05:36.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:38.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 10.00398477s
Mar 30 14:05:38.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:40.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 12.004727314s
Mar 30 14:05:40.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:42.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 14.003687768s
Mar 30 14:05:42.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:44.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 16.004621015s
Mar 30 14:05:44.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:46.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 18.003817458s
Mar 30 14:05:46.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:48.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 20.004841399s
Mar 30 14:05:48.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:50.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 22.00462054s
Mar 30 14:05:50.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:52.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 24.004637835s
Mar 30 14:05:52.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:54.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 26.00352587s
Mar 30 14:05:54.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:56.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 28.004061129s
Mar 30 14:05:56.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:05:58.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 30.004805609s
Mar 30 14:05:58.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
Mar 30 14:06:00.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=true. Elapsed: 32.004702356s
Mar 30 14:06:00.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = true)
Mar 30 14:06:00.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429" satisfied condition "running and ready"
Mar 30 14:06:00.808: INFO: Container started at 2023-03-30 14:05:29 +0000 UTC, pod became ready at 2023-03-30 14:05:59 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 30 14:06:00.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4171" for this suite. 03/30/23 14:06:00.81
------------------------------
â€¢ [SLOW TEST] [32.023 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:05:28.789
    Mar 30 14:05:28.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-probe 03/30/23 14:05:28.79
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:05:28.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:05:28.796
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Mar 30 14:05:28.801: INFO: Waiting up to 5m0s for pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429" in namespace "container-probe-4171" to be "running and ready"
    Mar 30 14:05:28.803: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Pending", Reason="", readiness=false. Elapsed: 1.518147ms
    Mar 30 14:05:28.803: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 14:05:30.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 2.003653586s
    Mar 30 14:05:30.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:32.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 4.003577163s
    Mar 30 14:05:32.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:34.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 6.004388718s
    Mar 30 14:05:34.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:36.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 8.003867076s
    Mar 30 14:05:36.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:38.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 10.00398477s
    Mar 30 14:05:38.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:40.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 12.004727314s
    Mar 30 14:05:40.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:42.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 14.003687768s
    Mar 30 14:05:42.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:44.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 16.004621015s
    Mar 30 14:05:44.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:46.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 18.003817458s
    Mar 30 14:05:46.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:48.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 20.004841399s
    Mar 30 14:05:48.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:50.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 22.00462054s
    Mar 30 14:05:50.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:52.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 24.004637835s
    Mar 30 14:05:52.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:54.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 26.00352587s
    Mar 30 14:05:54.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:56.805: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 28.004061129s
    Mar 30 14:05:56.805: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:05:58.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=false. Elapsed: 30.004805609s
    Mar 30 14:05:58.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = false)
    Mar 30 14:06:00.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429": Phase="Running", Reason="", readiness=true. Elapsed: 32.004702356s
    Mar 30 14:06:00.806: INFO: The phase of Pod test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429 is Running (Ready = true)
    Mar 30 14:06:00.806: INFO: Pod "test-webserver-da1e9986-fbda-4bad-85f9-a00c4271f429" satisfied condition "running and ready"
    Mar 30 14:06:00.808: INFO: Container started at 2023-03-30 14:05:29 +0000 UTC, pod became ready at 2023-03-30 14:05:59 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:06:00.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4171" for this suite. 03/30/23 14:06:00.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:06:00.813
Mar 30 14:06:00.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 14:06:00.814
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:00.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:00.821
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 03/30/23 14:06:00.822
Mar 30 14:06:00.826: INFO: Waiting up to 5m0s for pod "pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c" in namespace "emptydir-2578" to be "Succeeded or Failed"
Mar 30 14:06:00.827: INFO: Pod "pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.316164ms
Mar 30 14:06:02.830: INFO: Pod "pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004223764s
Mar 30 14:06:04.830: INFO: Pod "pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003864213s
STEP: Saw pod success 03/30/23 14:06:04.83
Mar 30 14:06:04.830: INFO: Pod "pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c" satisfied condition "Succeeded or Failed"
Mar 30 14:06:04.832: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c container test-container: <nil>
STEP: delete the pod 03/30/23 14:06:04.842
Mar 30 14:06:04.847: INFO: Waiting for pod pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c to disappear
Mar 30 14:06:04.849: INFO: Pod pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 14:06:04.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2578" for this suite. 03/30/23 14:06:04.851
------------------------------
â€¢ [4.041 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:06:00.813
    Mar 30 14:06:00.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 14:06:00.814
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:00.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:00.821
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/30/23 14:06:00.822
    Mar 30 14:06:00.826: INFO: Waiting up to 5m0s for pod "pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c" in namespace "emptydir-2578" to be "Succeeded or Failed"
    Mar 30 14:06:00.827: INFO: Pod "pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.316164ms
    Mar 30 14:06:02.830: INFO: Pod "pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004223764s
    Mar 30 14:06:04.830: INFO: Pod "pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003864213s
    STEP: Saw pod success 03/30/23 14:06:04.83
    Mar 30 14:06:04.830: INFO: Pod "pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c" satisfied condition "Succeeded or Failed"
    Mar 30 14:06:04.832: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c container test-container: <nil>
    STEP: delete the pod 03/30/23 14:06:04.842
    Mar 30 14:06:04.847: INFO: Waiting for pod pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c to disappear
    Mar 30 14:06:04.849: INFO: Pod pod-2754c33d-16ff-49f6-a6ca-1f5d17d0354c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:06:04.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2578" for this suite. 03/30/23 14:06:04.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:06:04.855
Mar 30 14:06:04.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename svc-latency 03/30/23 14:06:04.856
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:04.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:04.863
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Mar 30 14:06:04.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7354 03/30/23 14:06:04.865
I0330 14:06:04.868068      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7354, replica count: 1
I0330 14:06:05.919613      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 14:06:06.025: INFO: Created: latency-svc-cjk6t
Mar 30 14:06:06.028: INFO: Got endpoints: latency-svc-cjk6t [8.024801ms]
Mar 30 14:06:06.033: INFO: Created: latency-svc-rwqxj
Mar 30 14:06:06.036: INFO: Got endpoints: latency-svc-rwqxj [7.276003ms]
Mar 30 14:06:06.036: INFO: Created: latency-svc-68q82
Mar 30 14:06:06.038: INFO: Got endpoints: latency-svc-68q82 [10.117607ms]
Mar 30 14:06:06.039: INFO: Created: latency-svc-mtxhr
Mar 30 14:06:06.041: INFO: Got endpoints: latency-svc-mtxhr [12.727376ms]
Mar 30 14:06:06.042: INFO: Created: latency-svc-kdk6x
Mar 30 14:06:06.044: INFO: Got endpoints: latency-svc-kdk6x [15.761879ms]
Mar 30 14:06:06.046: INFO: Created: latency-svc-qffwf
Mar 30 14:06:06.047: INFO: Got endpoints: latency-svc-qffwf [18.827667ms]
Mar 30 14:06:06.049: INFO: Created: latency-svc-bmbm9
Mar 30 14:06:06.051: INFO: Got endpoints: latency-svc-bmbm9 [22.264102ms]
Mar 30 14:06:06.051: INFO: Created: latency-svc-w5k4d
Mar 30 14:06:06.053: INFO: Got endpoints: latency-svc-w5k4d [24.753676ms]
Mar 30 14:06:06.055: INFO: Created: latency-svc-kkf2w
Mar 30 14:06:06.056: INFO: Got endpoints: latency-svc-kkf2w [27.806834ms]
Mar 30 14:06:06.058: INFO: Created: latency-svc-g45xk
Mar 30 14:06:06.060: INFO: Got endpoints: latency-svc-g45xk [31.180095ms]
Mar 30 14:06:06.060: INFO: Created: latency-svc-4x7l4
Mar 30 14:06:06.063: INFO: Got endpoints: latency-svc-4x7l4 [34.501869ms]
Mar 30 14:06:06.063: INFO: Created: latency-svc-2wd6z
Mar 30 14:06:06.065: INFO: Got endpoints: latency-svc-2wd6z [36.449437ms]
Mar 30 14:06:06.066: INFO: Created: latency-svc-xxzdw
Mar 30 14:06:06.070: INFO: Got endpoints: latency-svc-xxzdw [41.308425ms]
Mar 30 14:06:06.071: INFO: Created: latency-svc-xccxj
Mar 30 14:06:06.073: INFO: Got endpoints: latency-svc-xccxj [44.721439ms]
Mar 30 14:06:06.074: INFO: Created: latency-svc-pt65j
Mar 30 14:06:06.075: INFO: Got endpoints: latency-svc-pt65j [46.679082ms]
Mar 30 14:06:06.076: INFO: Created: latency-svc-wtx8p
Mar 30 14:06:06.078: INFO: Got endpoints: latency-svc-wtx8p [49.56549ms]
Mar 30 14:06:06.079: INFO: Created: latency-svc-hbsqp
Mar 30 14:06:06.081: INFO: Got endpoints: latency-svc-hbsqp [45.797252ms]
Mar 30 14:06:06.082: INFO: Created: latency-svc-66scg
Mar 30 14:06:06.085: INFO: Got endpoints: latency-svc-66scg [46.300443ms]
Mar 30 14:06:06.086: INFO: Created: latency-svc-bgpgn
Mar 30 14:06:06.088: INFO: Got endpoints: latency-svc-bgpgn [46.991386ms]
Mar 30 14:06:06.089: INFO: Created: latency-svc-4dq44
Mar 30 14:06:06.091: INFO: Got endpoints: latency-svc-4dq44 [46.777569ms]
Mar 30 14:06:06.092: INFO: Created: latency-svc-wnr2g
Mar 30 14:06:06.096: INFO: Got endpoints: latency-svc-wnr2g [48.483756ms]
Mar 30 14:06:06.096: INFO: Created: latency-svc-dvmnf
Mar 30 14:06:06.103: INFO: Got endpoints: latency-svc-dvmnf [52.098974ms]
Mar 30 14:06:06.104: INFO: Created: latency-svc-wmlbv
Mar 30 14:06:06.107: INFO: Got endpoints: latency-svc-wmlbv [54.059148ms]
Mar 30 14:06:06.109: INFO: Created: latency-svc-ctbfj
Mar 30 14:06:06.110: INFO: Got endpoints: latency-svc-ctbfj [53.862227ms]
Mar 30 14:06:06.112: INFO: Created: latency-svc-jlf79
Mar 30 14:06:06.114: INFO: Got endpoints: latency-svc-jlf79 [54.043834ms]
Mar 30 14:06:06.115: INFO: Created: latency-svc-cm6fh
Mar 30 14:06:06.117: INFO: Got endpoints: latency-svc-cm6fh [53.788373ms]
Mar 30 14:06:06.118: INFO: Created: latency-svc-6tdjd
Mar 30 14:06:06.120: INFO: Got endpoints: latency-svc-6tdjd [55.155833ms]
Mar 30 14:06:06.121: INFO: Created: latency-svc-stq6q
Mar 30 14:06:06.123: INFO: Got endpoints: latency-svc-stq6q [53.102603ms]
Mar 30 14:06:06.124: INFO: Created: latency-svc-pg4pr
Mar 30 14:06:06.126: INFO: Got endpoints: latency-svc-pg4pr [52.238773ms]
Mar 30 14:06:06.127: INFO: Created: latency-svc-h92dg
Mar 30 14:06:06.129: INFO: Got endpoints: latency-svc-h92dg [53.513603ms]
Mar 30 14:06:06.129: INFO: Created: latency-svc-scv44
Mar 30 14:06:06.131: INFO: Got endpoints: latency-svc-scv44 [53.144736ms]
Mar 30 14:06:06.133: INFO: Created: latency-svc-nf4h5
Mar 30 14:06:06.134: INFO: Got endpoints: latency-svc-nf4h5 [52.59003ms]
Mar 30 14:06:06.140: INFO: Created: latency-svc-n2jhc
Mar 30 14:06:06.142: INFO: Created: latency-svc-88spj
Mar 30 14:06:06.143: INFO: Created: latency-svc-cmw26
Mar 30 14:06:06.155: INFO: Created: latency-svc-s8ksk
Mar 30 14:06:06.156: INFO: Created: latency-svc-vvjvw
Mar 30 14:06:06.159: INFO: Created: latency-svc-dz5sw
Mar 30 14:06:06.161: INFO: Created: latency-svc-cr5h5
Mar 30 14:06:06.164: INFO: Created: latency-svc-j2ktj
Mar 30 14:06:06.166: INFO: Created: latency-svc-rlf7f
Mar 30 14:06:06.169: INFO: Created: latency-svc-swfz9
Mar 30 14:06:06.171: INFO: Created: latency-svc-d7zrw
Mar 30 14:06:06.174: INFO: Created: latency-svc-bznpf
Mar 30 14:06:06.178: INFO: Created: latency-svc-m67kl
Mar 30 14:06:06.178: INFO: Got endpoints: latency-svc-n2jhc [93.569891ms]
Mar 30 14:06:06.181: INFO: Created: latency-svc-qkbb9
Mar 30 14:06:06.183: INFO: Created: latency-svc-4bp9d
Mar 30 14:06:06.186: INFO: Created: latency-svc-n5ngg
Mar 30 14:06:06.228: INFO: Got endpoints: latency-svc-88spj [139.968643ms]
Mar 30 14:06:06.233: INFO: Created: latency-svc-jj626
Mar 30 14:06:06.278: INFO: Got endpoints: latency-svc-cmw26 [187.512738ms]
Mar 30 14:06:06.283: INFO: Created: latency-svc-l8w2l
Mar 30 14:06:06.328: INFO: Got endpoints: latency-svc-s8ksk [232.042017ms]
Mar 30 14:06:06.333: INFO: Created: latency-svc-7xtp8
Mar 30 14:06:06.379: INFO: Got endpoints: latency-svc-vvjvw [276.116042ms]
Mar 30 14:06:06.384: INFO: Created: latency-svc-cllfw
Mar 30 14:06:06.429: INFO: Got endpoints: latency-svc-dz5sw [321.363498ms]
Mar 30 14:06:06.433: INFO: Created: latency-svc-cffj7
Mar 30 14:06:06.478: INFO: Got endpoints: latency-svc-cr5h5 [368.288773ms]
Mar 30 14:06:06.484: INFO: Created: latency-svc-pqtbt
Mar 30 14:06:06.528: INFO: Got endpoints: latency-svc-j2ktj [414.662538ms]
Mar 30 14:06:06.534: INFO: Created: latency-svc-95qdr
Mar 30 14:06:06.578: INFO: Got endpoints: latency-svc-rlf7f [461.410628ms]
Mar 30 14:06:06.583: INFO: Created: latency-svc-c2nqh
Mar 30 14:06:06.628: INFO: Got endpoints: latency-svc-swfz9 [507.940118ms]
Mar 30 14:06:06.634: INFO: Created: latency-svc-fsdlt
Mar 30 14:06:06.678: INFO: Got endpoints: latency-svc-d7zrw [555.339577ms]
Mar 30 14:06:06.684: INFO: Created: latency-svc-zcmpk
Mar 30 14:06:06.728: INFO: Got endpoints: latency-svc-bznpf [602.639472ms]
Mar 30 14:06:06.733: INFO: Created: latency-svc-9z9s4
Mar 30 14:06:06.780: INFO: Got endpoints: latency-svc-m67kl [650.978229ms]
Mar 30 14:06:06.785: INFO: Created: latency-svc-6s645
Mar 30 14:06:06.829: INFO: Got endpoints: latency-svc-qkbb9 [698.151111ms]
Mar 30 14:06:06.835: INFO: Created: latency-svc-xz24c
Mar 30 14:06:06.878: INFO: Got endpoints: latency-svc-4bp9d [744.494391ms]
Mar 30 14:06:06.883: INFO: Created: latency-svc-zfzgh
Mar 30 14:06:06.929: INFO: Got endpoints: latency-svc-n5ngg [750.171995ms]
Mar 30 14:06:06.934: INFO: Created: latency-svc-979xk
Mar 30 14:06:06.978: INFO: Got endpoints: latency-svc-jj626 [749.854925ms]
Mar 30 14:06:06.983: INFO: Created: latency-svc-szjw5
Mar 30 14:06:07.028: INFO: Got endpoints: latency-svc-l8w2l [749.831922ms]
Mar 30 14:06:07.033: INFO: Created: latency-svc-qnj67
Mar 30 14:06:07.079: INFO: Got endpoints: latency-svc-7xtp8 [750.877809ms]
Mar 30 14:06:07.084: INFO: Created: latency-svc-5jrkp
Mar 30 14:06:07.129: INFO: Got endpoints: latency-svc-cllfw [750.081824ms]
Mar 30 14:06:07.134: INFO: Created: latency-svc-t4fnv
Mar 30 14:06:07.179: INFO: Got endpoints: latency-svc-cffj7 [750.147031ms]
Mar 30 14:06:07.184: INFO: Created: latency-svc-vcjn4
Mar 30 14:06:07.228: INFO: Got endpoints: latency-svc-pqtbt [749.979062ms]
Mar 30 14:06:07.234: INFO: Created: latency-svc-n62tq
Mar 30 14:06:07.279: INFO: Got endpoints: latency-svc-95qdr [750.394855ms]
Mar 30 14:06:07.284: INFO: Created: latency-svc-pkgbz
Mar 30 14:06:07.328: INFO: Got endpoints: latency-svc-c2nqh [750.14661ms]
Mar 30 14:06:07.333: INFO: Created: latency-svc-96hcm
Mar 30 14:06:07.378: INFO: Got endpoints: latency-svc-fsdlt [749.459147ms]
Mar 30 14:06:07.383: INFO: Created: latency-svc-7kktr
Mar 30 14:06:07.428: INFO: Got endpoints: latency-svc-zcmpk [750.043483ms]
Mar 30 14:06:07.434: INFO: Created: latency-svc-2gjch
Mar 30 14:06:07.478: INFO: Got endpoints: latency-svc-9z9s4 [749.825272ms]
Mar 30 14:06:07.483: INFO: Created: latency-svc-xhvns
Mar 30 14:06:07.528: INFO: Got endpoints: latency-svc-6s645 [748.629721ms]
Mar 30 14:06:07.533: INFO: Created: latency-svc-kfxkx
Mar 30 14:06:07.579: INFO: Got endpoints: latency-svc-xz24c [749.253636ms]
Mar 30 14:06:07.584: INFO: Created: latency-svc-qbj8g
Mar 30 14:06:07.628: INFO: Got endpoints: latency-svc-zfzgh [749.578485ms]
Mar 30 14:06:07.633: INFO: Created: latency-svc-rxwtv
Mar 30 14:06:07.679: INFO: Got endpoints: latency-svc-979xk [750.005621ms]
Mar 30 14:06:07.684: INFO: Created: latency-svc-tnz5l
Mar 30 14:06:07.729: INFO: Got endpoints: latency-svc-szjw5 [750.837985ms]
Mar 30 14:06:07.734: INFO: Created: latency-svc-hx69j
Mar 30 14:06:07.778: INFO: Got endpoints: latency-svc-qnj67 [750.122884ms]
Mar 30 14:06:07.783: INFO: Created: latency-svc-m7czt
Mar 30 14:06:07.828: INFO: Got endpoints: latency-svc-5jrkp [749.124399ms]
Mar 30 14:06:07.833: INFO: Created: latency-svc-sgx75
Mar 30 14:06:07.878: INFO: Got endpoints: latency-svc-t4fnv [749.318283ms]
Mar 30 14:06:07.884: INFO: Created: latency-svc-7njtl
Mar 30 14:06:07.929: INFO: Got endpoints: latency-svc-vcjn4 [750.310779ms]
Mar 30 14:06:07.934: INFO: Created: latency-svc-h4rks
Mar 30 14:06:07.978: INFO: Got endpoints: latency-svc-n62tq [749.554886ms]
Mar 30 14:06:07.984: INFO: Created: latency-svc-rcrc2
Mar 30 14:06:08.029: INFO: Got endpoints: latency-svc-pkgbz [749.974357ms]
Mar 30 14:06:08.034: INFO: Created: latency-svc-ccbdd
Mar 30 14:06:08.079: INFO: Got endpoints: latency-svc-96hcm [750.427064ms]
Mar 30 14:06:08.084: INFO: Created: latency-svc-vlpfg
Mar 30 14:06:08.128: INFO: Got endpoints: latency-svc-7kktr [750.560037ms]
Mar 30 14:06:08.133: INFO: Created: latency-svc-7wzvq
Mar 30 14:06:08.179: INFO: Got endpoints: latency-svc-2gjch [750.169087ms]
Mar 30 14:06:08.183: INFO: Created: latency-svc-vjfvp
Mar 30 14:06:08.228: INFO: Got endpoints: latency-svc-xhvns [749.902767ms]
Mar 30 14:06:08.233: INFO: Created: latency-svc-5rv5b
Mar 30 14:06:08.278: INFO: Got endpoints: latency-svc-kfxkx [749.117764ms]
Mar 30 14:06:08.283: INFO: Created: latency-svc-h4sv5
Mar 30 14:06:08.329: INFO: Got endpoints: latency-svc-qbj8g [749.802608ms]
Mar 30 14:06:08.334: INFO: Created: latency-svc-q8xc9
Mar 30 14:06:08.379: INFO: Got endpoints: latency-svc-rxwtv [751.098473ms]
Mar 30 14:06:08.384: INFO: Created: latency-svc-dj798
Mar 30 14:06:08.428: INFO: Got endpoints: latency-svc-tnz5l [749.795958ms]
Mar 30 14:06:08.435: INFO: Created: latency-svc-bzw7b
Mar 30 14:06:08.479: INFO: Got endpoints: latency-svc-hx69j [750.220318ms]
Mar 30 14:06:08.484: INFO: Created: latency-svc-t2nrm
Mar 30 14:06:08.530: INFO: Got endpoints: latency-svc-m7czt [751.578385ms]
Mar 30 14:06:08.535: INFO: Created: latency-svc-qt66q
Mar 30 14:06:08.578: INFO: Got endpoints: latency-svc-sgx75 [750.223307ms]
Mar 30 14:06:08.583: INFO: Created: latency-svc-27b92
Mar 30 14:06:08.629: INFO: Got endpoints: latency-svc-7njtl [750.225737ms]
Mar 30 14:06:08.634: INFO: Created: latency-svc-8svgm
Mar 30 14:06:08.678: INFO: Got endpoints: latency-svc-h4rks [749.08175ms]
Mar 30 14:06:08.683: INFO: Created: latency-svc-q6fbq
Mar 30 14:06:08.728: INFO: Got endpoints: latency-svc-rcrc2 [749.763856ms]
Mar 30 14:06:08.734: INFO: Created: latency-svc-2wjr5
Mar 30 14:06:08.779: INFO: Got endpoints: latency-svc-ccbdd [749.817374ms]
Mar 30 14:06:08.784: INFO: Created: latency-svc-z8gsn
Mar 30 14:06:08.829: INFO: Got endpoints: latency-svc-vlpfg [749.751259ms]
Mar 30 14:06:08.833: INFO: Created: latency-svc-lr82x
Mar 30 14:06:08.878: INFO: Got endpoints: latency-svc-7wzvq [749.920593ms]
Mar 30 14:06:08.883: INFO: Created: latency-svc-dmm4r
Mar 30 14:06:08.929: INFO: Got endpoints: latency-svc-vjfvp [749.989633ms]
Mar 30 14:06:08.933: INFO: Created: latency-svc-dnl6h
Mar 30 14:06:08.978: INFO: Got endpoints: latency-svc-5rv5b [750.251076ms]
Mar 30 14:06:08.983: INFO: Created: latency-svc-tblnx
Mar 30 14:06:09.028: INFO: Got endpoints: latency-svc-h4sv5 [750.39839ms]
Mar 30 14:06:09.033: INFO: Created: latency-svc-fwgwr
Mar 30 14:06:09.078: INFO: Got endpoints: latency-svc-q8xc9 [749.0977ms]
Mar 30 14:06:09.084: INFO: Created: latency-svc-qh9cj
Mar 30 14:06:09.128: INFO: Got endpoints: latency-svc-dj798 [748.835307ms]
Mar 30 14:06:09.133: INFO: Created: latency-svc-r29ws
Mar 30 14:06:09.178: INFO: Got endpoints: latency-svc-bzw7b [749.920693ms]
Mar 30 14:06:09.183: INFO: Created: latency-svc-mlp74
Mar 30 14:06:09.228: INFO: Got endpoints: latency-svc-t2nrm [749.404161ms]
Mar 30 14:06:09.234: INFO: Created: latency-svc-lkc4f
Mar 30 14:06:09.278: INFO: Got endpoints: latency-svc-qt66q [748.312711ms]
Mar 30 14:06:09.283: INFO: Created: latency-svc-h7sh9
Mar 30 14:06:09.328: INFO: Got endpoints: latency-svc-27b92 [749.984269ms]
Mar 30 14:06:09.333: INFO: Created: latency-svc-pvj65
Mar 30 14:06:09.378: INFO: Got endpoints: latency-svc-8svgm [749.739058ms]
Mar 30 14:06:09.385: INFO: Created: latency-svc-xg7kt
Mar 30 14:06:09.428: INFO: Got endpoints: latency-svc-q6fbq [749.457447ms]
Mar 30 14:06:09.433: INFO: Created: latency-svc-htdzz
Mar 30 14:06:09.478: INFO: Got endpoints: latency-svc-2wjr5 [750.068271ms]
Mar 30 14:06:09.483: INFO: Created: latency-svc-bp6z8
Mar 30 14:06:09.528: INFO: Got endpoints: latency-svc-z8gsn [749.369941ms]
Mar 30 14:06:09.534: INFO: Created: latency-svc-kzp5s
Mar 30 14:06:09.579: INFO: Got endpoints: latency-svc-lr82x [749.899939ms]
Mar 30 14:06:09.583: INFO: Created: latency-svc-rc25q
Mar 30 14:06:09.628: INFO: Got endpoints: latency-svc-dmm4r [749.902984ms]
Mar 30 14:06:09.633: INFO: Created: latency-svc-hhb2q
Mar 30 14:06:09.678: INFO: Got endpoints: latency-svc-dnl6h [749.718039ms]
Mar 30 14:06:09.684: INFO: Created: latency-svc-jqjsn
Mar 30 14:06:09.728: INFO: Got endpoints: latency-svc-tblnx [750.189259ms]
Mar 30 14:06:09.733: INFO: Created: latency-svc-c2k4h
Mar 30 14:06:09.779: INFO: Got endpoints: latency-svc-fwgwr [750.915093ms]
Mar 30 14:06:09.784: INFO: Created: latency-svc-rrvjd
Mar 30 14:06:09.829: INFO: Got endpoints: latency-svc-qh9cj [750.851441ms]
Mar 30 14:06:09.833: INFO: Created: latency-svc-d8sf6
Mar 30 14:06:09.879: INFO: Got endpoints: latency-svc-r29ws [750.494925ms]
Mar 30 14:06:09.883: INFO: Created: latency-svc-bbbj5
Mar 30 14:06:09.928: INFO: Got endpoints: latency-svc-mlp74 [749.353608ms]
Mar 30 14:06:09.933: INFO: Created: latency-svc-42lsc
Mar 30 14:06:09.979: INFO: Got endpoints: latency-svc-lkc4f [750.237822ms]
Mar 30 14:06:09.985: INFO: Created: latency-svc-dqq6d
Mar 30 14:06:10.028: INFO: Got endpoints: latency-svc-h7sh9 [749.983552ms]
Mar 30 14:06:10.033: INFO: Created: latency-svc-b7bzz
Mar 30 14:06:10.079: INFO: Got endpoints: latency-svc-pvj65 [750.506578ms]
Mar 30 14:06:10.083: INFO: Created: latency-svc-2wwrk
Mar 30 14:06:10.128: INFO: Got endpoints: latency-svc-xg7kt [749.966121ms]
Mar 30 14:06:10.133: INFO: Created: latency-svc-7rvfs
Mar 30 14:06:10.179: INFO: Got endpoints: latency-svc-htdzz [751.187741ms]
Mar 30 14:06:10.187: INFO: Created: latency-svc-zgz88
Mar 30 14:06:10.228: INFO: Got endpoints: latency-svc-bp6z8 [750.036263ms]
Mar 30 14:06:10.234: INFO: Created: latency-svc-2s4gq
Mar 30 14:06:10.278: INFO: Got endpoints: latency-svc-kzp5s [750.311133ms]
Mar 30 14:06:10.284: INFO: Created: latency-svc-h6rcw
Mar 30 14:06:10.328: INFO: Got endpoints: latency-svc-rc25q [749.651547ms]
Mar 30 14:06:10.333: INFO: Created: latency-svc-cxzcc
Mar 30 14:06:10.378: INFO: Got endpoints: latency-svc-hhb2q [750.352567ms]
Mar 30 14:06:10.384: INFO: Created: latency-svc-bqvk5
Mar 30 14:06:10.429: INFO: Got endpoints: latency-svc-jqjsn [750.279833ms]
Mar 30 14:06:10.437: INFO: Created: latency-svc-mfn4h
Mar 30 14:06:10.479: INFO: Got endpoints: latency-svc-c2k4h [750.302114ms]
Mar 30 14:06:10.483: INFO: Created: latency-svc-s4dgn
Mar 30 14:06:10.528: INFO: Got endpoints: latency-svc-rrvjd [749.115643ms]
Mar 30 14:06:10.533: INFO: Created: latency-svc-xjzpk
Mar 30 14:06:10.578: INFO: Got endpoints: latency-svc-d8sf6 [749.584057ms]
Mar 30 14:06:10.583: INFO: Created: latency-svc-kcb47
Mar 30 14:06:10.629: INFO: Got endpoints: latency-svc-bbbj5 [750.154984ms]
Mar 30 14:06:10.633: INFO: Created: latency-svc-2624k
Mar 30 14:06:10.678: INFO: Got endpoints: latency-svc-42lsc [750.690739ms]
Mar 30 14:06:10.685: INFO: Created: latency-svc-pb7db
Mar 30 14:06:10.729: INFO: Got endpoints: latency-svc-dqq6d [750.649984ms]
Mar 30 14:06:10.735: INFO: Created: latency-svc-zm7fv
Mar 30 14:06:10.778: INFO: Got endpoints: latency-svc-b7bzz [749.278728ms]
Mar 30 14:06:10.782: INFO: Created: latency-svc-9sbd4
Mar 30 14:06:10.828: INFO: Got endpoints: latency-svc-2wwrk [749.411784ms]
Mar 30 14:06:10.833: INFO: Created: latency-svc-cnln4
Mar 30 14:06:10.878: INFO: Got endpoints: latency-svc-7rvfs [749.921834ms]
Mar 30 14:06:10.883: INFO: Created: latency-svc-t6vnh
Mar 30 14:06:10.928: INFO: Got endpoints: latency-svc-zgz88 [749.350951ms]
Mar 30 14:06:10.933: INFO: Created: latency-svc-gndwj
Mar 30 14:06:10.978: INFO: Got endpoints: latency-svc-2s4gq [749.995145ms]
Mar 30 14:06:10.983: INFO: Created: latency-svc-fqm7x
Mar 30 14:06:11.028: INFO: Got endpoints: latency-svc-h6rcw [749.458216ms]
Mar 30 14:06:11.033: INFO: Created: latency-svc-v42d7
Mar 30 14:06:11.079: INFO: Got endpoints: latency-svc-cxzcc [750.422806ms]
Mar 30 14:06:11.083: INFO: Created: latency-svc-72ttl
Mar 30 14:06:11.128: INFO: Got endpoints: latency-svc-bqvk5 [749.422874ms]
Mar 30 14:06:11.133: INFO: Created: latency-svc-96z4w
Mar 30 14:06:11.178: INFO: Got endpoints: latency-svc-mfn4h [749.198977ms]
Mar 30 14:06:11.183: INFO: Created: latency-svc-s98p6
Mar 30 14:06:11.228: INFO: Got endpoints: latency-svc-s4dgn [749.098079ms]
Mar 30 14:06:11.233: INFO: Created: latency-svc-5crl2
Mar 30 14:06:11.279: INFO: Got endpoints: latency-svc-xjzpk [751.351739ms]
Mar 30 14:06:11.284: INFO: Created: latency-svc-rp2dq
Mar 30 14:06:11.329: INFO: Got endpoints: latency-svc-kcb47 [750.92618ms]
Mar 30 14:06:11.335: INFO: Created: latency-svc-6h7b5
Mar 30 14:06:11.379: INFO: Got endpoints: latency-svc-2624k [749.731091ms]
Mar 30 14:06:11.384: INFO: Created: latency-svc-55v7q
Mar 30 14:06:11.428: INFO: Got endpoints: latency-svc-pb7db [749.710308ms]
Mar 30 14:06:11.433: INFO: Created: latency-svc-ntzr4
Mar 30 14:06:11.478: INFO: Got endpoints: latency-svc-zm7fv [748.117914ms]
Mar 30 14:06:11.483: INFO: Created: latency-svc-ll4wg
Mar 30 14:06:11.529: INFO: Got endpoints: latency-svc-9sbd4 [750.947173ms]
Mar 30 14:06:11.533: INFO: Created: latency-svc-wwb6t
Mar 30 14:06:11.579: INFO: Got endpoints: latency-svc-cnln4 [750.784466ms]
Mar 30 14:06:11.584: INFO: Created: latency-svc-ldgsk
Mar 30 14:06:11.629: INFO: Got endpoints: latency-svc-t6vnh [750.38453ms]
Mar 30 14:06:11.634: INFO: Created: latency-svc-g4249
Mar 30 14:06:11.678: INFO: Got endpoints: latency-svc-gndwj [749.994097ms]
Mar 30 14:06:11.683: INFO: Created: latency-svc-4625k
Mar 30 14:06:11.728: INFO: Got endpoints: latency-svc-fqm7x [750.207693ms]
Mar 30 14:06:11.733: INFO: Created: latency-svc-rvtgq
Mar 30 14:06:11.779: INFO: Got endpoints: latency-svc-v42d7 [750.961817ms]
Mar 30 14:06:11.784: INFO: Created: latency-svc-czcb8
Mar 30 14:06:11.828: INFO: Got endpoints: latency-svc-72ttl [749.642508ms]
Mar 30 14:06:11.833: INFO: Created: latency-svc-vsmkc
Mar 30 14:06:11.878: INFO: Got endpoints: latency-svc-96z4w [749.912727ms]
Mar 30 14:06:11.883: INFO: Created: latency-svc-6txpw
Mar 30 14:06:11.928: INFO: Got endpoints: latency-svc-s98p6 [750.369262ms]
Mar 30 14:06:11.933: INFO: Created: latency-svc-bsbnf
Mar 30 14:06:11.978: INFO: Got endpoints: latency-svc-5crl2 [749.998836ms]
Mar 30 14:06:11.983: INFO: Created: latency-svc-5txjl
Mar 30 14:06:12.028: INFO: Got endpoints: latency-svc-rp2dq [748.960123ms]
Mar 30 14:06:12.033: INFO: Created: latency-svc-kpxf9
Mar 30 14:06:12.078: INFO: Got endpoints: latency-svc-6h7b5 [749.164067ms]
Mar 30 14:06:12.083: INFO: Created: latency-svc-82lmd
Mar 30 14:06:12.129: INFO: Got endpoints: latency-svc-55v7q [750.074893ms]
Mar 30 14:06:12.133: INFO: Created: latency-svc-nqvrs
Mar 30 14:06:12.178: INFO: Got endpoints: latency-svc-ntzr4 [750.284096ms]
Mar 30 14:06:12.184: INFO: Created: latency-svc-zndcx
Mar 30 14:06:12.229: INFO: Got endpoints: latency-svc-ll4wg [751.378334ms]
Mar 30 14:06:12.234: INFO: Created: latency-svc-zldg6
Mar 30 14:06:12.279: INFO: Got endpoints: latency-svc-wwb6t [750.182322ms]
Mar 30 14:06:12.284: INFO: Created: latency-svc-t6wgd
Mar 30 14:06:12.329: INFO: Got endpoints: latency-svc-ldgsk [749.787017ms]
Mar 30 14:06:12.334: INFO: Created: latency-svc-r2k8r
Mar 30 14:06:12.378: INFO: Got endpoints: latency-svc-g4249 [749.32844ms]
Mar 30 14:06:12.383: INFO: Created: latency-svc-jcb22
Mar 30 14:06:12.428: INFO: Got endpoints: latency-svc-4625k [749.345092ms]
Mar 30 14:06:12.433: INFO: Created: latency-svc-x6nbx
Mar 30 14:06:12.479: INFO: Got endpoints: latency-svc-rvtgq [750.577727ms]
Mar 30 14:06:12.485: INFO: Created: latency-svc-kn7tg
Mar 30 14:06:12.528: INFO: Got endpoints: latency-svc-czcb8 [748.807988ms]
Mar 30 14:06:12.533: INFO: Created: latency-svc-xjjgt
Mar 30 14:06:12.579: INFO: Got endpoints: latency-svc-vsmkc [750.287092ms]
Mar 30 14:06:12.584: INFO: Created: latency-svc-m4r7k
Mar 30 14:06:12.629: INFO: Got endpoints: latency-svc-6txpw [751.017658ms]
Mar 30 14:06:12.634: INFO: Created: latency-svc-vk7zp
Mar 30 14:06:12.679: INFO: Got endpoints: latency-svc-bsbnf [750.253077ms]
Mar 30 14:06:12.684: INFO: Created: latency-svc-mkpn5
Mar 30 14:06:12.729: INFO: Got endpoints: latency-svc-5txjl [750.910018ms]
Mar 30 14:06:12.734: INFO: Created: latency-svc-lpw95
Mar 30 14:06:12.779: INFO: Got endpoints: latency-svc-kpxf9 [750.853642ms]
Mar 30 14:06:12.785: INFO: Created: latency-svc-47t77
Mar 30 14:06:12.829: INFO: Got endpoints: latency-svc-82lmd [751.006797ms]
Mar 30 14:06:12.834: INFO: Created: latency-svc-7pbhs
Mar 30 14:06:12.879: INFO: Got endpoints: latency-svc-nqvrs [750.251217ms]
Mar 30 14:06:12.884: INFO: Created: latency-svc-tvhbg
Mar 30 14:06:12.929: INFO: Got endpoints: latency-svc-zndcx [750.223456ms]
Mar 30 14:06:12.934: INFO: Created: latency-svc-5r9qr
Mar 30 14:06:12.978: INFO: Got endpoints: latency-svc-zldg6 [749.449036ms]
Mar 30 14:06:12.984: INFO: Created: latency-svc-9nzsw
Mar 30 14:06:13.028: INFO: Got endpoints: latency-svc-t6wgd [749.207098ms]
Mar 30 14:06:13.033: INFO: Created: latency-svc-nkd68
Mar 30 14:06:13.078: INFO: Got endpoints: latency-svc-r2k8r [749.335144ms]
Mar 30 14:06:13.083: INFO: Created: latency-svc-lg4sq
Mar 30 14:06:13.128: INFO: Got endpoints: latency-svc-jcb22 [749.922261ms]
Mar 30 14:06:13.133: INFO: Created: latency-svc-bkmv6
Mar 30 14:06:13.179: INFO: Got endpoints: latency-svc-x6nbx [750.925173ms]
Mar 30 14:06:13.184: INFO: Created: latency-svc-zt4kz
Mar 30 14:06:13.229: INFO: Got endpoints: latency-svc-kn7tg [749.867013ms]
Mar 30 14:06:13.233: INFO: Created: latency-svc-22hdc
Mar 30 14:06:13.279: INFO: Got endpoints: latency-svc-xjjgt [751.191218ms]
Mar 30 14:06:13.284: INFO: Created: latency-svc-bq4sh
Mar 30 14:06:13.328: INFO: Got endpoints: latency-svc-m4r7k [749.392609ms]
Mar 30 14:06:13.333: INFO: Created: latency-svc-7pfrk
Mar 30 14:06:13.378: INFO: Got endpoints: latency-svc-vk7zp [749.456524ms]
Mar 30 14:06:13.383: INFO: Created: latency-svc-bwgfh
Mar 30 14:06:13.428: INFO: Got endpoints: latency-svc-mkpn5 [749.249251ms]
Mar 30 14:06:13.433: INFO: Created: latency-svc-jfzc2
Mar 30 14:06:13.478: INFO: Got endpoints: latency-svc-lpw95 [749.345182ms]
Mar 30 14:06:13.483: INFO: Created: latency-svc-nkmj2
Mar 30 14:06:13.528: INFO: Got endpoints: latency-svc-47t77 [749.086671ms]
Mar 30 14:06:13.534: INFO: Created: latency-svc-cqcq6
Mar 30 14:06:13.579: INFO: Got endpoints: latency-svc-7pbhs [749.463484ms]
Mar 30 14:06:13.584: INFO: Created: latency-svc-vjrwx
Mar 30 14:06:13.629: INFO: Got endpoints: latency-svc-tvhbg [749.906999ms]
Mar 30 14:06:13.634: INFO: Created: latency-svc-f4mnt
Mar 30 14:06:13.679: INFO: Got endpoints: latency-svc-5r9qr [749.985509ms]
Mar 30 14:06:13.684: INFO: Created: latency-svc-nxzqb
Mar 30 14:06:13.729: INFO: Got endpoints: latency-svc-9nzsw [750.075371ms]
Mar 30 14:06:13.734: INFO: Created: latency-svc-zmkzb
Mar 30 14:06:13.779: INFO: Got endpoints: latency-svc-nkd68 [750.856212ms]
Mar 30 14:06:13.784: INFO: Created: latency-svc-l7kw6
Mar 30 14:06:13.829: INFO: Got endpoints: latency-svc-lg4sq [750.678906ms]
Mar 30 14:06:13.833: INFO: Created: latency-svc-kxlwp
Mar 30 14:06:13.879: INFO: Got endpoints: latency-svc-bkmv6 [750.511126ms]
Mar 30 14:06:13.929: INFO: Got endpoints: latency-svc-zt4kz [749.74856ms]
Mar 30 14:06:13.979: INFO: Got endpoints: latency-svc-22hdc [750.248569ms]
Mar 30 14:06:14.028: INFO: Got endpoints: latency-svc-bq4sh [749.111238ms]
Mar 30 14:06:14.078: INFO: Got endpoints: latency-svc-7pfrk [749.875227ms]
Mar 30 14:06:14.128: INFO: Got endpoints: latency-svc-bwgfh [750.024346ms]
Mar 30 14:06:14.178: INFO: Got endpoints: latency-svc-jfzc2 [750.221191ms]
Mar 30 14:06:14.228: INFO: Got endpoints: latency-svc-nkmj2 [749.721938ms]
Mar 30 14:06:14.278: INFO: Got endpoints: latency-svc-cqcq6 [749.424192ms]
Mar 30 14:06:14.329: INFO: Got endpoints: latency-svc-vjrwx [750.247197ms]
Mar 30 14:06:14.379: INFO: Got endpoints: latency-svc-f4mnt [749.630142ms]
Mar 30 14:06:14.429: INFO: Got endpoints: latency-svc-nxzqb [750.185643ms]
Mar 30 14:06:14.478: INFO: Got endpoints: latency-svc-zmkzb [749.393893ms]
Mar 30 14:06:14.529: INFO: Got endpoints: latency-svc-l7kw6 [749.76706ms]
Mar 30 14:06:14.579: INFO: Got endpoints: latency-svc-kxlwp [750.048391ms]
Mar 30 14:06:14.579: INFO: Latencies: [7.276003ms 10.117607ms 12.727376ms 15.761879ms 18.827667ms 22.264102ms 24.753676ms 27.806834ms 31.180095ms 34.501869ms 36.449437ms 41.308425ms 44.721439ms 45.797252ms 46.300443ms 46.679082ms 46.777569ms 46.991386ms 48.483756ms 49.56549ms 52.098974ms 52.238773ms 52.59003ms 53.102603ms 53.144736ms 53.513603ms 53.788373ms 53.862227ms 54.043834ms 54.059148ms 55.155833ms 93.569891ms 139.968643ms 187.512738ms 232.042017ms 276.116042ms 321.363498ms 368.288773ms 414.662538ms 461.410628ms 507.940118ms 555.339577ms 602.639472ms 650.978229ms 698.151111ms 744.494391ms 748.117914ms 748.312711ms 748.629721ms 748.807988ms 748.835307ms 748.960123ms 749.08175ms 749.086671ms 749.0977ms 749.098079ms 749.111238ms 749.115643ms 749.117764ms 749.124399ms 749.164067ms 749.198977ms 749.207098ms 749.249251ms 749.253636ms 749.278728ms 749.318283ms 749.32844ms 749.335144ms 749.345092ms 749.345182ms 749.350951ms 749.353608ms 749.369941ms 749.392609ms 749.393893ms 749.404161ms 749.411784ms 749.422874ms 749.424192ms 749.449036ms 749.456524ms 749.457447ms 749.458216ms 749.459147ms 749.463484ms 749.554886ms 749.578485ms 749.584057ms 749.630142ms 749.642508ms 749.651547ms 749.710308ms 749.718039ms 749.721938ms 749.731091ms 749.739058ms 749.74856ms 749.751259ms 749.763856ms 749.76706ms 749.787017ms 749.795958ms 749.802608ms 749.817374ms 749.825272ms 749.831922ms 749.854925ms 749.867013ms 749.875227ms 749.899939ms 749.902767ms 749.902984ms 749.906999ms 749.912727ms 749.920593ms 749.920693ms 749.921834ms 749.922261ms 749.966121ms 749.974357ms 749.979062ms 749.983552ms 749.984269ms 749.985509ms 749.989633ms 749.994097ms 749.995145ms 749.998836ms 750.005621ms 750.024346ms 750.036263ms 750.043483ms 750.048391ms 750.068271ms 750.074893ms 750.075371ms 750.081824ms 750.122884ms 750.14661ms 750.147031ms 750.154984ms 750.169087ms 750.171995ms 750.182322ms 750.185643ms 750.189259ms 750.207693ms 750.220318ms 750.221191ms 750.223307ms 750.223456ms 750.225737ms 750.237822ms 750.247197ms 750.248569ms 750.251076ms 750.251217ms 750.253077ms 750.279833ms 750.284096ms 750.287092ms 750.302114ms 750.310779ms 750.311133ms 750.352567ms 750.369262ms 750.38453ms 750.394855ms 750.39839ms 750.422806ms 750.427064ms 750.494925ms 750.506578ms 750.511126ms 750.560037ms 750.577727ms 750.649984ms 750.678906ms 750.690739ms 750.784466ms 750.837985ms 750.851441ms 750.853642ms 750.856212ms 750.877809ms 750.910018ms 750.915093ms 750.925173ms 750.92618ms 750.947173ms 750.961817ms 751.006797ms 751.017658ms 751.098473ms 751.187741ms 751.191218ms 751.351739ms 751.378334ms 751.578385ms]
Mar 30 14:06:14.579: INFO: 50 %ile: 749.76706ms
Mar 30 14:06:14.579: INFO: 90 %ile: 750.784466ms
Mar 30 14:06:14.579: INFO: 99 %ile: 751.378334ms
Mar 30 14:06:14.579: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Mar 30 14:06:14.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-7354" for this suite. 03/30/23 14:06:14.583
------------------------------
â€¢ [SLOW TEST] [9.732 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:06:04.855
    Mar 30 14:06:04.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename svc-latency 03/30/23 14:06:04.856
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:04.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:04.863
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Mar 30 14:06:04.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7354 03/30/23 14:06:04.865
    I0330 14:06:04.868068      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7354, replica count: 1
    I0330 14:06:05.919613      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 30 14:06:06.025: INFO: Created: latency-svc-cjk6t
    Mar 30 14:06:06.028: INFO: Got endpoints: latency-svc-cjk6t [8.024801ms]
    Mar 30 14:06:06.033: INFO: Created: latency-svc-rwqxj
    Mar 30 14:06:06.036: INFO: Got endpoints: latency-svc-rwqxj [7.276003ms]
    Mar 30 14:06:06.036: INFO: Created: latency-svc-68q82
    Mar 30 14:06:06.038: INFO: Got endpoints: latency-svc-68q82 [10.117607ms]
    Mar 30 14:06:06.039: INFO: Created: latency-svc-mtxhr
    Mar 30 14:06:06.041: INFO: Got endpoints: latency-svc-mtxhr [12.727376ms]
    Mar 30 14:06:06.042: INFO: Created: latency-svc-kdk6x
    Mar 30 14:06:06.044: INFO: Got endpoints: latency-svc-kdk6x [15.761879ms]
    Mar 30 14:06:06.046: INFO: Created: latency-svc-qffwf
    Mar 30 14:06:06.047: INFO: Got endpoints: latency-svc-qffwf [18.827667ms]
    Mar 30 14:06:06.049: INFO: Created: latency-svc-bmbm9
    Mar 30 14:06:06.051: INFO: Got endpoints: latency-svc-bmbm9 [22.264102ms]
    Mar 30 14:06:06.051: INFO: Created: latency-svc-w5k4d
    Mar 30 14:06:06.053: INFO: Got endpoints: latency-svc-w5k4d [24.753676ms]
    Mar 30 14:06:06.055: INFO: Created: latency-svc-kkf2w
    Mar 30 14:06:06.056: INFO: Got endpoints: latency-svc-kkf2w [27.806834ms]
    Mar 30 14:06:06.058: INFO: Created: latency-svc-g45xk
    Mar 30 14:06:06.060: INFO: Got endpoints: latency-svc-g45xk [31.180095ms]
    Mar 30 14:06:06.060: INFO: Created: latency-svc-4x7l4
    Mar 30 14:06:06.063: INFO: Got endpoints: latency-svc-4x7l4 [34.501869ms]
    Mar 30 14:06:06.063: INFO: Created: latency-svc-2wd6z
    Mar 30 14:06:06.065: INFO: Got endpoints: latency-svc-2wd6z [36.449437ms]
    Mar 30 14:06:06.066: INFO: Created: latency-svc-xxzdw
    Mar 30 14:06:06.070: INFO: Got endpoints: latency-svc-xxzdw [41.308425ms]
    Mar 30 14:06:06.071: INFO: Created: latency-svc-xccxj
    Mar 30 14:06:06.073: INFO: Got endpoints: latency-svc-xccxj [44.721439ms]
    Mar 30 14:06:06.074: INFO: Created: latency-svc-pt65j
    Mar 30 14:06:06.075: INFO: Got endpoints: latency-svc-pt65j [46.679082ms]
    Mar 30 14:06:06.076: INFO: Created: latency-svc-wtx8p
    Mar 30 14:06:06.078: INFO: Got endpoints: latency-svc-wtx8p [49.56549ms]
    Mar 30 14:06:06.079: INFO: Created: latency-svc-hbsqp
    Mar 30 14:06:06.081: INFO: Got endpoints: latency-svc-hbsqp [45.797252ms]
    Mar 30 14:06:06.082: INFO: Created: latency-svc-66scg
    Mar 30 14:06:06.085: INFO: Got endpoints: latency-svc-66scg [46.300443ms]
    Mar 30 14:06:06.086: INFO: Created: latency-svc-bgpgn
    Mar 30 14:06:06.088: INFO: Got endpoints: latency-svc-bgpgn [46.991386ms]
    Mar 30 14:06:06.089: INFO: Created: latency-svc-4dq44
    Mar 30 14:06:06.091: INFO: Got endpoints: latency-svc-4dq44 [46.777569ms]
    Mar 30 14:06:06.092: INFO: Created: latency-svc-wnr2g
    Mar 30 14:06:06.096: INFO: Got endpoints: latency-svc-wnr2g [48.483756ms]
    Mar 30 14:06:06.096: INFO: Created: latency-svc-dvmnf
    Mar 30 14:06:06.103: INFO: Got endpoints: latency-svc-dvmnf [52.098974ms]
    Mar 30 14:06:06.104: INFO: Created: latency-svc-wmlbv
    Mar 30 14:06:06.107: INFO: Got endpoints: latency-svc-wmlbv [54.059148ms]
    Mar 30 14:06:06.109: INFO: Created: latency-svc-ctbfj
    Mar 30 14:06:06.110: INFO: Got endpoints: latency-svc-ctbfj [53.862227ms]
    Mar 30 14:06:06.112: INFO: Created: latency-svc-jlf79
    Mar 30 14:06:06.114: INFO: Got endpoints: latency-svc-jlf79 [54.043834ms]
    Mar 30 14:06:06.115: INFO: Created: latency-svc-cm6fh
    Mar 30 14:06:06.117: INFO: Got endpoints: latency-svc-cm6fh [53.788373ms]
    Mar 30 14:06:06.118: INFO: Created: latency-svc-6tdjd
    Mar 30 14:06:06.120: INFO: Got endpoints: latency-svc-6tdjd [55.155833ms]
    Mar 30 14:06:06.121: INFO: Created: latency-svc-stq6q
    Mar 30 14:06:06.123: INFO: Got endpoints: latency-svc-stq6q [53.102603ms]
    Mar 30 14:06:06.124: INFO: Created: latency-svc-pg4pr
    Mar 30 14:06:06.126: INFO: Got endpoints: latency-svc-pg4pr [52.238773ms]
    Mar 30 14:06:06.127: INFO: Created: latency-svc-h92dg
    Mar 30 14:06:06.129: INFO: Got endpoints: latency-svc-h92dg [53.513603ms]
    Mar 30 14:06:06.129: INFO: Created: latency-svc-scv44
    Mar 30 14:06:06.131: INFO: Got endpoints: latency-svc-scv44 [53.144736ms]
    Mar 30 14:06:06.133: INFO: Created: latency-svc-nf4h5
    Mar 30 14:06:06.134: INFO: Got endpoints: latency-svc-nf4h5 [52.59003ms]
    Mar 30 14:06:06.140: INFO: Created: latency-svc-n2jhc
    Mar 30 14:06:06.142: INFO: Created: latency-svc-88spj
    Mar 30 14:06:06.143: INFO: Created: latency-svc-cmw26
    Mar 30 14:06:06.155: INFO: Created: latency-svc-s8ksk
    Mar 30 14:06:06.156: INFO: Created: latency-svc-vvjvw
    Mar 30 14:06:06.159: INFO: Created: latency-svc-dz5sw
    Mar 30 14:06:06.161: INFO: Created: latency-svc-cr5h5
    Mar 30 14:06:06.164: INFO: Created: latency-svc-j2ktj
    Mar 30 14:06:06.166: INFO: Created: latency-svc-rlf7f
    Mar 30 14:06:06.169: INFO: Created: latency-svc-swfz9
    Mar 30 14:06:06.171: INFO: Created: latency-svc-d7zrw
    Mar 30 14:06:06.174: INFO: Created: latency-svc-bznpf
    Mar 30 14:06:06.178: INFO: Created: latency-svc-m67kl
    Mar 30 14:06:06.178: INFO: Got endpoints: latency-svc-n2jhc [93.569891ms]
    Mar 30 14:06:06.181: INFO: Created: latency-svc-qkbb9
    Mar 30 14:06:06.183: INFO: Created: latency-svc-4bp9d
    Mar 30 14:06:06.186: INFO: Created: latency-svc-n5ngg
    Mar 30 14:06:06.228: INFO: Got endpoints: latency-svc-88spj [139.968643ms]
    Mar 30 14:06:06.233: INFO: Created: latency-svc-jj626
    Mar 30 14:06:06.278: INFO: Got endpoints: latency-svc-cmw26 [187.512738ms]
    Mar 30 14:06:06.283: INFO: Created: latency-svc-l8w2l
    Mar 30 14:06:06.328: INFO: Got endpoints: latency-svc-s8ksk [232.042017ms]
    Mar 30 14:06:06.333: INFO: Created: latency-svc-7xtp8
    Mar 30 14:06:06.379: INFO: Got endpoints: latency-svc-vvjvw [276.116042ms]
    Mar 30 14:06:06.384: INFO: Created: latency-svc-cllfw
    Mar 30 14:06:06.429: INFO: Got endpoints: latency-svc-dz5sw [321.363498ms]
    Mar 30 14:06:06.433: INFO: Created: latency-svc-cffj7
    Mar 30 14:06:06.478: INFO: Got endpoints: latency-svc-cr5h5 [368.288773ms]
    Mar 30 14:06:06.484: INFO: Created: latency-svc-pqtbt
    Mar 30 14:06:06.528: INFO: Got endpoints: latency-svc-j2ktj [414.662538ms]
    Mar 30 14:06:06.534: INFO: Created: latency-svc-95qdr
    Mar 30 14:06:06.578: INFO: Got endpoints: latency-svc-rlf7f [461.410628ms]
    Mar 30 14:06:06.583: INFO: Created: latency-svc-c2nqh
    Mar 30 14:06:06.628: INFO: Got endpoints: latency-svc-swfz9 [507.940118ms]
    Mar 30 14:06:06.634: INFO: Created: latency-svc-fsdlt
    Mar 30 14:06:06.678: INFO: Got endpoints: latency-svc-d7zrw [555.339577ms]
    Mar 30 14:06:06.684: INFO: Created: latency-svc-zcmpk
    Mar 30 14:06:06.728: INFO: Got endpoints: latency-svc-bznpf [602.639472ms]
    Mar 30 14:06:06.733: INFO: Created: latency-svc-9z9s4
    Mar 30 14:06:06.780: INFO: Got endpoints: latency-svc-m67kl [650.978229ms]
    Mar 30 14:06:06.785: INFO: Created: latency-svc-6s645
    Mar 30 14:06:06.829: INFO: Got endpoints: latency-svc-qkbb9 [698.151111ms]
    Mar 30 14:06:06.835: INFO: Created: latency-svc-xz24c
    Mar 30 14:06:06.878: INFO: Got endpoints: latency-svc-4bp9d [744.494391ms]
    Mar 30 14:06:06.883: INFO: Created: latency-svc-zfzgh
    Mar 30 14:06:06.929: INFO: Got endpoints: latency-svc-n5ngg [750.171995ms]
    Mar 30 14:06:06.934: INFO: Created: latency-svc-979xk
    Mar 30 14:06:06.978: INFO: Got endpoints: latency-svc-jj626 [749.854925ms]
    Mar 30 14:06:06.983: INFO: Created: latency-svc-szjw5
    Mar 30 14:06:07.028: INFO: Got endpoints: latency-svc-l8w2l [749.831922ms]
    Mar 30 14:06:07.033: INFO: Created: latency-svc-qnj67
    Mar 30 14:06:07.079: INFO: Got endpoints: latency-svc-7xtp8 [750.877809ms]
    Mar 30 14:06:07.084: INFO: Created: latency-svc-5jrkp
    Mar 30 14:06:07.129: INFO: Got endpoints: latency-svc-cllfw [750.081824ms]
    Mar 30 14:06:07.134: INFO: Created: latency-svc-t4fnv
    Mar 30 14:06:07.179: INFO: Got endpoints: latency-svc-cffj7 [750.147031ms]
    Mar 30 14:06:07.184: INFO: Created: latency-svc-vcjn4
    Mar 30 14:06:07.228: INFO: Got endpoints: latency-svc-pqtbt [749.979062ms]
    Mar 30 14:06:07.234: INFO: Created: latency-svc-n62tq
    Mar 30 14:06:07.279: INFO: Got endpoints: latency-svc-95qdr [750.394855ms]
    Mar 30 14:06:07.284: INFO: Created: latency-svc-pkgbz
    Mar 30 14:06:07.328: INFO: Got endpoints: latency-svc-c2nqh [750.14661ms]
    Mar 30 14:06:07.333: INFO: Created: latency-svc-96hcm
    Mar 30 14:06:07.378: INFO: Got endpoints: latency-svc-fsdlt [749.459147ms]
    Mar 30 14:06:07.383: INFO: Created: latency-svc-7kktr
    Mar 30 14:06:07.428: INFO: Got endpoints: latency-svc-zcmpk [750.043483ms]
    Mar 30 14:06:07.434: INFO: Created: latency-svc-2gjch
    Mar 30 14:06:07.478: INFO: Got endpoints: latency-svc-9z9s4 [749.825272ms]
    Mar 30 14:06:07.483: INFO: Created: latency-svc-xhvns
    Mar 30 14:06:07.528: INFO: Got endpoints: latency-svc-6s645 [748.629721ms]
    Mar 30 14:06:07.533: INFO: Created: latency-svc-kfxkx
    Mar 30 14:06:07.579: INFO: Got endpoints: latency-svc-xz24c [749.253636ms]
    Mar 30 14:06:07.584: INFO: Created: latency-svc-qbj8g
    Mar 30 14:06:07.628: INFO: Got endpoints: latency-svc-zfzgh [749.578485ms]
    Mar 30 14:06:07.633: INFO: Created: latency-svc-rxwtv
    Mar 30 14:06:07.679: INFO: Got endpoints: latency-svc-979xk [750.005621ms]
    Mar 30 14:06:07.684: INFO: Created: latency-svc-tnz5l
    Mar 30 14:06:07.729: INFO: Got endpoints: latency-svc-szjw5 [750.837985ms]
    Mar 30 14:06:07.734: INFO: Created: latency-svc-hx69j
    Mar 30 14:06:07.778: INFO: Got endpoints: latency-svc-qnj67 [750.122884ms]
    Mar 30 14:06:07.783: INFO: Created: latency-svc-m7czt
    Mar 30 14:06:07.828: INFO: Got endpoints: latency-svc-5jrkp [749.124399ms]
    Mar 30 14:06:07.833: INFO: Created: latency-svc-sgx75
    Mar 30 14:06:07.878: INFO: Got endpoints: latency-svc-t4fnv [749.318283ms]
    Mar 30 14:06:07.884: INFO: Created: latency-svc-7njtl
    Mar 30 14:06:07.929: INFO: Got endpoints: latency-svc-vcjn4 [750.310779ms]
    Mar 30 14:06:07.934: INFO: Created: latency-svc-h4rks
    Mar 30 14:06:07.978: INFO: Got endpoints: latency-svc-n62tq [749.554886ms]
    Mar 30 14:06:07.984: INFO: Created: latency-svc-rcrc2
    Mar 30 14:06:08.029: INFO: Got endpoints: latency-svc-pkgbz [749.974357ms]
    Mar 30 14:06:08.034: INFO: Created: latency-svc-ccbdd
    Mar 30 14:06:08.079: INFO: Got endpoints: latency-svc-96hcm [750.427064ms]
    Mar 30 14:06:08.084: INFO: Created: latency-svc-vlpfg
    Mar 30 14:06:08.128: INFO: Got endpoints: latency-svc-7kktr [750.560037ms]
    Mar 30 14:06:08.133: INFO: Created: latency-svc-7wzvq
    Mar 30 14:06:08.179: INFO: Got endpoints: latency-svc-2gjch [750.169087ms]
    Mar 30 14:06:08.183: INFO: Created: latency-svc-vjfvp
    Mar 30 14:06:08.228: INFO: Got endpoints: latency-svc-xhvns [749.902767ms]
    Mar 30 14:06:08.233: INFO: Created: latency-svc-5rv5b
    Mar 30 14:06:08.278: INFO: Got endpoints: latency-svc-kfxkx [749.117764ms]
    Mar 30 14:06:08.283: INFO: Created: latency-svc-h4sv5
    Mar 30 14:06:08.329: INFO: Got endpoints: latency-svc-qbj8g [749.802608ms]
    Mar 30 14:06:08.334: INFO: Created: latency-svc-q8xc9
    Mar 30 14:06:08.379: INFO: Got endpoints: latency-svc-rxwtv [751.098473ms]
    Mar 30 14:06:08.384: INFO: Created: latency-svc-dj798
    Mar 30 14:06:08.428: INFO: Got endpoints: latency-svc-tnz5l [749.795958ms]
    Mar 30 14:06:08.435: INFO: Created: latency-svc-bzw7b
    Mar 30 14:06:08.479: INFO: Got endpoints: latency-svc-hx69j [750.220318ms]
    Mar 30 14:06:08.484: INFO: Created: latency-svc-t2nrm
    Mar 30 14:06:08.530: INFO: Got endpoints: latency-svc-m7czt [751.578385ms]
    Mar 30 14:06:08.535: INFO: Created: latency-svc-qt66q
    Mar 30 14:06:08.578: INFO: Got endpoints: latency-svc-sgx75 [750.223307ms]
    Mar 30 14:06:08.583: INFO: Created: latency-svc-27b92
    Mar 30 14:06:08.629: INFO: Got endpoints: latency-svc-7njtl [750.225737ms]
    Mar 30 14:06:08.634: INFO: Created: latency-svc-8svgm
    Mar 30 14:06:08.678: INFO: Got endpoints: latency-svc-h4rks [749.08175ms]
    Mar 30 14:06:08.683: INFO: Created: latency-svc-q6fbq
    Mar 30 14:06:08.728: INFO: Got endpoints: latency-svc-rcrc2 [749.763856ms]
    Mar 30 14:06:08.734: INFO: Created: latency-svc-2wjr5
    Mar 30 14:06:08.779: INFO: Got endpoints: latency-svc-ccbdd [749.817374ms]
    Mar 30 14:06:08.784: INFO: Created: latency-svc-z8gsn
    Mar 30 14:06:08.829: INFO: Got endpoints: latency-svc-vlpfg [749.751259ms]
    Mar 30 14:06:08.833: INFO: Created: latency-svc-lr82x
    Mar 30 14:06:08.878: INFO: Got endpoints: latency-svc-7wzvq [749.920593ms]
    Mar 30 14:06:08.883: INFO: Created: latency-svc-dmm4r
    Mar 30 14:06:08.929: INFO: Got endpoints: latency-svc-vjfvp [749.989633ms]
    Mar 30 14:06:08.933: INFO: Created: latency-svc-dnl6h
    Mar 30 14:06:08.978: INFO: Got endpoints: latency-svc-5rv5b [750.251076ms]
    Mar 30 14:06:08.983: INFO: Created: latency-svc-tblnx
    Mar 30 14:06:09.028: INFO: Got endpoints: latency-svc-h4sv5 [750.39839ms]
    Mar 30 14:06:09.033: INFO: Created: latency-svc-fwgwr
    Mar 30 14:06:09.078: INFO: Got endpoints: latency-svc-q8xc9 [749.0977ms]
    Mar 30 14:06:09.084: INFO: Created: latency-svc-qh9cj
    Mar 30 14:06:09.128: INFO: Got endpoints: latency-svc-dj798 [748.835307ms]
    Mar 30 14:06:09.133: INFO: Created: latency-svc-r29ws
    Mar 30 14:06:09.178: INFO: Got endpoints: latency-svc-bzw7b [749.920693ms]
    Mar 30 14:06:09.183: INFO: Created: latency-svc-mlp74
    Mar 30 14:06:09.228: INFO: Got endpoints: latency-svc-t2nrm [749.404161ms]
    Mar 30 14:06:09.234: INFO: Created: latency-svc-lkc4f
    Mar 30 14:06:09.278: INFO: Got endpoints: latency-svc-qt66q [748.312711ms]
    Mar 30 14:06:09.283: INFO: Created: latency-svc-h7sh9
    Mar 30 14:06:09.328: INFO: Got endpoints: latency-svc-27b92 [749.984269ms]
    Mar 30 14:06:09.333: INFO: Created: latency-svc-pvj65
    Mar 30 14:06:09.378: INFO: Got endpoints: latency-svc-8svgm [749.739058ms]
    Mar 30 14:06:09.385: INFO: Created: latency-svc-xg7kt
    Mar 30 14:06:09.428: INFO: Got endpoints: latency-svc-q6fbq [749.457447ms]
    Mar 30 14:06:09.433: INFO: Created: latency-svc-htdzz
    Mar 30 14:06:09.478: INFO: Got endpoints: latency-svc-2wjr5 [750.068271ms]
    Mar 30 14:06:09.483: INFO: Created: latency-svc-bp6z8
    Mar 30 14:06:09.528: INFO: Got endpoints: latency-svc-z8gsn [749.369941ms]
    Mar 30 14:06:09.534: INFO: Created: latency-svc-kzp5s
    Mar 30 14:06:09.579: INFO: Got endpoints: latency-svc-lr82x [749.899939ms]
    Mar 30 14:06:09.583: INFO: Created: latency-svc-rc25q
    Mar 30 14:06:09.628: INFO: Got endpoints: latency-svc-dmm4r [749.902984ms]
    Mar 30 14:06:09.633: INFO: Created: latency-svc-hhb2q
    Mar 30 14:06:09.678: INFO: Got endpoints: latency-svc-dnl6h [749.718039ms]
    Mar 30 14:06:09.684: INFO: Created: latency-svc-jqjsn
    Mar 30 14:06:09.728: INFO: Got endpoints: latency-svc-tblnx [750.189259ms]
    Mar 30 14:06:09.733: INFO: Created: latency-svc-c2k4h
    Mar 30 14:06:09.779: INFO: Got endpoints: latency-svc-fwgwr [750.915093ms]
    Mar 30 14:06:09.784: INFO: Created: latency-svc-rrvjd
    Mar 30 14:06:09.829: INFO: Got endpoints: latency-svc-qh9cj [750.851441ms]
    Mar 30 14:06:09.833: INFO: Created: latency-svc-d8sf6
    Mar 30 14:06:09.879: INFO: Got endpoints: latency-svc-r29ws [750.494925ms]
    Mar 30 14:06:09.883: INFO: Created: latency-svc-bbbj5
    Mar 30 14:06:09.928: INFO: Got endpoints: latency-svc-mlp74 [749.353608ms]
    Mar 30 14:06:09.933: INFO: Created: latency-svc-42lsc
    Mar 30 14:06:09.979: INFO: Got endpoints: latency-svc-lkc4f [750.237822ms]
    Mar 30 14:06:09.985: INFO: Created: latency-svc-dqq6d
    Mar 30 14:06:10.028: INFO: Got endpoints: latency-svc-h7sh9 [749.983552ms]
    Mar 30 14:06:10.033: INFO: Created: latency-svc-b7bzz
    Mar 30 14:06:10.079: INFO: Got endpoints: latency-svc-pvj65 [750.506578ms]
    Mar 30 14:06:10.083: INFO: Created: latency-svc-2wwrk
    Mar 30 14:06:10.128: INFO: Got endpoints: latency-svc-xg7kt [749.966121ms]
    Mar 30 14:06:10.133: INFO: Created: latency-svc-7rvfs
    Mar 30 14:06:10.179: INFO: Got endpoints: latency-svc-htdzz [751.187741ms]
    Mar 30 14:06:10.187: INFO: Created: latency-svc-zgz88
    Mar 30 14:06:10.228: INFO: Got endpoints: latency-svc-bp6z8 [750.036263ms]
    Mar 30 14:06:10.234: INFO: Created: latency-svc-2s4gq
    Mar 30 14:06:10.278: INFO: Got endpoints: latency-svc-kzp5s [750.311133ms]
    Mar 30 14:06:10.284: INFO: Created: latency-svc-h6rcw
    Mar 30 14:06:10.328: INFO: Got endpoints: latency-svc-rc25q [749.651547ms]
    Mar 30 14:06:10.333: INFO: Created: latency-svc-cxzcc
    Mar 30 14:06:10.378: INFO: Got endpoints: latency-svc-hhb2q [750.352567ms]
    Mar 30 14:06:10.384: INFO: Created: latency-svc-bqvk5
    Mar 30 14:06:10.429: INFO: Got endpoints: latency-svc-jqjsn [750.279833ms]
    Mar 30 14:06:10.437: INFO: Created: latency-svc-mfn4h
    Mar 30 14:06:10.479: INFO: Got endpoints: latency-svc-c2k4h [750.302114ms]
    Mar 30 14:06:10.483: INFO: Created: latency-svc-s4dgn
    Mar 30 14:06:10.528: INFO: Got endpoints: latency-svc-rrvjd [749.115643ms]
    Mar 30 14:06:10.533: INFO: Created: latency-svc-xjzpk
    Mar 30 14:06:10.578: INFO: Got endpoints: latency-svc-d8sf6 [749.584057ms]
    Mar 30 14:06:10.583: INFO: Created: latency-svc-kcb47
    Mar 30 14:06:10.629: INFO: Got endpoints: latency-svc-bbbj5 [750.154984ms]
    Mar 30 14:06:10.633: INFO: Created: latency-svc-2624k
    Mar 30 14:06:10.678: INFO: Got endpoints: latency-svc-42lsc [750.690739ms]
    Mar 30 14:06:10.685: INFO: Created: latency-svc-pb7db
    Mar 30 14:06:10.729: INFO: Got endpoints: latency-svc-dqq6d [750.649984ms]
    Mar 30 14:06:10.735: INFO: Created: latency-svc-zm7fv
    Mar 30 14:06:10.778: INFO: Got endpoints: latency-svc-b7bzz [749.278728ms]
    Mar 30 14:06:10.782: INFO: Created: latency-svc-9sbd4
    Mar 30 14:06:10.828: INFO: Got endpoints: latency-svc-2wwrk [749.411784ms]
    Mar 30 14:06:10.833: INFO: Created: latency-svc-cnln4
    Mar 30 14:06:10.878: INFO: Got endpoints: latency-svc-7rvfs [749.921834ms]
    Mar 30 14:06:10.883: INFO: Created: latency-svc-t6vnh
    Mar 30 14:06:10.928: INFO: Got endpoints: latency-svc-zgz88 [749.350951ms]
    Mar 30 14:06:10.933: INFO: Created: latency-svc-gndwj
    Mar 30 14:06:10.978: INFO: Got endpoints: latency-svc-2s4gq [749.995145ms]
    Mar 30 14:06:10.983: INFO: Created: latency-svc-fqm7x
    Mar 30 14:06:11.028: INFO: Got endpoints: latency-svc-h6rcw [749.458216ms]
    Mar 30 14:06:11.033: INFO: Created: latency-svc-v42d7
    Mar 30 14:06:11.079: INFO: Got endpoints: latency-svc-cxzcc [750.422806ms]
    Mar 30 14:06:11.083: INFO: Created: latency-svc-72ttl
    Mar 30 14:06:11.128: INFO: Got endpoints: latency-svc-bqvk5 [749.422874ms]
    Mar 30 14:06:11.133: INFO: Created: latency-svc-96z4w
    Mar 30 14:06:11.178: INFO: Got endpoints: latency-svc-mfn4h [749.198977ms]
    Mar 30 14:06:11.183: INFO: Created: latency-svc-s98p6
    Mar 30 14:06:11.228: INFO: Got endpoints: latency-svc-s4dgn [749.098079ms]
    Mar 30 14:06:11.233: INFO: Created: latency-svc-5crl2
    Mar 30 14:06:11.279: INFO: Got endpoints: latency-svc-xjzpk [751.351739ms]
    Mar 30 14:06:11.284: INFO: Created: latency-svc-rp2dq
    Mar 30 14:06:11.329: INFO: Got endpoints: latency-svc-kcb47 [750.92618ms]
    Mar 30 14:06:11.335: INFO: Created: latency-svc-6h7b5
    Mar 30 14:06:11.379: INFO: Got endpoints: latency-svc-2624k [749.731091ms]
    Mar 30 14:06:11.384: INFO: Created: latency-svc-55v7q
    Mar 30 14:06:11.428: INFO: Got endpoints: latency-svc-pb7db [749.710308ms]
    Mar 30 14:06:11.433: INFO: Created: latency-svc-ntzr4
    Mar 30 14:06:11.478: INFO: Got endpoints: latency-svc-zm7fv [748.117914ms]
    Mar 30 14:06:11.483: INFO: Created: latency-svc-ll4wg
    Mar 30 14:06:11.529: INFO: Got endpoints: latency-svc-9sbd4 [750.947173ms]
    Mar 30 14:06:11.533: INFO: Created: latency-svc-wwb6t
    Mar 30 14:06:11.579: INFO: Got endpoints: latency-svc-cnln4 [750.784466ms]
    Mar 30 14:06:11.584: INFO: Created: latency-svc-ldgsk
    Mar 30 14:06:11.629: INFO: Got endpoints: latency-svc-t6vnh [750.38453ms]
    Mar 30 14:06:11.634: INFO: Created: latency-svc-g4249
    Mar 30 14:06:11.678: INFO: Got endpoints: latency-svc-gndwj [749.994097ms]
    Mar 30 14:06:11.683: INFO: Created: latency-svc-4625k
    Mar 30 14:06:11.728: INFO: Got endpoints: latency-svc-fqm7x [750.207693ms]
    Mar 30 14:06:11.733: INFO: Created: latency-svc-rvtgq
    Mar 30 14:06:11.779: INFO: Got endpoints: latency-svc-v42d7 [750.961817ms]
    Mar 30 14:06:11.784: INFO: Created: latency-svc-czcb8
    Mar 30 14:06:11.828: INFO: Got endpoints: latency-svc-72ttl [749.642508ms]
    Mar 30 14:06:11.833: INFO: Created: latency-svc-vsmkc
    Mar 30 14:06:11.878: INFO: Got endpoints: latency-svc-96z4w [749.912727ms]
    Mar 30 14:06:11.883: INFO: Created: latency-svc-6txpw
    Mar 30 14:06:11.928: INFO: Got endpoints: latency-svc-s98p6 [750.369262ms]
    Mar 30 14:06:11.933: INFO: Created: latency-svc-bsbnf
    Mar 30 14:06:11.978: INFO: Got endpoints: latency-svc-5crl2 [749.998836ms]
    Mar 30 14:06:11.983: INFO: Created: latency-svc-5txjl
    Mar 30 14:06:12.028: INFO: Got endpoints: latency-svc-rp2dq [748.960123ms]
    Mar 30 14:06:12.033: INFO: Created: latency-svc-kpxf9
    Mar 30 14:06:12.078: INFO: Got endpoints: latency-svc-6h7b5 [749.164067ms]
    Mar 30 14:06:12.083: INFO: Created: latency-svc-82lmd
    Mar 30 14:06:12.129: INFO: Got endpoints: latency-svc-55v7q [750.074893ms]
    Mar 30 14:06:12.133: INFO: Created: latency-svc-nqvrs
    Mar 30 14:06:12.178: INFO: Got endpoints: latency-svc-ntzr4 [750.284096ms]
    Mar 30 14:06:12.184: INFO: Created: latency-svc-zndcx
    Mar 30 14:06:12.229: INFO: Got endpoints: latency-svc-ll4wg [751.378334ms]
    Mar 30 14:06:12.234: INFO: Created: latency-svc-zldg6
    Mar 30 14:06:12.279: INFO: Got endpoints: latency-svc-wwb6t [750.182322ms]
    Mar 30 14:06:12.284: INFO: Created: latency-svc-t6wgd
    Mar 30 14:06:12.329: INFO: Got endpoints: latency-svc-ldgsk [749.787017ms]
    Mar 30 14:06:12.334: INFO: Created: latency-svc-r2k8r
    Mar 30 14:06:12.378: INFO: Got endpoints: latency-svc-g4249 [749.32844ms]
    Mar 30 14:06:12.383: INFO: Created: latency-svc-jcb22
    Mar 30 14:06:12.428: INFO: Got endpoints: latency-svc-4625k [749.345092ms]
    Mar 30 14:06:12.433: INFO: Created: latency-svc-x6nbx
    Mar 30 14:06:12.479: INFO: Got endpoints: latency-svc-rvtgq [750.577727ms]
    Mar 30 14:06:12.485: INFO: Created: latency-svc-kn7tg
    Mar 30 14:06:12.528: INFO: Got endpoints: latency-svc-czcb8 [748.807988ms]
    Mar 30 14:06:12.533: INFO: Created: latency-svc-xjjgt
    Mar 30 14:06:12.579: INFO: Got endpoints: latency-svc-vsmkc [750.287092ms]
    Mar 30 14:06:12.584: INFO: Created: latency-svc-m4r7k
    Mar 30 14:06:12.629: INFO: Got endpoints: latency-svc-6txpw [751.017658ms]
    Mar 30 14:06:12.634: INFO: Created: latency-svc-vk7zp
    Mar 30 14:06:12.679: INFO: Got endpoints: latency-svc-bsbnf [750.253077ms]
    Mar 30 14:06:12.684: INFO: Created: latency-svc-mkpn5
    Mar 30 14:06:12.729: INFO: Got endpoints: latency-svc-5txjl [750.910018ms]
    Mar 30 14:06:12.734: INFO: Created: latency-svc-lpw95
    Mar 30 14:06:12.779: INFO: Got endpoints: latency-svc-kpxf9 [750.853642ms]
    Mar 30 14:06:12.785: INFO: Created: latency-svc-47t77
    Mar 30 14:06:12.829: INFO: Got endpoints: latency-svc-82lmd [751.006797ms]
    Mar 30 14:06:12.834: INFO: Created: latency-svc-7pbhs
    Mar 30 14:06:12.879: INFO: Got endpoints: latency-svc-nqvrs [750.251217ms]
    Mar 30 14:06:12.884: INFO: Created: latency-svc-tvhbg
    Mar 30 14:06:12.929: INFO: Got endpoints: latency-svc-zndcx [750.223456ms]
    Mar 30 14:06:12.934: INFO: Created: latency-svc-5r9qr
    Mar 30 14:06:12.978: INFO: Got endpoints: latency-svc-zldg6 [749.449036ms]
    Mar 30 14:06:12.984: INFO: Created: latency-svc-9nzsw
    Mar 30 14:06:13.028: INFO: Got endpoints: latency-svc-t6wgd [749.207098ms]
    Mar 30 14:06:13.033: INFO: Created: latency-svc-nkd68
    Mar 30 14:06:13.078: INFO: Got endpoints: latency-svc-r2k8r [749.335144ms]
    Mar 30 14:06:13.083: INFO: Created: latency-svc-lg4sq
    Mar 30 14:06:13.128: INFO: Got endpoints: latency-svc-jcb22 [749.922261ms]
    Mar 30 14:06:13.133: INFO: Created: latency-svc-bkmv6
    Mar 30 14:06:13.179: INFO: Got endpoints: latency-svc-x6nbx [750.925173ms]
    Mar 30 14:06:13.184: INFO: Created: latency-svc-zt4kz
    Mar 30 14:06:13.229: INFO: Got endpoints: latency-svc-kn7tg [749.867013ms]
    Mar 30 14:06:13.233: INFO: Created: latency-svc-22hdc
    Mar 30 14:06:13.279: INFO: Got endpoints: latency-svc-xjjgt [751.191218ms]
    Mar 30 14:06:13.284: INFO: Created: latency-svc-bq4sh
    Mar 30 14:06:13.328: INFO: Got endpoints: latency-svc-m4r7k [749.392609ms]
    Mar 30 14:06:13.333: INFO: Created: latency-svc-7pfrk
    Mar 30 14:06:13.378: INFO: Got endpoints: latency-svc-vk7zp [749.456524ms]
    Mar 30 14:06:13.383: INFO: Created: latency-svc-bwgfh
    Mar 30 14:06:13.428: INFO: Got endpoints: latency-svc-mkpn5 [749.249251ms]
    Mar 30 14:06:13.433: INFO: Created: latency-svc-jfzc2
    Mar 30 14:06:13.478: INFO: Got endpoints: latency-svc-lpw95 [749.345182ms]
    Mar 30 14:06:13.483: INFO: Created: latency-svc-nkmj2
    Mar 30 14:06:13.528: INFO: Got endpoints: latency-svc-47t77 [749.086671ms]
    Mar 30 14:06:13.534: INFO: Created: latency-svc-cqcq6
    Mar 30 14:06:13.579: INFO: Got endpoints: latency-svc-7pbhs [749.463484ms]
    Mar 30 14:06:13.584: INFO: Created: latency-svc-vjrwx
    Mar 30 14:06:13.629: INFO: Got endpoints: latency-svc-tvhbg [749.906999ms]
    Mar 30 14:06:13.634: INFO: Created: latency-svc-f4mnt
    Mar 30 14:06:13.679: INFO: Got endpoints: latency-svc-5r9qr [749.985509ms]
    Mar 30 14:06:13.684: INFO: Created: latency-svc-nxzqb
    Mar 30 14:06:13.729: INFO: Got endpoints: latency-svc-9nzsw [750.075371ms]
    Mar 30 14:06:13.734: INFO: Created: latency-svc-zmkzb
    Mar 30 14:06:13.779: INFO: Got endpoints: latency-svc-nkd68 [750.856212ms]
    Mar 30 14:06:13.784: INFO: Created: latency-svc-l7kw6
    Mar 30 14:06:13.829: INFO: Got endpoints: latency-svc-lg4sq [750.678906ms]
    Mar 30 14:06:13.833: INFO: Created: latency-svc-kxlwp
    Mar 30 14:06:13.879: INFO: Got endpoints: latency-svc-bkmv6 [750.511126ms]
    Mar 30 14:06:13.929: INFO: Got endpoints: latency-svc-zt4kz [749.74856ms]
    Mar 30 14:06:13.979: INFO: Got endpoints: latency-svc-22hdc [750.248569ms]
    Mar 30 14:06:14.028: INFO: Got endpoints: latency-svc-bq4sh [749.111238ms]
    Mar 30 14:06:14.078: INFO: Got endpoints: latency-svc-7pfrk [749.875227ms]
    Mar 30 14:06:14.128: INFO: Got endpoints: latency-svc-bwgfh [750.024346ms]
    Mar 30 14:06:14.178: INFO: Got endpoints: latency-svc-jfzc2 [750.221191ms]
    Mar 30 14:06:14.228: INFO: Got endpoints: latency-svc-nkmj2 [749.721938ms]
    Mar 30 14:06:14.278: INFO: Got endpoints: latency-svc-cqcq6 [749.424192ms]
    Mar 30 14:06:14.329: INFO: Got endpoints: latency-svc-vjrwx [750.247197ms]
    Mar 30 14:06:14.379: INFO: Got endpoints: latency-svc-f4mnt [749.630142ms]
    Mar 30 14:06:14.429: INFO: Got endpoints: latency-svc-nxzqb [750.185643ms]
    Mar 30 14:06:14.478: INFO: Got endpoints: latency-svc-zmkzb [749.393893ms]
    Mar 30 14:06:14.529: INFO: Got endpoints: latency-svc-l7kw6 [749.76706ms]
    Mar 30 14:06:14.579: INFO: Got endpoints: latency-svc-kxlwp [750.048391ms]
    Mar 30 14:06:14.579: INFO: Latencies: [7.276003ms 10.117607ms 12.727376ms 15.761879ms 18.827667ms 22.264102ms 24.753676ms 27.806834ms 31.180095ms 34.501869ms 36.449437ms 41.308425ms 44.721439ms 45.797252ms 46.300443ms 46.679082ms 46.777569ms 46.991386ms 48.483756ms 49.56549ms 52.098974ms 52.238773ms 52.59003ms 53.102603ms 53.144736ms 53.513603ms 53.788373ms 53.862227ms 54.043834ms 54.059148ms 55.155833ms 93.569891ms 139.968643ms 187.512738ms 232.042017ms 276.116042ms 321.363498ms 368.288773ms 414.662538ms 461.410628ms 507.940118ms 555.339577ms 602.639472ms 650.978229ms 698.151111ms 744.494391ms 748.117914ms 748.312711ms 748.629721ms 748.807988ms 748.835307ms 748.960123ms 749.08175ms 749.086671ms 749.0977ms 749.098079ms 749.111238ms 749.115643ms 749.117764ms 749.124399ms 749.164067ms 749.198977ms 749.207098ms 749.249251ms 749.253636ms 749.278728ms 749.318283ms 749.32844ms 749.335144ms 749.345092ms 749.345182ms 749.350951ms 749.353608ms 749.369941ms 749.392609ms 749.393893ms 749.404161ms 749.411784ms 749.422874ms 749.424192ms 749.449036ms 749.456524ms 749.457447ms 749.458216ms 749.459147ms 749.463484ms 749.554886ms 749.578485ms 749.584057ms 749.630142ms 749.642508ms 749.651547ms 749.710308ms 749.718039ms 749.721938ms 749.731091ms 749.739058ms 749.74856ms 749.751259ms 749.763856ms 749.76706ms 749.787017ms 749.795958ms 749.802608ms 749.817374ms 749.825272ms 749.831922ms 749.854925ms 749.867013ms 749.875227ms 749.899939ms 749.902767ms 749.902984ms 749.906999ms 749.912727ms 749.920593ms 749.920693ms 749.921834ms 749.922261ms 749.966121ms 749.974357ms 749.979062ms 749.983552ms 749.984269ms 749.985509ms 749.989633ms 749.994097ms 749.995145ms 749.998836ms 750.005621ms 750.024346ms 750.036263ms 750.043483ms 750.048391ms 750.068271ms 750.074893ms 750.075371ms 750.081824ms 750.122884ms 750.14661ms 750.147031ms 750.154984ms 750.169087ms 750.171995ms 750.182322ms 750.185643ms 750.189259ms 750.207693ms 750.220318ms 750.221191ms 750.223307ms 750.223456ms 750.225737ms 750.237822ms 750.247197ms 750.248569ms 750.251076ms 750.251217ms 750.253077ms 750.279833ms 750.284096ms 750.287092ms 750.302114ms 750.310779ms 750.311133ms 750.352567ms 750.369262ms 750.38453ms 750.394855ms 750.39839ms 750.422806ms 750.427064ms 750.494925ms 750.506578ms 750.511126ms 750.560037ms 750.577727ms 750.649984ms 750.678906ms 750.690739ms 750.784466ms 750.837985ms 750.851441ms 750.853642ms 750.856212ms 750.877809ms 750.910018ms 750.915093ms 750.925173ms 750.92618ms 750.947173ms 750.961817ms 751.006797ms 751.017658ms 751.098473ms 751.187741ms 751.191218ms 751.351739ms 751.378334ms 751.578385ms]
    Mar 30 14:06:14.579: INFO: 50 %ile: 749.76706ms
    Mar 30 14:06:14.579: INFO: 90 %ile: 750.784466ms
    Mar 30 14:06:14.579: INFO: 99 %ile: 751.378334ms
    Mar 30 14:06:14.579: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:06:14.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-7354" for this suite. 03/30/23 14:06:14.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:06:14.587
Mar 30 14:06:14.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename containers 03/30/23 14:06:14.588
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:14.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:14.596
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 03/30/23 14:06:14.598
Mar 30 14:06:14.601: INFO: Waiting up to 5m0s for pod "client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb" in namespace "containers-3533" to be "Succeeded or Failed"
Mar 30 14:06:14.603: INFO: Pod "client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.653241ms
Mar 30 14:06:16.606: INFO: Pod "client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004498421s
Mar 30 14:06:18.606: INFO: Pod "client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004771292s
STEP: Saw pod success 03/30/23 14:06:18.606
Mar 30 14:06:18.606: INFO: Pod "client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb" satisfied condition "Succeeded or Failed"
Mar 30 14:06:18.608: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb container agnhost-container: <nil>
STEP: delete the pod 03/30/23 14:06:18.611
Mar 30 14:06:18.616: INFO: Waiting for pod client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb to disappear
Mar 30 14:06:18.617: INFO: Pod client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 30 14:06:18.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3533" for this suite. 03/30/23 14:06:18.619
------------------------------
â€¢ [4.034 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:06:14.587
    Mar 30 14:06:14.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename containers 03/30/23 14:06:14.588
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:14.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:14.596
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 03/30/23 14:06:14.598
    Mar 30 14:06:14.601: INFO: Waiting up to 5m0s for pod "client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb" in namespace "containers-3533" to be "Succeeded or Failed"
    Mar 30 14:06:14.603: INFO: Pod "client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.653241ms
    Mar 30 14:06:16.606: INFO: Pod "client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004498421s
    Mar 30 14:06:18.606: INFO: Pod "client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004771292s
    STEP: Saw pod success 03/30/23 14:06:18.606
    Mar 30 14:06:18.606: INFO: Pod "client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb" satisfied condition "Succeeded or Failed"
    Mar 30 14:06:18.608: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 14:06:18.611
    Mar 30 14:06:18.616: INFO: Waiting for pod client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb to disappear
    Mar 30 14:06:18.617: INFO: Pod client-containers-f1695577-53e4-4cfa-842a-b425b6a919eb no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:06:18.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3533" for this suite. 03/30/23 14:06:18.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:06:18.622
Mar 30 14:06:18.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 14:06:18.623
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:18.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:18.629
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-325f6b70-4b9e-4c30-9182-7223abccd65d 03/30/23 14:06:18.631
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 14:06:18.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5338" for this suite. 03/30/23 14:06:18.634
------------------------------
â€¢ [0.014 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:06:18.622
    Mar 30 14:06:18.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 14:06:18.623
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:18.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:18.629
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-325f6b70-4b9e-4c30-9182-7223abccd65d 03/30/23 14:06:18.631
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:06:18.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5338" for this suite. 03/30/23 14:06:18.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:06:18.637
Mar 30 14:06:18.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pods 03/30/23 14:06:18.637
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:18.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:18.644
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 03/30/23 14:06:18.645
STEP: submitting the pod to kubernetes 03/30/23 14:06:18.646
STEP: verifying QOS class is set on the pod 03/30/23 14:06:18.649
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Mar 30 14:06:18.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8399" for this suite. 03/30/23 14:06:18.654
------------------------------
â€¢ [0.020 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:06:18.637
    Mar 30 14:06:18.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pods 03/30/23 14:06:18.637
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:18.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:18.644
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 03/30/23 14:06:18.645
    STEP: submitting the pod to kubernetes 03/30/23 14:06:18.646
    STEP: verifying QOS class is set on the pod 03/30/23 14:06:18.649
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:06:18.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8399" for this suite. 03/30/23 14:06:18.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:06:18.658
Mar 30 14:06:18.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename replication-controller 03/30/23 14:06:18.659
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:18.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:18.665
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Mar 30 14:06:18.667: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/30/23 14:06:19.674
STEP: Checking rc "condition-test" has the desired failure condition set 03/30/23 14:06:19.677
STEP: Scaling down rc "condition-test" to satisfy pod quota 03/30/23 14:06:20.682
Mar 30 14:06:20.686: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 03/30/23 14:06:20.686
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 30 14:06:21.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5044" for this suite. 03/30/23 14:06:21.692
------------------------------
â€¢ [3.037 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:06:18.658
    Mar 30 14:06:18.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename replication-controller 03/30/23 14:06:18.659
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:18.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:18.665
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Mar 30 14:06:18.667: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/30/23 14:06:19.674
    STEP: Checking rc "condition-test" has the desired failure condition set 03/30/23 14:06:19.677
    STEP: Scaling down rc "condition-test" to satisfy pod quota 03/30/23 14:06:20.682
    Mar 30 14:06:20.686: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 03/30/23 14:06:20.686
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:06:21.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5044" for this suite. 03/30/23 14:06:21.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:06:21.696
Mar 30 14:06:21.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 14:06:21.697
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:21.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:21.704
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 03/30/23 14:06:21.706
Mar 30 14:06:21.710: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed" in namespace "downward-api-97" to be "Succeeded or Failed"
Mar 30 14:06:21.711: INFO: Pod "downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed": Phase="Pending", Reason="", readiness=false. Elapsed: 1.442876ms
Mar 30 14:06:23.714: INFO: Pod "downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003806748s
Mar 30 14:06:25.714: INFO: Pod "downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003902993s
STEP: Saw pod success 03/30/23 14:06:25.714
Mar 30 14:06:25.714: INFO: Pod "downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed" satisfied condition "Succeeded or Failed"
Mar 30 14:06:25.715: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed container client-container: <nil>
STEP: delete the pod 03/30/23 14:06:25.718
Mar 30 14:06:25.722: INFO: Waiting for pod downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed to disappear
Mar 30 14:06:25.724: INFO: Pod downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 30 14:06:25.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-97" for this suite. 03/30/23 14:06:25.726
------------------------------
â€¢ [4.032 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:06:21.696
    Mar 30 14:06:21.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 14:06:21.697
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:21.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:21.704
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 03/30/23 14:06:21.706
    Mar 30 14:06:21.710: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed" in namespace "downward-api-97" to be "Succeeded or Failed"
    Mar 30 14:06:21.711: INFO: Pod "downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed": Phase="Pending", Reason="", readiness=false. Elapsed: 1.442876ms
    Mar 30 14:06:23.714: INFO: Pod "downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003806748s
    Mar 30 14:06:25.714: INFO: Pod "downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003902993s
    STEP: Saw pod success 03/30/23 14:06:25.714
    Mar 30 14:06:25.714: INFO: Pod "downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed" satisfied condition "Succeeded or Failed"
    Mar 30 14:06:25.715: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed container client-container: <nil>
    STEP: delete the pod 03/30/23 14:06:25.718
    Mar 30 14:06:25.722: INFO: Waiting for pod downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed to disappear
    Mar 30 14:06:25.724: INFO: Pod downwardapi-volume-14636421-4244-4fff-8ba2-0e3ebe761aed no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:06:25.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-97" for this suite. 03/30/23 14:06:25.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:06:25.73
Mar 30 14:06:25.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename job 03/30/23 14:06:25.731
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:25.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:25.737
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 03/30/23 14:06:25.739
STEP: Ensuring job reaches completions 03/30/23 14:06:25.742
STEP: Ensuring pods with index for job exist 03/30/23 14:06:33.744
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 30 14:06:33.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3518" for this suite. 03/30/23 14:06:33.749
------------------------------
â€¢ [SLOW TEST] [8.021 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:06:25.73
    Mar 30 14:06:25.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename job 03/30/23 14:06:25.731
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:25.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:25.737
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 03/30/23 14:06:25.739
    STEP: Ensuring job reaches completions 03/30/23 14:06:25.742
    STEP: Ensuring pods with index for job exist 03/30/23 14:06:33.744
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:06:33.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3518" for this suite. 03/30/23 14:06:33.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:06:33.753
Mar 30 14:06:33.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename deployment 03/30/23 14:06:33.753
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:33.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:33.76
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Mar 30 14:06:33.762: INFO: Creating deployment "test-recreate-deployment"
Mar 30 14:06:33.764: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 30 14:06:33.767: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 30 14:06:35.771: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 30 14:06:35.772: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 30 14:06:35.777: INFO: Updating deployment test-recreate-deployment
Mar 30 14:06:35.777: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 30 14:06:35.803: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9856  4f6e17f1-6b79-4cb4-9998-04a3aec48e44 86042 2 2023-03-30 14:06:33 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003778808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-30 14:06:35 +0000 UTC,LastTransitionTime:2023-03-30 14:06:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-03-30 14:06:35 +0000 UTC,LastTransitionTime:2023-03-30 14:06:33 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 30 14:06:35.805: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9856  320e162e-dfd7-44a1-beae-91340681b425 86040 1 2023-03-30 14:06:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4f6e17f1-6b79-4cb4-9998-04a3aec48e44 0xc0040a0e70 0xc0040a0e71}] [] [{kube-controller-manager Update apps/v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f6e17f1-6b79-4cb4-9998-04a3aec48e44\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040a0f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 14:06:35.805: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 30 14:06:35.805: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9856  b8528392-1250-4336-af7e-319a2cbb828f 86031 2 2023-03-30 14:06:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4f6e17f1-6b79-4cb4-9998-04a3aec48e44 0xc0040a0d57 0xc0040a0d58}] [] [{kube-controller-manager Update apps/v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f6e17f1-6b79-4cb4-9998-04a3aec48e44\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040a0e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 30 14:06:35.806: INFO: Pod "test-recreate-deployment-cff6dc657-lhmfp" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-lhmfp test-recreate-deployment-cff6dc657- deployment-9856  83cdc10c-8756-4c9b-a794-6554069738c1 86043 0 2023-03-30 14:06:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 320e162e-dfd7-44a1-beae-91340681b425 0xc006bd29b0 0xc006bd29b1}] [] [{kube-controller-manager Update v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"320e162e-dfd7-44a1-beae-91340681b425\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rf5pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rf5pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:06:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:06:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:06:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:06:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:,StartTime:2023-03-30 14:06:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 30 14:06:35.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9856" for this suite. 03/30/23 14:06:35.809
------------------------------
â€¢ [2.058 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:06:33.753
    Mar 30 14:06:33.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename deployment 03/30/23 14:06:33.753
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:33.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:33.76
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Mar 30 14:06:33.762: INFO: Creating deployment "test-recreate-deployment"
    Mar 30 14:06:33.764: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Mar 30 14:06:33.767: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Mar 30 14:06:35.771: INFO: Waiting deployment "test-recreate-deployment" to complete
    Mar 30 14:06:35.772: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Mar 30 14:06:35.777: INFO: Updating deployment test-recreate-deployment
    Mar 30 14:06:35.777: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 30 14:06:35.803: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9856  4f6e17f1-6b79-4cb4-9998-04a3aec48e44 86042 2 2023-03-30 14:06:33 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003778808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-30 14:06:35 +0000 UTC,LastTransitionTime:2023-03-30 14:06:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-03-30 14:06:35 +0000 UTC,LastTransitionTime:2023-03-30 14:06:33 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Mar 30 14:06:35.805: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9856  320e162e-dfd7-44a1-beae-91340681b425 86040 1 2023-03-30 14:06:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4f6e17f1-6b79-4cb4-9998-04a3aec48e44 0xc0040a0e70 0xc0040a0e71}] [] [{kube-controller-manager Update apps/v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f6e17f1-6b79-4cb4-9998-04a3aec48e44\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040a0f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 14:06:35.805: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Mar 30 14:06:35.805: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9856  b8528392-1250-4336-af7e-319a2cbb828f 86031 2 2023-03-30 14:06:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4f6e17f1-6b79-4cb4-9998-04a3aec48e44 0xc0040a0d57 0xc0040a0d58}] [] [{kube-controller-manager Update apps/v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f6e17f1-6b79-4cb4-9998-04a3aec48e44\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040a0e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 14:06:35.806: INFO: Pod "test-recreate-deployment-cff6dc657-lhmfp" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-lhmfp test-recreate-deployment-cff6dc657- deployment-9856  83cdc10c-8756-4c9b-a794-6554069738c1 86043 0 2023-03-30 14:06:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 320e162e-dfd7-44a1-beae-91340681b425 0xc006bd29b0 0xc006bd29b1}] [] [{kube-controller-manager Update v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"320e162e-dfd7-44a1-beae-91340681b425\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 14:06:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rf5pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rf5pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:06:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:06:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:06:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:06:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:,StartTime:2023-03-30 14:06:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:06:35.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9856" for this suite. 03/30/23 14:06:35.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:06:35.811
Mar 30 14:06:35.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename replication-controller 03/30/23 14:06:35.812
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:35.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:35.818
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 03/30/23 14:06:35.82
Mar 30 14:06:35.823: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9483" to be "running and ready"
Mar 30 14:06:35.825: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.544848ms
Mar 30 14:06:35.825: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar 30 14:06:37.827: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.00391922s
Mar 30 14:06:37.827: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Mar 30 14:06:37.827: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 03/30/23 14:06:37.829
STEP: Then the orphan pod is adopted 03/30/23 14:06:37.831
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 30 14:06:38.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9483" for this suite. 03/30/23 14:06:38.837
------------------------------
â€¢ [3.029 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:06:35.811
    Mar 30 14:06:35.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename replication-controller 03/30/23 14:06:35.812
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:35.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:35.818
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 03/30/23 14:06:35.82
    Mar 30 14:06:35.823: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9483" to be "running and ready"
    Mar 30 14:06:35.825: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.544848ms
    Mar 30 14:06:35.825: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 14:06:37.827: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.00391922s
    Mar 30 14:06:37.827: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Mar 30 14:06:37.827: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 03/30/23 14:06:37.829
    STEP: Then the orphan pod is adopted 03/30/23 14:06:37.831
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:06:38.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9483" for this suite. 03/30/23 14:06:38.837
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:06:38.84
Mar 30 14:06:38.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 14:06:38.841
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:38.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:38.848
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 03/30/23 14:06:38.85
Mar 30 14:06:38.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d" in namespace "downward-api-7592" to be "Succeeded or Failed"
Mar 30 14:06:38.856: INFO: Pod "downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.403782ms
Mar 30 14:06:40.859: INFO: Pod "downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003753082s
Mar 30 14:06:42.859: INFO: Pod "downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004104972s
STEP: Saw pod success 03/30/23 14:06:42.859
Mar 30 14:06:42.859: INFO: Pod "downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d" satisfied condition "Succeeded or Failed"
Mar 30 14:06:42.861: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d container client-container: <nil>
STEP: delete the pod 03/30/23 14:06:42.864
Mar 30 14:06:42.869: INFO: Waiting for pod downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d to disappear
Mar 30 14:06:42.870: INFO: Pod downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 30 14:06:42.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7592" for this suite. 03/30/23 14:06:42.872
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:06:38.84
    Mar 30 14:06:38.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 14:06:38.841
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:38.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:38.848
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 03/30/23 14:06:38.85
    Mar 30 14:06:38.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d" in namespace "downward-api-7592" to be "Succeeded or Failed"
    Mar 30 14:06:38.856: INFO: Pod "downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.403782ms
    Mar 30 14:06:40.859: INFO: Pod "downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003753082s
    Mar 30 14:06:42.859: INFO: Pod "downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004104972s
    STEP: Saw pod success 03/30/23 14:06:42.859
    Mar 30 14:06:42.859: INFO: Pod "downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d" satisfied condition "Succeeded or Failed"
    Mar 30 14:06:42.861: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d container client-container: <nil>
    STEP: delete the pod 03/30/23 14:06:42.864
    Mar 30 14:06:42.869: INFO: Waiting for pod downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d to disappear
    Mar 30 14:06:42.870: INFO: Pod downwardapi-volume-c924e90a-e836-4a4a-9130-73515ae21e6d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:06:42.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7592" for this suite. 03/30/23 14:06:42.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:06:42.876
Mar 30 14:06:42.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename subpath 03/30/23 14:06:42.876
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:42.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:42.883
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/30/23 14:06:42.884
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-pjqd 03/30/23 14:06:42.888
STEP: Creating a pod to test atomic-volume-subpath 03/30/23 14:06:42.888
Mar 30 14:06:42.892: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pjqd" in namespace "subpath-7731" to be "Succeeded or Failed"
Mar 30 14:06:42.893: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.491324ms
Mar 30 14:06:44.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 2.004274672s
Mar 30 14:06:46.895: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 4.003963709s
Mar 30 14:06:48.895: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 6.003665718s
Mar 30 14:06:50.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 8.004349961s
Mar 30 14:06:52.895: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 10.003884289s
Mar 30 14:06:54.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 12.004330344s
Mar 30 14:06:56.895: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 14.00393231s
Mar 30 14:06:58.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 16.004363584s
Mar 30 14:07:00.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 18.004420395s
Mar 30 14:07:02.895: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 20.003531505s
Mar 30 14:07:04.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=false. Elapsed: 22.004499135s
Mar 30 14:07:06.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004037067s
STEP: Saw pod success 03/30/23 14:07:06.896
Mar 30 14:07:06.896: INFO: Pod "pod-subpath-test-projected-pjqd" satisfied condition "Succeeded or Failed"
Mar 30 14:07:06.897: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-subpath-test-projected-pjqd container test-container-subpath-projected-pjqd: <nil>
STEP: delete the pod 03/30/23 14:07:06.9
Mar 30 14:07:06.905: INFO: Waiting for pod pod-subpath-test-projected-pjqd to disappear
Mar 30 14:07:06.906: INFO: Pod pod-subpath-test-projected-pjqd no longer exists
STEP: Deleting pod pod-subpath-test-projected-pjqd 03/30/23 14:07:06.906
Mar 30 14:07:06.906: INFO: Deleting pod "pod-subpath-test-projected-pjqd" in namespace "subpath-7731"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 30 14:07:06.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7731" for this suite. 03/30/23 14:07:06.91
------------------------------
â€¢ [SLOW TEST] [24.037 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:06:42.876
    Mar 30 14:06:42.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename subpath 03/30/23 14:06:42.876
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:06:42.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:06:42.883
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/30/23 14:06:42.884
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-pjqd 03/30/23 14:06:42.888
    STEP: Creating a pod to test atomic-volume-subpath 03/30/23 14:06:42.888
    Mar 30 14:06:42.892: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pjqd" in namespace "subpath-7731" to be "Succeeded or Failed"
    Mar 30 14:06:42.893: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.491324ms
    Mar 30 14:06:44.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 2.004274672s
    Mar 30 14:06:46.895: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 4.003963709s
    Mar 30 14:06:48.895: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 6.003665718s
    Mar 30 14:06:50.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 8.004349961s
    Mar 30 14:06:52.895: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 10.003884289s
    Mar 30 14:06:54.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 12.004330344s
    Mar 30 14:06:56.895: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 14.00393231s
    Mar 30 14:06:58.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 16.004363584s
    Mar 30 14:07:00.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 18.004420395s
    Mar 30 14:07:02.895: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=true. Elapsed: 20.003531505s
    Mar 30 14:07:04.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Running", Reason="", readiness=false. Elapsed: 22.004499135s
    Mar 30 14:07:06.896: INFO: Pod "pod-subpath-test-projected-pjqd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004037067s
    STEP: Saw pod success 03/30/23 14:07:06.896
    Mar 30 14:07:06.896: INFO: Pod "pod-subpath-test-projected-pjqd" satisfied condition "Succeeded or Failed"
    Mar 30 14:07:06.897: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-subpath-test-projected-pjqd container test-container-subpath-projected-pjqd: <nil>
    STEP: delete the pod 03/30/23 14:07:06.9
    Mar 30 14:07:06.905: INFO: Waiting for pod pod-subpath-test-projected-pjqd to disappear
    Mar 30 14:07:06.906: INFO: Pod pod-subpath-test-projected-pjqd no longer exists
    STEP: Deleting pod pod-subpath-test-projected-pjqd 03/30/23 14:07:06.906
    Mar 30 14:07:06.906: INFO: Deleting pod "pod-subpath-test-projected-pjqd" in namespace "subpath-7731"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:07:06.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7731" for this suite. 03/30/23 14:07:06.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:07:06.913
Mar 30 14:07:06.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 14:07:06.913
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:07:06.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:07:06.92
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/30/23 14:07:06.922
Mar 30 14:07:06.925: INFO: Waiting up to 5m0s for pod "pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b" in namespace "emptydir-859" to be "Succeeded or Failed"
Mar 30 14:07:06.926: INFO: Pod "pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.426683ms
Mar 30 14:07:08.929: INFO: Pod "pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004188853s
Mar 30 14:07:10.930: INFO: Pod "pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005049602s
STEP: Saw pod success 03/30/23 14:07:10.93
Mar 30 14:07:10.930: INFO: Pod "pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b" satisfied condition "Succeeded or Failed"
Mar 30 14:07:10.932: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b container test-container: <nil>
STEP: delete the pod 03/30/23 14:07:10.935
Mar 30 14:07:10.939: INFO: Waiting for pod pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b to disappear
Mar 30 14:07:10.940: INFO: Pod pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 14:07:10.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-859" for this suite. 03/30/23 14:07:10.943
------------------------------
â€¢ [4.033 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:07:06.913
    Mar 30 14:07:06.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 14:07:06.913
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:07:06.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:07:06.92
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/30/23 14:07:06.922
    Mar 30 14:07:06.925: INFO: Waiting up to 5m0s for pod "pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b" in namespace "emptydir-859" to be "Succeeded or Failed"
    Mar 30 14:07:06.926: INFO: Pod "pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.426683ms
    Mar 30 14:07:08.929: INFO: Pod "pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004188853s
    Mar 30 14:07:10.930: INFO: Pod "pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005049602s
    STEP: Saw pod success 03/30/23 14:07:10.93
    Mar 30 14:07:10.930: INFO: Pod "pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b" satisfied condition "Succeeded or Failed"
    Mar 30 14:07:10.932: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b container test-container: <nil>
    STEP: delete the pod 03/30/23 14:07:10.935
    Mar 30 14:07:10.939: INFO: Waiting for pod pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b to disappear
    Mar 30 14:07:10.940: INFO: Pod pod-4d03ffc2-f08d-429e-82f3-7870c8f1360b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:07:10.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-859" for this suite. 03/30/23 14:07:10.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:07:10.946
Mar 30 14:07:10.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 14:07:10.947
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:07:10.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:07:10.953
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 14:07:10.959
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:07:11.411
STEP: Deploying the webhook pod 03/30/23 14:07:11.415
STEP: Wait for the deployment to be ready 03/30/23 14:07:11.42
Mar 30 14:07:11.423: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 14:07:13.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:07:15.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:07:17.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:07:19.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:07:21.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/30/23 14:07:23.431
STEP: Verifying the service has paired with the endpoint 03/30/23 14:07:23.437
Mar 30 14:07:24.437: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 03/30/23 14:07:24.464
STEP: Creating a configMap that should be mutated 03/30/23 14:07:24.472
STEP: Deleting the collection of validation webhooks 03/30/23 14:07:24.486
STEP: Creating a configMap that should not be mutated 03/30/23 14:07:24.504
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:07:24.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7840" for this suite. 03/30/23 14:07:24.528
STEP: Destroying namespace "webhook-7840-markers" for this suite. 03/30/23 14:07:24.531
------------------------------
â€¢ [SLOW TEST] [13.588 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:07:10.946
    Mar 30 14:07:10.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 14:07:10.947
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:07:10.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:07:10.953
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 14:07:10.959
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:07:11.411
    STEP: Deploying the webhook pod 03/30/23 14:07:11.415
    STEP: Wait for the deployment to be ready 03/30/23 14:07:11.42
    Mar 30 14:07:11.423: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 30 14:07:13.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:07:15.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:07:17.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:07:19.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:07:21.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/30/23 14:07:23.431
    STEP: Verifying the service has paired with the endpoint 03/30/23 14:07:23.437
    Mar 30 14:07:24.437: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 03/30/23 14:07:24.464
    STEP: Creating a configMap that should be mutated 03/30/23 14:07:24.472
    STEP: Deleting the collection of validation webhooks 03/30/23 14:07:24.486
    STEP: Creating a configMap that should not be mutated 03/30/23 14:07:24.504
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:07:24.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7840" for this suite. 03/30/23 14:07:24.528
    STEP: Destroying namespace "webhook-7840-markers" for this suite. 03/30/23 14:07:24.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:07:24.534
Mar 30 14:07:24.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename dns 03/30/23 14:07:24.535
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:07:24.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:07:24.542
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 03/30/23 14:07:24.544
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local;sleep 1; done
 03/30/23 14:07:24.546
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-889.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local;sleep 1; done
 03/30/23 14:07:24.547
STEP: creating a pod to probe DNS 03/30/23 14:07:24.547
STEP: submitting the pod to kubernetes 03/30/23 14:07:24.547
Mar 30 14:07:24.551: INFO: Waiting up to 15m0s for pod "dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5" in namespace "dns-889" to be "running"
Mar 30 14:07:24.554: INFO: Pod "dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.416323ms
Mar 30 14:07:26.556: INFO: Pod "dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004795342s
Mar 30 14:07:26.556: INFO: Pod "dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5" satisfied condition "running"
STEP: retrieving the pod 03/30/23 14:07:26.556
STEP: looking for the results for each expected name from probers 03/30/23 14:07:26.558
Mar 30 14:07:26.561: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:26.562: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:26.564: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:26.566: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:26.567: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:26.569: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:26.570: INFO: Unable to read jessie_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:26.572: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:26.572: INFO: Lookups using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local]

Mar 30 14:07:31.578: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:31.580: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:31.582: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:31.584: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:31.586: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:31.588: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:31.589: INFO: Unable to read jessie_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:31.591: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:31.591: INFO: Lookups using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local]

Mar 30 14:07:36.577: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:36.579: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:36.581: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:36.583: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:36.584: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:36.586: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:36.588: INFO: Unable to read jessie_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:36.589: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:36.589: INFO: Lookups using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local]

Mar 30 14:07:41.578: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:41.580: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:41.582: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:41.584: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:41.585: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:41.587: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:41.588: INFO: Unable to read jessie_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:41.590: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:41.590: INFO: Lookups using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local]

Mar 30 14:07:46.578: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:46.580: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:46.582: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:46.584: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:46.586: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:46.587: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:46.589: INFO: Unable to read jessie_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:46.591: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:46.591: INFO: Lookups using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local]

Mar 30 14:07:51.579: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:51.580: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:51.582: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:51.584: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:51.585: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:51.587: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:51.588: INFO: Unable to read jessie_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:51.590: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
Mar 30 14:07:51.590: INFO: Lookups using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local]

Mar 30 14:07:56.590: INFO: DNS probes using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 succeeded

STEP: deleting the pod 03/30/23 14:07:56.59
STEP: deleting the test headless service 03/30/23 14:07:56.594
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 30 14:07:56.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-889" for this suite. 03/30/23 14:07:56.604
------------------------------
â€¢ [SLOW TEST] [32.072 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:07:24.534
    Mar 30 14:07:24.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename dns 03/30/23 14:07:24.535
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:07:24.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:07:24.542
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 03/30/23 14:07:24.544
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local;sleep 1; done
     03/30/23 14:07:24.546
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-889.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-889.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local;sleep 1; done
     03/30/23 14:07:24.547
    STEP: creating a pod to probe DNS 03/30/23 14:07:24.547
    STEP: submitting the pod to kubernetes 03/30/23 14:07:24.547
    Mar 30 14:07:24.551: INFO: Waiting up to 15m0s for pod "dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5" in namespace "dns-889" to be "running"
    Mar 30 14:07:24.554: INFO: Pod "dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.416323ms
    Mar 30 14:07:26.556: INFO: Pod "dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004795342s
    Mar 30 14:07:26.556: INFO: Pod "dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5" satisfied condition "running"
    STEP: retrieving the pod 03/30/23 14:07:26.556
    STEP: looking for the results for each expected name from probers 03/30/23 14:07:26.558
    Mar 30 14:07:26.561: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:26.562: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:26.564: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:26.566: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:26.567: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:26.569: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:26.570: INFO: Unable to read jessie_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:26.572: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:26.572: INFO: Lookups using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local]

    Mar 30 14:07:31.578: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:31.580: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:31.582: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:31.584: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:31.586: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:31.588: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:31.589: INFO: Unable to read jessie_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:31.591: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:31.591: INFO: Lookups using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local]

    Mar 30 14:07:36.577: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:36.579: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:36.581: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:36.583: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:36.584: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:36.586: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:36.588: INFO: Unable to read jessie_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:36.589: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:36.589: INFO: Lookups using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local]

    Mar 30 14:07:41.578: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:41.580: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:41.582: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:41.584: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:41.585: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:41.587: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:41.588: INFO: Unable to read jessie_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:41.590: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:41.590: INFO: Lookups using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local]

    Mar 30 14:07:46.578: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:46.580: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:46.582: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:46.584: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:46.586: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:46.587: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:46.589: INFO: Unable to read jessie_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:46.591: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:46.591: INFO: Lookups using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local]

    Mar 30 14:07:51.579: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:51.580: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:51.582: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:51.584: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:51.585: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:51.587: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:51.588: INFO: Unable to read jessie_udp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:51.590: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local from pod dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5: the server could not find the requested resource (get pods dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5)
    Mar 30 14:07:51.590: INFO: Lookups using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local wheezy_udp@dns-test-service-2.dns-889.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-889.svc.cluster.local jessie_udp@dns-test-service-2.dns-889.svc.cluster.local jessie_tcp@dns-test-service-2.dns-889.svc.cluster.local]

    Mar 30 14:07:56.590: INFO: DNS probes using dns-889/dns-test-322d8d94-9499-4d48-8ad8-519fcccea0c5 succeeded

    STEP: deleting the pod 03/30/23 14:07:56.59
    STEP: deleting the test headless service 03/30/23 14:07:56.594
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:07:56.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-889" for this suite. 03/30/23 14:07:56.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:07:56.608
Mar 30 14:07:56.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename resourcequota 03/30/23 14:07:56.608
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:07:56.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:07:56.615
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 03/30/23 14:07:56.616
STEP: Ensuring ResourceQuota status is calculated 03/30/23 14:07:56.618
STEP: Creating a ResourceQuota with not terminating scope 03/30/23 14:07:58.621
STEP: Ensuring ResourceQuota status is calculated 03/30/23 14:07:58.623
STEP: Creating a long running pod 03/30/23 14:08:00.626
STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/30/23 14:08:00.632
STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/30/23 14:08:02.635
STEP: Deleting the pod 03/30/23 14:08:04.638
STEP: Ensuring resource quota status released the pod usage 03/30/23 14:08:04.643
STEP: Creating a terminating pod 03/30/23 14:08:06.646
STEP: Ensuring resource quota with terminating scope captures the pod usage 03/30/23 14:08:06.651
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/30/23 14:08:08.654
STEP: Deleting the pod 03/30/23 14:08:10.657
STEP: Ensuring resource quota status released the pod usage 03/30/23 14:08:10.661
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 30 14:08:12.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9072" for this suite. 03/30/23 14:08:12.666
------------------------------
â€¢ [SLOW TEST] [16.061 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:07:56.608
    Mar 30 14:07:56.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename resourcequota 03/30/23 14:07:56.608
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:07:56.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:07:56.615
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 03/30/23 14:07:56.616
    STEP: Ensuring ResourceQuota status is calculated 03/30/23 14:07:56.618
    STEP: Creating a ResourceQuota with not terminating scope 03/30/23 14:07:58.621
    STEP: Ensuring ResourceQuota status is calculated 03/30/23 14:07:58.623
    STEP: Creating a long running pod 03/30/23 14:08:00.626
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/30/23 14:08:00.632
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/30/23 14:08:02.635
    STEP: Deleting the pod 03/30/23 14:08:04.638
    STEP: Ensuring resource quota status released the pod usage 03/30/23 14:08:04.643
    STEP: Creating a terminating pod 03/30/23 14:08:06.646
    STEP: Ensuring resource quota with terminating scope captures the pod usage 03/30/23 14:08:06.651
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/30/23 14:08:08.654
    STEP: Deleting the pod 03/30/23 14:08:10.657
    STEP: Ensuring resource quota status released the pod usage 03/30/23 14:08:10.661
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:08:12.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9072" for this suite. 03/30/23 14:08:12.666
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:08:12.669
Mar 30 14:08:12.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename resourcequota 03/30/23 14:08:12.67
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:08:12.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:08:12.677
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-crzch" 03/30/23 14:08:12.68
Mar 30 14:08:12.683: INFO: Resource quota "e2e-rq-status-crzch" reports spec: hard cpu limit of 500m
Mar 30 14:08:12.683: INFO: Resource quota "e2e-rq-status-crzch" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-crzch" /status 03/30/23 14:08:12.683
STEP: Confirm /status for "e2e-rq-status-crzch" resourceQuota via watch 03/30/23 14:08:12.686
Mar 30 14:08:12.687: INFO: observed resourceQuota "e2e-rq-status-crzch" in namespace "resourcequota-8960" with hard status: v1.ResourceList(nil)
Mar 30 14:08:12.687: INFO: Found resourceQuota "e2e-rq-status-crzch" in namespace "resourcequota-8960" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Mar 30 14:08:12.687: INFO: ResourceQuota "e2e-rq-status-crzch" /status was updated
STEP: Patching hard spec values for cpu & memory 03/30/23 14:08:12.689
Mar 30 14:08:12.691: INFO: Resource quota "e2e-rq-status-crzch" reports spec: hard cpu limit of 1
Mar 30 14:08:12.691: INFO: Resource quota "e2e-rq-status-crzch" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-crzch" /status 03/30/23 14:08:12.691
STEP: Confirm /status for "e2e-rq-status-crzch" resourceQuota via watch 03/30/23 14:08:12.694
Mar 30 14:08:12.695: INFO: observed resourceQuota "e2e-rq-status-crzch" in namespace "resourcequota-8960" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Mar 30 14:08:12.695: INFO: Found resourceQuota "e2e-rq-status-crzch" in namespace "resourcequota-8960" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Mar 30 14:08:12.695: INFO: ResourceQuota "e2e-rq-status-crzch" /status was patched
STEP: Get "e2e-rq-status-crzch" /status 03/30/23 14:08:12.695
Mar 30 14:08:12.697: INFO: Resourcequota "e2e-rq-status-crzch" reports status: hard cpu of 1
Mar 30 14:08:12.697: INFO: Resourcequota "e2e-rq-status-crzch" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-crzch" /status before checking Spec is unchanged 03/30/23 14:08:12.698
Mar 30 14:08:12.700: INFO: Resourcequota "e2e-rq-status-crzch" reports status: hard cpu of 2
Mar 30 14:08:12.700: INFO: Resourcequota "e2e-rq-status-crzch" reports status: hard memory of 2Gi
Mar 30 14:08:12.701: INFO: Found resourceQuota "e2e-rq-status-crzch" in namespace "resourcequota-8960" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Mar 30 14:12:12.705: INFO: ResourceQuota "e2e-rq-status-crzch" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 30 14:12:12.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8960" for this suite. 03/30/23 14:12:12.707
------------------------------
â€¢ [SLOW TEST] [240.041 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:08:12.669
    Mar 30 14:08:12.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename resourcequota 03/30/23 14:08:12.67
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:08:12.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:08:12.677
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-crzch" 03/30/23 14:08:12.68
    Mar 30 14:08:12.683: INFO: Resource quota "e2e-rq-status-crzch" reports spec: hard cpu limit of 500m
    Mar 30 14:08:12.683: INFO: Resource quota "e2e-rq-status-crzch" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-crzch" /status 03/30/23 14:08:12.683
    STEP: Confirm /status for "e2e-rq-status-crzch" resourceQuota via watch 03/30/23 14:08:12.686
    Mar 30 14:08:12.687: INFO: observed resourceQuota "e2e-rq-status-crzch" in namespace "resourcequota-8960" with hard status: v1.ResourceList(nil)
    Mar 30 14:08:12.687: INFO: Found resourceQuota "e2e-rq-status-crzch" in namespace "resourcequota-8960" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Mar 30 14:08:12.687: INFO: ResourceQuota "e2e-rq-status-crzch" /status was updated
    STEP: Patching hard spec values for cpu & memory 03/30/23 14:08:12.689
    Mar 30 14:08:12.691: INFO: Resource quota "e2e-rq-status-crzch" reports spec: hard cpu limit of 1
    Mar 30 14:08:12.691: INFO: Resource quota "e2e-rq-status-crzch" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-crzch" /status 03/30/23 14:08:12.691
    STEP: Confirm /status for "e2e-rq-status-crzch" resourceQuota via watch 03/30/23 14:08:12.694
    Mar 30 14:08:12.695: INFO: observed resourceQuota "e2e-rq-status-crzch" in namespace "resourcequota-8960" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Mar 30 14:08:12.695: INFO: Found resourceQuota "e2e-rq-status-crzch" in namespace "resourcequota-8960" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Mar 30 14:08:12.695: INFO: ResourceQuota "e2e-rq-status-crzch" /status was patched
    STEP: Get "e2e-rq-status-crzch" /status 03/30/23 14:08:12.695
    Mar 30 14:08:12.697: INFO: Resourcequota "e2e-rq-status-crzch" reports status: hard cpu of 1
    Mar 30 14:08:12.697: INFO: Resourcequota "e2e-rq-status-crzch" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-crzch" /status before checking Spec is unchanged 03/30/23 14:08:12.698
    Mar 30 14:08:12.700: INFO: Resourcequota "e2e-rq-status-crzch" reports status: hard cpu of 2
    Mar 30 14:08:12.700: INFO: Resourcequota "e2e-rq-status-crzch" reports status: hard memory of 2Gi
    Mar 30 14:08:12.701: INFO: Found resourceQuota "e2e-rq-status-crzch" in namespace "resourcequota-8960" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Mar 30 14:12:12.705: INFO: ResourceQuota "e2e-rq-status-crzch" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:12:12.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8960" for this suite. 03/30/23 14:12:12.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:12:12.71
Mar 30 14:12:12.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-probe 03/30/23 14:12:12.711
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:12:12.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:12:12.718
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-9654fe67-7f00-4326-8b65-40d55343315a in namespace container-probe-9126 03/30/23 14:12:12.719
Mar 30 14:12:12.723: INFO: Waiting up to 5m0s for pod "busybox-9654fe67-7f00-4326-8b65-40d55343315a" in namespace "container-probe-9126" to be "not pending"
Mar 30 14:12:12.725: INFO: Pod "busybox-9654fe67-7f00-4326-8b65-40d55343315a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10908ms
Mar 30 14:12:14.728: INFO: Pod "busybox-9654fe67-7f00-4326-8b65-40d55343315a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004944946s
Mar 30 14:12:14.728: INFO: Pod "busybox-9654fe67-7f00-4326-8b65-40d55343315a" satisfied condition "not pending"
Mar 30 14:12:14.728: INFO: Started pod busybox-9654fe67-7f00-4326-8b65-40d55343315a in namespace container-probe-9126
STEP: checking the pod's current state and verifying that restartCount is present 03/30/23 14:12:14.728
Mar 30 14:12:14.730: INFO: Initial restart count of pod busybox-9654fe67-7f00-4326-8b65-40d55343315a is 0
Mar 30 14:13:04.793: INFO: Restart count of pod container-probe-9126/busybox-9654fe67-7f00-4326-8b65-40d55343315a is now 1 (50.063242357s elapsed)
STEP: deleting the pod 03/30/23 14:13:04.793
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 30 14:13:04.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9126" for this suite. 03/30/23 14:13:04.8
------------------------------
â€¢ [SLOW TEST] [52.092 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:12:12.71
    Mar 30 14:12:12.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-probe 03/30/23 14:12:12.711
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:12:12.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:12:12.718
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-9654fe67-7f00-4326-8b65-40d55343315a in namespace container-probe-9126 03/30/23 14:12:12.719
    Mar 30 14:12:12.723: INFO: Waiting up to 5m0s for pod "busybox-9654fe67-7f00-4326-8b65-40d55343315a" in namespace "container-probe-9126" to be "not pending"
    Mar 30 14:12:12.725: INFO: Pod "busybox-9654fe67-7f00-4326-8b65-40d55343315a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10908ms
    Mar 30 14:12:14.728: INFO: Pod "busybox-9654fe67-7f00-4326-8b65-40d55343315a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004944946s
    Mar 30 14:12:14.728: INFO: Pod "busybox-9654fe67-7f00-4326-8b65-40d55343315a" satisfied condition "not pending"
    Mar 30 14:12:14.728: INFO: Started pod busybox-9654fe67-7f00-4326-8b65-40d55343315a in namespace container-probe-9126
    STEP: checking the pod's current state and verifying that restartCount is present 03/30/23 14:12:14.728
    Mar 30 14:12:14.730: INFO: Initial restart count of pod busybox-9654fe67-7f00-4326-8b65-40d55343315a is 0
    Mar 30 14:13:04.793: INFO: Restart count of pod container-probe-9126/busybox-9654fe67-7f00-4326-8b65-40d55343315a is now 1 (50.063242357s elapsed)
    STEP: deleting the pod 03/30/23 14:13:04.793
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:13:04.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9126" for this suite. 03/30/23 14:13:04.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:13:04.803
Mar 30 14:13:04.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 14:13:04.804
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:13:04.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:13:04.811
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 03/30/23 14:13:04.812
Mar 30 14:13:04.816: INFO: Waiting up to 5m0s for pod "annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5" in namespace "projected-7346" to be "running and ready"
Mar 30 14:13:04.817: INFO: Pod "annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.541424ms
Mar 30 14:13:04.818: INFO: The phase of Pod annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 14:13:06.820: INFO: Pod "annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5": Phase="Running", Reason="", readiness=true. Elapsed: 2.003754603s
Mar 30 14:13:06.820: INFO: The phase of Pod annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5 is Running (Ready = true)
Mar 30 14:13:06.820: INFO: Pod "annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5" satisfied condition "running and ready"
Mar 30 14:13:07.339: INFO: Successfully updated pod "annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 30 14:13:09.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7346" for this suite. 03/30/23 14:13:09.348
------------------------------
â€¢ [4.548 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:13:04.803
    Mar 30 14:13:04.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 14:13:04.804
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:13:04.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:13:04.811
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 03/30/23 14:13:04.812
    Mar 30 14:13:04.816: INFO: Waiting up to 5m0s for pod "annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5" in namespace "projected-7346" to be "running and ready"
    Mar 30 14:13:04.817: INFO: Pod "annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.541424ms
    Mar 30 14:13:04.818: INFO: The phase of Pod annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 14:13:06.820: INFO: Pod "annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5": Phase="Running", Reason="", readiness=true. Elapsed: 2.003754603s
    Mar 30 14:13:06.820: INFO: The phase of Pod annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5 is Running (Ready = true)
    Mar 30 14:13:06.820: INFO: Pod "annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5" satisfied condition "running and ready"
    Mar 30 14:13:07.339: INFO: Successfully updated pod "annotationupdated6f2f3d3-3380-4ffa-9349-642b7861aec5"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:13:09.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7346" for this suite. 03/30/23 14:13:09.348
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:13:09.351
Mar 30 14:13:09.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 14:13:09.352
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:13:09.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:13:09.358
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-77423e5d-13d6-4ccf-a063-0acc88898a8e 03/30/23 14:13:09.36
STEP: Creating a pod to test consume secrets 03/30/23 14:13:09.362
Mar 30 14:13:09.366: INFO: Waiting up to 5m0s for pod "pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04" in namespace "secrets-6860" to be "Succeeded or Failed"
Mar 30 14:13:09.367: INFO: Pod "pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04": Phase="Pending", Reason="", readiness=false. Elapsed: 1.369666ms
Mar 30 14:13:11.369: INFO: Pod "pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00330553s
Mar 30 14:13:13.369: INFO: Pod "pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003416673s
STEP: Saw pod success 03/30/23 14:13:13.369
Mar 30 14:13:13.369: INFO: Pod "pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04" satisfied condition "Succeeded or Failed"
Mar 30 14:13:13.371: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04 container secret-env-test: <nil>
STEP: delete the pod 03/30/23 14:13:13.374
Mar 30 14:13:13.379: INFO: Waiting for pod pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04 to disappear
Mar 30 14:13:13.380: INFO: Pod pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 14:13:13.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6860" for this suite. 03/30/23 14:13:13.383
------------------------------
â€¢ [4.034 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:13:09.351
    Mar 30 14:13:09.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 14:13:09.352
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:13:09.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:13:09.358
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-77423e5d-13d6-4ccf-a063-0acc88898a8e 03/30/23 14:13:09.36
    STEP: Creating a pod to test consume secrets 03/30/23 14:13:09.362
    Mar 30 14:13:09.366: INFO: Waiting up to 5m0s for pod "pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04" in namespace "secrets-6860" to be "Succeeded or Failed"
    Mar 30 14:13:09.367: INFO: Pod "pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04": Phase="Pending", Reason="", readiness=false. Elapsed: 1.369666ms
    Mar 30 14:13:11.369: INFO: Pod "pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00330553s
    Mar 30 14:13:13.369: INFO: Pod "pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003416673s
    STEP: Saw pod success 03/30/23 14:13:13.369
    Mar 30 14:13:13.369: INFO: Pod "pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04" satisfied condition "Succeeded or Failed"
    Mar 30 14:13:13.371: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04 container secret-env-test: <nil>
    STEP: delete the pod 03/30/23 14:13:13.374
    Mar 30 14:13:13.379: INFO: Waiting for pod pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04 to disappear
    Mar 30 14:13:13.380: INFO: Pod pod-secrets-97ee21c8-d088-4bf7-afa8-b3576d056d04 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:13:13.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6860" for this suite. 03/30/23 14:13:13.383
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:13:13.385
Mar 30 14:13:13.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 14:13:13.386
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:13:13.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:13:13.392
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-b07305f9-b5e8-449e-90cb-5db2c060b15d 03/30/23 14:13:13.394
STEP: Creating a pod to test consume configMaps 03/30/23 14:13:13.396
Mar 30 14:13:13.399: INFO: Waiting up to 5m0s for pod "pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635" in namespace "configmap-9066" to be "Succeeded or Failed"
Mar 30 14:13:13.401: INFO: Pod "pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635": Phase="Pending", Reason="", readiness=false. Elapsed: 1.883414ms
Mar 30 14:13:15.404: INFO: Pod "pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004640127s
Mar 30 14:13:17.403: INFO: Pod "pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003810503s
STEP: Saw pod success 03/30/23 14:13:17.403
Mar 30 14:13:17.403: INFO: Pod "pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635" satisfied condition "Succeeded or Failed"
Mar 30 14:13:17.405: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635 container agnhost-container: <nil>
STEP: delete the pod 03/30/23 14:13:17.408
Mar 30 14:13:17.412: INFO: Waiting for pod pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635 to disappear
Mar 30 14:13:17.413: INFO: Pod pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 14:13:17.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9066" for this suite. 03/30/23 14:13:17.416
------------------------------
â€¢ [4.033 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:13:13.385
    Mar 30 14:13:13.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 14:13:13.386
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:13:13.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:13:13.392
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-b07305f9-b5e8-449e-90cb-5db2c060b15d 03/30/23 14:13:13.394
    STEP: Creating a pod to test consume configMaps 03/30/23 14:13:13.396
    Mar 30 14:13:13.399: INFO: Waiting up to 5m0s for pod "pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635" in namespace "configmap-9066" to be "Succeeded or Failed"
    Mar 30 14:13:13.401: INFO: Pod "pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635": Phase="Pending", Reason="", readiness=false. Elapsed: 1.883414ms
    Mar 30 14:13:15.404: INFO: Pod "pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004640127s
    Mar 30 14:13:17.403: INFO: Pod "pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003810503s
    STEP: Saw pod success 03/30/23 14:13:17.403
    Mar 30 14:13:17.403: INFO: Pod "pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635" satisfied condition "Succeeded or Failed"
    Mar 30 14:13:17.405: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635 container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 14:13:17.408
    Mar 30 14:13:17.412: INFO: Waiting for pod pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635 to disappear
    Mar 30 14:13:17.413: INFO: Pod pod-configmaps-5ff26853-225e-414a-8f55-98be75d04635 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:13:17.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9066" for this suite. 03/30/23 14:13:17.416
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:13:17.419
Mar 30 14:13:17.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 14:13:17.419
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:13:17.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:13:17.426
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-1a552eae-56a0-42cc-bbbb-0e977493e623 03/30/23 14:13:17.428
STEP: Creating a pod to test consume configMaps 03/30/23 14:13:17.43
Mar 30 14:13:17.433: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e" in namespace "projected-3635" to be "Succeeded or Failed"
Mar 30 14:13:17.436: INFO: Pod "pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051999ms
Mar 30 14:13:19.438: INFO: Pod "pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00476941s
Mar 30 14:13:21.439: INFO: Pod "pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005419755s
STEP: Saw pod success 03/30/23 14:13:21.439
Mar 30 14:13:21.439: INFO: Pod "pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e" satisfied condition "Succeeded or Failed"
Mar 30 14:13:21.441: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e container projected-configmap-volume-test: <nil>
STEP: delete the pod 03/30/23 14:13:21.444
Mar 30 14:13:21.448: INFO: Waiting for pod pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e to disappear
Mar 30 14:13:21.450: INFO: Pod pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 30 14:13:21.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3635" for this suite. 03/30/23 14:13:21.452
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:13:17.419
    Mar 30 14:13:17.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 14:13:17.419
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:13:17.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:13:17.426
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-1a552eae-56a0-42cc-bbbb-0e977493e623 03/30/23 14:13:17.428
    STEP: Creating a pod to test consume configMaps 03/30/23 14:13:17.43
    Mar 30 14:13:17.433: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e" in namespace "projected-3635" to be "Succeeded or Failed"
    Mar 30 14:13:17.436: INFO: Pod "pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051999ms
    Mar 30 14:13:19.438: INFO: Pod "pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00476941s
    Mar 30 14:13:21.439: INFO: Pod "pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005419755s
    STEP: Saw pod success 03/30/23 14:13:21.439
    Mar 30 14:13:21.439: INFO: Pod "pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e" satisfied condition "Succeeded or Failed"
    Mar 30 14:13:21.441: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e container projected-configmap-volume-test: <nil>
    STEP: delete the pod 03/30/23 14:13:21.444
    Mar 30 14:13:21.448: INFO: Waiting for pod pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e to disappear
    Mar 30 14:13:21.450: INFO: Pod pod-projected-configmaps-0e1f44b9-7c11-4b43-9656-0267a4f8692e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:13:21.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3635" for this suite. 03/30/23 14:13:21.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:13:21.455
Mar 30 14:13:21.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pod-network-test 03/30/23 14:13:21.456
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:13:21.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:13:21.462
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-4704 03/30/23 14:13:21.464
STEP: creating a selector 03/30/23 14:13:21.464
STEP: Creating the service pods in kubernetes 03/30/23 14:13:21.464
Mar 30 14:13:21.464: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 30 14:13:21.495: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4704" to be "running and ready"
Mar 30 14:13:21.497: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.948397ms
Mar 30 14:13:21.497: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 14:13:23.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005393245s
Mar 30 14:13:23.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:13:25.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004690456s
Mar 30 14:13:25.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:13:27.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004768375s
Mar 30 14:13:27.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:13:29.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004551743s
Mar 30 14:13:29.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:13:31.499: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004042127s
Mar 30 14:13:31.499: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:13:33.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.004985701s
Mar 30 14:13:33.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:13:35.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005070061s
Mar 30 14:13:35.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:13:37.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.004734119s
Mar 30 14:13:37.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:13:39.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004966182s
Mar 30 14:13:39.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:13:41.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004963752s
Mar 30 14:13:41.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:13:43.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.005080664s
Mar 30 14:13:43.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:13:45.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 24.00471504s
Mar 30 14:13:45.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:13:47.501: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 26.005448192s
Mar 30 14:13:47.501: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 30 14:13:47.501: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 30 14:13:47.502: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4704" to be "running and ready"
Mar 30 14:13:47.504: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.460287ms
Mar 30 14:13:47.504: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 30 14:13:47.504: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 30 14:13:47.505: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4704" to be "running and ready"
Mar 30 14:13:47.506: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 1.345976ms
Mar 30 14:13:47.506: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar 30 14:13:49.509: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.003732456s
Mar 30 14:13:49.509: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar 30 14:13:51.509: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.004462606s
Mar 30 14:13:51.509: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar 30 14:13:53.509: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.003921629s
Mar 30 14:13:53.509: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar 30 14:13:55.509: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.004282791s
Mar 30 14:13:55.509: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar 30 14:13:57.509: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.004227715s
Mar 30 14:13:57.509: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 30 14:13:57.509: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/30/23 14:13:57.511
Mar 30 14:13:57.515: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4704" to be "running"
Mar 30 14:13:57.517: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.577049ms
Mar 30 14:13:59.519: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003804972s
Mar 30 14:13:59.519: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 30 14:13:59.521: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 30 14:13:59.521: INFO: Breadth first check of 10.29.1.71 on host 192.168.0.3...
Mar 30 14:13:59.522: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.29.1.34:9080/dial?request=hostname&protocol=udp&host=10.29.1.71&port=8081&tries=1'] Namespace:pod-network-test-4704 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 14:13:59.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 14:13:59.523: INFO: ExecWithOptions: Clientset creation
Mar 30 14:13:59.523: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-4704/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.29.1.34%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.29.1.71%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 30 14:13:59.571: INFO: Waiting for responses: map[]
Mar 30 14:13:59.571: INFO: reached 10.29.1.71 after 0/1 tries
Mar 30 14:13:59.571: INFO: Breadth first check of 10.29.0.207 on host 192.168.0.4...
Mar 30 14:13:59.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.29.1.34:9080/dial?request=hostname&protocol=udp&host=10.29.0.207&port=8081&tries=1'] Namespace:pod-network-test-4704 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 14:13:59.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 14:13:59.572: INFO: ExecWithOptions: Clientset creation
Mar 30 14:13:59.572: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-4704/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.29.1.34%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.29.0.207%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 30 14:13:59.614: INFO: Waiting for responses: map[]
Mar 30 14:13:59.614: INFO: reached 10.29.0.207 after 0/1 tries
Mar 30 14:13:59.614: INFO: Breadth first check of 10.29.1.33 on host 192.168.0.5...
Mar 30 14:13:59.616: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.29.1.34:9080/dial?request=hostname&protocol=udp&host=10.29.1.33&port=8081&tries=1'] Namespace:pod-network-test-4704 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 14:13:59.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 14:13:59.616: INFO: ExecWithOptions: Clientset creation
Mar 30 14:13:59.616: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-4704/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.29.1.34%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.29.1.33%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 30 14:13:59.660: INFO: Waiting for responses: map[]
Mar 30 14:13:59.660: INFO: reached 10.29.1.33 after 0/1 tries
Mar 30 14:13:59.660: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 30 14:13:59.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4704" for this suite. 03/30/23 14:13:59.662
------------------------------
â€¢ [SLOW TEST] [38.210 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:13:21.455
    Mar 30 14:13:21.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pod-network-test 03/30/23 14:13:21.456
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:13:21.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:13:21.462
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-4704 03/30/23 14:13:21.464
    STEP: creating a selector 03/30/23 14:13:21.464
    STEP: Creating the service pods in kubernetes 03/30/23 14:13:21.464
    Mar 30 14:13:21.464: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 30 14:13:21.495: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4704" to be "running and ready"
    Mar 30 14:13:21.497: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.948397ms
    Mar 30 14:13:21.497: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 14:13:23.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005393245s
    Mar 30 14:13:23.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:13:25.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004690456s
    Mar 30 14:13:25.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:13:27.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004768375s
    Mar 30 14:13:27.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:13:29.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004551743s
    Mar 30 14:13:29.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:13:31.499: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004042127s
    Mar 30 14:13:31.499: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:13:33.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.004985701s
    Mar 30 14:13:33.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:13:35.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005070061s
    Mar 30 14:13:35.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:13:37.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.004734119s
    Mar 30 14:13:37.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:13:39.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004966182s
    Mar 30 14:13:39.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:13:41.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004963752s
    Mar 30 14:13:41.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:13:43.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.005080664s
    Mar 30 14:13:43.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:13:45.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 24.00471504s
    Mar 30 14:13:45.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:13:47.501: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 26.005448192s
    Mar 30 14:13:47.501: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 30 14:13:47.501: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 30 14:13:47.502: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4704" to be "running and ready"
    Mar 30 14:13:47.504: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.460287ms
    Mar 30 14:13:47.504: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 30 14:13:47.504: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 30 14:13:47.505: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4704" to be "running and ready"
    Mar 30 14:13:47.506: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 1.345976ms
    Mar 30 14:13:47.506: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar 30 14:13:49.509: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.003732456s
    Mar 30 14:13:49.509: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar 30 14:13:51.509: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.004462606s
    Mar 30 14:13:51.509: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar 30 14:13:53.509: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.003921629s
    Mar 30 14:13:53.509: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar 30 14:13:55.509: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.004282791s
    Mar 30 14:13:55.509: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar 30 14:13:57.509: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.004227715s
    Mar 30 14:13:57.509: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 30 14:13:57.509: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/30/23 14:13:57.511
    Mar 30 14:13:57.515: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4704" to be "running"
    Mar 30 14:13:57.517: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.577049ms
    Mar 30 14:13:59.519: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003804972s
    Mar 30 14:13:59.519: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 30 14:13:59.521: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 30 14:13:59.521: INFO: Breadth first check of 10.29.1.71 on host 192.168.0.3...
    Mar 30 14:13:59.522: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.29.1.34:9080/dial?request=hostname&protocol=udp&host=10.29.1.71&port=8081&tries=1'] Namespace:pod-network-test-4704 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 14:13:59.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 14:13:59.523: INFO: ExecWithOptions: Clientset creation
    Mar 30 14:13:59.523: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-4704/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.29.1.34%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.29.1.71%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 30 14:13:59.571: INFO: Waiting for responses: map[]
    Mar 30 14:13:59.571: INFO: reached 10.29.1.71 after 0/1 tries
    Mar 30 14:13:59.571: INFO: Breadth first check of 10.29.0.207 on host 192.168.0.4...
    Mar 30 14:13:59.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.29.1.34:9080/dial?request=hostname&protocol=udp&host=10.29.0.207&port=8081&tries=1'] Namespace:pod-network-test-4704 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 14:13:59.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 14:13:59.572: INFO: ExecWithOptions: Clientset creation
    Mar 30 14:13:59.572: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-4704/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.29.1.34%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.29.0.207%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 30 14:13:59.614: INFO: Waiting for responses: map[]
    Mar 30 14:13:59.614: INFO: reached 10.29.0.207 after 0/1 tries
    Mar 30 14:13:59.614: INFO: Breadth first check of 10.29.1.33 on host 192.168.0.5...
    Mar 30 14:13:59.616: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.29.1.34:9080/dial?request=hostname&protocol=udp&host=10.29.1.33&port=8081&tries=1'] Namespace:pod-network-test-4704 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 14:13:59.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 14:13:59.616: INFO: ExecWithOptions: Clientset creation
    Mar 30 14:13:59.616: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-4704/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.29.1.34%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.29.1.33%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 30 14:13:59.660: INFO: Waiting for responses: map[]
    Mar 30 14:13:59.660: INFO: reached 10.29.1.33 after 0/1 tries
    Mar 30 14:13:59.660: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:13:59.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4704" for this suite. 03/30/23 14:13:59.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:13:59.667
Mar 30 14:13:59.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 14:13:59.668
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:13:59.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:13:59.675
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 03/30/23 14:13:59.676
Mar 30 14:13:59.679: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c" in namespace "projected-5350" to be "Succeeded or Failed"
Mar 30 14:13:59.681: INFO: Pod "downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.453142ms
Mar 30 14:14:01.683: INFO: Pod "downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003897457s
Mar 30 14:14:03.684: INFO: Pod "downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004314282s
STEP: Saw pod success 03/30/23 14:14:03.684
Mar 30 14:14:03.684: INFO: Pod "downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c" satisfied condition "Succeeded or Failed"
Mar 30 14:14:03.685: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c container client-container: <nil>
STEP: delete the pod 03/30/23 14:14:03.688
Mar 30 14:14:03.693: INFO: Waiting for pod downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c to disappear
Mar 30 14:14:03.695: INFO: Pod downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 30 14:14:03.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5350" for this suite. 03/30/23 14:14:03.697
------------------------------
â€¢ [4.032 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:13:59.667
    Mar 30 14:13:59.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 14:13:59.668
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:13:59.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:13:59.675
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 03/30/23 14:13:59.676
    Mar 30 14:13:59.679: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c" in namespace "projected-5350" to be "Succeeded or Failed"
    Mar 30 14:13:59.681: INFO: Pod "downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.453142ms
    Mar 30 14:14:01.683: INFO: Pod "downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003897457s
    Mar 30 14:14:03.684: INFO: Pod "downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004314282s
    STEP: Saw pod success 03/30/23 14:14:03.684
    Mar 30 14:14:03.684: INFO: Pod "downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c" satisfied condition "Succeeded or Failed"
    Mar 30 14:14:03.685: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c container client-container: <nil>
    STEP: delete the pod 03/30/23 14:14:03.688
    Mar 30 14:14:03.693: INFO: Waiting for pod downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c to disappear
    Mar 30 14:14:03.695: INFO: Pod downwardapi-volume-8938dc68-59d8-4d2e-adb6-767549ee7a4c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:14:03.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5350" for this suite. 03/30/23 14:14:03.697
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:14:03.699
Mar 30 14:14:03.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename certificates 03/30/23 14:14:03.7
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:14:03.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:14:03.706
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 03/30/23 14:14:04.835
STEP: getting /apis/certificates.k8s.io 03/30/23 14:14:04.837
STEP: getting /apis/certificates.k8s.io/v1 03/30/23 14:14:04.837
STEP: creating 03/30/23 14:14:04.838
STEP: getting 03/30/23 14:14:04.845
STEP: listing 03/30/23 14:14:04.846
STEP: watching 03/30/23 14:14:04.847
Mar 30 14:14:04.847: INFO: starting watch
STEP: patching 03/30/23 14:14:04.848
STEP: updating 03/30/23 14:14:04.851
Mar 30 14:14:04.853: INFO: waiting for watch events with expected annotations
Mar 30 14:14:04.853: INFO: saw patched and updated annotations
STEP: getting /approval 03/30/23 14:14:04.853
STEP: patching /approval 03/30/23 14:14:04.855
STEP: updating /approval 03/30/23 14:14:04.857
STEP: getting /status 03/30/23 14:14:04.86
STEP: patching /status 03/30/23 14:14:04.861
STEP: updating /status 03/30/23 14:14:04.865
STEP: deleting 03/30/23 14:14:04.868
STEP: deleting a collection 03/30/23 14:14:04.872
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:14:04.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-5085" for this suite. 03/30/23 14:14:04.88
------------------------------
â€¢ [1.183 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:14:03.699
    Mar 30 14:14:03.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename certificates 03/30/23 14:14:03.7
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:14:03.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:14:03.706
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 03/30/23 14:14:04.835
    STEP: getting /apis/certificates.k8s.io 03/30/23 14:14:04.837
    STEP: getting /apis/certificates.k8s.io/v1 03/30/23 14:14:04.837
    STEP: creating 03/30/23 14:14:04.838
    STEP: getting 03/30/23 14:14:04.845
    STEP: listing 03/30/23 14:14:04.846
    STEP: watching 03/30/23 14:14:04.847
    Mar 30 14:14:04.847: INFO: starting watch
    STEP: patching 03/30/23 14:14:04.848
    STEP: updating 03/30/23 14:14:04.851
    Mar 30 14:14:04.853: INFO: waiting for watch events with expected annotations
    Mar 30 14:14:04.853: INFO: saw patched and updated annotations
    STEP: getting /approval 03/30/23 14:14:04.853
    STEP: patching /approval 03/30/23 14:14:04.855
    STEP: updating /approval 03/30/23 14:14:04.857
    STEP: getting /status 03/30/23 14:14:04.86
    STEP: patching /status 03/30/23 14:14:04.861
    STEP: updating /status 03/30/23 14:14:04.865
    STEP: deleting 03/30/23 14:14:04.868
    STEP: deleting a collection 03/30/23 14:14:04.872
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:14:04.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-5085" for this suite. 03/30/23 14:14:04.88
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:14:04.883
Mar 30 14:14:04.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename runtimeclass 03/30/23 14:14:04.884
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:14:04.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:14:04.893
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-569-delete-me 03/30/23 14:14:04.896
STEP: Waiting for the RuntimeClass to disappear 03/30/23 14:14:04.898
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 30 14:14:04.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-569" for this suite. 03/30/23 14:14:04.905
------------------------------
â€¢ [0.025 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:14:04.883
    Mar 30 14:14:04.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename runtimeclass 03/30/23 14:14:04.884
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:14:04.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:14:04.893
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-569-delete-me 03/30/23 14:14:04.896
    STEP: Waiting for the RuntimeClass to disappear 03/30/23 14:14:04.898
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:14:04.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-569" for this suite. 03/30/23 14:14:04.905
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:14:04.908
Mar 30 14:14:04.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-probe 03/30/23 14:14:04.908
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:14:04.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:14:04.915
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 30 14:15:04.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5038" for this suite. 03/30/23 14:15:04.924
------------------------------
â€¢ [SLOW TEST] [60.019 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:14:04.908
    Mar 30 14:14:04.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-probe 03/30/23 14:14:04.908
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:14:04.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:14:04.915
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:15:04.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5038" for this suite. 03/30/23 14:15:04.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:15:04.927
Mar 30 14:15:04.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 14:15:04.928
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:15:04.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:15:04.935
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/30/23 14:15:04.936
Mar 30 14:15:04.940: INFO: Waiting up to 5m0s for pod "pod-7782fa3e-525f-4fdf-8667-31a273443fc2" in namespace "emptydir-2085" to be "Succeeded or Failed"
Mar 30 14:15:04.942: INFO: Pod "pod-7782fa3e-525f-4fdf-8667-31a273443fc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066314ms
Mar 30 14:15:06.944: INFO: Pod "pod-7782fa3e-525f-4fdf-8667-31a273443fc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004288966s
Mar 30 14:15:08.945: INFO: Pod "pod-7782fa3e-525f-4fdf-8667-31a273443fc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004640056s
STEP: Saw pod success 03/30/23 14:15:08.945
Mar 30 14:15:08.945: INFO: Pod "pod-7782fa3e-525f-4fdf-8667-31a273443fc2" satisfied condition "Succeeded or Failed"
Mar 30 14:15:08.946: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-7782fa3e-525f-4fdf-8667-31a273443fc2 container test-container: <nil>
STEP: delete the pod 03/30/23 14:15:08.95
Mar 30 14:15:08.954: INFO: Waiting for pod pod-7782fa3e-525f-4fdf-8667-31a273443fc2 to disappear
Mar 30 14:15:08.955: INFO: Pod pod-7782fa3e-525f-4fdf-8667-31a273443fc2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 14:15:08.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2085" for this suite. 03/30/23 14:15:08.957
------------------------------
â€¢ [4.032 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:15:04.927
    Mar 30 14:15:04.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 14:15:04.928
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:15:04.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:15:04.935
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/30/23 14:15:04.936
    Mar 30 14:15:04.940: INFO: Waiting up to 5m0s for pod "pod-7782fa3e-525f-4fdf-8667-31a273443fc2" in namespace "emptydir-2085" to be "Succeeded or Failed"
    Mar 30 14:15:04.942: INFO: Pod "pod-7782fa3e-525f-4fdf-8667-31a273443fc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066314ms
    Mar 30 14:15:06.944: INFO: Pod "pod-7782fa3e-525f-4fdf-8667-31a273443fc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004288966s
    Mar 30 14:15:08.945: INFO: Pod "pod-7782fa3e-525f-4fdf-8667-31a273443fc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004640056s
    STEP: Saw pod success 03/30/23 14:15:08.945
    Mar 30 14:15:08.945: INFO: Pod "pod-7782fa3e-525f-4fdf-8667-31a273443fc2" satisfied condition "Succeeded or Failed"
    Mar 30 14:15:08.946: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-7782fa3e-525f-4fdf-8667-31a273443fc2 container test-container: <nil>
    STEP: delete the pod 03/30/23 14:15:08.95
    Mar 30 14:15:08.954: INFO: Waiting for pod pod-7782fa3e-525f-4fdf-8667-31a273443fc2 to disappear
    Mar 30 14:15:08.955: INFO: Pod pod-7782fa3e-525f-4fdf-8667-31a273443fc2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:15:08.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2085" for this suite. 03/30/23 14:15:08.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:15:08.961
Mar 30 14:15:08.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 14:15:08.962
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:15:08.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:15:08.968
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 03/30/23 14:15:08.97
Mar 30 14:15:08.973: INFO: Waiting up to 5m0s for pod "annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f" in namespace "downward-api-1587" to be "running and ready"
Mar 30 14:15:08.975: INFO: Pod "annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.992126ms
Mar 30 14:15:08.976: INFO: The phase of Pod annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f is Pending, waiting for it to be Running (with Ready = true)
Mar 30 14:15:10.978: INFO: Pod "annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004625067s
Mar 30 14:15:10.978: INFO: The phase of Pod annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f is Running (Ready = true)
Mar 30 14:15:10.978: INFO: Pod "annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f" satisfied condition "running and ready"
Mar 30 14:15:11.488: INFO: Successfully updated pod "annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 30 14:15:15.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1587" for this suite. 03/30/23 14:15:15.505
------------------------------
â€¢ [SLOW TEST] [6.547 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:15:08.961
    Mar 30 14:15:08.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 14:15:08.962
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:15:08.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:15:08.968
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 03/30/23 14:15:08.97
    Mar 30 14:15:08.973: INFO: Waiting up to 5m0s for pod "annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f" in namespace "downward-api-1587" to be "running and ready"
    Mar 30 14:15:08.975: INFO: Pod "annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.992126ms
    Mar 30 14:15:08.976: INFO: The phase of Pod annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 14:15:10.978: INFO: Pod "annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004625067s
    Mar 30 14:15:10.978: INFO: The phase of Pod annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f is Running (Ready = true)
    Mar 30 14:15:10.978: INFO: Pod "annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f" satisfied condition "running and ready"
    Mar 30 14:15:11.488: INFO: Successfully updated pod "annotationupdatea6696574-a0bf-45d2-9e2a-51a05a3b1d4f"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:15:15.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1587" for this suite. 03/30/23 14:15:15.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:15:15.508
Mar 30 14:15:15.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename daemonsets 03/30/23 14:15:15.509
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:15:15.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:15:15.516
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Mar 30 14:15:15.526: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 03/30/23 14:15:15.528
Mar 30 14:15:15.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 14:15:15.530: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 03/30/23 14:15:15.53
Mar 30 14:15:15.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 14:15:15.542: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
Mar 30 14:15:16.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 30 14:15:16.545: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 03/30/23 14:15:16.546
Mar 30 14:15:16.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 30 14:15:16.554: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Mar 30 14:15:17.557: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 14:15:17.557: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/30/23 14:15:17.557
Mar 30 14:15:17.564: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 14:15:17.564: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
Mar 30 14:15:18.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 14:15:18.567: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
Mar 30 14:15:19.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 14:15:19.567: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
Mar 30 14:15:20.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 30 14:15:20.567: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/30/23 14:15:20.569
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4647, will wait for the garbage collector to delete the pods 03/30/23 14:15:20.569
Mar 30 14:15:20.624: INFO: Deleting DaemonSet.extensions daemon-set took: 2.566124ms
Mar 30 14:15:20.724: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.267914ms
Mar 30 14:15:23.526: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 14:15:23.526: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 30 14:15:23.528: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"89274"},"items":null}

Mar 30 14:15:23.529: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"89274"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:15:23.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4647" for this suite. 03/30/23 14:15:23.542
------------------------------
â€¢ [SLOW TEST] [8.036 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:15:15.508
    Mar 30 14:15:15.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename daemonsets 03/30/23 14:15:15.509
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:15:15.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:15:15.516
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Mar 30 14:15:15.526: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 03/30/23 14:15:15.528
    Mar 30 14:15:15.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 14:15:15.530: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 03/30/23 14:15:15.53
    Mar 30 14:15:15.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 14:15:15.542: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
    Mar 30 14:15:16.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 30 14:15:16.545: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 03/30/23 14:15:16.546
    Mar 30 14:15:16.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 30 14:15:16.554: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Mar 30 14:15:17.557: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 14:15:17.557: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/30/23 14:15:17.557
    Mar 30 14:15:17.564: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 14:15:17.564: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
    Mar 30 14:15:18.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 14:15:18.567: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
    Mar 30 14:15:19.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 14:15:19.567: INFO: Node cn-hongkong.192.168.0.5 is running 0 daemon pod, expected 1
    Mar 30 14:15:20.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 30 14:15:20.567: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/30/23 14:15:20.569
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4647, will wait for the garbage collector to delete the pods 03/30/23 14:15:20.569
    Mar 30 14:15:20.624: INFO: Deleting DaemonSet.extensions daemon-set took: 2.566124ms
    Mar 30 14:15:20.724: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.267914ms
    Mar 30 14:15:23.526: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 14:15:23.526: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 30 14:15:23.528: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"89274"},"items":null}

    Mar 30 14:15:23.529: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"89274"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:15:23.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4647" for this suite. 03/30/23 14:15:23.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:15:23.55
Mar 30 14:15:23.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename replicaset 03/30/23 14:15:23.551
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:15:23.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:15:23.558
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 03/30/23 14:15:23.561
STEP: Verify that the required pods have come up. 03/30/23 14:15:23.563
Mar 30 14:15:23.564: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 30 14:15:28.568: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/30/23 14:15:28.568
STEP: Getting /status 03/30/23 14:15:28.568
Mar 30 14:15:28.570: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 03/30/23 14:15:28.57
Mar 30 14:15:28.573: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 03/30/23 14:15:28.573
Mar 30 14:15:28.574: INFO: Observed &ReplicaSet event: ADDED
Mar 30 14:15:28.575: INFO: Observed &ReplicaSet event: MODIFIED
Mar 30 14:15:28.575: INFO: Observed &ReplicaSet event: MODIFIED
Mar 30 14:15:28.575: INFO: Observed &ReplicaSet event: MODIFIED
Mar 30 14:15:28.575: INFO: Found replicaset test-rs in namespace replicaset-1145 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 30 14:15:28.575: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 03/30/23 14:15:28.575
Mar 30 14:15:28.575: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 30 14:15:28.578: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 03/30/23 14:15:28.578
Mar 30 14:15:28.579: INFO: Observed &ReplicaSet event: ADDED
Mar 30 14:15:28.579: INFO: Observed &ReplicaSet event: MODIFIED
Mar 30 14:15:28.579: INFO: Observed &ReplicaSet event: MODIFIED
Mar 30 14:15:28.579: INFO: Observed &ReplicaSet event: MODIFIED
Mar 30 14:15:28.579: INFO: Observed replicaset test-rs in namespace replicaset-1145 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 30 14:15:28.579: INFO: Observed &ReplicaSet event: MODIFIED
Mar 30 14:15:28.579: INFO: Found replicaset test-rs in namespace replicaset-1145 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar 30 14:15:28.579: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 30 14:15:28.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1145" for this suite. 03/30/23 14:15:28.582
------------------------------
â€¢ [SLOW TEST] [5.034 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:15:23.55
    Mar 30 14:15:23.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename replicaset 03/30/23 14:15:23.551
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:15:23.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:15:23.558
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 03/30/23 14:15:23.561
    STEP: Verify that the required pods have come up. 03/30/23 14:15:23.563
    Mar 30 14:15:23.564: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 30 14:15:28.568: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/30/23 14:15:28.568
    STEP: Getting /status 03/30/23 14:15:28.568
    Mar 30 14:15:28.570: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 03/30/23 14:15:28.57
    Mar 30 14:15:28.573: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 03/30/23 14:15:28.573
    Mar 30 14:15:28.574: INFO: Observed &ReplicaSet event: ADDED
    Mar 30 14:15:28.575: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 30 14:15:28.575: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 30 14:15:28.575: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 30 14:15:28.575: INFO: Found replicaset test-rs in namespace replicaset-1145 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 30 14:15:28.575: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 03/30/23 14:15:28.575
    Mar 30 14:15:28.575: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 30 14:15:28.578: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 03/30/23 14:15:28.578
    Mar 30 14:15:28.579: INFO: Observed &ReplicaSet event: ADDED
    Mar 30 14:15:28.579: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 30 14:15:28.579: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 30 14:15:28.579: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 30 14:15:28.579: INFO: Observed replicaset test-rs in namespace replicaset-1145 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 30 14:15:28.579: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 30 14:15:28.579: INFO: Found replicaset test-rs in namespace replicaset-1145 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Mar 30 14:15:28.579: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:15:28.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1145" for this suite. 03/30/23 14:15:28.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:15:28.585
Mar 30 14:15:28.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 14:15:28.585
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:15:28.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:15:28.591
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 03/30/23 14:15:28.593
Mar 30 14:15:28.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-6695 create -f -'
Mar 30 14:15:29.129: INFO: stderr: ""
Mar 30 14:15:29.129: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 03/30/23 14:15:29.129
Mar 30 14:15:29.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-6695 diff -f -'
Mar 30 14:15:29.686: INFO: rc: 1
Mar 30 14:15:29.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-6695 delete -f -'
Mar 30 14:15:29.739: INFO: stderr: ""
Mar 30 14:15:29.739: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 14:15:29.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6695" for this suite. 03/30/23 14:15:29.741
------------------------------
â€¢ [1.160 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:15:28.585
    Mar 30 14:15:28.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 14:15:28.585
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:15:28.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:15:28.591
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 03/30/23 14:15:28.593
    Mar 30 14:15:28.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-6695 create -f -'
    Mar 30 14:15:29.129: INFO: stderr: ""
    Mar 30 14:15:29.129: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 03/30/23 14:15:29.129
    Mar 30 14:15:29.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-6695 diff -f -'
    Mar 30 14:15:29.686: INFO: rc: 1
    Mar 30 14:15:29.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-6695 delete -f -'
    Mar 30 14:15:29.739: INFO: stderr: ""
    Mar 30 14:15:29.739: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:15:29.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6695" for this suite. 03/30/23 14:15:29.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:15:29.745
Mar 30 14:15:29.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename dns 03/30/23 14:15:29.746
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:15:29.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:15:29.752
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 03/30/23 14:15:29.754
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3515 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3515;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3515 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3515;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3515.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3515.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3515.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3515.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3515.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3515.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3515.svc;check="$$(dig +notcp +noall +answer +search 127.16.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.16.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.16.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.16.127_tcp@PTR;sleep 1; done
 03/30/23 14:15:29.762
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3515 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3515;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3515 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3515;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3515.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3515.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3515.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3515.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3515.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3515.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3515.svc;check="$$(dig +notcp +noall +answer +search 127.16.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.16.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.16.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.16.127_tcp@PTR;sleep 1; done
 03/30/23 14:15:29.762
STEP: creating a pod to probe DNS 03/30/23 14:15:29.762
STEP: submitting the pod to kubernetes 03/30/23 14:15:29.762
Mar 30 14:15:29.766: INFO: Waiting up to 15m0s for pod "dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe" in namespace "dns-3515" to be "running"
Mar 30 14:15:29.768: INFO: Pod "dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463066ms
Mar 30 14:15:31.770: INFO: Pod "dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe": Phase="Running", Reason="", readiness=true. Elapsed: 2.003808204s
Mar 30 14:15:31.770: INFO: Pod "dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe" satisfied condition "running"
STEP: retrieving the pod 03/30/23 14:15:31.77
STEP: looking for the results for each expected name from probers 03/30/23 14:15:31.772
Mar 30 14:15:31.774: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.776: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.778: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.779: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.781: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.782: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.784: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.785: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.793: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.795: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.796: INFO: Unable to read jessie_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.798: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.799: INFO: Unable to read jessie_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.801: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.803: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.804: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:31.810: INFO: Lookups using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3515 wheezy_tcp@dns-test-service.dns-3515 wheezy_udp@dns-test-service.dns-3515.svc wheezy_tcp@dns-test-service.dns-3515.svc wheezy_udp@_http._tcp.dns-test-service.dns-3515.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3515 jessie_tcp@dns-test-service.dns-3515 jessie_udp@dns-test-service.dns-3515.svc jessie_tcp@dns-test-service.dns-3515.svc jessie_udp@_http._tcp.dns-test-service.dns-3515.svc jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc]

Mar 30 14:15:36.814: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.815: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.817: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.819: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.820: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.822: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.825: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.833: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.834: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.836: INFO: Unable to read jessie_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.837: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.839: INFO: Unable to read jessie_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:36.849: INFO: Lookups using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3515 wheezy_tcp@dns-test-service.dns-3515 wheezy_udp@dns-test-service.dns-3515.svc wheezy_tcp@dns-test-service.dns-3515.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3515 jessie_tcp@dns-test-service.dns-3515 jessie_udp@dns-test-service.dns-3515.svc jessie_tcp@dns-test-service.dns-3515.svc jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc]

Mar 30 14:15:41.813: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.815: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.817: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.818: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.820: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.821: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.825: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.832: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.834: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.835: INFO: Unable to read jessie_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.837: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.838: INFO: Unable to read jessie_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:41.849: INFO: Lookups using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3515 wheezy_tcp@dns-test-service.dns-3515 wheezy_udp@dns-test-service.dns-3515.svc wheezy_tcp@dns-test-service.dns-3515.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3515 jessie_tcp@dns-test-service.dns-3515 jessie_udp@dns-test-service.dns-3515.svc jessie_tcp@dns-test-service.dns-3515.svc jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc]

Mar 30 14:15:46.817: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.819: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.821: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.822: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.824: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.825: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.828: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.836: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.837: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.839: INFO: Unable to read jessie_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.842: INFO: Unable to read jessie_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.843: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.846: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:46.853: INFO: Lookups using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3515 wheezy_tcp@dns-test-service.dns-3515 wheezy_udp@dns-test-service.dns-3515.svc wheezy_tcp@dns-test-service.dns-3515.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3515 jessie_tcp@dns-test-service.dns-3515 jessie_udp@dns-test-service.dns-3515.svc jessie_tcp@dns-test-service.dns-3515.svc jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc]

Mar 30 14:15:51.813: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.815: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.816: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.818: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.820: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.821: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.824: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.832: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.834: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.835: INFO: Unable to read jessie_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.837: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.838: INFO: Unable to read jessie_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.839: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:51.849: INFO: Lookups using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3515 wheezy_tcp@dns-test-service.dns-3515 wheezy_udp@dns-test-service.dns-3515.svc wheezy_tcp@dns-test-service.dns-3515.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3515 jessie_tcp@dns-test-service.dns-3515 jessie_udp@dns-test-service.dns-3515.svc jessie_tcp@dns-test-service.dns-3515.svc jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc]

Mar 30 14:15:56.814: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.816: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.817: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.819: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.820: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.822: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.825: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.833: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.834: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.836: INFO: Unable to read jessie_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.837: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.839: INFO: Unable to read jessie_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
Mar 30 14:15:56.849: INFO: Lookups using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3515 wheezy_tcp@dns-test-service.dns-3515 wheezy_udp@dns-test-service.dns-3515.svc wheezy_tcp@dns-test-service.dns-3515.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3515 jessie_tcp@dns-test-service.dns-3515 jessie_udp@dns-test-service.dns-3515.svc jessie_tcp@dns-test-service.dns-3515.svc jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc]

Mar 30 14:16:01.849: INFO: DNS probes using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe succeeded

STEP: deleting the pod 03/30/23 14:16:01.849
STEP: deleting the test service 03/30/23 14:16:01.853
STEP: deleting the test headless service 03/30/23 14:16:01.866
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 30 14:16:01.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3515" for this suite. 03/30/23 14:16:01.874
------------------------------
â€¢ [SLOW TEST] [32.132 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:15:29.745
    Mar 30 14:15:29.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename dns 03/30/23 14:15:29.746
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:15:29.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:15:29.752
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 03/30/23 14:15:29.754
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3515 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3515;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3515 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3515;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3515.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3515.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3515.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3515.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3515.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3515.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3515.svc;check="$$(dig +notcp +noall +answer +search 127.16.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.16.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.16.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.16.127_tcp@PTR;sleep 1; done
     03/30/23 14:15:29.762
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3515 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3515;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3515 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3515;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3515.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3515.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3515.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3515.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3515.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3515.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3515.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3515.svc;check="$$(dig +notcp +noall +answer +search 127.16.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.16.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.16.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.16.127_tcp@PTR;sleep 1; done
     03/30/23 14:15:29.762
    STEP: creating a pod to probe DNS 03/30/23 14:15:29.762
    STEP: submitting the pod to kubernetes 03/30/23 14:15:29.762
    Mar 30 14:15:29.766: INFO: Waiting up to 15m0s for pod "dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe" in namespace "dns-3515" to be "running"
    Mar 30 14:15:29.768: INFO: Pod "dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463066ms
    Mar 30 14:15:31.770: INFO: Pod "dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe": Phase="Running", Reason="", readiness=true. Elapsed: 2.003808204s
    Mar 30 14:15:31.770: INFO: Pod "dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe" satisfied condition "running"
    STEP: retrieving the pod 03/30/23 14:15:31.77
    STEP: looking for the results for each expected name from probers 03/30/23 14:15:31.772
    Mar 30 14:15:31.774: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.776: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.778: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.779: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.781: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.782: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.784: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.785: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.793: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.795: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.796: INFO: Unable to read jessie_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.798: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.799: INFO: Unable to read jessie_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.801: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.803: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.804: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:31.810: INFO: Lookups using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3515 wheezy_tcp@dns-test-service.dns-3515 wheezy_udp@dns-test-service.dns-3515.svc wheezy_tcp@dns-test-service.dns-3515.svc wheezy_udp@_http._tcp.dns-test-service.dns-3515.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3515 jessie_tcp@dns-test-service.dns-3515 jessie_udp@dns-test-service.dns-3515.svc jessie_tcp@dns-test-service.dns-3515.svc jessie_udp@_http._tcp.dns-test-service.dns-3515.svc jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc]

    Mar 30 14:15:36.814: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.815: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.817: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.819: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.820: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.822: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.825: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.833: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.834: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.836: INFO: Unable to read jessie_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.837: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.839: INFO: Unable to read jessie_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:36.849: INFO: Lookups using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3515 wheezy_tcp@dns-test-service.dns-3515 wheezy_udp@dns-test-service.dns-3515.svc wheezy_tcp@dns-test-service.dns-3515.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3515 jessie_tcp@dns-test-service.dns-3515 jessie_udp@dns-test-service.dns-3515.svc jessie_tcp@dns-test-service.dns-3515.svc jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc]

    Mar 30 14:15:41.813: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.815: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.817: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.818: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.820: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.821: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.825: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.832: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.834: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.835: INFO: Unable to read jessie_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.837: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.838: INFO: Unable to read jessie_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:41.849: INFO: Lookups using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3515 wheezy_tcp@dns-test-service.dns-3515 wheezy_udp@dns-test-service.dns-3515.svc wheezy_tcp@dns-test-service.dns-3515.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3515 jessie_tcp@dns-test-service.dns-3515 jessie_udp@dns-test-service.dns-3515.svc jessie_tcp@dns-test-service.dns-3515.svc jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc]

    Mar 30 14:15:46.817: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.819: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.821: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.822: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.824: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.825: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.828: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.836: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.837: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.839: INFO: Unable to read jessie_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.842: INFO: Unable to read jessie_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.843: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.846: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:46.853: INFO: Lookups using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3515 wheezy_tcp@dns-test-service.dns-3515 wheezy_udp@dns-test-service.dns-3515.svc wheezy_tcp@dns-test-service.dns-3515.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3515 jessie_tcp@dns-test-service.dns-3515 jessie_udp@dns-test-service.dns-3515.svc jessie_tcp@dns-test-service.dns-3515.svc jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc]

    Mar 30 14:15:51.813: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.815: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.816: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.818: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.820: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.821: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.824: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.832: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.834: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.835: INFO: Unable to read jessie_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.837: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.838: INFO: Unable to read jessie_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.839: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:51.849: INFO: Lookups using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3515 wheezy_tcp@dns-test-service.dns-3515 wheezy_udp@dns-test-service.dns-3515.svc wheezy_tcp@dns-test-service.dns-3515.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3515 jessie_tcp@dns-test-service.dns-3515 jessie_udp@dns-test-service.dns-3515.svc jessie_tcp@dns-test-service.dns-3515.svc jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc]

    Mar 30 14:15:56.814: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.816: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.817: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.819: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.820: INFO: Unable to read wheezy_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.822: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.825: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.833: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.834: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.836: INFO: Unable to read jessie_udp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.837: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515 from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.839: INFO: Unable to read jessie_udp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc from pod dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe: the server could not find the requested resource (get pods dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe)
    Mar 30 14:15:56.849: INFO: Lookups using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3515 wheezy_tcp@dns-test-service.dns-3515 wheezy_udp@dns-test-service.dns-3515.svc wheezy_tcp@dns-test-service.dns-3515.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3515.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3515 jessie_tcp@dns-test-service.dns-3515 jessie_udp@dns-test-service.dns-3515.svc jessie_tcp@dns-test-service.dns-3515.svc jessie_tcp@_http._tcp.dns-test-service.dns-3515.svc]

    Mar 30 14:16:01.849: INFO: DNS probes using dns-3515/dns-test-7a739ca4-e25e-4eb4-a5ef-e47c17612ffe succeeded

    STEP: deleting the pod 03/30/23 14:16:01.849
    STEP: deleting the test service 03/30/23 14:16:01.853
    STEP: deleting the test headless service 03/30/23 14:16:01.866
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:16:01.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3515" for this suite. 03/30/23 14:16:01.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:16:01.878
Mar 30 14:16:01.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename containers 03/30/23 14:16:01.879
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:01.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:01.887
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 03/30/23 14:16:01.888
Mar 30 14:16:01.891: INFO: Waiting up to 5m0s for pod "client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e" in namespace "containers-8526" to be "Succeeded or Failed"
Mar 30 14:16:01.893: INFO: Pod "client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702618ms
Mar 30 14:16:03.896: INFO: Pod "client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004443844s
Mar 30 14:16:05.896: INFO: Pod "client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004458619s
STEP: Saw pod success 03/30/23 14:16:05.896
Mar 30 14:16:05.896: INFO: Pod "client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e" satisfied condition "Succeeded or Failed"
Mar 30 14:16:05.898: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e container agnhost-container: <nil>
STEP: delete the pod 03/30/23 14:16:05.901
Mar 30 14:16:05.906: INFO: Waiting for pod client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e to disappear
Mar 30 14:16:05.907: INFO: Pod client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 30 14:16:05.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8526" for this suite. 03/30/23 14:16:05.909
------------------------------
â€¢ [4.034 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:16:01.878
    Mar 30 14:16:01.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename containers 03/30/23 14:16:01.879
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:01.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:01.887
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 03/30/23 14:16:01.888
    Mar 30 14:16:01.891: INFO: Waiting up to 5m0s for pod "client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e" in namespace "containers-8526" to be "Succeeded or Failed"
    Mar 30 14:16:01.893: INFO: Pod "client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702618ms
    Mar 30 14:16:03.896: INFO: Pod "client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004443844s
    Mar 30 14:16:05.896: INFO: Pod "client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004458619s
    STEP: Saw pod success 03/30/23 14:16:05.896
    Mar 30 14:16:05.896: INFO: Pod "client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e" satisfied condition "Succeeded or Failed"
    Mar 30 14:16:05.898: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 14:16:05.901
    Mar 30 14:16:05.906: INFO: Waiting for pod client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e to disappear
    Mar 30 14:16:05.907: INFO: Pod client-containers-758c6d1c-d6f3-41e1-9a77-e8b8ceeff60e no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:16:05.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8526" for this suite. 03/30/23 14:16:05.909
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:16:05.912
Mar 30 14:16:05.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-probe 03/30/23 14:16:05.913
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:05.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:05.919
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-ab49fe79-b53c-43b5-8345-240ac5b68883 in namespace container-probe-8540 03/30/23 14:16:05.92
Mar 30 14:16:05.924: INFO: Waiting up to 5m0s for pod "liveness-ab49fe79-b53c-43b5-8345-240ac5b68883" in namespace "container-probe-8540" to be "not pending"
Mar 30 14:16:05.926: INFO: Pod "liveness-ab49fe79-b53c-43b5-8345-240ac5b68883": Phase="Pending", Reason="", readiness=false. Elapsed: 1.942841ms
Mar 30 14:16:07.928: INFO: Pod "liveness-ab49fe79-b53c-43b5-8345-240ac5b68883": Phase="Running", Reason="", readiness=true. Elapsed: 2.004010309s
Mar 30 14:16:07.928: INFO: Pod "liveness-ab49fe79-b53c-43b5-8345-240ac5b68883" satisfied condition "not pending"
Mar 30 14:16:07.928: INFO: Started pod liveness-ab49fe79-b53c-43b5-8345-240ac5b68883 in namespace container-probe-8540
STEP: checking the pod's current state and verifying that restartCount is present 03/30/23 14:16:07.928
Mar 30 14:16:07.929: INFO: Initial restart count of pod liveness-ab49fe79-b53c-43b5-8345-240ac5b68883 is 0
Mar 30 14:16:27.957: INFO: Restart count of pod container-probe-8540/liveness-ab49fe79-b53c-43b5-8345-240ac5b68883 is now 1 (20.027360605s elapsed)
STEP: deleting the pod 03/30/23 14:16:27.957
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 30 14:16:27.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8540" for this suite. 03/30/23 14:16:27.964
------------------------------
â€¢ [SLOW TEST] [22.054 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:16:05.912
    Mar 30 14:16:05.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-probe 03/30/23 14:16:05.913
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:05.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:05.919
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-ab49fe79-b53c-43b5-8345-240ac5b68883 in namespace container-probe-8540 03/30/23 14:16:05.92
    Mar 30 14:16:05.924: INFO: Waiting up to 5m0s for pod "liveness-ab49fe79-b53c-43b5-8345-240ac5b68883" in namespace "container-probe-8540" to be "not pending"
    Mar 30 14:16:05.926: INFO: Pod "liveness-ab49fe79-b53c-43b5-8345-240ac5b68883": Phase="Pending", Reason="", readiness=false. Elapsed: 1.942841ms
    Mar 30 14:16:07.928: INFO: Pod "liveness-ab49fe79-b53c-43b5-8345-240ac5b68883": Phase="Running", Reason="", readiness=true. Elapsed: 2.004010309s
    Mar 30 14:16:07.928: INFO: Pod "liveness-ab49fe79-b53c-43b5-8345-240ac5b68883" satisfied condition "not pending"
    Mar 30 14:16:07.928: INFO: Started pod liveness-ab49fe79-b53c-43b5-8345-240ac5b68883 in namespace container-probe-8540
    STEP: checking the pod's current state and verifying that restartCount is present 03/30/23 14:16:07.928
    Mar 30 14:16:07.929: INFO: Initial restart count of pod liveness-ab49fe79-b53c-43b5-8345-240ac5b68883 is 0
    Mar 30 14:16:27.957: INFO: Restart count of pod container-probe-8540/liveness-ab49fe79-b53c-43b5-8345-240ac5b68883 is now 1 (20.027360605s elapsed)
    STEP: deleting the pod 03/30/23 14:16:27.957
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:16:27.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8540" for this suite. 03/30/23 14:16:27.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:16:27.967
Mar 30 14:16:27.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename var-expansion 03/30/23 14:16:27.967
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:27.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:27.974
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 03/30/23 14:16:27.976
Mar 30 14:16:27.979: INFO: Waiting up to 5m0s for pod "var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3" in namespace "var-expansion-8138" to be "Succeeded or Failed"
Mar 30 14:16:27.980: INFO: Pod "var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.376749ms
Mar 30 14:16:29.983: INFO: Pod "var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003856395s
Mar 30 14:16:31.983: INFO: Pod "var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003914725s
STEP: Saw pod success 03/30/23 14:16:31.983
Mar 30 14:16:31.983: INFO: Pod "var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3" satisfied condition "Succeeded or Failed"
Mar 30 14:16:31.984: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3 container dapi-container: <nil>
STEP: delete the pod 03/30/23 14:16:31.988
Mar 30 14:16:31.992: INFO: Waiting for pod var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3 to disappear
Mar 30 14:16:31.993: INFO: Pod var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 30 14:16:31.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8138" for this suite. 03/30/23 14:16:31.996
------------------------------
â€¢ [4.031 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:16:27.967
    Mar 30 14:16:27.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename var-expansion 03/30/23 14:16:27.967
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:27.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:27.974
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 03/30/23 14:16:27.976
    Mar 30 14:16:27.979: INFO: Waiting up to 5m0s for pod "var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3" in namespace "var-expansion-8138" to be "Succeeded or Failed"
    Mar 30 14:16:27.980: INFO: Pod "var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.376749ms
    Mar 30 14:16:29.983: INFO: Pod "var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003856395s
    Mar 30 14:16:31.983: INFO: Pod "var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003914725s
    STEP: Saw pod success 03/30/23 14:16:31.983
    Mar 30 14:16:31.983: INFO: Pod "var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3" satisfied condition "Succeeded or Failed"
    Mar 30 14:16:31.984: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3 container dapi-container: <nil>
    STEP: delete the pod 03/30/23 14:16:31.988
    Mar 30 14:16:31.992: INFO: Waiting for pod var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3 to disappear
    Mar 30 14:16:31.993: INFO: Pod var-expansion-9d2f6eb6-8122-4f45-bc01-968736ef6fc3 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:16:31.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8138" for this suite. 03/30/23 14:16:31.996
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:16:31.998
Mar 30 14:16:31.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pods 03/30/23 14:16:31.999
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:32.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:32.006
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 03/30/23 14:16:32.008
STEP: submitting the pod to kubernetes 03/30/23 14:16:32.008
Mar 30 14:16:32.012: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de" in namespace "pods-1771" to be "running and ready"
Mar 30 14:16:32.013: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de": Phase="Pending", Reason="", readiness=false. Elapsed: 1.420528ms
Mar 30 14:16:32.013: INFO: The phase of Pod pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de is Pending, waiting for it to be Running (with Ready = true)
Mar 30 14:16:34.016: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de": Phase="Running", Reason="", readiness=true. Elapsed: 2.004367046s
Mar 30 14:16:34.016: INFO: The phase of Pod pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de is Running (Ready = true)
Mar 30 14:16:34.016: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/30/23 14:16:34.018
STEP: updating the pod 03/30/23 14:16:34.019
Mar 30 14:16:34.526: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de"
Mar 30 14:16:34.526: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de" in namespace "pods-1771" to be "terminated with reason DeadlineExceeded"
Mar 30 14:16:34.528: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de": Phase="Running", Reason="", readiness=true. Elapsed: 1.364243ms
Mar 30 14:16:36.530: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de": Phase="Running", Reason="", readiness=true. Elapsed: 2.00407292s
Mar 30 14:16:38.530: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de": Phase="Running", Reason="", readiness=false. Elapsed: 4.003499362s
Mar 30 14:16:40.530: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.003750594s
Mar 30 14:16:40.530: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 30 14:16:40.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1771" for this suite. 03/30/23 14:16:40.533
------------------------------
â€¢ [SLOW TEST] [8.537 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:16:31.998
    Mar 30 14:16:31.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pods 03/30/23 14:16:31.999
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:32.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:32.006
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 03/30/23 14:16:32.008
    STEP: submitting the pod to kubernetes 03/30/23 14:16:32.008
    Mar 30 14:16:32.012: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de" in namespace "pods-1771" to be "running and ready"
    Mar 30 14:16:32.013: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de": Phase="Pending", Reason="", readiness=false. Elapsed: 1.420528ms
    Mar 30 14:16:32.013: INFO: The phase of Pod pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 14:16:34.016: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de": Phase="Running", Reason="", readiness=true. Elapsed: 2.004367046s
    Mar 30 14:16:34.016: INFO: The phase of Pod pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de is Running (Ready = true)
    Mar 30 14:16:34.016: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/30/23 14:16:34.018
    STEP: updating the pod 03/30/23 14:16:34.019
    Mar 30 14:16:34.526: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de"
    Mar 30 14:16:34.526: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de" in namespace "pods-1771" to be "terminated with reason DeadlineExceeded"
    Mar 30 14:16:34.528: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de": Phase="Running", Reason="", readiness=true. Elapsed: 1.364243ms
    Mar 30 14:16:36.530: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de": Phase="Running", Reason="", readiness=true. Elapsed: 2.00407292s
    Mar 30 14:16:38.530: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de": Phase="Running", Reason="", readiness=false. Elapsed: 4.003499362s
    Mar 30 14:16:40.530: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.003750594s
    Mar 30 14:16:40.530: INFO: Pod "pod-update-activedeadlineseconds-d7fc26b6-5313-4a19-9c9f-03ec016235de" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:16:40.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1771" for this suite. 03/30/23 14:16:40.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:16:40.536
Mar 30 14:16:40.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 14:16:40.536
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:40.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:40.543
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-f01b8dbd-49e2-4365-928c-ac3d2fd90805 03/30/23 14:16:40.551
STEP: Creating a pod to test consume secrets 03/30/23 14:16:40.553
Mar 30 14:16:40.556: INFO: Waiting up to 5m0s for pod "pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6" in namespace "secrets-6275" to be "Succeeded or Failed"
Mar 30 14:16:40.558: INFO: Pod "pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.686933ms
Mar 30 14:16:42.560: INFO: Pod "pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003725956s
Mar 30 14:16:44.561: INFO: Pod "pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004722642s
STEP: Saw pod success 03/30/23 14:16:44.561
Mar 30 14:16:44.561: INFO: Pod "pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6" satisfied condition "Succeeded or Failed"
Mar 30 14:16:44.563: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6 container secret-volume-test: <nil>
STEP: delete the pod 03/30/23 14:16:44.566
Mar 30 14:16:44.570: INFO: Waiting for pod pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6 to disappear
Mar 30 14:16:44.571: INFO: Pod pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 14:16:44.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6275" for this suite. 03/30/23 14:16:44.574
STEP: Destroying namespace "secret-namespace-1828" for this suite. 03/30/23 14:16:44.576
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:16:40.536
    Mar 30 14:16:40.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 14:16:40.536
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:40.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:40.543
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-f01b8dbd-49e2-4365-928c-ac3d2fd90805 03/30/23 14:16:40.551
    STEP: Creating a pod to test consume secrets 03/30/23 14:16:40.553
    Mar 30 14:16:40.556: INFO: Waiting up to 5m0s for pod "pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6" in namespace "secrets-6275" to be "Succeeded or Failed"
    Mar 30 14:16:40.558: INFO: Pod "pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.686933ms
    Mar 30 14:16:42.560: INFO: Pod "pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003725956s
    Mar 30 14:16:44.561: INFO: Pod "pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004722642s
    STEP: Saw pod success 03/30/23 14:16:44.561
    Mar 30 14:16:44.561: INFO: Pod "pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6" satisfied condition "Succeeded or Failed"
    Mar 30 14:16:44.563: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6 container secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 14:16:44.566
    Mar 30 14:16:44.570: INFO: Waiting for pod pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6 to disappear
    Mar 30 14:16:44.571: INFO: Pod pod-secrets-2342a28d-3560-45c2-ae5f-4ff3fbd4b8c6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:16:44.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6275" for this suite. 03/30/23 14:16:44.574
    STEP: Destroying namespace "secret-namespace-1828" for this suite. 03/30/23 14:16:44.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:16:44.579
Mar 30 14:16:44.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename resourcequota 03/30/23 14:16:44.579
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:44.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:44.586
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 03/30/23 14:16:44.587
STEP: Getting a ResourceQuota 03/30/23 14:16:44.589
STEP: Updating a ResourceQuota 03/30/23 14:16:44.591
STEP: Verifying a ResourceQuota was modified 03/30/23 14:16:44.593
STEP: Deleting a ResourceQuota 03/30/23 14:16:44.594
STEP: Verifying the deleted ResourceQuota 03/30/23 14:16:44.596
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 30 14:16:44.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4122" for this suite. 03/30/23 14:16:44.6
------------------------------
â€¢ [0.024 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:16:44.579
    Mar 30 14:16:44.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename resourcequota 03/30/23 14:16:44.579
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:44.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:44.586
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 03/30/23 14:16:44.587
    STEP: Getting a ResourceQuota 03/30/23 14:16:44.589
    STEP: Updating a ResourceQuota 03/30/23 14:16:44.591
    STEP: Verifying a ResourceQuota was modified 03/30/23 14:16:44.593
    STEP: Deleting a ResourceQuota 03/30/23 14:16:44.594
    STEP: Verifying the deleted ResourceQuota 03/30/23 14:16:44.596
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:16:44.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4122" for this suite. 03/30/23 14:16:44.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:16:44.603
Mar 30 14:16:44.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename security-context 03/30/23 14:16:44.604
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:44.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:44.61
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/30/23 14:16:44.612
Mar 30 14:16:44.616: INFO: Waiting up to 5m0s for pod "security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd" in namespace "security-context-3371" to be "Succeeded or Failed"
Mar 30 14:16:44.617: INFO: Pod "security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.897638ms
Mar 30 14:16:46.620: INFO: Pod "security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004204612s
Mar 30 14:16:48.620: INFO: Pod "security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004609978s
STEP: Saw pod success 03/30/23 14:16:48.62
Mar 30 14:16:48.620: INFO: Pod "security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd" satisfied condition "Succeeded or Failed"
Mar 30 14:16:48.622: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd container test-container: <nil>
STEP: delete the pod 03/30/23 14:16:48.625
Mar 30 14:16:48.630: INFO: Waiting for pod security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd to disappear
Mar 30 14:16:48.631: INFO: Pod security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 30 14:16:48.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-3371" for this suite. 03/30/23 14:16:48.634
------------------------------
â€¢ [4.033 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:16:44.603
    Mar 30 14:16:44.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename security-context 03/30/23 14:16:44.604
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:44.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:44.61
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/30/23 14:16:44.612
    Mar 30 14:16:44.616: INFO: Waiting up to 5m0s for pod "security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd" in namespace "security-context-3371" to be "Succeeded or Failed"
    Mar 30 14:16:44.617: INFO: Pod "security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.897638ms
    Mar 30 14:16:46.620: INFO: Pod "security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004204612s
    Mar 30 14:16:48.620: INFO: Pod "security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004609978s
    STEP: Saw pod success 03/30/23 14:16:48.62
    Mar 30 14:16:48.620: INFO: Pod "security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd" satisfied condition "Succeeded or Failed"
    Mar 30 14:16:48.622: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd container test-container: <nil>
    STEP: delete the pod 03/30/23 14:16:48.625
    Mar 30 14:16:48.630: INFO: Waiting for pod security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd to disappear
    Mar 30 14:16:48.631: INFO: Pod security-context-1f6df93d-68b7-4baa-bc8a-6f151c3afdbd no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:16:48.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-3371" for this suite. 03/30/23 14:16:48.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:16:48.637
Mar 30 14:16:48.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 14:16:48.638
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:48.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:48.644
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-39b9d78f-8d51-418d-8108-fdfc4a355ad1 03/30/23 14:16:48.646
STEP: Creating a pod to test consume secrets 03/30/23 14:16:48.647
Mar 30 14:16:48.651: INFO: Waiting up to 5m0s for pod "pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440" in namespace "secrets-5579" to be "Succeeded or Failed"
Mar 30 14:16:48.653: INFO: Pod "pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440": Phase="Pending", Reason="", readiness=false. Elapsed: 1.542653ms
Mar 30 14:16:50.655: INFO: Pod "pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003976851s
Mar 30 14:16:52.655: INFO: Pod "pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003940252s
STEP: Saw pod success 03/30/23 14:16:52.655
Mar 30 14:16:52.655: INFO: Pod "pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440" satisfied condition "Succeeded or Failed"
Mar 30 14:16:52.657: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440 container secret-volume-test: <nil>
STEP: delete the pod 03/30/23 14:16:52.66
Mar 30 14:16:52.664: INFO: Waiting for pod pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440 to disappear
Mar 30 14:16:52.666: INFO: Pod pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 14:16:52.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5579" for this suite. 03/30/23 14:16:52.668
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:16:48.637
    Mar 30 14:16:48.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 14:16:48.638
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:48.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:48.644
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-39b9d78f-8d51-418d-8108-fdfc4a355ad1 03/30/23 14:16:48.646
    STEP: Creating a pod to test consume secrets 03/30/23 14:16:48.647
    Mar 30 14:16:48.651: INFO: Waiting up to 5m0s for pod "pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440" in namespace "secrets-5579" to be "Succeeded or Failed"
    Mar 30 14:16:48.653: INFO: Pod "pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440": Phase="Pending", Reason="", readiness=false. Elapsed: 1.542653ms
    Mar 30 14:16:50.655: INFO: Pod "pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003976851s
    Mar 30 14:16:52.655: INFO: Pod "pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003940252s
    STEP: Saw pod success 03/30/23 14:16:52.655
    Mar 30 14:16:52.655: INFO: Pod "pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440" satisfied condition "Succeeded or Failed"
    Mar 30 14:16:52.657: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440 container secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 14:16:52.66
    Mar 30 14:16:52.664: INFO: Waiting for pod pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440 to disappear
    Mar 30 14:16:52.666: INFO: Pod pod-secrets-dd167ac3-57e1-4e00-8785-010e44625440 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:16:52.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5579" for this suite. 03/30/23 14:16:52.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:16:52.671
Mar 30 14:16:52.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename configmap 03/30/23 14:16:52.672
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:52.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:52.678
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-77026d69-1f52-4b6d-87b2-969542672e74 03/30/23 14:16:52.68
STEP: Creating a pod to test consume configMaps 03/30/23 14:16:52.681
Mar 30 14:16:52.685: INFO: Waiting up to 5m0s for pod "pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378" in namespace "configmap-4346" to be "Succeeded or Failed"
Mar 30 14:16:52.686: INFO: Pod "pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378": Phase="Pending", Reason="", readiness=false. Elapsed: 1.566297ms
Mar 30 14:16:54.689: INFO: Pod "pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004371004s
Mar 30 14:16:56.688: INFO: Pod "pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003560596s
STEP: Saw pod success 03/30/23 14:16:56.688
Mar 30 14:16:56.688: INFO: Pod "pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378" satisfied condition "Succeeded or Failed"
Mar 30 14:16:56.690: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378 container configmap-volume-test: <nil>
STEP: delete the pod 03/30/23 14:16:56.693
Mar 30 14:16:56.697: INFO: Waiting for pod pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378 to disappear
Mar 30 14:16:56.698: INFO: Pod pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 30 14:16:56.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4346" for this suite. 03/30/23 14:16:56.701
------------------------------
â€¢ [4.032 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:16:52.671
    Mar 30 14:16:52.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename configmap 03/30/23 14:16:52.672
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:52.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:52.678
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-77026d69-1f52-4b6d-87b2-969542672e74 03/30/23 14:16:52.68
    STEP: Creating a pod to test consume configMaps 03/30/23 14:16:52.681
    Mar 30 14:16:52.685: INFO: Waiting up to 5m0s for pod "pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378" in namespace "configmap-4346" to be "Succeeded or Failed"
    Mar 30 14:16:52.686: INFO: Pod "pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378": Phase="Pending", Reason="", readiness=false. Elapsed: 1.566297ms
    Mar 30 14:16:54.689: INFO: Pod "pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004371004s
    Mar 30 14:16:56.688: INFO: Pod "pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003560596s
    STEP: Saw pod success 03/30/23 14:16:56.688
    Mar 30 14:16:56.688: INFO: Pod "pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378" satisfied condition "Succeeded or Failed"
    Mar 30 14:16:56.690: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378 container configmap-volume-test: <nil>
    STEP: delete the pod 03/30/23 14:16:56.693
    Mar 30 14:16:56.697: INFO: Waiting for pod pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378 to disappear
    Mar 30 14:16:56.698: INFO: Pod pod-configmaps-5930b575-6c2c-435b-83fa-c51fa35e6378 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:16:56.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4346" for this suite. 03/30/23 14:16:56.701
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:16:56.703
Mar 30 14:16:56.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 14:16:56.704
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:56.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:56.71
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-b60f7e97-e295-405a-b93a-5ab485b92f89 03/30/23 14:16:56.712
STEP: Creating a pod to test consume secrets 03/30/23 14:16:56.714
Mar 30 14:16:56.717: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1" in namespace "projected-434" to be "Succeeded or Failed"
Mar 30 14:16:56.718: INFO: Pod "pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.347362ms
Mar 30 14:16:58.721: INFO: Pod "pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00431909s
Mar 30 14:17:00.722: INFO: Pod "pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004821164s
STEP: Saw pod success 03/30/23 14:17:00.722
Mar 30 14:17:00.722: INFO: Pod "pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1" satisfied condition "Succeeded or Failed"
Mar 30 14:17:00.723: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/30/23 14:17:00.727
Mar 30 14:17:00.731: INFO: Waiting for pod pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1 to disappear
Mar 30 14:17:00.732: INFO: Pod pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 30 14:17:00.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-434" for this suite. 03/30/23 14:17:00.735
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:16:56.703
    Mar 30 14:16:56.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 14:16:56.704
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:16:56.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:16:56.71
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-b60f7e97-e295-405a-b93a-5ab485b92f89 03/30/23 14:16:56.712
    STEP: Creating a pod to test consume secrets 03/30/23 14:16:56.714
    Mar 30 14:16:56.717: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1" in namespace "projected-434" to be "Succeeded or Failed"
    Mar 30 14:16:56.718: INFO: Pod "pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.347362ms
    Mar 30 14:16:58.721: INFO: Pod "pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00431909s
    Mar 30 14:17:00.722: INFO: Pod "pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004821164s
    STEP: Saw pod success 03/30/23 14:17:00.722
    Mar 30 14:17:00.722: INFO: Pod "pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1" satisfied condition "Succeeded or Failed"
    Mar 30 14:17:00.723: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 14:17:00.727
    Mar 30 14:17:00.731: INFO: Waiting for pod pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1 to disappear
    Mar 30 14:17:00.732: INFO: Pod pod-projected-secrets-50252abf-87b0-4c00-b7b2-9e3163bd71d1 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:17:00.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-434" for this suite. 03/30/23 14:17:00.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:17:00.738
Mar 30 14:17:00.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename daemonsets 03/30/23 14:17:00.738
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:17:00.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:17:00.744
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 03/30/23 14:17:00.754
STEP: Check that daemon pods launch on every node of the cluster. 03/30/23 14:17:00.757
Mar 30 14:17:00.759: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:00.759: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:00.759: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:00.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 14:17:00.760: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
Mar 30 14:17:01.763: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:01.763: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:01.763: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:01.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 30 14:17:01.765: INFO: Node cn-hongkong.192.168.0.4 is running 0 daemon pod, expected 1
Mar 30 14:17:02.764: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:02.764: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:02.764: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:02.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 30 14:17:02.766: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/30/23 14:17:02.767
Mar 30 14:17:02.775: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:02.775: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:02.775: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:02.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 30 14:17:02.777: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
Mar 30 14:17:03.780: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:03.780: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:03.780: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:17:03.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 30 14:17:03.782: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 03/30/23 14:17:03.782
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/30/23 14:17:03.785
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4214, will wait for the garbage collector to delete the pods 03/30/23 14:17:03.785
Mar 30 14:17:03.839: INFO: Deleting DaemonSet.extensions daemon-set took: 2.668311ms
Mar 30 14:17:03.940: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.957721ms
Mar 30 14:17:06.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 30 14:17:06.543: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 30 14:17:06.545: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"90246"},"items":null}

Mar 30 14:17:06.546: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"90246"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:17:06.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4214" for this suite. 03/30/23 14:17:06.554
------------------------------
â€¢ [SLOW TEST] [5.819 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:17:00.738
    Mar 30 14:17:00.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename daemonsets 03/30/23 14:17:00.738
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:17:00.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:17:00.744
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 03/30/23 14:17:00.754
    STEP: Check that daemon pods launch on every node of the cluster. 03/30/23 14:17:00.757
    Mar 30 14:17:00.759: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:00.759: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:00.759: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:00.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 14:17:00.760: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
    Mar 30 14:17:01.763: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:01.763: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:01.763: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:01.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 30 14:17:01.765: INFO: Node cn-hongkong.192.168.0.4 is running 0 daemon pod, expected 1
    Mar 30 14:17:02.764: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:02.764: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:02.764: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:02.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 30 14:17:02.766: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/30/23 14:17:02.767
    Mar 30 14:17:02.775: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:02.775: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:02.775: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:02.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 30 14:17:02.777: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
    Mar 30 14:17:03.780: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:03.780: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:03.780: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:17:03.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 30 14:17:03.782: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 03/30/23 14:17:03.782
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/30/23 14:17:03.785
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4214, will wait for the garbage collector to delete the pods 03/30/23 14:17:03.785
    Mar 30 14:17:03.839: INFO: Deleting DaemonSet.extensions daemon-set took: 2.668311ms
    Mar 30 14:17:03.940: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.957721ms
    Mar 30 14:17:06.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 30 14:17:06.543: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 30 14:17:06.545: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"90246"},"items":null}

    Mar 30 14:17:06.546: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"90246"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:17:06.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4214" for this suite. 03/30/23 14:17:06.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:17:06.557
Mar 30 14:17:06.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 14:17:06.558
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:17:06.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:17:06.564
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-dc455c16-124d-44dc-9416-eac7dfd1dcb2 03/30/23 14:17:06.568
STEP: Creating configMap with name cm-test-opt-upd-8f5fbcbd-8cd3-41d1-a5b9-d2db48314375 03/30/23 14:17:06.569
STEP: Creating the pod 03/30/23 14:17:06.571
Mar 30 14:17:06.575: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac78e845-2bda-4dbf-a108-f3b9bfab5434" in namespace "projected-2951" to be "running and ready"
Mar 30 14:17:06.577: INFO: Pod "pod-projected-configmaps-ac78e845-2bda-4dbf-a108-f3b9bfab5434": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013309ms
Mar 30 14:17:06.577: INFO: The phase of Pod pod-projected-configmaps-ac78e845-2bda-4dbf-a108-f3b9bfab5434 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 14:17:08.580: INFO: Pod "pod-projected-configmaps-ac78e845-2bda-4dbf-a108-f3b9bfab5434": Phase="Running", Reason="", readiness=true. Elapsed: 2.00465278s
Mar 30 14:17:08.580: INFO: The phase of Pod pod-projected-configmaps-ac78e845-2bda-4dbf-a108-f3b9bfab5434 is Running (Ready = true)
Mar 30 14:17:08.580: INFO: Pod "pod-projected-configmaps-ac78e845-2bda-4dbf-a108-f3b9bfab5434" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-dc455c16-124d-44dc-9416-eac7dfd1dcb2 03/30/23 14:17:08.59
STEP: Updating configmap cm-test-opt-upd-8f5fbcbd-8cd3-41d1-a5b9-d2db48314375 03/30/23 14:17:08.593
STEP: Creating configMap with name cm-test-opt-create-1e485854-c94e-4ccf-8656-7b0f8ca02e53 03/30/23 14:17:08.595
STEP: waiting to observe update in volume 03/30/23 14:17:08.597
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 30 14:17:10.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2951" for this suite. 03/30/23 14:17:10.613
------------------------------
â€¢ [4.059 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:17:06.557
    Mar 30 14:17:06.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 14:17:06.558
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:17:06.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:17:06.564
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-dc455c16-124d-44dc-9416-eac7dfd1dcb2 03/30/23 14:17:06.568
    STEP: Creating configMap with name cm-test-opt-upd-8f5fbcbd-8cd3-41d1-a5b9-d2db48314375 03/30/23 14:17:06.569
    STEP: Creating the pod 03/30/23 14:17:06.571
    Mar 30 14:17:06.575: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac78e845-2bda-4dbf-a108-f3b9bfab5434" in namespace "projected-2951" to be "running and ready"
    Mar 30 14:17:06.577: INFO: Pod "pod-projected-configmaps-ac78e845-2bda-4dbf-a108-f3b9bfab5434": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013309ms
    Mar 30 14:17:06.577: INFO: The phase of Pod pod-projected-configmaps-ac78e845-2bda-4dbf-a108-f3b9bfab5434 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 14:17:08.580: INFO: Pod "pod-projected-configmaps-ac78e845-2bda-4dbf-a108-f3b9bfab5434": Phase="Running", Reason="", readiness=true. Elapsed: 2.00465278s
    Mar 30 14:17:08.580: INFO: The phase of Pod pod-projected-configmaps-ac78e845-2bda-4dbf-a108-f3b9bfab5434 is Running (Ready = true)
    Mar 30 14:17:08.580: INFO: Pod "pod-projected-configmaps-ac78e845-2bda-4dbf-a108-f3b9bfab5434" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-dc455c16-124d-44dc-9416-eac7dfd1dcb2 03/30/23 14:17:08.59
    STEP: Updating configmap cm-test-opt-upd-8f5fbcbd-8cd3-41d1-a5b9-d2db48314375 03/30/23 14:17:08.593
    STEP: Creating configMap with name cm-test-opt-create-1e485854-c94e-4ccf-8656-7b0f8ca02e53 03/30/23 14:17:08.595
    STEP: waiting to observe update in volume 03/30/23 14:17:08.597
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:17:10.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2951" for this suite. 03/30/23 14:17:10.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:17:10.616
Mar 30 14:17:10.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 14:17:10.617
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:17:10.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:17:10.623
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/30/23 14:17:10.624
Mar 30 14:17:10.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 14:17:17.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:17:28.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9170" for this suite. 03/30/23 14:17:28.382
------------------------------
â€¢ [SLOW TEST] [17.769 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:17:10.616
    Mar 30 14:17:10.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 14:17:10.617
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:17:10.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:17:10.623
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/30/23 14:17:10.624
    Mar 30 14:17:10.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 14:17:17.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:17:28.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9170" for this suite. 03/30/23 14:17:28.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:17:28.385
Mar 30 14:17:28.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename dns 03/30/23 14:17:28.386
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:17:28.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:17:28.393
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 03/30/23 14:17:28.395
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3858.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local; sleep 1; done
 03/30/23 14:17:28.4
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3858.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local; sleep 1; done
 03/30/23 14:17:28.4
STEP: creating a pod to probe DNS 03/30/23 14:17:28.401
STEP: submitting the pod to kubernetes 03/30/23 14:17:28.401
Mar 30 14:17:28.423: INFO: Waiting up to 15m0s for pod "dns-test-e30911a4-5eeb-41db-aed6-ac1332aac639" in namespace "dns-3858" to be "running"
Mar 30 14:17:28.425: INFO: Pod "dns-test-e30911a4-5eeb-41db-aed6-ac1332aac639": Phase="Pending", Reason="", readiness=false. Elapsed: 2.61258ms
Mar 30 14:17:30.428: INFO: Pod "dns-test-e30911a4-5eeb-41db-aed6-ac1332aac639": Phase="Running", Reason="", readiness=true. Elapsed: 2.004990427s
Mar 30 14:17:30.428: INFO: Pod "dns-test-e30911a4-5eeb-41db-aed6-ac1332aac639" satisfied condition "running"
STEP: retrieving the pod 03/30/23 14:17:30.428
STEP: looking for the results for each expected name from probers 03/30/23 14:17:30.43
Mar 30 14:17:30.434: INFO: DNS probes using dns-test-e30911a4-5eeb-41db-aed6-ac1332aac639 succeeded

STEP: deleting the pod 03/30/23 14:17:30.434
STEP: changing the externalName to bar.example.com 03/30/23 14:17:30.438
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3858.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local; sleep 1; done
 03/30/23 14:17:30.442
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3858.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local; sleep 1; done
 03/30/23 14:17:30.442
STEP: creating a second pod to probe DNS 03/30/23 14:17:30.442
STEP: submitting the pod to kubernetes 03/30/23 14:17:30.442
Mar 30 14:17:30.445: INFO: Waiting up to 15m0s for pod "dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5" in namespace "dns-3858" to be "running"
Mar 30 14:17:30.447: INFO: Pod "dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.950648ms
Mar 30 14:17:32.449: INFO: Pod "dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.003967421s
Mar 30 14:17:32.449: INFO: Pod "dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5" satisfied condition "running"
STEP: retrieving the pod 03/30/23 14:17:32.449
STEP: looking for the results for each expected name from probers 03/30/23 14:17:32.451
Mar 30 14:17:32.453: INFO: File wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 14:17:32.455: INFO: File jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 14:17:32.455: INFO: Lookups using dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 failed for: [wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local]

Mar 30 14:17:37.458: INFO: File wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 14:17:37.460: INFO: File jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 14:17:37.460: INFO: Lookups using dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 failed for: [wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local]

Mar 30 14:17:42.458: INFO: File wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 14:17:42.460: INFO: File jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 14:17:42.460: INFO: Lookups using dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 failed for: [wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local]

Mar 30 14:17:47.458: INFO: File wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 14:17:47.460: INFO: File jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 14:17:47.460: INFO: Lookups using dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 failed for: [wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local]

Mar 30 14:17:52.458: INFO: File wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 14:17:52.460: INFO: File jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 14:17:52.460: INFO: Lookups using dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 failed for: [wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local]

Mar 30 14:17:57.458: INFO: File wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 14:17:57.460: INFO: File jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 30 14:17:57.460: INFO: Lookups using dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 failed for: [wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local]

Mar 30 14:18:02.460: INFO: DNS probes using dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 succeeded

STEP: deleting the pod 03/30/23 14:18:02.46
STEP: changing the service to type=ClusterIP 03/30/23 14:18:02.465
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3858.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local; sleep 1; done
 03/30/23 14:18:02.471
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3858.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local; sleep 1; done
 03/30/23 14:18:02.471
STEP: creating a third pod to probe DNS 03/30/23 14:18:02.471
STEP: submitting the pod to kubernetes 03/30/23 14:18:02.473
Mar 30 14:18:02.478: INFO: Waiting up to 15m0s for pod "dns-test-822f3e94-10de-466d-9c69-c1afd277d135" in namespace "dns-3858" to be "running"
Mar 30 14:18:02.480: INFO: Pod "dns-test-822f3e94-10de-466d-9c69-c1afd277d135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025924ms
Mar 30 14:18:04.482: INFO: Pod "dns-test-822f3e94-10de-466d-9c69-c1afd277d135": Phase="Running", Reason="", readiness=true. Elapsed: 2.004449274s
Mar 30 14:18:04.482: INFO: Pod "dns-test-822f3e94-10de-466d-9c69-c1afd277d135" satisfied condition "running"
STEP: retrieving the pod 03/30/23 14:18:04.482
STEP: looking for the results for each expected name from probers 03/30/23 14:18:04.484
Mar 30 14:18:04.488: INFO: DNS probes using dns-test-822f3e94-10de-466d-9c69-c1afd277d135 succeeded

STEP: deleting the pod 03/30/23 14:18:04.488
STEP: deleting the test externalName service 03/30/23 14:18:04.493
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 30 14:18:04.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3858" for this suite. 03/30/23 14:18:04.501
------------------------------
â€¢ [SLOW TEST] [36.118 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:17:28.385
    Mar 30 14:17:28.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename dns 03/30/23 14:17:28.386
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:17:28.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:17:28.393
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 03/30/23 14:17:28.395
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3858.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local; sleep 1; done
     03/30/23 14:17:28.4
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3858.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local; sleep 1; done
     03/30/23 14:17:28.4
    STEP: creating a pod to probe DNS 03/30/23 14:17:28.401
    STEP: submitting the pod to kubernetes 03/30/23 14:17:28.401
    Mar 30 14:17:28.423: INFO: Waiting up to 15m0s for pod "dns-test-e30911a4-5eeb-41db-aed6-ac1332aac639" in namespace "dns-3858" to be "running"
    Mar 30 14:17:28.425: INFO: Pod "dns-test-e30911a4-5eeb-41db-aed6-ac1332aac639": Phase="Pending", Reason="", readiness=false. Elapsed: 2.61258ms
    Mar 30 14:17:30.428: INFO: Pod "dns-test-e30911a4-5eeb-41db-aed6-ac1332aac639": Phase="Running", Reason="", readiness=true. Elapsed: 2.004990427s
    Mar 30 14:17:30.428: INFO: Pod "dns-test-e30911a4-5eeb-41db-aed6-ac1332aac639" satisfied condition "running"
    STEP: retrieving the pod 03/30/23 14:17:30.428
    STEP: looking for the results for each expected name from probers 03/30/23 14:17:30.43
    Mar 30 14:17:30.434: INFO: DNS probes using dns-test-e30911a4-5eeb-41db-aed6-ac1332aac639 succeeded

    STEP: deleting the pod 03/30/23 14:17:30.434
    STEP: changing the externalName to bar.example.com 03/30/23 14:17:30.438
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3858.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local; sleep 1; done
     03/30/23 14:17:30.442
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3858.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local; sleep 1; done
     03/30/23 14:17:30.442
    STEP: creating a second pod to probe DNS 03/30/23 14:17:30.442
    STEP: submitting the pod to kubernetes 03/30/23 14:17:30.442
    Mar 30 14:17:30.445: INFO: Waiting up to 15m0s for pod "dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5" in namespace "dns-3858" to be "running"
    Mar 30 14:17:30.447: INFO: Pod "dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.950648ms
    Mar 30 14:17:32.449: INFO: Pod "dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.003967421s
    Mar 30 14:17:32.449: INFO: Pod "dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5" satisfied condition "running"
    STEP: retrieving the pod 03/30/23 14:17:32.449
    STEP: looking for the results for each expected name from probers 03/30/23 14:17:32.451
    Mar 30 14:17:32.453: INFO: File wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 30 14:17:32.455: INFO: File jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 30 14:17:32.455: INFO: Lookups using dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 failed for: [wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local]

    Mar 30 14:17:37.458: INFO: File wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 30 14:17:37.460: INFO: File jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 30 14:17:37.460: INFO: Lookups using dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 failed for: [wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local]

    Mar 30 14:17:42.458: INFO: File wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 30 14:17:42.460: INFO: File jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 30 14:17:42.460: INFO: Lookups using dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 failed for: [wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local]

    Mar 30 14:17:47.458: INFO: File wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 30 14:17:47.460: INFO: File jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 30 14:17:47.460: INFO: Lookups using dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 failed for: [wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local]

    Mar 30 14:17:52.458: INFO: File wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 30 14:17:52.460: INFO: File jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 30 14:17:52.460: INFO: Lookups using dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 failed for: [wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local]

    Mar 30 14:17:57.458: INFO: File wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 30 14:17:57.460: INFO: File jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local from pod  dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 30 14:17:57.460: INFO: Lookups using dns-3858/dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 failed for: [wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local]

    Mar 30 14:18:02.460: INFO: DNS probes using dns-test-e42cd4cf-391a-403b-ac78-be72d28326b5 succeeded

    STEP: deleting the pod 03/30/23 14:18:02.46
    STEP: changing the service to type=ClusterIP 03/30/23 14:18:02.465
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3858.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3858.svc.cluster.local; sleep 1; done
     03/30/23 14:18:02.471
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3858.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3858.svc.cluster.local; sleep 1; done
     03/30/23 14:18:02.471
    STEP: creating a third pod to probe DNS 03/30/23 14:18:02.471
    STEP: submitting the pod to kubernetes 03/30/23 14:18:02.473
    Mar 30 14:18:02.478: INFO: Waiting up to 15m0s for pod "dns-test-822f3e94-10de-466d-9c69-c1afd277d135" in namespace "dns-3858" to be "running"
    Mar 30 14:18:02.480: INFO: Pod "dns-test-822f3e94-10de-466d-9c69-c1afd277d135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025924ms
    Mar 30 14:18:04.482: INFO: Pod "dns-test-822f3e94-10de-466d-9c69-c1afd277d135": Phase="Running", Reason="", readiness=true. Elapsed: 2.004449274s
    Mar 30 14:18:04.482: INFO: Pod "dns-test-822f3e94-10de-466d-9c69-c1afd277d135" satisfied condition "running"
    STEP: retrieving the pod 03/30/23 14:18:04.482
    STEP: looking for the results for each expected name from probers 03/30/23 14:18:04.484
    Mar 30 14:18:04.488: INFO: DNS probes using dns-test-822f3e94-10de-466d-9c69-c1afd277d135 succeeded

    STEP: deleting the pod 03/30/23 14:18:04.488
    STEP: deleting the test externalName service 03/30/23 14:18:04.493
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:18:04.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3858" for this suite. 03/30/23 14:18:04.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:18:04.504
Mar 30 14:18:04.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename tables 03/30/23 14:18:04.504
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:18:04.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:18:04.512
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Mar 30 14:18:04.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-4704" for this suite. 03/30/23 14:18:04.519
------------------------------
â€¢ [0.019 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:18:04.504
    Mar 30 14:18:04.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename tables 03/30/23 14:18:04.504
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:18:04.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:18:04.512
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:18:04.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-4704" for this suite. 03/30/23 14:18:04.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:18:04.524
Mar 30 14:18:04.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-probe 03/30/23 14:18:04.525
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:18:04.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:18:04.532
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 in namespace container-probe-7673 03/30/23 14:18:04.534
Mar 30 14:18:04.538: INFO: Waiting up to 5m0s for pod "liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0" in namespace "container-probe-7673" to be "not pending"
Mar 30 14:18:04.539: INFO: Pod "liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580011ms
Mar 30 14:18:06.542: INFO: Pod "liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.003979104s
Mar 30 14:18:06.542: INFO: Pod "liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0" satisfied condition "not pending"
Mar 30 14:18:06.542: INFO: Started pod liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 in namespace container-probe-7673
STEP: checking the pod's current state and verifying that restartCount is present 03/30/23 14:18:06.542
Mar 30 14:18:06.543: INFO: Initial restart count of pod liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 is 0
Mar 30 14:18:26.569: INFO: Restart count of pod container-probe-7673/liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 is now 1 (20.025454544s elapsed)
Mar 30 14:18:46.596: INFO: Restart count of pod container-probe-7673/liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 is now 2 (40.052721172s elapsed)
Mar 30 14:19:06.622: INFO: Restart count of pod container-probe-7673/liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 is now 3 (1m0.078423605s elapsed)
Mar 30 14:19:26.648: INFO: Restart count of pod container-probe-7673/liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 is now 4 (1m20.104622278s elapsed)
Mar 30 14:20:30.733: INFO: Restart count of pod container-probe-7673/liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 is now 5 (2m24.189350929s elapsed)
STEP: deleting the pod 03/30/23 14:20:30.733
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 30 14:20:30.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7673" for this suite. 03/30/23 14:20:30.74
------------------------------
â€¢ [SLOW TEST] [146.219 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:18:04.524
    Mar 30 14:18:04.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-probe 03/30/23 14:18:04.525
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:18:04.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:18:04.532
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 in namespace container-probe-7673 03/30/23 14:18:04.534
    Mar 30 14:18:04.538: INFO: Waiting up to 5m0s for pod "liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0" in namespace "container-probe-7673" to be "not pending"
    Mar 30 14:18:04.539: INFO: Pod "liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580011ms
    Mar 30 14:18:06.542: INFO: Pod "liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.003979104s
    Mar 30 14:18:06.542: INFO: Pod "liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0" satisfied condition "not pending"
    Mar 30 14:18:06.542: INFO: Started pod liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 in namespace container-probe-7673
    STEP: checking the pod's current state and verifying that restartCount is present 03/30/23 14:18:06.542
    Mar 30 14:18:06.543: INFO: Initial restart count of pod liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 is 0
    Mar 30 14:18:26.569: INFO: Restart count of pod container-probe-7673/liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 is now 1 (20.025454544s elapsed)
    Mar 30 14:18:46.596: INFO: Restart count of pod container-probe-7673/liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 is now 2 (40.052721172s elapsed)
    Mar 30 14:19:06.622: INFO: Restart count of pod container-probe-7673/liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 is now 3 (1m0.078423605s elapsed)
    Mar 30 14:19:26.648: INFO: Restart count of pod container-probe-7673/liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 is now 4 (1m20.104622278s elapsed)
    Mar 30 14:20:30.733: INFO: Restart count of pod container-probe-7673/liveness-a168a38c-a23e-426b-8a2c-ced1707d08f0 is now 5 (2m24.189350929s elapsed)
    STEP: deleting the pod 03/30/23 14:20:30.733
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:20:30.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7673" for this suite. 03/30/23 14:20:30.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:20:30.744
Mar 30 14:20:30.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename cronjob 03/30/23 14:20:30.744
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:20:30.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:20:30.751
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 03/30/23 14:20:30.753
STEP: Ensuring a job is scheduled 03/30/23 14:20:30.755
STEP: Ensuring exactly one is scheduled 03/30/23 14:21:00.759
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/30/23 14:21:00.761
STEP: Ensuring the job is replaced with a new one 03/30/23 14:21:00.762
STEP: Removing cronjob 03/30/23 14:22:00.765
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 30 14:22:00.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6856" for this suite. 03/30/23 14:22:00.77
------------------------------
â€¢ [SLOW TEST] [90.028 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:20:30.744
    Mar 30 14:20:30.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename cronjob 03/30/23 14:20:30.744
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:20:30.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:20:30.751
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 03/30/23 14:20:30.753
    STEP: Ensuring a job is scheduled 03/30/23 14:20:30.755
    STEP: Ensuring exactly one is scheduled 03/30/23 14:21:00.759
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/30/23 14:21:00.761
    STEP: Ensuring the job is replaced with a new one 03/30/23 14:21:00.762
    STEP: Removing cronjob 03/30/23 14:22:00.765
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:22:00.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6856" for this suite. 03/30/23 14:22:00.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:22:00.773
Mar 30 14:22:00.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 14:22:00.773
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:00.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:00.781
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 03/30/23 14:22:00.782
Mar 30 14:22:00.782: INFO: Creating e2e-svc-a-8xx9x
Mar 30 14:22:00.786: INFO: Creating e2e-svc-b-cp57c
Mar 30 14:22:00.791: INFO: Creating e2e-svc-c-t98g4
STEP: deleting service collection 03/30/23 14:22:00.797
Mar 30 14:22:00.812: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 14:22:00.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3683" for this suite. 03/30/23 14:22:00.815
------------------------------
â€¢ [0.045 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:22:00.773
    Mar 30 14:22:00.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 14:22:00.773
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:00.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:00.781
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 03/30/23 14:22:00.782
    Mar 30 14:22:00.782: INFO: Creating e2e-svc-a-8xx9x
    Mar 30 14:22:00.786: INFO: Creating e2e-svc-b-cp57c
    Mar 30 14:22:00.791: INFO: Creating e2e-svc-c-t98g4
    STEP: deleting service collection 03/30/23 14:22:00.797
    Mar 30 14:22:00.812: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:22:00.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3683" for this suite. 03/30/23 14:22:00.815
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:22:00.818
Mar 30 14:22:00.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename subpath 03/30/23 14:22:00.818
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:00.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:00.826
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/30/23 14:22:00.827
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-x4bj 03/30/23 14:22:00.831
STEP: Creating a pod to test atomic-volume-subpath 03/30/23 14:22:00.831
Mar 30 14:22:00.835: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-x4bj" in namespace "subpath-9695" to be "Succeeded or Failed"
Mar 30 14:22:00.837: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Pending", Reason="", readiness=false. Elapsed: 1.577735ms
Mar 30 14:22:02.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 2.003697696s
Mar 30 14:22:04.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 4.003820375s
Mar 30 14:22:06.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 6.004030908s
Mar 30 14:22:08.840: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 8.004693089s
Mar 30 14:22:10.840: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 10.004358712s
Mar 30 14:22:12.840: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 12.004493251s
Mar 30 14:22:14.840: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 14.004448888s
Mar 30 14:22:16.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 16.004057506s
Mar 30 14:22:18.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 18.003416404s
Mar 30 14:22:20.840: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 20.004901876s
Mar 30 14:22:22.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=false. Elapsed: 22.003707815s
Mar 30 14:22:24.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003641741s
STEP: Saw pod success 03/30/23 14:22:24.839
Mar 30 14:22:24.839: INFO: Pod "pod-subpath-test-configmap-x4bj" satisfied condition "Succeeded or Failed"
Mar 30 14:22:24.841: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-subpath-test-configmap-x4bj container test-container-subpath-configmap-x4bj: <nil>
STEP: delete the pod 03/30/23 14:22:24.851
Mar 30 14:22:24.855: INFO: Waiting for pod pod-subpath-test-configmap-x4bj to disappear
Mar 30 14:22:24.857: INFO: Pod pod-subpath-test-configmap-x4bj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-x4bj 03/30/23 14:22:24.857
Mar 30 14:22:24.857: INFO: Deleting pod "pod-subpath-test-configmap-x4bj" in namespace "subpath-9695"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 30 14:22:24.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9695" for this suite. 03/30/23 14:22:24.86
------------------------------
â€¢ [SLOW TEST] [24.045 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:22:00.818
    Mar 30 14:22:00.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename subpath 03/30/23 14:22:00.818
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:00.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:00.826
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/30/23 14:22:00.827
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-x4bj 03/30/23 14:22:00.831
    STEP: Creating a pod to test atomic-volume-subpath 03/30/23 14:22:00.831
    Mar 30 14:22:00.835: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-x4bj" in namespace "subpath-9695" to be "Succeeded or Failed"
    Mar 30 14:22:00.837: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Pending", Reason="", readiness=false. Elapsed: 1.577735ms
    Mar 30 14:22:02.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 2.003697696s
    Mar 30 14:22:04.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 4.003820375s
    Mar 30 14:22:06.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 6.004030908s
    Mar 30 14:22:08.840: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 8.004693089s
    Mar 30 14:22:10.840: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 10.004358712s
    Mar 30 14:22:12.840: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 12.004493251s
    Mar 30 14:22:14.840: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 14.004448888s
    Mar 30 14:22:16.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 16.004057506s
    Mar 30 14:22:18.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 18.003416404s
    Mar 30 14:22:20.840: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=true. Elapsed: 20.004901876s
    Mar 30 14:22:22.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Running", Reason="", readiness=false. Elapsed: 22.003707815s
    Mar 30 14:22:24.839: INFO: Pod "pod-subpath-test-configmap-x4bj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003641741s
    STEP: Saw pod success 03/30/23 14:22:24.839
    Mar 30 14:22:24.839: INFO: Pod "pod-subpath-test-configmap-x4bj" satisfied condition "Succeeded or Failed"
    Mar 30 14:22:24.841: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-subpath-test-configmap-x4bj container test-container-subpath-configmap-x4bj: <nil>
    STEP: delete the pod 03/30/23 14:22:24.851
    Mar 30 14:22:24.855: INFO: Waiting for pod pod-subpath-test-configmap-x4bj to disappear
    Mar 30 14:22:24.857: INFO: Pod pod-subpath-test-configmap-x4bj no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-x4bj 03/30/23 14:22:24.857
    Mar 30 14:22:24.857: INFO: Deleting pod "pod-subpath-test-configmap-x4bj" in namespace "subpath-9695"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:22:24.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9695" for this suite. 03/30/23 14:22:24.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:22:24.864
Mar 30 14:22:24.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename downward-api 03/30/23 14:22:24.864
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:24.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:24.871
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 03/30/23 14:22:24.873
Mar 30 14:22:24.876: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a" in namespace "downward-api-8877" to be "Succeeded or Failed"
Mar 30 14:22:24.878: INFO: Pod "downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.459531ms
Mar 30 14:22:26.880: INFO: Pod "downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003793926s
Mar 30 14:22:28.880: INFO: Pod "downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003675345s
STEP: Saw pod success 03/30/23 14:22:28.88
Mar 30 14:22:28.880: INFO: Pod "downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a" satisfied condition "Succeeded or Failed"
Mar 30 14:22:28.882: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a container client-container: <nil>
STEP: delete the pod 03/30/23 14:22:28.885
Mar 30 14:22:28.890: INFO: Waiting for pod downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a to disappear
Mar 30 14:22:28.891: INFO: Pod downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 30 14:22:28.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8877" for this suite. 03/30/23 14:22:28.893
------------------------------
â€¢ [4.032 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:22:24.864
    Mar 30 14:22:24.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename downward-api 03/30/23 14:22:24.864
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:24.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:24.871
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 03/30/23 14:22:24.873
    Mar 30 14:22:24.876: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a" in namespace "downward-api-8877" to be "Succeeded or Failed"
    Mar 30 14:22:24.878: INFO: Pod "downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.459531ms
    Mar 30 14:22:26.880: INFO: Pod "downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003793926s
    Mar 30 14:22:28.880: INFO: Pod "downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003675345s
    STEP: Saw pod success 03/30/23 14:22:28.88
    Mar 30 14:22:28.880: INFO: Pod "downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a" satisfied condition "Succeeded or Failed"
    Mar 30 14:22:28.882: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a container client-container: <nil>
    STEP: delete the pod 03/30/23 14:22:28.885
    Mar 30 14:22:28.890: INFO: Waiting for pod downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a to disappear
    Mar 30 14:22:28.891: INFO: Pod downwardapi-volume-9d08eba0-8199-4bcf-8336-4d050d17da4a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:22:28.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8877" for this suite. 03/30/23 14:22:28.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:22:28.896
Mar 30 14:22:28.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename namespaces 03/30/23 14:22:28.897
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:28.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:28.903
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 03/30/23 14:22:28.905
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:28.91
STEP: Creating a service in the namespace 03/30/23 14:22:28.911
STEP: Deleting the namespace 03/30/23 14:22:28.916
STEP: Waiting for the namespace to be removed. 03/30/23 14:22:28.919
STEP: Recreating the namespace 03/30/23 14:22:34.921
STEP: Verifying there is no service in the namespace 03/30/23 14:22:34.927
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:22:34.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5966" for this suite. 03/30/23 14:22:34.931
STEP: Destroying namespace "nsdeletetest-4587" for this suite. 03/30/23 14:22:34.934
Mar 30 14:22:34.936: INFO: Namespace nsdeletetest-4587 was already deleted
STEP: Destroying namespace "nsdeletetest-9272" for this suite. 03/30/23 14:22:34.936
------------------------------
â€¢ [SLOW TEST] [6.042 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:22:28.896
    Mar 30 14:22:28.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename namespaces 03/30/23 14:22:28.897
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:28.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:28.903
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 03/30/23 14:22:28.905
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:28.91
    STEP: Creating a service in the namespace 03/30/23 14:22:28.911
    STEP: Deleting the namespace 03/30/23 14:22:28.916
    STEP: Waiting for the namespace to be removed. 03/30/23 14:22:28.919
    STEP: Recreating the namespace 03/30/23 14:22:34.921
    STEP: Verifying there is no service in the namespace 03/30/23 14:22:34.927
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:22:34.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5966" for this suite. 03/30/23 14:22:34.931
    STEP: Destroying namespace "nsdeletetest-4587" for this suite. 03/30/23 14:22:34.934
    Mar 30 14:22:34.936: INFO: Namespace nsdeletetest-4587 was already deleted
    STEP: Destroying namespace "nsdeletetest-9272" for this suite. 03/30/23 14:22:34.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:22:34.939
Mar 30 14:22:34.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 14:22:34.939
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:34.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:34.947
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 03/30/23 14:22:34.948
Mar 30 14:22:34.948: INFO: namespace kubectl-938
Mar 30 14:22:34.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-938 create -f -'
Mar 30 14:22:35.331: INFO: stderr: ""
Mar 30 14:22:35.331: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/30/23 14:22:35.331
Mar 30 14:22:36.334: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 14:22:36.334: INFO: Found 1 / 1
Mar 30 14:22:36.334: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 30 14:22:36.336: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 14:22:36.336: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 30 14:22:36.336: INFO: wait on agnhost-primary startup in kubectl-938 
Mar 30 14:22:36.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-938 logs agnhost-primary-8h8ct agnhost-primary'
Mar 30 14:22:36.393: INFO: stderr: ""
Mar 30 14:22:36.393: INFO: stdout: "Paused\n"
STEP: exposing RC 03/30/23 14:22:36.393
Mar 30 14:22:36.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-938 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 30 14:22:36.448: INFO: stderr: ""
Mar 30 14:22:36.448: INFO: stdout: "service/rm2 exposed\n"
Mar 30 14:22:36.450: INFO: Service rm2 in namespace kubectl-938 found.
STEP: exposing service 03/30/23 14:22:38.454
Mar 30 14:22:38.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-938 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 30 14:22:38.509: INFO: stderr: ""
Mar 30 14:22:38.509: INFO: stdout: "service/rm3 exposed\n"
Mar 30 14:22:38.511: INFO: Service rm3 in namespace kubectl-938 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 14:22:40.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-938" for this suite. 03/30/23 14:22:40.518
------------------------------
â€¢ [SLOW TEST] [5.583 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:22:34.939
    Mar 30 14:22:34.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 14:22:34.939
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:34.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:34.947
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 03/30/23 14:22:34.948
    Mar 30 14:22:34.948: INFO: namespace kubectl-938
    Mar 30 14:22:34.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-938 create -f -'
    Mar 30 14:22:35.331: INFO: stderr: ""
    Mar 30 14:22:35.331: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/30/23 14:22:35.331
    Mar 30 14:22:36.334: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 30 14:22:36.334: INFO: Found 1 / 1
    Mar 30 14:22:36.334: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 30 14:22:36.336: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 30 14:22:36.336: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 30 14:22:36.336: INFO: wait on agnhost-primary startup in kubectl-938 
    Mar 30 14:22:36.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-938 logs agnhost-primary-8h8ct agnhost-primary'
    Mar 30 14:22:36.393: INFO: stderr: ""
    Mar 30 14:22:36.393: INFO: stdout: "Paused\n"
    STEP: exposing RC 03/30/23 14:22:36.393
    Mar 30 14:22:36.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-938 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Mar 30 14:22:36.448: INFO: stderr: ""
    Mar 30 14:22:36.448: INFO: stdout: "service/rm2 exposed\n"
    Mar 30 14:22:36.450: INFO: Service rm2 in namespace kubectl-938 found.
    STEP: exposing service 03/30/23 14:22:38.454
    Mar 30 14:22:38.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-938 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Mar 30 14:22:38.509: INFO: stderr: ""
    Mar 30 14:22:38.509: INFO: stdout: "service/rm3 exposed\n"
    Mar 30 14:22:38.511: INFO: Service rm3 in namespace kubectl-938 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:22:40.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-938" for this suite. 03/30/23 14:22:40.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:22:40.522
Mar 30 14:22:40.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename job 03/30/23 14:22:40.523
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:40.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:40.529
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 03/30/23 14:22:40.531
STEP: Ensure pods equal to parallelism count is attached to the job 03/30/23 14:22:40.533
STEP: patching /status 03/30/23 14:22:42.536
STEP: updating /status 03/30/23 14:22:42.54
STEP: get /status 03/30/23 14:22:42.544
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 30 14:22:42.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-465" for this suite. 03/30/23 14:22:42.548
------------------------------
â€¢ [2.028 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:22:40.522
    Mar 30 14:22:40.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename job 03/30/23 14:22:40.523
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:40.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:40.529
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 03/30/23 14:22:40.531
    STEP: Ensure pods equal to parallelism count is attached to the job 03/30/23 14:22:40.533
    STEP: patching /status 03/30/23 14:22:42.536
    STEP: updating /status 03/30/23 14:22:42.54
    STEP: get /status 03/30/23 14:22:42.544
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:22:42.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-465" for this suite. 03/30/23 14:22:42.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:22:42.551
Mar 30 14:22:42.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pod-network-test 03/30/23 14:22:42.551
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:42.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:42.558
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-7547 03/30/23 14:22:42.559
STEP: creating a selector 03/30/23 14:22:42.559
STEP: Creating the service pods in kubernetes 03/30/23 14:22:42.56
Mar 30 14:22:42.560: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 30 14:22:42.572: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7547" to be "running and ready"
Mar 30 14:22:42.574: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.979744ms
Mar 30 14:22:42.574: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 14:22:44.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004086408s
Mar 30 14:22:44.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:22:46.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004471375s
Mar 30 14:22:46.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:22:48.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004178422s
Mar 30 14:22:48.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:22:50.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004537273s
Mar 30 14:22:50.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:22:52.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004430768s
Mar 30 14:22:52.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:22:54.577: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005337652s
Mar 30 14:22:54.577: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:22:56.577: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005006769s
Mar 30 14:22:56.577: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:22:58.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.004219371s
Mar 30 14:22:58.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:23:00.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004258138s
Mar 30 14:23:00.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:23:02.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004328494s
Mar 30 14:23:02.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 30 14:23:04.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004402551s
Mar 30 14:23:04.576: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 30 14:23:04.576: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 30 14:23:04.577: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7547" to be "running and ready"
Mar 30 14:23:04.579: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.524537ms
Mar 30 14:23:04.579: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 30 14:23:04.579: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 30 14:23:04.580: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7547" to be "running and ready"
Mar 30 14:23:04.582: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 1.218297ms
Mar 30 14:23:04.582: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar 30 14:23:06.584: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.003904831s
Mar 30 14:23:06.584: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar 30 14:23:08.585: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.004469397s
Mar 30 14:23:08.585: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar 30 14:23:10.584: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.003461988s
Mar 30 14:23:10.584: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar 30 14:23:12.584: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.003411179s
Mar 30 14:23:12.584: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar 30 14:23:14.584: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 10.00373538s
Mar 30 14:23:14.584: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar 30 14:23:16.584: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 12.003805362s
Mar 30 14:23:16.584: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Mar 30 14:23:18.584: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 14.003564015s
Mar 30 14:23:18.584: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 30 14:23:18.584: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/30/23 14:23:18.586
Mar 30 14:23:18.593: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7547" to be "running"
Mar 30 14:23:18.595: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.616138ms
Mar 30 14:23:20.597: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00418268s
Mar 30 14:23:20.597: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 30 14:23:20.599: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7547" to be "running"
Mar 30 14:23:20.600: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.335777ms
Mar 30 14:23:20.600: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 30 14:23:20.601: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 30 14:23:20.601: INFO: Going to poll 10.29.1.74 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 30 14:23:20.603: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.29.1.74 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7547 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 14:23:20.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 14:23:20.603: INFO: ExecWithOptions: Clientset creation
Mar 30 14:23:20.603: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-7547/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.29.1.74+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 30 14:23:21.645: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 30 14:23:21.645: INFO: Going to poll 10.29.0.209 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 30 14:23:21.647: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.29.0.209 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7547 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 14:23:21.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 14:23:21.647: INFO: ExecWithOptions: Clientset creation
Mar 30 14:23:21.647: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-7547/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.29.0.209+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 30 14:23:22.687: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 30 14:23:22.687: INFO: Going to poll 10.29.1.6 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 30 14:23:22.689: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.29.1.6 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7547 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 14:23:22.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 14:23:22.689: INFO: ExecWithOptions: Clientset creation
Mar 30 14:23:22.689: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-7547/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.29.1.6+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 30 14:23:23.731: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 30 14:23:23.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7547" for this suite. 03/30/23 14:23:23.733
------------------------------
â€¢ [SLOW TEST] [41.186 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:22:42.551
    Mar 30 14:22:42.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pod-network-test 03/30/23 14:22:42.551
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:22:42.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:22:42.558
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-7547 03/30/23 14:22:42.559
    STEP: creating a selector 03/30/23 14:22:42.559
    STEP: Creating the service pods in kubernetes 03/30/23 14:22:42.56
    Mar 30 14:22:42.560: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 30 14:22:42.572: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7547" to be "running and ready"
    Mar 30 14:22:42.574: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.979744ms
    Mar 30 14:22:42.574: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 14:22:44.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004086408s
    Mar 30 14:22:44.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:22:46.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004471375s
    Mar 30 14:22:46.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:22:48.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004178422s
    Mar 30 14:22:48.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:22:50.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004537273s
    Mar 30 14:22:50.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:22:52.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004430768s
    Mar 30 14:22:52.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:22:54.577: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005337652s
    Mar 30 14:22:54.577: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:22:56.577: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005006769s
    Mar 30 14:22:56.577: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:22:58.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.004219371s
    Mar 30 14:22:58.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:23:00.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004258138s
    Mar 30 14:23:00.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:23:02.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004328494s
    Mar 30 14:23:02.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 30 14:23:04.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004402551s
    Mar 30 14:23:04.576: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 30 14:23:04.576: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 30 14:23:04.577: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7547" to be "running and ready"
    Mar 30 14:23:04.579: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.524537ms
    Mar 30 14:23:04.579: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 30 14:23:04.579: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 30 14:23:04.580: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7547" to be "running and ready"
    Mar 30 14:23:04.582: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 1.218297ms
    Mar 30 14:23:04.582: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar 30 14:23:06.584: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.003904831s
    Mar 30 14:23:06.584: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar 30 14:23:08.585: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.004469397s
    Mar 30 14:23:08.585: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar 30 14:23:10.584: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.003461988s
    Mar 30 14:23:10.584: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar 30 14:23:12.584: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.003411179s
    Mar 30 14:23:12.584: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar 30 14:23:14.584: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 10.00373538s
    Mar 30 14:23:14.584: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar 30 14:23:16.584: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 12.003805362s
    Mar 30 14:23:16.584: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Mar 30 14:23:18.584: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 14.003564015s
    Mar 30 14:23:18.584: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 30 14:23:18.584: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/30/23 14:23:18.586
    Mar 30 14:23:18.593: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7547" to be "running"
    Mar 30 14:23:18.595: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.616138ms
    Mar 30 14:23:20.597: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00418268s
    Mar 30 14:23:20.597: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 30 14:23:20.599: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7547" to be "running"
    Mar 30 14:23:20.600: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.335777ms
    Mar 30 14:23:20.600: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 30 14:23:20.601: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 30 14:23:20.601: INFO: Going to poll 10.29.1.74 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 30 14:23:20.603: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.29.1.74 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7547 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 14:23:20.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 14:23:20.603: INFO: ExecWithOptions: Clientset creation
    Mar 30 14:23:20.603: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-7547/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.29.1.74+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 30 14:23:21.645: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 30 14:23:21.645: INFO: Going to poll 10.29.0.209 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 30 14:23:21.647: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.29.0.209 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7547 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 14:23:21.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 14:23:21.647: INFO: ExecWithOptions: Clientset creation
    Mar 30 14:23:21.647: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-7547/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.29.0.209+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 30 14:23:22.687: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 30 14:23:22.687: INFO: Going to poll 10.29.1.6 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 30 14:23:22.689: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.29.1.6 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7547 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 14:23:22.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 14:23:22.689: INFO: ExecWithOptions: Clientset creation
    Mar 30 14:23:22.689: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/pod-network-test-7547/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.29.1.6+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 30 14:23:23.731: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:23:23.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7547" for this suite. 03/30/23 14:23:23.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:23:23.737
Mar 30 14:23:23.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename init-container 03/30/23 14:23:23.738
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:23:23.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:23:23.746
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 03/30/23 14:23:23.747
Mar 30 14:23:23.747: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:23:28.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2018" for this suite. 03/30/23 14:23:28.217
------------------------------
â€¢ [4.482 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:23:23.737
    Mar 30 14:23:23.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename init-container 03/30/23 14:23:23.738
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:23:23.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:23:23.746
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 03/30/23 14:23:23.747
    Mar 30 14:23:23.747: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:23:28.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2018" for this suite. 03/30/23 14:23:28.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:23:28.22
Mar 30 14:23:28.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 14:23:28.221
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:23:28.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:23:28.227
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 03/30/23 14:23:28.229
Mar 30 14:23:28.232: INFO: Waiting up to 5m0s for pod "pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e" in namespace "emptydir-4477" to be "Succeeded or Failed"
Mar 30 14:23:28.233: INFO: Pod "pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.420913ms
Mar 30 14:23:30.235: INFO: Pod "pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003272667s
Mar 30 14:23:32.236: INFO: Pod "pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003684349s
STEP: Saw pod success 03/30/23 14:23:32.236
Mar 30 14:23:32.236: INFO: Pod "pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e" satisfied condition "Succeeded or Failed"
Mar 30 14:23:32.237: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e container test-container: <nil>
STEP: delete the pod 03/30/23 14:23:32.241
Mar 30 14:23:32.245: INFO: Waiting for pod pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e to disappear
Mar 30 14:23:32.246: INFO: Pod pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 14:23:32.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4477" for this suite. 03/30/23 14:23:32.249
------------------------------
â€¢ [4.031 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:23:28.22
    Mar 30 14:23:28.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 14:23:28.221
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:23:28.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:23:28.227
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 03/30/23 14:23:28.229
    Mar 30 14:23:28.232: INFO: Waiting up to 5m0s for pod "pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e" in namespace "emptydir-4477" to be "Succeeded or Failed"
    Mar 30 14:23:28.233: INFO: Pod "pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.420913ms
    Mar 30 14:23:30.235: INFO: Pod "pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003272667s
    Mar 30 14:23:32.236: INFO: Pod "pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003684349s
    STEP: Saw pod success 03/30/23 14:23:32.236
    Mar 30 14:23:32.236: INFO: Pod "pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e" satisfied condition "Succeeded or Failed"
    Mar 30 14:23:32.237: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e container test-container: <nil>
    STEP: delete the pod 03/30/23 14:23:32.241
    Mar 30 14:23:32.245: INFO: Waiting for pod pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e to disappear
    Mar 30 14:23:32.246: INFO: Pod pod-ac0affcb-e48d-47c1-8493-a9a43eb5456e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:23:32.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4477" for this suite. 03/30/23 14:23:32.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:23:32.252
Mar 30 14:23:32.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 14:23:32.252
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:23:32.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:23:32.259
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 14:23:32.265
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:23:32.664
STEP: Deploying the webhook pod 03/30/23 14:23:32.668
STEP: Wait for the deployment to be ready 03/30/23 14:23:32.673
Mar 30 14:23:32.677: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 14:23:34.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:23:36.685: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:23:38.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:23:40.685: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/30/23 14:23:42.685
STEP: Verifying the service has paired with the endpoint 03/30/23 14:23:42.69
Mar 30 14:23:43.691: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Mar 30 14:23:43.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9173-crds.webhook.example.com via the AdmissionRegistration API 03/30/23 14:23:49.2
STEP: Creating a custom resource that should be mutated by the webhook 03/30/23 14:23:49.209
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:23:51.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2197" for this suite. 03/30/23 14:23:51.774
STEP: Destroying namespace "webhook-2197-markers" for this suite. 03/30/23 14:23:51.778
------------------------------
â€¢ [SLOW TEST] [19.529 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:23:32.252
    Mar 30 14:23:32.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 14:23:32.252
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:23:32.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:23:32.259
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 14:23:32.265
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:23:32.664
    STEP: Deploying the webhook pod 03/30/23 14:23:32.668
    STEP: Wait for the deployment to be ready 03/30/23 14:23:32.673
    Mar 30 14:23:32.677: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 30 14:23:34.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:23:36.685: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:23:38.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:23:40.685: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 23, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/30/23 14:23:42.685
    STEP: Verifying the service has paired with the endpoint 03/30/23 14:23:42.69
    Mar 30 14:23:43.691: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Mar 30 14:23:43.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9173-crds.webhook.example.com via the AdmissionRegistration API 03/30/23 14:23:49.2
    STEP: Creating a custom resource that should be mutated by the webhook 03/30/23 14:23:49.209
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:23:51.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2197" for this suite. 03/30/23 14:23:51.774
    STEP: Destroying namespace "webhook-2197-markers" for this suite. 03/30/23 14:23:51.778
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:23:51.781
Mar 30 14:23:51.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 14:23:51.781
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:23:51.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:23:51.789
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Mar 30 14:23:51.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 create -f -'
Mar 30 14:23:52.348: INFO: stderr: ""
Mar 30 14:23:52.348: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 30 14:23:52.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 create -f -'
Mar 30 14:23:52.893: INFO: stderr: ""
Mar 30 14:23:52.893: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/30/23 14:23:52.893
Mar 30 14:23:53.896: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 14:23:53.896: INFO: Found 1 / 1
Mar 30 14:23:53.896: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 30 14:23:53.898: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 30 14:23:53.898: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 30 14:23:53.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 describe pod agnhost-primary-gl7mg'
Mar 30 14:23:53.955: INFO: stderr: ""
Mar 30 14:23:53.955: INFO: stdout: "Name:             agnhost-primary-gl7mg\nNamespace:        kubectl-3256\nPriority:         0\nService Account:  default\nNode:             cn-hongkong.192.168.0.5/192.168.0.5\nStart Time:       Thu, 30 Mar 2023 14:23:52 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.29.1.11\nIPs:\n  IP:           10.29.1.11\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://5ae2bd223d2e2a2233857a686fccf4dd643e78ab4d6db3b717b60ebc3ff42586\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 30 Mar 2023 14:23:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zxgsf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-zxgsf:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-3256/agnhost-primary-gl7mg to cn-hongkong.192.168.0.5\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Mar 30 14:23:53.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 describe rc agnhost-primary'
Mar 30 14:23:54.013: INFO: stderr: ""
Mar 30 14:23:54.013: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3256\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-gl7mg\n"
Mar 30 14:23:54.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 describe service agnhost-primary'
Mar 30 14:23:54.067: INFO: stderr: ""
Mar 30 14:23:54.067: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3256\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.16.1.11\nIPs:               172.16.1.11\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.29.1.11:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 30 14:23:54.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 describe node cn-hongkong.192.168.0.1'
Mar 30 14:23:54.139: INFO: stderr: ""
Mar 30 14:23:54.139: INFO: stdout: "Name:               cn-hongkong.192.168.0.1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ecs.r7.2xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=cn-hongkong\n                    failure-domain.beta.kubernetes.io/zone=cn-hongkong-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cn-hongkong.192.168.0.1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.csi.alibabacloud.com/disktype.cloud_auto=available\n                    node.csi.alibabacloud.com/disktype.cloud_essd=available\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=ecs.r7.2xlarge\n                    topology.diskplugin.csi.alibabacloud.com/zone=cn-hongkong-b\n                    topology.kubernetes.io/region=cn-hongkong\n                    topology.kubernetes.io/zone=cn-hongkong-b\nAnnotations:        csi.volume.kubernetes.io/nodeid:\n                      {\"diskplugin.csi.alibabacloud.com\":\"i-j6ceb0ii9lu82ty9v92x\",\"nasplugin.csi.alibabacloud.com\":\"i-j6ceb0ii9lu82ty9v92x\",\"ossplugin.csi.aliba...\n                    flannel.alpha.coreos.com/backend-data: null\n                    flannel.alpha.coreos.com/backend-type: \n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.0.1\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 30 Mar 2023 10:02:34 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  cn-hongkong.192.168.0.1\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 30 Mar 2023 14:23:49 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure       False   Thu, 30 Mar 2023 14:20:40 +0000   Thu, 30 Mar 2023 10:02:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 30 Mar 2023 14:20:40 +0000   Thu, 30 Mar 2023 10:02:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 30 Mar 2023 14:20:40 +0000   Thu, 30 Mar 2023 10:02:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 30 Mar 2023 14:20:40 +0000   Thu, 30 Mar 2023 10:06:33 +0000   KubeletReady                 kubelet is posting ready status\n  NetworkUnavailable   False   Thu, 30 Mar 2023 10:06:27 +0000   Thu, 30 Mar 2023 10:06:27 +0000   RouteCreated                 RouteController created a route\nAddresses:\n  InternalIP:  192.168.0.1\n  Hostname:    cn-hongkong.192.168.0.1\nCapacity:\n  cpu:                8\n  ephemeral-storage:  123722704Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             64612608Ki\n  pods:               64\nAllocatable:\n  cpu:                7800m\n  ephemeral-storage:  114022843818\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             58710272Ki\n  pods:               64\nSystem Info:\n  Machine ID:                 20221103114305325897320765277378\n  System UUID:                04216887-f163-4e33-8192-e97d14b05e1c\n  Boot ID:                    304ed091-e063-4cbc-be60-3d7ef1cf79c6\n  Kernel Version:             4.19.91-26.6.al7.x86_64\n  OS Image:                   Alibaba Cloud Linux (Aliyun Linux) 2.1903 LTS (Hunting Beagle)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.13\n  Kubelet Version:            v1.26.3-aliyun.1\n  Kube-Proxy Version:         v1.26.3-aliyun.1\nPodCIDR:                      10.29.0.64/26\nPodCIDRs:                     10.29.0.64/26\nProviderID:                   cn-hongkong.i-j6ceb0ii9lu82ty9v92x\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cloud-controller-manager-g5cxx                             100m (1%)     1 (12%)     200Mi (0%)       1Gi (1%)       4h17m\n  kube-system                 csi-plugin-h2ncs                                           130m (1%)     2 (25%)     176Mi (0%)       4Gi (7%)       4h17m\n  kube-system                 kube-apiserver-cn-hongkong.192.168.0.1                     250m (3%)     0 (0%)      0 (0%)           0 (0%)         4h21m\n  kube-system                 kube-controller-manager-cn-hongkong.192.168.0.1            200m (2%)     0 (0%)      0 (0%)           0 (0%)         4h21m\n  kube-system                 kube-flannel-ds-4vn48                                      100m (1%)     100m (1%)   100Mi (0%)       256Mi (0%)     4h17m\n  kube-system                 kube-proxy-master-dnq6l                                    100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         4h17m\n  kube-system                 kube-scheduler-cn-hongkong.192.168.0.1                     100m (1%)     0 (0%)      0 (0%)           0 (0%)         4h21m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-zfjvx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         75m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                980m (12%)  3100m (39%)\n  memory             576Mi (1%)  5376Mi (9%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Mar 30 14:23:54.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 describe namespace kubectl-3256'
Mar 30 14:23:54.192: INFO: stderr: ""
Mar 30 14:23:54.192: INFO: stdout: "Name:         kubectl-3256\nLabels:       e2e-framework=kubectl\n              e2e-run=2220ea2c-1613-43b9-bb5d-81b7f052acb2\n              kubernetes.io/metadata.name=kubectl-3256\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 14:23:54.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3256" for this suite. 03/30/23 14:23:54.195
------------------------------
â€¢ [2.417 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:23:51.781
    Mar 30 14:23:51.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 14:23:51.781
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:23:51.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:23:51.789
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Mar 30 14:23:51.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 create -f -'
    Mar 30 14:23:52.348: INFO: stderr: ""
    Mar 30 14:23:52.348: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Mar 30 14:23:52.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 create -f -'
    Mar 30 14:23:52.893: INFO: stderr: ""
    Mar 30 14:23:52.893: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/30/23 14:23:52.893
    Mar 30 14:23:53.896: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 30 14:23:53.896: INFO: Found 1 / 1
    Mar 30 14:23:53.896: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 30 14:23:53.898: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 30 14:23:53.898: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 30 14:23:53.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 describe pod agnhost-primary-gl7mg'
    Mar 30 14:23:53.955: INFO: stderr: ""
    Mar 30 14:23:53.955: INFO: stdout: "Name:             agnhost-primary-gl7mg\nNamespace:        kubectl-3256\nPriority:         0\nService Account:  default\nNode:             cn-hongkong.192.168.0.5/192.168.0.5\nStart Time:       Thu, 30 Mar 2023 14:23:52 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.29.1.11\nIPs:\n  IP:           10.29.1.11\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://5ae2bd223d2e2a2233857a686fccf4dd643e78ab4d6db3b717b60ebc3ff42586\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 30 Mar 2023 14:23:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zxgsf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-zxgsf:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-3256/agnhost-primary-gl7mg to cn-hongkong.192.168.0.5\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Mar 30 14:23:53.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 describe rc agnhost-primary'
    Mar 30 14:23:54.013: INFO: stderr: ""
    Mar 30 14:23:54.013: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3256\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-gl7mg\n"
    Mar 30 14:23:54.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 describe service agnhost-primary'
    Mar 30 14:23:54.067: INFO: stderr: ""
    Mar 30 14:23:54.067: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3256\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.16.1.11\nIPs:               172.16.1.11\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.29.1.11:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Mar 30 14:23:54.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 describe node cn-hongkong.192.168.0.1'
    Mar 30 14:23:54.139: INFO: stderr: ""
    Mar 30 14:23:54.139: INFO: stdout: "Name:               cn-hongkong.192.168.0.1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ecs.r7.2xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=cn-hongkong\n                    failure-domain.beta.kubernetes.io/zone=cn-hongkong-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cn-hongkong.192.168.0.1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.csi.alibabacloud.com/disktype.cloud_auto=available\n                    node.csi.alibabacloud.com/disktype.cloud_essd=available\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=ecs.r7.2xlarge\n                    topology.diskplugin.csi.alibabacloud.com/zone=cn-hongkong-b\n                    topology.kubernetes.io/region=cn-hongkong\n                    topology.kubernetes.io/zone=cn-hongkong-b\nAnnotations:        csi.volume.kubernetes.io/nodeid:\n                      {\"diskplugin.csi.alibabacloud.com\":\"i-j6ceb0ii9lu82ty9v92x\",\"nasplugin.csi.alibabacloud.com\":\"i-j6ceb0ii9lu82ty9v92x\",\"ossplugin.csi.aliba...\n                    flannel.alpha.coreos.com/backend-data: null\n                    flannel.alpha.coreos.com/backend-type: \n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.0.1\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 30 Mar 2023 10:02:34 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  cn-hongkong.192.168.0.1\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 30 Mar 2023 14:23:49 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure       False   Thu, 30 Mar 2023 14:20:40 +0000   Thu, 30 Mar 2023 10:02:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 30 Mar 2023 14:20:40 +0000   Thu, 30 Mar 2023 10:02:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 30 Mar 2023 14:20:40 +0000   Thu, 30 Mar 2023 10:02:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 30 Mar 2023 14:20:40 +0000   Thu, 30 Mar 2023 10:06:33 +0000   KubeletReady                 kubelet is posting ready status\n  NetworkUnavailable   False   Thu, 30 Mar 2023 10:06:27 +0000   Thu, 30 Mar 2023 10:06:27 +0000   RouteCreated                 RouteController created a route\nAddresses:\n  InternalIP:  192.168.0.1\n  Hostname:    cn-hongkong.192.168.0.1\nCapacity:\n  cpu:                8\n  ephemeral-storage:  123722704Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             64612608Ki\n  pods:               64\nAllocatable:\n  cpu:                7800m\n  ephemeral-storage:  114022843818\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             58710272Ki\n  pods:               64\nSystem Info:\n  Machine ID:                 20221103114305325897320765277378\n  System UUID:                04216887-f163-4e33-8192-e97d14b05e1c\n  Boot ID:                    304ed091-e063-4cbc-be60-3d7ef1cf79c6\n  Kernel Version:             4.19.91-26.6.al7.x86_64\n  OS Image:                   Alibaba Cloud Linux (Aliyun Linux) 2.1903 LTS (Hunting Beagle)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.13\n  Kubelet Version:            v1.26.3-aliyun.1\n  Kube-Proxy Version:         v1.26.3-aliyun.1\nPodCIDR:                      10.29.0.64/26\nPodCIDRs:                     10.29.0.64/26\nProviderID:                   cn-hongkong.i-j6ceb0ii9lu82ty9v92x\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cloud-controller-manager-g5cxx                             100m (1%)     1 (12%)     200Mi (0%)       1Gi (1%)       4h17m\n  kube-system                 csi-plugin-h2ncs                                           130m (1%)     2 (25%)     176Mi (0%)       4Gi (7%)       4h17m\n  kube-system                 kube-apiserver-cn-hongkong.192.168.0.1                     250m (3%)     0 (0%)      0 (0%)           0 (0%)         4h21m\n  kube-system                 kube-controller-manager-cn-hongkong.192.168.0.1            200m (2%)     0 (0%)      0 (0%)           0 (0%)         4h21m\n  kube-system                 kube-flannel-ds-4vn48                                      100m (1%)     100m (1%)   100Mi (0%)       256Mi (0%)     4h17m\n  kube-system                 kube-proxy-master-dnq6l                                    100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         4h17m\n  kube-system                 kube-scheduler-cn-hongkong.192.168.0.1                     100m (1%)     0 (0%)      0 (0%)           0 (0%)         4h21m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-1ee23301b0c74a08-zfjvx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         75m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                980m (12%)  3100m (39%)\n  memory             576Mi (1%)  5376Mi (9%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
    Mar 30 14:23:54.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-3256 describe namespace kubectl-3256'
    Mar 30 14:23:54.192: INFO: stderr: ""
    Mar 30 14:23:54.192: INFO: stdout: "Name:         kubectl-3256\nLabels:       e2e-framework=kubectl\n              e2e-run=2220ea2c-1613-43b9-bb5d-81b7f052acb2\n              kubernetes.io/metadata.name=kubectl-3256\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:23:54.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3256" for this suite. 03/30/23 14:23:54.195
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:23:54.198
Mar 30 14:23:54.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename cronjob 03/30/23 14:23:54.198
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:23:54.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:23:54.206
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 03/30/23 14:23:54.207
STEP: Ensuring no jobs are scheduled 03/30/23 14:23:54.209
STEP: Ensuring no job exists by listing jobs explicitly 03/30/23 14:28:54.214
STEP: Removing cronjob 03/30/23 14:28:54.215
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 30 14:28:54.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8951" for this suite. 03/30/23 14:28:54.22
------------------------------
â€¢ [SLOW TEST] [300.025 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:23:54.198
    Mar 30 14:23:54.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename cronjob 03/30/23 14:23:54.198
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:23:54.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:23:54.206
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 03/30/23 14:23:54.207
    STEP: Ensuring no jobs are scheduled 03/30/23 14:23:54.209
    STEP: Ensuring no job exists by listing jobs explicitly 03/30/23 14:28:54.214
    STEP: Removing cronjob 03/30/23 14:28:54.215
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:28:54.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8951" for this suite. 03/30/23 14:28:54.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:28:54.223
Mar 30 14:28:54.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename crd-webhook 03/30/23 14:28:54.223
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:28:54.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:28:54.231
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/30/23 14:28:54.232
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/30/23 14:28:54.772
STEP: Deploying the custom resource conversion webhook pod 03/30/23 14:28:54.776
STEP: Wait for the deployment to be ready 03/30/23 14:28:54.781
Mar 30 14:28:54.784: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/30/23 14:28:56.789
STEP: Verifying the service has paired with the endpoint 03/30/23 14:28:56.793
Mar 30 14:28:57.794: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Mar 30 14:28:57.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Creating a v1 custom resource 03/30/23 14:29:05.347
STEP: v2 custom resource should be converted 03/30/23 14:29:05.35
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:29:05.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-5055" for this suite. 03/30/23 14:29:05.879
------------------------------
â€¢ [SLOW TEST] [11.659 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:28:54.223
    Mar 30 14:28:54.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename crd-webhook 03/30/23 14:28:54.223
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:28:54.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:28:54.231
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/30/23 14:28:54.232
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/30/23 14:28:54.772
    STEP: Deploying the custom resource conversion webhook pod 03/30/23 14:28:54.776
    STEP: Wait for the deployment to be ready 03/30/23 14:28:54.781
    Mar 30 14:28:54.784: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/30/23 14:28:56.789
    STEP: Verifying the service has paired with the endpoint 03/30/23 14:28:56.793
    Mar 30 14:28:57.794: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Mar 30 14:28:57.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Creating a v1 custom resource 03/30/23 14:29:05.347
    STEP: v2 custom resource should be converted 03/30/23 14:29:05.35
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:29:05.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-5055" for this suite. 03/30/23 14:29:05.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:29:05.884
Mar 30 14:29:05.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 14:29:05.884
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:29:05.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:29:05.892
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-ead7c263-ca8f-403e-b44d-c64dcbe51973 03/30/23 14:29:05.893
STEP: Creating a pod to test consume secrets 03/30/23 14:29:05.896
Mar 30 14:29:05.900: INFO: Waiting up to 5m0s for pod "pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005" in namespace "secrets-9475" to be "Succeeded or Failed"
Mar 30 14:29:05.902: INFO: Pod "pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066839ms
Mar 30 14:29:07.904: INFO: Pod "pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004187416s
Mar 30 14:29:09.905: INFO: Pod "pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005267557s
STEP: Saw pod success 03/30/23 14:29:09.905
Mar 30 14:29:09.905: INFO: Pod "pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005" satisfied condition "Succeeded or Failed"
Mar 30 14:29:09.907: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005 container secret-volume-test: <nil>
STEP: delete the pod 03/30/23 14:29:09.917
Mar 30 14:29:09.921: INFO: Waiting for pod pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005 to disappear
Mar 30 14:29:09.923: INFO: Pod pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 14:29:09.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9475" for this suite. 03/30/23 14:29:09.925
------------------------------
â€¢ [4.044 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:29:05.884
    Mar 30 14:29:05.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 14:29:05.884
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:29:05.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:29:05.892
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-ead7c263-ca8f-403e-b44d-c64dcbe51973 03/30/23 14:29:05.893
    STEP: Creating a pod to test consume secrets 03/30/23 14:29:05.896
    Mar 30 14:29:05.900: INFO: Waiting up to 5m0s for pod "pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005" in namespace "secrets-9475" to be "Succeeded or Failed"
    Mar 30 14:29:05.902: INFO: Pod "pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066839ms
    Mar 30 14:29:07.904: INFO: Pod "pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004187416s
    Mar 30 14:29:09.905: INFO: Pod "pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005267557s
    STEP: Saw pod success 03/30/23 14:29:09.905
    Mar 30 14:29:09.905: INFO: Pod "pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005" satisfied condition "Succeeded or Failed"
    Mar 30 14:29:09.907: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005 container secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 14:29:09.917
    Mar 30 14:29:09.921: INFO: Waiting for pod pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005 to disappear
    Mar 30 14:29:09.923: INFO: Pod pod-secrets-86c71827-d9c5-45b6-9357-b26a1cc9c005 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:29:09.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9475" for this suite. 03/30/23 14:29:09.925
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:29:09.928
Mar 30 14:29:09.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename controllerrevisions 03/30/23 14:29:09.929
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:29:09.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:29:09.935
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-8mnb9-daemon-set" 03/30/23 14:29:09.945
STEP: Check that daemon pods launch on every node of the cluster. 03/30/23 14:29:09.948
Mar 30 14:29:09.950: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:29:09.950: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:29:09.950: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:29:09.952: INFO: Number of nodes with available pods controlled by daemonset e2e-8mnb9-daemon-set: 0
Mar 30 14:29:09.952: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
Mar 30 14:29:10.954: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:29:10.954: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:29:10.955: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:29:10.957: INFO: Number of nodes with available pods controlled by daemonset e2e-8mnb9-daemon-set: 2
Mar 30 14:29:10.957: INFO: Node cn-hongkong.192.168.0.4 is running 0 daemon pod, expected 1
Mar 30 14:29:11.955: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:29:11.956: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:29:11.956: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 30 14:29:11.957: INFO: Number of nodes with available pods controlled by daemonset e2e-8mnb9-daemon-set: 3
Mar 30 14:29:11.957: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-8mnb9-daemon-set
STEP: Confirm DaemonSet "e2e-8mnb9-daemon-set" successfully created with "daemonset-name=e2e-8mnb9-daemon-set" label 03/30/23 14:29:11.959
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-8mnb9-daemon-set" 03/30/23 14:29:11.962
Mar 30 14:29:11.964: INFO: Located ControllerRevision: "e2e-8mnb9-daemon-set-5c9764469f"
STEP: Patching ControllerRevision "e2e-8mnb9-daemon-set-5c9764469f" 03/30/23 14:29:11.965
Mar 30 14:29:11.968: INFO: e2e-8mnb9-daemon-set-5c9764469f has been patched
STEP: Create a new ControllerRevision 03/30/23 14:29:11.968
Mar 30 14:29:11.970: INFO: Created ControllerRevision: e2e-8mnb9-daemon-set-595db7d66c
STEP: Confirm that there are two ControllerRevisions 03/30/23 14:29:11.97
Mar 30 14:29:11.970: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 30 14:29:11.972: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-8mnb9-daemon-set-5c9764469f" 03/30/23 14:29:11.972
STEP: Confirm that there is only one ControllerRevision 03/30/23 14:29:11.974
Mar 30 14:29:11.974: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 30 14:29:11.976: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-8mnb9-daemon-set-595db7d66c" 03/30/23 14:29:11.977
Mar 30 14:29:11.980: INFO: e2e-8mnb9-daemon-set-595db7d66c has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 03/30/23 14:29:11.98
W0330 14:29:11.983803      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 03/30/23 14:29:11.983
Mar 30 14:29:11.983: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 30 14:29:12.985: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 30 14:29:12.987: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-8mnb9-daemon-set-595db7d66c=updated" 03/30/23 14:29:12.988
STEP: Confirm that there is only one ControllerRevision 03/30/23 14:29:12.993
Mar 30 14:29:12.993: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 30 14:29:12.995: INFO: Found 1 ControllerRevisions
Mar 30 14:29:12.996: INFO: ControllerRevision "e2e-8mnb9-daemon-set-db8cbf678" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-8mnb9-daemon-set" 03/30/23 14:29:12.997
STEP: deleting DaemonSet.extensions e2e-8mnb9-daemon-set in namespace controllerrevisions-4189, will wait for the garbage collector to delete the pods 03/30/23 14:29:12.997
Mar 30 14:29:13.052: INFO: Deleting DaemonSet.extensions e2e-8mnb9-daemon-set took: 2.618829ms
Mar 30 14:29:13.153: INFO: Terminating DaemonSet.extensions e2e-8mnb9-daemon-set pods took: 100.874269ms
Mar 30 14:29:14.155: INFO: Number of nodes with available pods controlled by daemonset e2e-8mnb9-daemon-set: 0
Mar 30 14:29:14.155: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-8mnb9-daemon-set
Mar 30 14:29:14.157: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"94623"},"items":null}

Mar 30 14:29:14.158: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"94623"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:29:14.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-4189" for this suite. 03/30/23 14:29:14.167
------------------------------
â€¢ [4.241 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:29:09.928
    Mar 30 14:29:09.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename controllerrevisions 03/30/23 14:29:09.929
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:29:09.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:29:09.935
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-8mnb9-daemon-set" 03/30/23 14:29:09.945
    STEP: Check that daemon pods launch on every node of the cluster. 03/30/23 14:29:09.948
    Mar 30 14:29:09.950: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:29:09.950: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:29:09.950: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:29:09.952: INFO: Number of nodes with available pods controlled by daemonset e2e-8mnb9-daemon-set: 0
    Mar 30 14:29:09.952: INFO: Node cn-hongkong.192.168.0.3 is running 0 daemon pod, expected 1
    Mar 30 14:29:10.954: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:29:10.954: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:29:10.955: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:29:10.957: INFO: Number of nodes with available pods controlled by daemonset e2e-8mnb9-daemon-set: 2
    Mar 30 14:29:10.957: INFO: Node cn-hongkong.192.168.0.4 is running 0 daemon pod, expected 1
    Mar 30 14:29:11.955: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:29:11.956: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:29:11.956: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.252 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 30 14:29:11.957: INFO: Number of nodes with available pods controlled by daemonset e2e-8mnb9-daemon-set: 3
    Mar 30 14:29:11.957: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-8mnb9-daemon-set
    STEP: Confirm DaemonSet "e2e-8mnb9-daemon-set" successfully created with "daemonset-name=e2e-8mnb9-daemon-set" label 03/30/23 14:29:11.959
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-8mnb9-daemon-set" 03/30/23 14:29:11.962
    Mar 30 14:29:11.964: INFO: Located ControllerRevision: "e2e-8mnb9-daemon-set-5c9764469f"
    STEP: Patching ControllerRevision "e2e-8mnb9-daemon-set-5c9764469f" 03/30/23 14:29:11.965
    Mar 30 14:29:11.968: INFO: e2e-8mnb9-daemon-set-5c9764469f has been patched
    STEP: Create a new ControllerRevision 03/30/23 14:29:11.968
    Mar 30 14:29:11.970: INFO: Created ControllerRevision: e2e-8mnb9-daemon-set-595db7d66c
    STEP: Confirm that there are two ControllerRevisions 03/30/23 14:29:11.97
    Mar 30 14:29:11.970: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 30 14:29:11.972: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-8mnb9-daemon-set-5c9764469f" 03/30/23 14:29:11.972
    STEP: Confirm that there is only one ControllerRevision 03/30/23 14:29:11.974
    Mar 30 14:29:11.974: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 30 14:29:11.976: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-8mnb9-daemon-set-595db7d66c" 03/30/23 14:29:11.977
    Mar 30 14:29:11.980: INFO: e2e-8mnb9-daemon-set-595db7d66c has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 03/30/23 14:29:11.98
    W0330 14:29:11.983803      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 03/30/23 14:29:11.983
    Mar 30 14:29:11.983: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 30 14:29:12.985: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 30 14:29:12.987: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-8mnb9-daemon-set-595db7d66c=updated" 03/30/23 14:29:12.988
    STEP: Confirm that there is only one ControllerRevision 03/30/23 14:29:12.993
    Mar 30 14:29:12.993: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 30 14:29:12.995: INFO: Found 1 ControllerRevisions
    Mar 30 14:29:12.996: INFO: ControllerRevision "e2e-8mnb9-daemon-set-db8cbf678" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-8mnb9-daemon-set" 03/30/23 14:29:12.997
    STEP: deleting DaemonSet.extensions e2e-8mnb9-daemon-set in namespace controllerrevisions-4189, will wait for the garbage collector to delete the pods 03/30/23 14:29:12.997
    Mar 30 14:29:13.052: INFO: Deleting DaemonSet.extensions e2e-8mnb9-daemon-set took: 2.618829ms
    Mar 30 14:29:13.153: INFO: Terminating DaemonSet.extensions e2e-8mnb9-daemon-set pods took: 100.874269ms
    Mar 30 14:29:14.155: INFO: Number of nodes with available pods controlled by daemonset e2e-8mnb9-daemon-set: 0
    Mar 30 14:29:14.155: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-8mnb9-daemon-set
    Mar 30 14:29:14.157: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"94623"},"items":null}

    Mar 30 14:29:14.158: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"94623"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:29:14.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-4189" for this suite. 03/30/23 14:29:14.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:29:14.17
Mar 30 14:29:14.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-probe 03/30/23 14:29:14.171
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:29:14.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:29:14.178
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-a766293a-b721-48a7-acb7-8b24487ccba6 in namespace container-probe-9406 03/30/23 14:29:14.179
Mar 30 14:29:14.183: INFO: Waiting up to 5m0s for pod "liveness-a766293a-b721-48a7-acb7-8b24487ccba6" in namespace "container-probe-9406" to be "not pending"
Mar 30 14:29:14.184: INFO: Pod "liveness-a766293a-b721-48a7-acb7-8b24487ccba6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.277745ms
Mar 30 14:29:16.187: INFO: Pod "liveness-a766293a-b721-48a7-acb7-8b24487ccba6": Phase="Running", Reason="", readiness=true. Elapsed: 2.003929744s
Mar 30 14:29:16.187: INFO: Pod "liveness-a766293a-b721-48a7-acb7-8b24487ccba6" satisfied condition "not pending"
Mar 30 14:29:16.187: INFO: Started pod liveness-a766293a-b721-48a7-acb7-8b24487ccba6 in namespace container-probe-9406
STEP: checking the pod's current state and verifying that restartCount is present 03/30/23 14:29:16.187
Mar 30 14:29:16.188: INFO: Initial restart count of pod liveness-a766293a-b721-48a7-acb7-8b24487ccba6 is 0
STEP: deleting the pod 03/30/23 14:33:16.499
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 30 14:33:16.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9406" for this suite. 03/30/23 14:33:16.511
------------------------------
â€¢ [SLOW TEST] [242.343 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:29:14.17
    Mar 30 14:29:14.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-probe 03/30/23 14:29:14.171
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:29:14.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:29:14.178
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-a766293a-b721-48a7-acb7-8b24487ccba6 in namespace container-probe-9406 03/30/23 14:29:14.179
    Mar 30 14:29:14.183: INFO: Waiting up to 5m0s for pod "liveness-a766293a-b721-48a7-acb7-8b24487ccba6" in namespace "container-probe-9406" to be "not pending"
    Mar 30 14:29:14.184: INFO: Pod "liveness-a766293a-b721-48a7-acb7-8b24487ccba6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.277745ms
    Mar 30 14:29:16.187: INFO: Pod "liveness-a766293a-b721-48a7-acb7-8b24487ccba6": Phase="Running", Reason="", readiness=true. Elapsed: 2.003929744s
    Mar 30 14:29:16.187: INFO: Pod "liveness-a766293a-b721-48a7-acb7-8b24487ccba6" satisfied condition "not pending"
    Mar 30 14:29:16.187: INFO: Started pod liveness-a766293a-b721-48a7-acb7-8b24487ccba6 in namespace container-probe-9406
    STEP: checking the pod's current state and verifying that restartCount is present 03/30/23 14:29:16.187
    Mar 30 14:29:16.188: INFO: Initial restart count of pod liveness-a766293a-b721-48a7-acb7-8b24487ccba6 is 0
    STEP: deleting the pod 03/30/23 14:33:16.499
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:33:16.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9406" for this suite. 03/30/23 14:33:16.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:33:16.514
Mar 30 14:33:16.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 14:33:16.515
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:16.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:16.522
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 14:33:16.528
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:33:16.652
STEP: Deploying the webhook pod 03/30/23 14:33:16.656
STEP: Wait for the deployment to be ready 03/30/23 14:33:16.66
Mar 30 14:33:16.664: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/30/23 14:33:18.669
STEP: Verifying the service has paired with the endpoint 03/30/23 14:33:18.674
Mar 30 14:33:19.675: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 03/30/23 14:33:19.677
STEP: Creating a custom resource definition that should be denied by the webhook 03/30/23 14:33:19.686
Mar 30 14:33:19.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:33:19.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2745" for this suite. 03/30/23 14:33:19.713
STEP: Destroying namespace "webhook-2745-markers" for this suite. 03/30/23 14:33:19.716
------------------------------
â€¢ [3.204 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:33:16.514
    Mar 30 14:33:16.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 14:33:16.515
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:16.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:16.522
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 14:33:16.528
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:33:16.652
    STEP: Deploying the webhook pod 03/30/23 14:33:16.656
    STEP: Wait for the deployment to be ready 03/30/23 14:33:16.66
    Mar 30 14:33:16.664: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/30/23 14:33:18.669
    STEP: Verifying the service has paired with the endpoint 03/30/23 14:33:18.674
    Mar 30 14:33:19.675: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 03/30/23 14:33:19.677
    STEP: Creating a custom resource definition that should be denied by the webhook 03/30/23 14:33:19.686
    Mar 30 14:33:19.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:33:19.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2745" for this suite. 03/30/23 14:33:19.713
    STEP: Destroying namespace "webhook-2745-markers" for this suite. 03/30/23 14:33:19.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:33:19.72
Mar 30 14:33:19.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 14:33:19.72
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:19.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:19.728
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 14:33:19.735
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:33:20.164
STEP: Deploying the webhook pod 03/30/23 14:33:20.166
STEP: Wait for the deployment to be ready 03/30/23 14:33:20.172
Mar 30 14:33:20.175: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 14:33:22.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:33:24.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:33:26.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:33:28.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/30/23 14:33:30.182
STEP: Verifying the service has paired with the endpoint 03/30/23 14:33:30.186
Mar 30 14:33:31.187: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 03/30/23 14:33:31.189
STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/30/23 14:33:31.2
STEP: Creating a configMap that should not be mutated 03/30/23 14:33:31.203
STEP: Patching a mutating webhook configuration's rules to include the create operation 03/30/23 14:33:31.207
STEP: Creating a configMap that should be mutated 03/30/23 14:33:31.212
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:33:31.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-314" for this suite. 03/30/23 14:33:31.246
STEP: Destroying namespace "webhook-314-markers" for this suite. 03/30/23 14:33:31.249
------------------------------
â€¢ [SLOW TEST] [11.533 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:33:19.72
    Mar 30 14:33:19.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 14:33:19.72
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:19.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:19.728
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 14:33:19.735
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:33:20.164
    STEP: Deploying the webhook pod 03/30/23 14:33:20.166
    STEP: Wait for the deployment to be ready 03/30/23 14:33:20.172
    Mar 30 14:33:20.175: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 30 14:33:22.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:33:24.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:33:26.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:33:28.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 33, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/30/23 14:33:30.182
    STEP: Verifying the service has paired with the endpoint 03/30/23 14:33:30.186
    Mar 30 14:33:31.187: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 03/30/23 14:33:31.189
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/30/23 14:33:31.2
    STEP: Creating a configMap that should not be mutated 03/30/23 14:33:31.203
    STEP: Patching a mutating webhook configuration's rules to include the create operation 03/30/23 14:33:31.207
    STEP: Creating a configMap that should be mutated 03/30/23 14:33:31.212
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:33:31.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-314" for this suite. 03/30/23 14:33:31.246
    STEP: Destroying namespace "webhook-314-markers" for this suite. 03/30/23 14:33:31.249
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:33:31.253
Mar 30 14:33:31.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pods 03/30/23 14:33:31.254
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:31.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:31.261
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 03/30/23 14:33:31.263
Mar 30 14:33:31.267: INFO: created test-pod-1
Mar 30 14:33:31.269: INFO: created test-pod-2
Mar 30 14:33:31.273: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 03/30/23 14:33:31.273
Mar 30 14:33:31.273: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1517' to be running and ready
Mar 30 14:33:31.279: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 30 14:33:31.279: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 30 14:33:31.279: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 30 14:33:31.279: INFO: 0 / 3 pods in namespace 'pods-1517' are running and ready (0 seconds elapsed)
Mar 30 14:33:31.279: INFO: expected 0 pod replicas in namespace 'pods-1517', 0 are Running and Ready.
Mar 30 14:33:31.279: INFO: POD         NODE                     PHASE    GRACE  CONDITIONS
Mar 30 14:33:31.279: INFO: test-pod-1  cn-hongkong.192.168.0.5  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 14:33:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 14:33:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 14:33:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 14:33:31 +0000 UTC  }]
Mar 30 14:33:31.279: INFO: test-pod-2  cn-hongkong.192.168.0.5  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 14:33:31 +0000 UTC  }]
Mar 30 14:33:31.279: INFO: test-pod-3  cn-hongkong.192.168.0.5  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 14:33:31 +0000 UTC  }]
Mar 30 14:33:31.279: INFO: 
Mar 30 14:33:33.284: INFO: 3 / 3 pods in namespace 'pods-1517' are running and ready (2 seconds elapsed)
Mar 30 14:33:33.284: INFO: expected 0 pod replicas in namespace 'pods-1517', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 03/30/23 14:33:33.292
Mar 30 14:33:33.294: INFO: Pod quantity 3 is different from expected quantity 0
Mar 30 14:33:34.297: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 30 14:33:35.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1517" for this suite. 03/30/23 14:33:35.299
------------------------------
â€¢ [4.049 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:33:31.253
    Mar 30 14:33:31.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pods 03/30/23 14:33:31.254
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:31.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:31.261
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 03/30/23 14:33:31.263
    Mar 30 14:33:31.267: INFO: created test-pod-1
    Mar 30 14:33:31.269: INFO: created test-pod-2
    Mar 30 14:33:31.273: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 03/30/23 14:33:31.273
    Mar 30 14:33:31.273: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1517' to be running and ready
    Mar 30 14:33:31.279: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 30 14:33:31.279: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 30 14:33:31.279: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 30 14:33:31.279: INFO: 0 / 3 pods in namespace 'pods-1517' are running and ready (0 seconds elapsed)
    Mar 30 14:33:31.279: INFO: expected 0 pod replicas in namespace 'pods-1517', 0 are Running and Ready.
    Mar 30 14:33:31.279: INFO: POD         NODE                     PHASE    GRACE  CONDITIONS
    Mar 30 14:33:31.279: INFO: test-pod-1  cn-hongkong.192.168.0.5  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 14:33:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 14:33:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-30 14:33:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 14:33:31 +0000 UTC  }]
    Mar 30 14:33:31.279: INFO: test-pod-2  cn-hongkong.192.168.0.5  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 14:33:31 +0000 UTC  }]
    Mar 30 14:33:31.279: INFO: test-pod-3  cn-hongkong.192.168.0.5  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-30 14:33:31 +0000 UTC  }]
    Mar 30 14:33:31.279: INFO: 
    Mar 30 14:33:33.284: INFO: 3 / 3 pods in namespace 'pods-1517' are running and ready (2 seconds elapsed)
    Mar 30 14:33:33.284: INFO: expected 0 pod replicas in namespace 'pods-1517', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 03/30/23 14:33:33.292
    Mar 30 14:33:33.294: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 30 14:33:34.297: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:33:35.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1517" for this suite. 03/30/23 14:33:35.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:33:35.302
Mar 30 14:33:35.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pods 03/30/23 14:33:35.303
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:35.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:35.309
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Mar 30 14:33:35.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: creating the pod 03/30/23 14:33:35.311
STEP: submitting the pod to kubernetes 03/30/23 14:33:35.311
Mar 30 14:33:35.315: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-4fa31785-c158-4f6c-924d-9bb24690470d" in namespace "pods-9440" to be "running and ready"
Mar 30 14:33:35.316: INFO: Pod "pod-logs-websocket-4fa31785-c158-4f6c-924d-9bb24690470d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.432302ms
Mar 30 14:33:35.316: INFO: The phase of Pod pod-logs-websocket-4fa31785-c158-4f6c-924d-9bb24690470d is Pending, waiting for it to be Running (with Ready = true)
Mar 30 14:33:37.319: INFO: Pod "pod-logs-websocket-4fa31785-c158-4f6c-924d-9bb24690470d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003932928s
Mar 30 14:33:37.319: INFO: The phase of Pod pod-logs-websocket-4fa31785-c158-4f6c-924d-9bb24690470d is Running (Ready = true)
Mar 30 14:33:37.319: INFO: Pod "pod-logs-websocket-4fa31785-c158-4f6c-924d-9bb24690470d" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 30 14:33:37.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9440" for this suite. 03/30/23 14:33:37.333
------------------------------
â€¢ [2.033 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:33:35.302
    Mar 30 14:33:35.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pods 03/30/23 14:33:35.303
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:35.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:35.309
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Mar 30 14:33:35.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: creating the pod 03/30/23 14:33:35.311
    STEP: submitting the pod to kubernetes 03/30/23 14:33:35.311
    Mar 30 14:33:35.315: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-4fa31785-c158-4f6c-924d-9bb24690470d" in namespace "pods-9440" to be "running and ready"
    Mar 30 14:33:35.316: INFO: Pod "pod-logs-websocket-4fa31785-c158-4f6c-924d-9bb24690470d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.432302ms
    Mar 30 14:33:35.316: INFO: The phase of Pod pod-logs-websocket-4fa31785-c158-4f6c-924d-9bb24690470d is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 14:33:37.319: INFO: Pod "pod-logs-websocket-4fa31785-c158-4f6c-924d-9bb24690470d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003932928s
    Mar 30 14:33:37.319: INFO: The phase of Pod pod-logs-websocket-4fa31785-c158-4f6c-924d-9bb24690470d is Running (Ready = true)
    Mar 30 14:33:37.319: INFO: Pod "pod-logs-websocket-4fa31785-c158-4f6c-924d-9bb24690470d" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:33:37.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9440" for this suite. 03/30/23 14:33:37.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:33:37.336
Mar 30 14:33:37.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 14:33:37.337
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:37.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:37.343
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 14:33:37.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1658" for this suite. 03/30/23 14:33:37.362
------------------------------
â€¢ [0.028 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:33:37.336
    Mar 30 14:33:37.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 14:33:37.337
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:37.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:37.343
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:33:37.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1658" for this suite. 03/30/23 14:33:37.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:33:37.365
Mar 30 14:33:37.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename statefulset 03/30/23 14:33:37.366
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:37.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:37.373
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7410 03/30/23 14:33:37.374
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-7410 03/30/23 14:33:37.378
Mar 30 14:33:37.382: INFO: Found 0 stateful pods, waiting for 1
Mar 30 14:33:47.384: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 03/30/23 14:33:47.387
STEP: Getting /status 03/30/23 14:33:47.391
Mar 30 14:33:47.393: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 03/30/23 14:33:47.393
Mar 30 14:33:47.397: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 03/30/23 14:33:47.397
Mar 30 14:33:47.398: INFO: Observed &StatefulSet event: ADDED
Mar 30 14:33:47.398: INFO: Found Statefulset ss in namespace statefulset-7410 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 30 14:33:47.398: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 03/30/23 14:33:47.398
Mar 30 14:33:47.398: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 30 14:33:47.402: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 03/30/23 14:33:47.402
Mar 30 14:33:47.403: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 30 14:33:47.403: INFO: Deleting all statefulset in ns statefulset-7410
Mar 30 14:33:47.404: INFO: Scaling statefulset ss to 0
Mar 30 14:33:57.413: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 14:33:57.414: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 30 14:33:57.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7410" for this suite. 03/30/23 14:33:57.422
------------------------------
â€¢ [SLOW TEST] [20.060 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:33:37.365
    Mar 30 14:33:37.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename statefulset 03/30/23 14:33:37.366
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:37.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:37.373
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7410 03/30/23 14:33:37.374
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-7410 03/30/23 14:33:37.378
    Mar 30 14:33:37.382: INFO: Found 0 stateful pods, waiting for 1
    Mar 30 14:33:47.384: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 03/30/23 14:33:47.387
    STEP: Getting /status 03/30/23 14:33:47.391
    Mar 30 14:33:47.393: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 03/30/23 14:33:47.393
    Mar 30 14:33:47.397: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 03/30/23 14:33:47.397
    Mar 30 14:33:47.398: INFO: Observed &StatefulSet event: ADDED
    Mar 30 14:33:47.398: INFO: Found Statefulset ss in namespace statefulset-7410 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 30 14:33:47.398: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 03/30/23 14:33:47.398
    Mar 30 14:33:47.398: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 30 14:33:47.402: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 03/30/23 14:33:47.402
    Mar 30 14:33:47.403: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 30 14:33:47.403: INFO: Deleting all statefulset in ns statefulset-7410
    Mar 30 14:33:47.404: INFO: Scaling statefulset ss to 0
    Mar 30 14:33:57.413: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 30 14:33:57.414: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:33:57.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7410" for this suite. 03/30/23 14:33:57.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:33:57.426
Mar 30 14:33:57.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 14:33:57.426
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:57.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:57.433
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 03/30/23 14:33:57.436
STEP: watching for the Service to be added 03/30/23 14:33:57.44
Mar 30 14:33:57.441: INFO: Found Service test-service-ckvxj in namespace services-1603 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar 30 14:33:57.441: INFO: Service test-service-ckvxj created
STEP: Getting /status 03/30/23 14:33:57.441
Mar 30 14:33:57.443: INFO: Service test-service-ckvxj has LoadBalancer: {[]}
STEP: patching the ServiceStatus 03/30/23 14:33:57.443
STEP: watching for the Service to be patched 03/30/23 14:33:57.445
Mar 30 14:33:57.446: INFO: observed Service test-service-ckvxj in namespace services-1603 with annotations: map[] & LoadBalancer: {[]}
Mar 30 14:33:57.446: INFO: Found Service test-service-ckvxj in namespace services-1603 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar 30 14:33:57.446: INFO: Service test-service-ckvxj has service status patched
STEP: updating the ServiceStatus 03/30/23 14:33:57.446
Mar 30 14:33:57.450: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 03/30/23 14:33:57.45
Mar 30 14:33:57.451: INFO: Observed Service test-service-ckvxj in namespace services-1603 with annotations: map[] & Conditions: {[]}
Mar 30 14:33:57.451: INFO: Observed event: &Service{ObjectMeta:{test-service-ckvxj  services-1603  47a917dd-9b7b-4aeb-9570-77d6f978e81e 96297 0 2023-03-30 14:33:57 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-30 14:33:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-30 14:33:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.16.2.137,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.16.2.137],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar 30 14:33:57.451: INFO: Found Service test-service-ckvxj in namespace services-1603 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 30 14:33:57.451: INFO: Service test-service-ckvxj has service status updated
STEP: patching the service 03/30/23 14:33:57.451
STEP: watching for the Service to be patched 03/30/23 14:33:57.458
Mar 30 14:33:57.459: INFO: observed Service test-service-ckvxj in namespace services-1603 with labels: map[test-service-static:true]
Mar 30 14:33:57.459: INFO: observed Service test-service-ckvxj in namespace services-1603 with labels: map[test-service-static:true]
Mar 30 14:33:57.459: INFO: observed Service test-service-ckvxj in namespace services-1603 with labels: map[test-service-static:true]
Mar 30 14:33:57.459: INFO: Found Service test-service-ckvxj in namespace services-1603 with labels: map[test-service:patched test-service-static:true]
Mar 30 14:33:57.459: INFO: Service test-service-ckvxj patched
STEP: deleting the service 03/30/23 14:33:57.459
STEP: watching for the Service to be deleted 03/30/23 14:33:57.466
Mar 30 14:33:57.467: INFO: Observed event: ADDED
Mar 30 14:33:57.467: INFO: Observed event: MODIFIED
Mar 30 14:33:57.467: INFO: Observed event: MODIFIED
Mar 30 14:33:57.467: INFO: Observed event: MODIFIED
Mar 30 14:33:57.467: INFO: Found Service test-service-ckvxj in namespace services-1603 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar 30 14:33:57.467: INFO: Service test-service-ckvxj deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 14:33:57.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1603" for this suite. 03/30/23 14:33:57.47
------------------------------
â€¢ [0.047 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:33:57.426
    Mar 30 14:33:57.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 14:33:57.426
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:57.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:57.433
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 03/30/23 14:33:57.436
    STEP: watching for the Service to be added 03/30/23 14:33:57.44
    Mar 30 14:33:57.441: INFO: Found Service test-service-ckvxj in namespace services-1603 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Mar 30 14:33:57.441: INFO: Service test-service-ckvxj created
    STEP: Getting /status 03/30/23 14:33:57.441
    Mar 30 14:33:57.443: INFO: Service test-service-ckvxj has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 03/30/23 14:33:57.443
    STEP: watching for the Service to be patched 03/30/23 14:33:57.445
    Mar 30 14:33:57.446: INFO: observed Service test-service-ckvxj in namespace services-1603 with annotations: map[] & LoadBalancer: {[]}
    Mar 30 14:33:57.446: INFO: Found Service test-service-ckvxj in namespace services-1603 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Mar 30 14:33:57.446: INFO: Service test-service-ckvxj has service status patched
    STEP: updating the ServiceStatus 03/30/23 14:33:57.446
    Mar 30 14:33:57.450: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 03/30/23 14:33:57.45
    Mar 30 14:33:57.451: INFO: Observed Service test-service-ckvxj in namespace services-1603 with annotations: map[] & Conditions: {[]}
    Mar 30 14:33:57.451: INFO: Observed event: &Service{ObjectMeta:{test-service-ckvxj  services-1603  47a917dd-9b7b-4aeb-9570-77d6f978e81e 96297 0 2023-03-30 14:33:57 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-30 14:33:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-30 14:33:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.16.2.137,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.16.2.137],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Mar 30 14:33:57.451: INFO: Found Service test-service-ckvxj in namespace services-1603 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 30 14:33:57.451: INFO: Service test-service-ckvxj has service status updated
    STEP: patching the service 03/30/23 14:33:57.451
    STEP: watching for the Service to be patched 03/30/23 14:33:57.458
    Mar 30 14:33:57.459: INFO: observed Service test-service-ckvxj in namespace services-1603 with labels: map[test-service-static:true]
    Mar 30 14:33:57.459: INFO: observed Service test-service-ckvxj in namespace services-1603 with labels: map[test-service-static:true]
    Mar 30 14:33:57.459: INFO: observed Service test-service-ckvxj in namespace services-1603 with labels: map[test-service-static:true]
    Mar 30 14:33:57.459: INFO: Found Service test-service-ckvxj in namespace services-1603 with labels: map[test-service:patched test-service-static:true]
    Mar 30 14:33:57.459: INFO: Service test-service-ckvxj patched
    STEP: deleting the service 03/30/23 14:33:57.459
    STEP: watching for the Service to be deleted 03/30/23 14:33:57.466
    Mar 30 14:33:57.467: INFO: Observed event: ADDED
    Mar 30 14:33:57.467: INFO: Observed event: MODIFIED
    Mar 30 14:33:57.467: INFO: Observed event: MODIFIED
    Mar 30 14:33:57.467: INFO: Observed event: MODIFIED
    Mar 30 14:33:57.467: INFO: Found Service test-service-ckvxj in namespace services-1603 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Mar 30 14:33:57.467: INFO: Service test-service-ckvxj deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:33:57.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1603" for this suite. 03/30/23 14:33:57.47
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:33:57.473
Mar 30 14:33:57.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename gc 03/30/23 14:33:57.474
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:57.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:57.481
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 03/30/23 14:33:57.483
STEP: Wait for the Deployment to create new ReplicaSet 03/30/23 14:33:57.486
STEP: delete the deployment 03/30/23 14:33:57.99
STEP: wait for all rs to be garbage collected 03/30/23 14:33:57.992
STEP: expected 0 rs, got 1 rs 03/30/23 14:33:57.995
STEP: expected 0 pods, got 2 pods 03/30/23 14:33:57.997
STEP: Gathering metrics 03/30/23 14:33:58.502
Mar 30 14:33:58.517: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cn-hongkong.192.168.0.252" in namespace "kube-system" to be "running and ready"
Mar 30 14:33:58.518: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252": Phase="Running", Reason="", readiness=true. Elapsed: 1.817942ms
Mar 30 14:33:58.518: INFO: The phase of Pod kube-controller-manager-cn-hongkong.192.168.0.252 is Running (Ready = true)
Mar 30 14:33:58.518: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252" satisfied condition "running and ready"
Mar 30 14:33:58.556: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 30 14:33:58.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5055" for this suite. 03/30/23 14:33:58.558
------------------------------
â€¢ [1.089 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:33:57.473
    Mar 30 14:33:57.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename gc 03/30/23 14:33:57.474
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:57.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:57.481
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 03/30/23 14:33:57.483
    STEP: Wait for the Deployment to create new ReplicaSet 03/30/23 14:33:57.486
    STEP: delete the deployment 03/30/23 14:33:57.99
    STEP: wait for all rs to be garbage collected 03/30/23 14:33:57.992
    STEP: expected 0 rs, got 1 rs 03/30/23 14:33:57.995
    STEP: expected 0 pods, got 2 pods 03/30/23 14:33:57.997
    STEP: Gathering metrics 03/30/23 14:33:58.502
    Mar 30 14:33:58.517: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cn-hongkong.192.168.0.252" in namespace "kube-system" to be "running and ready"
    Mar 30 14:33:58.518: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252": Phase="Running", Reason="", readiness=true. Elapsed: 1.817942ms
    Mar 30 14:33:58.518: INFO: The phase of Pod kube-controller-manager-cn-hongkong.192.168.0.252 is Running (Ready = true)
    Mar 30 14:33:58.518: INFO: Pod "kube-controller-manager-cn-hongkong.192.168.0.252" satisfied condition "running and ready"
    Mar 30 14:33:58.556: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:33:58.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5055" for this suite. 03/30/23 14:33:58.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:33:58.563
Mar 30 14:33:58.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir-wrapper 03/30/23 14:33:58.564
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:58.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:58.57
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 03/30/23 14:33:58.571
STEP: Creating RC which spawns configmap-volume pods 03/30/23 14:33:58.817
Mar 30 14:33:58.919: INFO: Pod name wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/30/23 14:33:58.919
Mar 30 14:33:58.919: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:33:58.966: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 47.199896ms
Mar 30 14:34:00.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050336727s
Mar 30 14:34:02.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05050915s
Mar 30 14:34:04.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050300874s
Mar 30 14:34:06.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.049822928s
Mar 30 14:34:08.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050360831s
Mar 30 14:34:10.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 12.050321352s
Mar 30 14:34:12.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Running", Reason="", readiness=true. Elapsed: 14.050564727s
Mar 30 14:34:12.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv" satisfied condition "running"
Mar 30 14:34:12.969: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8vwxp" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:12.971: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8vwxp": Phase="Running", Reason="", readiness=true. Elapsed: 1.871945ms
Mar 30 14:34:12.971: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8vwxp" satisfied condition "running"
Mar 30 14:34:12.971: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-jbss9" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:12.973: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-jbss9": Phase="Running", Reason="", readiness=true. Elapsed: 1.623835ms
Mar 30 14:34:12.973: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-jbss9" satisfied condition "running"
Mar 30 14:34:12.973: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-njt5w" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:12.975: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-njt5w": Phase="Running", Reason="", readiness=true. Elapsed: 1.766422ms
Mar 30 14:34:12.975: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-njt5w" satisfied condition "running"
Mar 30 14:34:12.975: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-pf272" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:12.976: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-pf272": Phase="Running", Reason="", readiness=true. Elapsed: 1.680414ms
Mar 30 14:34:12.976: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-pf272" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe in namespace emptydir-wrapper-9380, will wait for the garbage collector to delete the pods 03/30/23 14:34:12.976
Mar 30 14:34:13.032: INFO: Deleting ReplicationController wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe took: 3.268182ms
Mar 30 14:34:13.133: INFO: Terminating ReplicationController wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe pods took: 100.792087ms
STEP: Creating RC which spawns configmap-volume pods 03/30/23 14:34:17.136
Mar 30 14:34:17.142: INFO: Pod name wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b: Found 0 pods out of 5
Mar 30 14:34:22.146: INFO: Pod name wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/30/23 14:34:22.146
Mar 30 14:34:22.146: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:22.148: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq": Phase="Pending", Reason="", readiness=false. Elapsed: 1.980271ms
Mar 30 14:34:24.151: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005098926s
Mar 30 14:34:26.152: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005739665s
Mar 30 14:34:28.151: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004862602s
Mar 30 14:34:30.151: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004811425s
Mar 30 14:34:32.151: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq": Phase="Running", Reason="", readiness=true. Elapsed: 10.004875591s
Mar 30 14:34:32.151: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq" satisfied condition "running"
Mar 30 14:34:32.151: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-9st6w" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:32.153: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-9st6w": Phase="Running", Reason="", readiness=true. Elapsed: 1.763758ms
Mar 30 14:34:32.153: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-9st6w" satisfied condition "running"
Mar 30 14:34:32.153: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-b55qb" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:32.155: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-b55qb": Phase="Running", Reason="", readiness=true. Elapsed: 1.640997ms
Mar 30 14:34:32.155: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-b55qb" satisfied condition "running"
Mar 30 14:34:32.155: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-bhm65" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:32.156: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-bhm65": Phase="Pending", Reason="", readiness=false. Elapsed: 1.698111ms
Mar 30 14:34:34.160: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-bhm65": Phase="Running", Reason="", readiness=true. Elapsed: 2.004841625s
Mar 30 14:34:34.160: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-bhm65" satisfied condition "running"
Mar 30 14:34:34.160: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-dc7t5" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:34.161: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-dc7t5": Phase="Running", Reason="", readiness=true. Elapsed: 1.84647ms
Mar 30 14:34:34.161: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-dc7t5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b in namespace emptydir-wrapper-9380, will wait for the garbage collector to delete the pods 03/30/23 14:34:34.161
Mar 30 14:34:34.217: INFO: Deleting ReplicationController wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b took: 2.817122ms
Mar 30 14:34:34.318: INFO: Terminating ReplicationController wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b pods took: 100.709094ms
STEP: Creating RC which spawns configmap-volume pods 03/30/23 14:34:37.121
Mar 30 14:34:37.130: INFO: Pod name wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872: Found 0 pods out of 5
Mar 30 14:34:42.134: INFO: Pod name wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/30/23 14:34:42.134
Mar 30 14:34:42.134: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:42.138: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052169ms
Mar 30 14:34:44.140: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006262021s
Mar 30 14:34:46.142: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007899064s
Mar 30 14:34:48.141: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006954428s
Mar 30 14:34:50.141: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007248161s
Mar 30 14:34:52.141: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f": Phase="Running", Reason="", readiness=true. Elapsed: 10.007022852s
Mar 30 14:34:52.141: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f" satisfied condition "running"
Mar 30 14:34:52.141: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-jlkhg" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:52.143: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-jlkhg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.782275ms
Mar 30 14:34:54.145: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-jlkhg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004272146s
Mar 30 14:34:54.145: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-jlkhg" satisfied condition "running"
Mar 30 14:34:54.145: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-m757l" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:54.147: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-m757l": Phase="Running", Reason="", readiness=true. Elapsed: 1.864055ms
Mar 30 14:34:54.147: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-m757l" satisfied condition "running"
Mar 30 14:34:54.147: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-qj49p" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:54.149: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-qj49p": Phase="Running", Reason="", readiness=true. Elapsed: 1.71596ms
Mar 30 14:34:54.149: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-qj49p" satisfied condition "running"
Mar 30 14:34:54.149: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-zf8wb" in namespace "emptydir-wrapper-9380" to be "running"
Mar 30 14:34:54.151: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-zf8wb": Phase="Running", Reason="", readiness=true. Elapsed: 1.677924ms
Mar 30 14:34:54.151: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-zf8wb" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872 in namespace emptydir-wrapper-9380, will wait for the garbage collector to delete the pods 03/30/23 14:34:54.151
Mar 30 14:34:54.206: INFO: Deleting ReplicationController wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872 took: 2.973901ms
Mar 30 14:34:54.306: INFO: Terminating ReplicationController wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872 pods took: 100.548731ms
STEP: Cleaning up the configMaps 03/30/23 14:34:56.908
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 14:34:57.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-9380" for this suite. 03/30/23 14:34:57.017
------------------------------
â€¢ [SLOW TEST] [58.457 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:33:58.563
    Mar 30 14:33:58.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir-wrapper 03/30/23 14:33:58.564
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:33:58.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:33:58.57
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 03/30/23 14:33:58.571
    STEP: Creating RC which spawns configmap-volume pods 03/30/23 14:33:58.817
    Mar 30 14:33:58.919: INFO: Pod name wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/30/23 14:33:58.919
    Mar 30 14:33:58.919: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:33:58.966: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 47.199896ms
    Mar 30 14:34:00.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050336727s
    Mar 30 14:34:02.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05050915s
    Mar 30 14:34:04.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050300874s
    Mar 30 14:34:06.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.049822928s
    Mar 30 14:34:08.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050360831s
    Mar 30 14:34:10.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Pending", Reason="", readiness=false. Elapsed: 12.050321352s
    Mar 30 14:34:12.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv": Phase="Running", Reason="", readiness=true. Elapsed: 14.050564727s
    Mar 30 14:34:12.969: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8jljv" satisfied condition "running"
    Mar 30 14:34:12.969: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8vwxp" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:12.971: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8vwxp": Phase="Running", Reason="", readiness=true. Elapsed: 1.871945ms
    Mar 30 14:34:12.971: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-8vwxp" satisfied condition "running"
    Mar 30 14:34:12.971: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-jbss9" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:12.973: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-jbss9": Phase="Running", Reason="", readiness=true. Elapsed: 1.623835ms
    Mar 30 14:34:12.973: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-jbss9" satisfied condition "running"
    Mar 30 14:34:12.973: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-njt5w" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:12.975: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-njt5w": Phase="Running", Reason="", readiness=true. Elapsed: 1.766422ms
    Mar 30 14:34:12.975: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-njt5w" satisfied condition "running"
    Mar 30 14:34:12.975: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-pf272" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:12.976: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-pf272": Phase="Running", Reason="", readiness=true. Elapsed: 1.680414ms
    Mar 30 14:34:12.976: INFO: Pod "wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe-pf272" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe in namespace emptydir-wrapper-9380, will wait for the garbage collector to delete the pods 03/30/23 14:34:12.976
    Mar 30 14:34:13.032: INFO: Deleting ReplicationController wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe took: 3.268182ms
    Mar 30 14:34:13.133: INFO: Terminating ReplicationController wrapped-volume-race-5a26d4f0-2f43-46ee-bc77-5c4e329cc6fe pods took: 100.792087ms
    STEP: Creating RC which spawns configmap-volume pods 03/30/23 14:34:17.136
    Mar 30 14:34:17.142: INFO: Pod name wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b: Found 0 pods out of 5
    Mar 30 14:34:22.146: INFO: Pod name wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/30/23 14:34:22.146
    Mar 30 14:34:22.146: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:22.148: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq": Phase="Pending", Reason="", readiness=false. Elapsed: 1.980271ms
    Mar 30 14:34:24.151: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005098926s
    Mar 30 14:34:26.152: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005739665s
    Mar 30 14:34:28.151: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004862602s
    Mar 30 14:34:30.151: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004811425s
    Mar 30 14:34:32.151: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq": Phase="Running", Reason="", readiness=true. Elapsed: 10.004875591s
    Mar 30 14:34:32.151: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-5w7hq" satisfied condition "running"
    Mar 30 14:34:32.151: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-9st6w" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:32.153: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-9st6w": Phase="Running", Reason="", readiness=true. Elapsed: 1.763758ms
    Mar 30 14:34:32.153: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-9st6w" satisfied condition "running"
    Mar 30 14:34:32.153: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-b55qb" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:32.155: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-b55qb": Phase="Running", Reason="", readiness=true. Elapsed: 1.640997ms
    Mar 30 14:34:32.155: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-b55qb" satisfied condition "running"
    Mar 30 14:34:32.155: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-bhm65" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:32.156: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-bhm65": Phase="Pending", Reason="", readiness=false. Elapsed: 1.698111ms
    Mar 30 14:34:34.160: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-bhm65": Phase="Running", Reason="", readiness=true. Elapsed: 2.004841625s
    Mar 30 14:34:34.160: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-bhm65" satisfied condition "running"
    Mar 30 14:34:34.160: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-dc7t5" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:34.161: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-dc7t5": Phase="Running", Reason="", readiness=true. Elapsed: 1.84647ms
    Mar 30 14:34:34.161: INFO: Pod "wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b-dc7t5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b in namespace emptydir-wrapper-9380, will wait for the garbage collector to delete the pods 03/30/23 14:34:34.161
    Mar 30 14:34:34.217: INFO: Deleting ReplicationController wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b took: 2.817122ms
    Mar 30 14:34:34.318: INFO: Terminating ReplicationController wrapped-volume-race-38bf359d-3bdb-440f-a57d-2561a9e0637b pods took: 100.709094ms
    STEP: Creating RC which spawns configmap-volume pods 03/30/23 14:34:37.121
    Mar 30 14:34:37.130: INFO: Pod name wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872: Found 0 pods out of 5
    Mar 30 14:34:42.134: INFO: Pod name wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/30/23 14:34:42.134
    Mar 30 14:34:42.134: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:42.138: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052169ms
    Mar 30 14:34:44.140: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006262021s
    Mar 30 14:34:46.142: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007899064s
    Mar 30 14:34:48.141: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006954428s
    Mar 30 14:34:50.141: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007248161s
    Mar 30 14:34:52.141: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f": Phase="Running", Reason="", readiness=true. Elapsed: 10.007022852s
    Mar 30 14:34:52.141: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-2nx6f" satisfied condition "running"
    Mar 30 14:34:52.141: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-jlkhg" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:52.143: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-jlkhg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.782275ms
    Mar 30 14:34:54.145: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-jlkhg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004272146s
    Mar 30 14:34:54.145: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-jlkhg" satisfied condition "running"
    Mar 30 14:34:54.145: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-m757l" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:54.147: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-m757l": Phase="Running", Reason="", readiness=true. Elapsed: 1.864055ms
    Mar 30 14:34:54.147: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-m757l" satisfied condition "running"
    Mar 30 14:34:54.147: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-qj49p" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:54.149: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-qj49p": Phase="Running", Reason="", readiness=true. Elapsed: 1.71596ms
    Mar 30 14:34:54.149: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-qj49p" satisfied condition "running"
    Mar 30 14:34:54.149: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-zf8wb" in namespace "emptydir-wrapper-9380" to be "running"
    Mar 30 14:34:54.151: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-zf8wb": Phase="Running", Reason="", readiness=true. Elapsed: 1.677924ms
    Mar 30 14:34:54.151: INFO: Pod "wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872-zf8wb" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872 in namespace emptydir-wrapper-9380, will wait for the garbage collector to delete the pods 03/30/23 14:34:54.151
    Mar 30 14:34:54.206: INFO: Deleting ReplicationController wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872 took: 2.973901ms
    Mar 30 14:34:54.306: INFO: Terminating ReplicationController wrapped-volume-race-bc5cd5a0-d4c4-4379-993f-48388ab96872 pods took: 100.548731ms
    STEP: Cleaning up the configMaps 03/30/23 14:34:56.908
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:34:57.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-9380" for this suite. 03/30/23 14:34:57.017
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:34:57.02
Mar 30 14:34:57.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 14:34:57.02
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:34:57.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:34:57.027
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Mar 30 14:34:57.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/30/23 14:35:03.665
Mar 30 14:35:03.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-1469 --namespace=crd-publish-openapi-1469 create -f -'
Mar 30 14:35:04.093: INFO: stderr: ""
Mar 30 14:35:04.093: INFO: stdout: "e2e-test-crd-publish-openapi-2466-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 30 14:35:04.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-1469 --namespace=crd-publish-openapi-1469 delete e2e-test-crd-publish-openapi-2466-crds test-cr'
Mar 30 14:35:04.146: INFO: stderr: ""
Mar 30 14:35:04.146: INFO: stdout: "e2e-test-crd-publish-openapi-2466-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 30 14:35:04.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-1469 --namespace=crd-publish-openapi-1469 apply -f -'
Mar 30 14:35:04.546: INFO: stderr: ""
Mar 30 14:35:04.546: INFO: stdout: "e2e-test-crd-publish-openapi-2466-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 30 14:35:04.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-1469 --namespace=crd-publish-openapi-1469 delete e2e-test-crd-publish-openapi-2466-crds test-cr'
Mar 30 14:35:04.599: INFO: stderr: ""
Mar 30 14:35:04.599: INFO: stdout: "e2e-test-crd-publish-openapi-2466-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/30/23 14:35:04.599
Mar 30 14:35:04.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-1469 explain e2e-test-crd-publish-openapi-2466-crds'
Mar 30 14:35:04.961: INFO: stderr: ""
Mar 30 14:35:04.961: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2466-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:35:06.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1469" for this suite. 03/30/23 14:35:06.583
------------------------------
â€¢ [SLOW TEST] [9.566 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:34:57.02
    Mar 30 14:34:57.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 14:34:57.02
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:34:57.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:34:57.027
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Mar 30 14:34:57.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/30/23 14:35:03.665
    Mar 30 14:35:03.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-1469 --namespace=crd-publish-openapi-1469 create -f -'
    Mar 30 14:35:04.093: INFO: stderr: ""
    Mar 30 14:35:04.093: INFO: stdout: "e2e-test-crd-publish-openapi-2466-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 30 14:35:04.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-1469 --namespace=crd-publish-openapi-1469 delete e2e-test-crd-publish-openapi-2466-crds test-cr'
    Mar 30 14:35:04.146: INFO: stderr: ""
    Mar 30 14:35:04.146: INFO: stdout: "e2e-test-crd-publish-openapi-2466-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Mar 30 14:35:04.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-1469 --namespace=crd-publish-openapi-1469 apply -f -'
    Mar 30 14:35:04.546: INFO: stderr: ""
    Mar 30 14:35:04.546: INFO: stdout: "e2e-test-crd-publish-openapi-2466-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 30 14:35:04.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-1469 --namespace=crd-publish-openapi-1469 delete e2e-test-crd-publish-openapi-2466-crds test-cr'
    Mar 30 14:35:04.599: INFO: stderr: ""
    Mar 30 14:35:04.599: INFO: stdout: "e2e-test-crd-publish-openapi-2466-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/30/23 14:35:04.599
    Mar 30 14:35:04.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=crd-publish-openapi-1469 explain e2e-test-crd-publish-openapi-2466-crds'
    Mar 30 14:35:04.961: INFO: stderr: ""
    Mar 30 14:35:04.961: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2466-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:35:06.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1469" for this suite. 03/30/23 14:35:06.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:35:06.587
Mar 30 14:35:06.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename statefulset 03/30/23 14:35:06.587
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:35:06.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:35:06.596
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4665 03/30/23 14:35:06.597
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 03/30/23 14:35:06.6
Mar 30 14:35:06.604: INFO: Found 0 stateful pods, waiting for 3
Mar 30 14:35:16.607: INFO: Found 2 stateful pods, waiting for 3
Mar 30 14:35:26.606: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 14:35:26.606: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 14:35:26.606: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/30/23 14:35:26.611
Mar 30 14:35:26.626: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/30/23 14:35:26.626
STEP: Not applying an update when the partition is greater than the number of replicas 03/30/23 14:35:36.633
STEP: Performing a canary update 03/30/23 14:35:36.633
Mar 30 14:35:36.648: INFO: Updating stateful set ss2
Mar 30 14:35:36.651: INFO: Waiting for Pod statefulset-4665/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 03/30/23 14:35:46.655
Mar 30 14:35:46.670: INFO: Found 2 stateful pods, waiting for 3
Mar 30 14:35:56.673: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 14:35:56.673: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 14:35:56.673: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 03/30/23 14:35:56.676
Mar 30 14:35:56.691: INFO: Updating stateful set ss2
Mar 30 14:35:56.694: INFO: Waiting for Pod statefulset-4665/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Mar 30 14:36:06.714: INFO: Updating stateful set ss2
Mar 30 14:36:06.718: INFO: Waiting for StatefulSet statefulset-4665/ss2 to complete update
Mar 30 14:36:06.718: INFO: Waiting for Pod statefulset-4665/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 30 14:36:16.722: INFO: Deleting all statefulset in ns statefulset-4665
Mar 30 14:36:16.724: INFO: Scaling statefulset ss2 to 0
Mar 30 14:36:26.732: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 14:36:26.734: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 30 14:36:26.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4665" for this suite. 03/30/23 14:36:26.743
------------------------------
â€¢ [SLOW TEST] [80.159 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:35:06.587
    Mar 30 14:35:06.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename statefulset 03/30/23 14:35:06.587
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:35:06.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:35:06.596
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4665 03/30/23 14:35:06.597
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 03/30/23 14:35:06.6
    Mar 30 14:35:06.604: INFO: Found 0 stateful pods, waiting for 3
    Mar 30 14:35:16.607: INFO: Found 2 stateful pods, waiting for 3
    Mar 30 14:35:26.606: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 30 14:35:26.606: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 30 14:35:26.606: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/30/23 14:35:26.611
    Mar 30 14:35:26.626: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/30/23 14:35:26.626
    STEP: Not applying an update when the partition is greater than the number of replicas 03/30/23 14:35:36.633
    STEP: Performing a canary update 03/30/23 14:35:36.633
    Mar 30 14:35:36.648: INFO: Updating stateful set ss2
    Mar 30 14:35:36.651: INFO: Waiting for Pod statefulset-4665/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 03/30/23 14:35:46.655
    Mar 30 14:35:46.670: INFO: Found 2 stateful pods, waiting for 3
    Mar 30 14:35:56.673: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 30 14:35:56.673: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 30 14:35:56.673: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 03/30/23 14:35:56.676
    Mar 30 14:35:56.691: INFO: Updating stateful set ss2
    Mar 30 14:35:56.694: INFO: Waiting for Pod statefulset-4665/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Mar 30 14:36:06.714: INFO: Updating stateful set ss2
    Mar 30 14:36:06.718: INFO: Waiting for StatefulSet statefulset-4665/ss2 to complete update
    Mar 30 14:36:06.718: INFO: Waiting for Pod statefulset-4665/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 30 14:36:16.722: INFO: Deleting all statefulset in ns statefulset-4665
    Mar 30 14:36:16.724: INFO: Scaling statefulset ss2 to 0
    Mar 30 14:36:26.732: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 30 14:36:26.734: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:36:26.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4665" for this suite. 03/30/23 14:36:26.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:36:26.746
Mar 30 14:36:26.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 14:36:26.747
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:36:26.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:36:26.755
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-9444/secret-test-7f020452-81cc-4064-8f15-c86e564947c9 03/30/23 14:36:26.757
STEP: Creating a pod to test consume secrets 03/30/23 14:36:26.759
Mar 30 14:36:26.762: INFO: Waiting up to 5m0s for pod "pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0" in namespace "secrets-9444" to be "Succeeded or Failed"
Mar 30 14:36:26.764: INFO: Pod "pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.503534ms
Mar 30 14:36:28.766: INFO: Pod "pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0": Phase="Running", Reason="", readiness=false. Elapsed: 2.003868976s
Mar 30 14:36:30.767: INFO: Pod "pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004400574s
STEP: Saw pod success 03/30/23 14:36:30.767
Mar 30 14:36:30.767: INFO: Pod "pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0" satisfied condition "Succeeded or Failed"
Mar 30 14:36:30.768: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0 container env-test: <nil>
STEP: delete the pod 03/30/23 14:36:30.779
Mar 30 14:36:30.784: INFO: Waiting for pod pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0 to disappear
Mar 30 14:36:30.785: INFO: Pod pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 14:36:30.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9444" for this suite. 03/30/23 14:36:30.788
------------------------------
â€¢ [4.044 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:36:26.746
    Mar 30 14:36:26.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 14:36:26.747
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:36:26.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:36:26.755
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-9444/secret-test-7f020452-81cc-4064-8f15-c86e564947c9 03/30/23 14:36:26.757
    STEP: Creating a pod to test consume secrets 03/30/23 14:36:26.759
    Mar 30 14:36:26.762: INFO: Waiting up to 5m0s for pod "pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0" in namespace "secrets-9444" to be "Succeeded or Failed"
    Mar 30 14:36:26.764: INFO: Pod "pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.503534ms
    Mar 30 14:36:28.766: INFO: Pod "pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0": Phase="Running", Reason="", readiness=false. Elapsed: 2.003868976s
    Mar 30 14:36:30.767: INFO: Pod "pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004400574s
    STEP: Saw pod success 03/30/23 14:36:30.767
    Mar 30 14:36:30.767: INFO: Pod "pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0" satisfied condition "Succeeded or Failed"
    Mar 30 14:36:30.768: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0 container env-test: <nil>
    STEP: delete the pod 03/30/23 14:36:30.779
    Mar 30 14:36:30.784: INFO: Waiting for pod pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0 to disappear
    Mar 30 14:36:30.785: INFO: Pod pod-configmaps-d98dd913-a2f3-4a8a-a4f9-3654221952d0 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:36:30.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9444" for this suite. 03/30/23 14:36:30.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:36:30.791
Mar 30 14:36:30.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 14:36:30.792
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:36:30.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:36:30.8
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 03/30/23 14:36:30.801
Mar 30 14:36:30.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: mark a version not serverd 03/30/23 14:36:39.419
STEP: check the unserved version gets removed 03/30/23 14:36:39.429
STEP: check the other version is not changed 03/30/23 14:36:41.027
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:36:43.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6221" for this suite. 03/30/23 14:36:43.953
------------------------------
â€¢ [SLOW TEST] [13.164 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:36:30.791
    Mar 30 14:36:30.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 14:36:30.792
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:36:30.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:36:30.8
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 03/30/23 14:36:30.801
    Mar 30 14:36:30.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: mark a version not serverd 03/30/23 14:36:39.419
    STEP: check the unserved version gets removed 03/30/23 14:36:39.429
    STEP: check the other version is not changed 03/30/23 14:36:41.027
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:36:43.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6221" for this suite. 03/30/23 14:36:43.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:36:43.956
Mar 30 14:36:43.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename namespaces 03/30/23 14:36:43.957
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:36:43.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:36:43.965
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-qghxs" 03/30/23 14:36:43.966
Mar 30 14:36:43.972: INFO: Namespace "e2e-ns-qghxs-9344" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-qghxs-9344" 03/30/23 14:36:43.972
Mar 30 14:36:43.976: INFO: Namespace "e2e-ns-qghxs-9344" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-qghxs-9344" 03/30/23 14:36:43.976
Mar 30 14:36:43.979: INFO: Namespace "e2e-ns-qghxs-9344" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:36:43.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4797" for this suite. 03/30/23 14:36:43.982
STEP: Destroying namespace "e2e-ns-qghxs-9344" for this suite. 03/30/23 14:36:43.984
------------------------------
â€¢ [0.030 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:36:43.956
    Mar 30 14:36:43.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename namespaces 03/30/23 14:36:43.957
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:36:43.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:36:43.965
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-qghxs" 03/30/23 14:36:43.966
    Mar 30 14:36:43.972: INFO: Namespace "e2e-ns-qghxs-9344" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-qghxs-9344" 03/30/23 14:36:43.972
    Mar 30 14:36:43.976: INFO: Namespace "e2e-ns-qghxs-9344" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-qghxs-9344" 03/30/23 14:36:43.976
    Mar 30 14:36:43.979: INFO: Namespace "e2e-ns-qghxs-9344" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:36:43.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4797" for this suite. 03/30/23 14:36:43.982
    STEP: Destroying namespace "e2e-ns-qghxs-9344" for this suite. 03/30/23 14:36:43.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:36:43.987
Mar 30 14:36:43.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 14:36:43.988
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:36:43.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:36:44.001
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 03/30/23 14:36:44.003
Mar 30 14:36:44.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 create -f -'
Mar 30 14:36:44.400: INFO: stderr: ""
Mar 30 14:36:44.400: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/30/23 14:36:44.4
Mar 30 14:36:44.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 30 14:36:44.455: INFO: stderr: ""
Mar 30 14:36:44.455: INFO: stdout: "update-demo-nautilus-2hp7n update-demo-nautilus-lv2jt "
Mar 30 14:36:44.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods update-demo-nautilus-2hp7n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 30 14:36:44.506: INFO: stderr: ""
Mar 30 14:36:44.506: INFO: stdout: ""
Mar 30 14:36:44.506: INFO: update-demo-nautilus-2hp7n is created but not running
Mar 30 14:36:49.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 30 14:36:49.560: INFO: stderr: ""
Mar 30 14:36:49.560: INFO: stdout: "update-demo-nautilus-2hp7n update-demo-nautilus-lv2jt "
Mar 30 14:36:49.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods update-demo-nautilus-2hp7n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 30 14:36:49.611: INFO: stderr: ""
Mar 30 14:36:49.611: INFO: stdout: "true"
Mar 30 14:36:49.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods update-demo-nautilus-2hp7n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 30 14:36:49.661: INFO: stderr: ""
Mar 30 14:36:49.661: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 30 14:36:49.661: INFO: validating pod update-demo-nautilus-2hp7n
Mar 30 14:37:04.930: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 14:37:04.930: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 14:37:04.930: INFO: update-demo-nautilus-2hp7n is verified up and running
Mar 30 14:37:04.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods update-demo-nautilus-lv2jt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 30 14:37:04.979: INFO: stderr: ""
Mar 30 14:37:04.979: INFO: stdout: "true"
Mar 30 14:37:04.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods update-demo-nautilus-lv2jt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 30 14:37:05.026: INFO: stderr: ""
Mar 30 14:37:05.026: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 30 14:37:05.026: INFO: validating pod update-demo-nautilus-lv2jt
Mar 30 14:37:05.030: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 30 14:37:05.030: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 30 14:37:05.030: INFO: update-demo-nautilus-lv2jt is verified up and running
STEP: using delete to clean up resources 03/30/23 14:37:05.03
Mar 30 14:37:05.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 delete --grace-period=0 --force -f -'
Mar 30 14:37:05.081: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 30 14:37:05.081: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 30 14:37:05.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get rc,svc -l name=update-demo --no-headers'
Mar 30 14:37:05.137: INFO: stderr: "No resources found in kubectl-9771 namespace.\n"
Mar 30 14:37:05.137: INFO: stdout: ""
Mar 30 14:37:05.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 30 14:37:05.192: INFO: stderr: ""
Mar 30 14:37:05.192: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 14:37:05.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9771" for this suite. 03/30/23 14:37:05.195
------------------------------
â€¢ [SLOW TEST] [21.210 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:36:43.987
    Mar 30 14:36:43.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 14:36:43.988
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:36:43.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:36:44.001
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 03/30/23 14:36:44.003
    Mar 30 14:36:44.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 create -f -'
    Mar 30 14:36:44.400: INFO: stderr: ""
    Mar 30 14:36:44.400: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/30/23 14:36:44.4
    Mar 30 14:36:44.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 30 14:36:44.455: INFO: stderr: ""
    Mar 30 14:36:44.455: INFO: stdout: "update-demo-nautilus-2hp7n update-demo-nautilus-lv2jt "
    Mar 30 14:36:44.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods update-demo-nautilus-2hp7n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 30 14:36:44.506: INFO: stderr: ""
    Mar 30 14:36:44.506: INFO: stdout: ""
    Mar 30 14:36:44.506: INFO: update-demo-nautilus-2hp7n is created but not running
    Mar 30 14:36:49.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 30 14:36:49.560: INFO: stderr: ""
    Mar 30 14:36:49.560: INFO: stdout: "update-demo-nautilus-2hp7n update-demo-nautilus-lv2jt "
    Mar 30 14:36:49.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods update-demo-nautilus-2hp7n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 30 14:36:49.611: INFO: stderr: ""
    Mar 30 14:36:49.611: INFO: stdout: "true"
    Mar 30 14:36:49.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods update-demo-nautilus-2hp7n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 30 14:36:49.661: INFO: stderr: ""
    Mar 30 14:36:49.661: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 30 14:36:49.661: INFO: validating pod update-demo-nautilus-2hp7n
    Mar 30 14:37:04.930: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 30 14:37:04.930: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 30 14:37:04.930: INFO: update-demo-nautilus-2hp7n is verified up and running
    Mar 30 14:37:04.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods update-demo-nautilus-lv2jt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 30 14:37:04.979: INFO: stderr: ""
    Mar 30 14:37:04.979: INFO: stdout: "true"
    Mar 30 14:37:04.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods update-demo-nautilus-lv2jt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 30 14:37:05.026: INFO: stderr: ""
    Mar 30 14:37:05.026: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 30 14:37:05.026: INFO: validating pod update-demo-nautilus-lv2jt
    Mar 30 14:37:05.030: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 30 14:37:05.030: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 30 14:37:05.030: INFO: update-demo-nautilus-lv2jt is verified up and running
    STEP: using delete to clean up resources 03/30/23 14:37:05.03
    Mar 30 14:37:05.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 delete --grace-period=0 --force -f -'
    Mar 30 14:37:05.081: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 30 14:37:05.081: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 30 14:37:05.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get rc,svc -l name=update-demo --no-headers'
    Mar 30 14:37:05.137: INFO: stderr: "No resources found in kubectl-9771 namespace.\n"
    Mar 30 14:37:05.137: INFO: stdout: ""
    Mar 30 14:37:05.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-9771 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 30 14:37:05.192: INFO: stderr: ""
    Mar 30 14:37:05.192: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:37:05.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9771" for this suite. 03/30/23 14:37:05.195
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:37:05.198
Mar 30 14:37:05.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename sysctl 03/30/23 14:37:05.198
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:37:05.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:37:05.206
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/30/23 14:37:05.208
STEP: Watching for error events or started pod 03/30/23 14:37:05.211
STEP: Waiting for pod completion 03/30/23 14:37:07.214
Mar 30 14:37:07.214: INFO: Waiting up to 3m0s for pod "sysctl-5255ecf1-e1a0-4aaa-829a-f484c256bd15" in namespace "sysctl-6935" to be "completed"
Mar 30 14:37:07.216: INFO: Pod "sysctl-5255ecf1-e1a0-4aaa-829a-f484c256bd15": Phase="Pending", Reason="", readiness=false. Elapsed: 1.44923ms
Mar 30 14:37:09.218: INFO: Pod "sysctl-5255ecf1-e1a0-4aaa-829a-f484c256bd15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004373407s
Mar 30 14:37:09.218: INFO: Pod "sysctl-5255ecf1-e1a0-4aaa-829a-f484c256bd15" satisfied condition "completed"
STEP: Checking that the pod succeeded 03/30/23 14:37:09.22
STEP: Getting logs from the pod 03/30/23 14:37:09.22
STEP: Checking that the sysctl is actually updated 03/30/23 14:37:09.223
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:37:09.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-6935" for this suite. 03/30/23 14:37:09.226
------------------------------
â€¢ [4.031 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:37:05.198
    Mar 30 14:37:05.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename sysctl 03/30/23 14:37:05.198
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:37:05.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:37:05.206
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/30/23 14:37:05.208
    STEP: Watching for error events or started pod 03/30/23 14:37:05.211
    STEP: Waiting for pod completion 03/30/23 14:37:07.214
    Mar 30 14:37:07.214: INFO: Waiting up to 3m0s for pod "sysctl-5255ecf1-e1a0-4aaa-829a-f484c256bd15" in namespace "sysctl-6935" to be "completed"
    Mar 30 14:37:07.216: INFO: Pod "sysctl-5255ecf1-e1a0-4aaa-829a-f484c256bd15": Phase="Pending", Reason="", readiness=false. Elapsed: 1.44923ms
    Mar 30 14:37:09.218: INFO: Pod "sysctl-5255ecf1-e1a0-4aaa-829a-f484c256bd15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004373407s
    Mar 30 14:37:09.218: INFO: Pod "sysctl-5255ecf1-e1a0-4aaa-829a-f484c256bd15" satisfied condition "completed"
    STEP: Checking that the pod succeeded 03/30/23 14:37:09.22
    STEP: Getting logs from the pod 03/30/23 14:37:09.22
    STEP: Checking that the sysctl is actually updated 03/30/23 14:37:09.223
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:37:09.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-6935" for this suite. 03/30/23 14:37:09.226
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:37:09.228
Mar 30 14:37:09.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename statefulset 03/30/23 14:37:09.229
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:37:09.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:37:09.238
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9606 03/30/23 14:37:09.239
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 03/30/23 14:37:09.242
Mar 30 14:37:09.246: INFO: Found 0 stateful pods, waiting for 3
Mar 30 14:37:19.249: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 14:37:19.249: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 14:37:19.249: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 30 14:37:19.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9606 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 14:37:19.354: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 14:37:19.354: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 14:37:19.354: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/30/23 14:37:29.362
Mar 30 14:37:29.377: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/30/23 14:37:29.377
STEP: Updating Pods in reverse ordinal order 03/30/23 14:37:39.387
Mar 30 14:37:39.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9606 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 14:37:39.480: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 14:37:39.480: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 14:37:39.480: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 14:37:49.492: INFO: Waiting for StatefulSet statefulset-9606/ss2 to complete update
STEP: Rolling back to a previous revision 03/30/23 14:37:59.497
Mar 30 14:37:59.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9606 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 30 14:37:59.589: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 30 14:37:59.589: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 30 14:37:59.589: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 30 14:38:09.611: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 03/30/23 14:38:19.619
Mar 30 14:38:19.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9606 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 30 14:38:19.722: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 30 14:38:19.722: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 30 14:38:19.722: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 30 14:38:29.734: INFO: Waiting for StatefulSet statefulset-9606/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 30 14:38:39.738: INFO: Deleting all statefulset in ns statefulset-9606
Mar 30 14:38:39.740: INFO: Scaling statefulset ss2 to 0
Mar 30 14:38:49.748: INFO: Waiting for statefulset status.replicas updated to 0
Mar 30 14:38:49.750: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 30 14:38:49.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9606" for this suite. 03/30/23 14:38:49.758
------------------------------
â€¢ [SLOW TEST] [100.532 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:37:09.228
    Mar 30 14:37:09.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename statefulset 03/30/23 14:37:09.229
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:37:09.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:37:09.238
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9606 03/30/23 14:37:09.239
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 03/30/23 14:37:09.242
    Mar 30 14:37:09.246: INFO: Found 0 stateful pods, waiting for 3
    Mar 30 14:37:19.249: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 30 14:37:19.249: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 30 14:37:19.249: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Mar 30 14:37:19.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9606 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 30 14:37:19.354: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 30 14:37:19.354: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 30 14:37:19.354: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/30/23 14:37:29.362
    Mar 30 14:37:29.377: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/30/23 14:37:29.377
    STEP: Updating Pods in reverse ordinal order 03/30/23 14:37:39.387
    Mar 30 14:37:39.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9606 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 30 14:37:39.480: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 30 14:37:39.480: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 30 14:37:39.480: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 30 14:37:49.492: INFO: Waiting for StatefulSet statefulset-9606/ss2 to complete update
    STEP: Rolling back to a previous revision 03/30/23 14:37:59.497
    Mar 30 14:37:59.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9606 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 30 14:37:59.589: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 30 14:37:59.589: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 30 14:37:59.589: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 30 14:38:09.611: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 03/30/23 14:38:19.619
    Mar 30 14:38:19.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=statefulset-9606 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 30 14:38:19.722: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 30 14:38:19.722: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 30 14:38:19.722: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 30 14:38:29.734: INFO: Waiting for StatefulSet statefulset-9606/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 30 14:38:39.738: INFO: Deleting all statefulset in ns statefulset-9606
    Mar 30 14:38:39.740: INFO: Scaling statefulset ss2 to 0
    Mar 30 14:38:49.748: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 30 14:38:49.750: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:38:49.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9606" for this suite. 03/30/23 14:38:49.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:38:49.761
Mar 30 14:38:49.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename events 03/30/23 14:38:49.762
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:38:49.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:38:49.773
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 03/30/23 14:38:49.774
STEP: listing events in all namespaces 03/30/23 14:38:49.777
STEP: listing events in test namespace 03/30/23 14:38:49.78
STEP: listing events with field selection filtering on source 03/30/23 14:38:49.781
STEP: listing events with field selection filtering on reportingController 03/30/23 14:38:49.782
STEP: getting the test event 03/30/23 14:38:49.784
STEP: patching the test event 03/30/23 14:38:49.785
STEP: getting the test event 03/30/23 14:38:49.789
STEP: updating the test event 03/30/23 14:38:49.791
STEP: getting the test event 03/30/23 14:38:49.793
STEP: deleting the test event 03/30/23 14:38:49.794
STEP: listing events in all namespaces 03/30/23 14:38:49.797
STEP: listing events in test namespace 03/30/23 14:38:49.799
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Mar 30 14:38:49.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6250" for this suite. 03/30/23 14:38:49.803
------------------------------
â€¢ [0.044 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:38:49.761
    Mar 30 14:38:49.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename events 03/30/23 14:38:49.762
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:38:49.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:38:49.773
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 03/30/23 14:38:49.774
    STEP: listing events in all namespaces 03/30/23 14:38:49.777
    STEP: listing events in test namespace 03/30/23 14:38:49.78
    STEP: listing events with field selection filtering on source 03/30/23 14:38:49.781
    STEP: listing events with field selection filtering on reportingController 03/30/23 14:38:49.782
    STEP: getting the test event 03/30/23 14:38:49.784
    STEP: patching the test event 03/30/23 14:38:49.785
    STEP: getting the test event 03/30/23 14:38:49.789
    STEP: updating the test event 03/30/23 14:38:49.791
    STEP: getting the test event 03/30/23 14:38:49.793
    STEP: deleting the test event 03/30/23 14:38:49.794
    STEP: listing events in all namespaces 03/30/23 14:38:49.797
    STEP: listing events in test namespace 03/30/23 14:38:49.799
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:38:49.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6250" for this suite. 03/30/23 14:38:49.803
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:38:49.805
Mar 30 14:38:49.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 14:38:49.806
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:38:49.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:38:49.815
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 14:38:49.821
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:38:50.237
STEP: Deploying the webhook pod 03/30/23 14:38:50.24
STEP: Wait for the deployment to be ready 03/30/23 14:38:50.244
Mar 30 14:38:50.248: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 14:38:52.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:38:54.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:38:56.256: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:38:58.256: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:39:00.256: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/30/23 14:39:02.255
STEP: Verifying the service has paired with the endpoint 03/30/23 14:39:02.26
Mar 30 14:39:03.260: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/30/23 14:39:03.262
STEP: create a namespace for the webhook 03/30/23 14:39:03.271
STEP: create a configmap should be unconditionally rejected by the webhook 03/30/23 14:39:03.274
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:39:03.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5520" for this suite. 03/30/23 14:39:03.31
STEP: Destroying namespace "webhook-5520-markers" for this suite. 03/30/23 14:39:03.313
------------------------------
â€¢ [SLOW TEST] [13.511 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:38:49.805
    Mar 30 14:38:49.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 14:38:49.806
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:38:49.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:38:49.815
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 14:38:49.821
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:38:50.237
    STEP: Deploying the webhook pod 03/30/23 14:38:50.24
    STEP: Wait for the deployment to be ready 03/30/23 14:38:50.244
    Mar 30 14:38:50.248: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 30 14:38:52.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:38:54.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:38:56.256: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:38:58.256: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:39:00.256: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 38, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/30/23 14:39:02.255
    STEP: Verifying the service has paired with the endpoint 03/30/23 14:39:02.26
    Mar 30 14:39:03.260: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/30/23 14:39:03.262
    STEP: create a namespace for the webhook 03/30/23 14:39:03.271
    STEP: create a configmap should be unconditionally rejected by the webhook 03/30/23 14:39:03.274
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:39:03.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5520" for this suite. 03/30/23 14:39:03.31
    STEP: Destroying namespace "webhook-5520-markers" for this suite. 03/30/23 14:39:03.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:39:03.317
Mar 30 14:39:03.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubelet-test 03/30/23 14:39:03.318
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:39:03.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:39:03.326
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 30 14:39:07.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8600" for this suite. 03/30/23 14:39:07.343
------------------------------
â€¢ [4.029 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:39:03.317
    Mar 30 14:39:03.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubelet-test 03/30/23 14:39:03.318
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:39:03.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:39:03.326
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:39:07.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8600" for this suite. 03/30/23 14:39:07.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:39:07.347
Mar 30 14:39:07.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename sched-preemption 03/30/23 14:39:07.348
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:39:07.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:39:07.356
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 30 14:39:07.363: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 30 14:40:07.394: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 03/30/23 14:40:07.395
Mar 30 14:40:07.406: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 30 14:40:07.409: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 30 14:40:07.417: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 30 14:40:07.420: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 30 14:40:07.429: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 30 14:40:07.431: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/30/23 14:40:07.431
Mar 30 14:40:07.431: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1340" to be "running"
Mar 30 14:40:07.433: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.873442ms
Mar 30 14:40:09.436: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.004983467s
Mar 30 14:40:09.436: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 30 14:40:09.436: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1340" to be "running"
Mar 30 14:40:09.438: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.525304ms
Mar 30 14:40:09.438: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 30 14:40:09.438: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1340" to be "running"
Mar 30 14:40:09.439: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.532918ms
Mar 30 14:40:09.439: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 30 14:40:09.439: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1340" to be "running"
Mar 30 14:40:09.441: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.333481ms
Mar 30 14:40:09.441: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 30 14:40:09.441: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1340" to be "running"
Mar 30 14:40:09.442: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.207232ms
Mar 30 14:40:09.442: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 30 14:40:09.442: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1340" to be "running"
Mar 30 14:40:09.443: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.346614ms
Mar 30 14:40:09.443: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 03/30/23 14:40:09.443
Mar 30 14:40:09.447: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Mar 30 14:40:09.449: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.330296ms
Mar 30 14:40:11.451: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00381255s
Mar 30 14:40:13.452: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004836977s
Mar 30 14:40:15.452: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.004616161s
Mar 30 14:40:15.452: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:40:15.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1340" for this suite. 03/30/23 14:40:15.491
------------------------------
â€¢ [SLOW TEST] [68.146 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:39:07.347
    Mar 30 14:39:07.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename sched-preemption 03/30/23 14:39:07.348
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:39:07.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:39:07.356
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 30 14:39:07.363: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 30 14:40:07.394: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 03/30/23 14:40:07.395
    Mar 30 14:40:07.406: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 30 14:40:07.409: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 30 14:40:07.417: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 30 14:40:07.420: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 30 14:40:07.429: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 30 14:40:07.431: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/30/23 14:40:07.431
    Mar 30 14:40:07.431: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1340" to be "running"
    Mar 30 14:40:07.433: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.873442ms
    Mar 30 14:40:09.436: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.004983467s
    Mar 30 14:40:09.436: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 30 14:40:09.436: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1340" to be "running"
    Mar 30 14:40:09.438: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.525304ms
    Mar 30 14:40:09.438: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 30 14:40:09.438: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1340" to be "running"
    Mar 30 14:40:09.439: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.532918ms
    Mar 30 14:40:09.439: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 30 14:40:09.439: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1340" to be "running"
    Mar 30 14:40:09.441: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.333481ms
    Mar 30 14:40:09.441: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 30 14:40:09.441: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1340" to be "running"
    Mar 30 14:40:09.442: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.207232ms
    Mar 30 14:40:09.442: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 30 14:40:09.442: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1340" to be "running"
    Mar 30 14:40:09.443: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.346614ms
    Mar 30 14:40:09.443: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 03/30/23 14:40:09.443
    Mar 30 14:40:09.447: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Mar 30 14:40:09.449: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.330296ms
    Mar 30 14:40:11.451: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00381255s
    Mar 30 14:40:13.452: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004836977s
    Mar 30 14:40:15.452: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.004616161s
    Mar 30 14:40:15.452: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:40:15.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1340" for this suite. 03/30/23 14:40:15.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:40:15.494
Mar 30 14:40:15.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename subpath 03/30/23 14:40:15.494
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:40:15.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:40:15.503
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/30/23 14:40:15.505
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-4frh 03/30/23 14:40:15.509
STEP: Creating a pod to test atomic-volume-subpath 03/30/23 14:40:15.509
Mar 30 14:40:15.513: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4frh" in namespace "subpath-3069" to be "Succeeded or Failed"
Mar 30 14:40:15.515: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029887ms
Mar 30 14:40:17.517: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 2.004336052s
Mar 30 14:40:19.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 4.004611953s
Mar 30 14:40:21.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 6.005159642s
Mar 30 14:40:23.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 8.00538152s
Mar 30 14:40:25.517: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 10.004333247s
Mar 30 14:40:27.517: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 12.003967295s
Mar 30 14:40:29.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 14.005094944s
Mar 30 14:40:31.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 16.005574579s
Mar 30 14:40:33.517: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 18.004312696s
Mar 30 14:40:35.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 20.005221297s
Mar 30 14:40:37.517: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=false. Elapsed: 22.003994495s
Mar 30 14:40:39.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005363373s
STEP: Saw pod success 03/30/23 14:40:39.518
Mar 30 14:40:39.518: INFO: Pod "pod-subpath-test-configmap-4frh" satisfied condition "Succeeded or Failed"
Mar 30 14:40:39.520: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-subpath-test-configmap-4frh container test-container-subpath-configmap-4frh: <nil>
STEP: delete the pod 03/30/23 14:40:39.528
Mar 30 14:40:39.533: INFO: Waiting for pod pod-subpath-test-configmap-4frh to disappear
Mar 30 14:40:39.535: INFO: Pod pod-subpath-test-configmap-4frh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4frh 03/30/23 14:40:39.535
Mar 30 14:40:39.535: INFO: Deleting pod "pod-subpath-test-configmap-4frh" in namespace "subpath-3069"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 30 14:40:39.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3069" for this suite. 03/30/23 14:40:39.538
------------------------------
â€¢ [SLOW TEST] [24.047 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:40:15.494
    Mar 30 14:40:15.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename subpath 03/30/23 14:40:15.494
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:40:15.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:40:15.503
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/30/23 14:40:15.505
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-4frh 03/30/23 14:40:15.509
    STEP: Creating a pod to test atomic-volume-subpath 03/30/23 14:40:15.509
    Mar 30 14:40:15.513: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4frh" in namespace "subpath-3069" to be "Succeeded or Failed"
    Mar 30 14:40:15.515: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029887ms
    Mar 30 14:40:17.517: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 2.004336052s
    Mar 30 14:40:19.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 4.004611953s
    Mar 30 14:40:21.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 6.005159642s
    Mar 30 14:40:23.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 8.00538152s
    Mar 30 14:40:25.517: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 10.004333247s
    Mar 30 14:40:27.517: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 12.003967295s
    Mar 30 14:40:29.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 14.005094944s
    Mar 30 14:40:31.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 16.005574579s
    Mar 30 14:40:33.517: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 18.004312696s
    Mar 30 14:40:35.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=true. Elapsed: 20.005221297s
    Mar 30 14:40:37.517: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Running", Reason="", readiness=false. Elapsed: 22.003994495s
    Mar 30 14:40:39.518: INFO: Pod "pod-subpath-test-configmap-4frh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005363373s
    STEP: Saw pod success 03/30/23 14:40:39.518
    Mar 30 14:40:39.518: INFO: Pod "pod-subpath-test-configmap-4frh" satisfied condition "Succeeded or Failed"
    Mar 30 14:40:39.520: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-subpath-test-configmap-4frh container test-container-subpath-configmap-4frh: <nil>
    STEP: delete the pod 03/30/23 14:40:39.528
    Mar 30 14:40:39.533: INFO: Waiting for pod pod-subpath-test-configmap-4frh to disappear
    Mar 30 14:40:39.535: INFO: Pod pod-subpath-test-configmap-4frh no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-4frh 03/30/23 14:40:39.535
    Mar 30 14:40:39.535: INFO: Deleting pod "pod-subpath-test-configmap-4frh" in namespace "subpath-3069"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:40:39.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3069" for this suite. 03/30/23 14:40:39.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:40:39.541
Mar 30 14:40:39.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename custom-resource-definition 03/30/23 14:40:39.542
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:40:39.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:40:39.55
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Mar 30 14:40:39.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:41:40.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2256" for this suite. 03/30/23 14:41:40.662
------------------------------
â€¢ [SLOW TEST] [61.123 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:40:39.541
    Mar 30 14:40:39.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename custom-resource-definition 03/30/23 14:40:39.542
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:40:39.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:40:39.55
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Mar 30 14:40:39.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:41:40.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2256" for this suite. 03/30/23 14:41:40.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:41:40.665
Mar 30 14:41:40.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename watch 03/30/23 14:41:40.666
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:41:40.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:41:40.674
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 03/30/23 14:41:40.676
STEP: creating a new configmap 03/30/23 14:41:40.677
STEP: modifying the configmap once 03/30/23 14:41:40.679
STEP: changing the label value of the configmap 03/30/23 14:41:40.682
STEP: Expecting to observe a delete notification for the watched object 03/30/23 14:41:40.685
Mar 30 14:41:40.685: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3522  5258e5fc-c8e9-4e91-994c-b2fe689f36fa 100515 0 2023-03-30 14:41:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-30 14:41:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 14:41:40.685: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3522  5258e5fc-c8e9-4e91-994c-b2fe689f36fa 100516 0 2023-03-30 14:41:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-30 14:41:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 14:41:40.685: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3522  5258e5fc-c8e9-4e91-994c-b2fe689f36fa 100517 0 2023-03-30 14:41:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-30 14:41:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 03/30/23 14:41:40.685
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/30/23 14:41:40.688
STEP: changing the label value of the configmap back 03/30/23 14:41:50.689
STEP: modifying the configmap a third time 03/30/23 14:41:50.693
STEP: deleting the configmap 03/30/23 14:41:50.697
STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/30/23 14:41:50.7
Mar 30 14:41:50.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3522  5258e5fc-c8e9-4e91-994c-b2fe689f36fa 100572 0 2023-03-30 14:41:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-30 14:41:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 14:41:50.700: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3522  5258e5fc-c8e9-4e91-994c-b2fe689f36fa 100573 0 2023-03-30 14:41:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-30 14:41:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 14:41:50.700: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3522  5258e5fc-c8e9-4e91-994c-b2fe689f36fa 100574 0 2023-03-30 14:41:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-30 14:41:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 30 14:41:50.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3522" for this suite. 03/30/23 14:41:50.702
------------------------------
â€¢ [SLOW TEST] [10.039 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:41:40.665
    Mar 30 14:41:40.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename watch 03/30/23 14:41:40.666
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:41:40.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:41:40.674
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 03/30/23 14:41:40.676
    STEP: creating a new configmap 03/30/23 14:41:40.677
    STEP: modifying the configmap once 03/30/23 14:41:40.679
    STEP: changing the label value of the configmap 03/30/23 14:41:40.682
    STEP: Expecting to observe a delete notification for the watched object 03/30/23 14:41:40.685
    Mar 30 14:41:40.685: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3522  5258e5fc-c8e9-4e91-994c-b2fe689f36fa 100515 0 2023-03-30 14:41:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-30 14:41:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 14:41:40.685: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3522  5258e5fc-c8e9-4e91-994c-b2fe689f36fa 100516 0 2023-03-30 14:41:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-30 14:41:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 14:41:40.685: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3522  5258e5fc-c8e9-4e91-994c-b2fe689f36fa 100517 0 2023-03-30 14:41:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-30 14:41:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 03/30/23 14:41:40.685
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/30/23 14:41:40.688
    STEP: changing the label value of the configmap back 03/30/23 14:41:50.689
    STEP: modifying the configmap a third time 03/30/23 14:41:50.693
    STEP: deleting the configmap 03/30/23 14:41:50.697
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/30/23 14:41:50.7
    Mar 30 14:41:50.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3522  5258e5fc-c8e9-4e91-994c-b2fe689f36fa 100572 0 2023-03-30 14:41:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-30 14:41:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 14:41:50.700: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3522  5258e5fc-c8e9-4e91-994c-b2fe689f36fa 100573 0 2023-03-30 14:41:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-30 14:41:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 14:41:50.700: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3522  5258e5fc-c8e9-4e91-994c-b2fe689f36fa 100574 0 2023-03-30 14:41:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-30 14:41:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:41:50.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3522" for this suite. 03/30/23 14:41:50.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:41:50.705
Mar 30 14:41:50.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename replicaset 03/30/23 14:41:50.706
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:41:50.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:41:50.714
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Mar 30 14:41:50.715: INFO: Creating ReplicaSet my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57
Mar 30 14:41:50.719: INFO: Pod name my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57: Found 0 pods out of 1
Mar 30 14:41:55.724: INFO: Pod name my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57: Found 1 pods out of 1
Mar 30 14:41:55.724: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57" is running
Mar 30 14:41:55.724: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57-xqnfc" in namespace "replicaset-2162" to be "running"
Mar 30 14:41:55.725: INFO: Pod "my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57-xqnfc": Phase="Running", Reason="", readiness=true. Elapsed: 1.403011ms
Mar 30 14:41:55.725: INFO: Pod "my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57-xqnfc" satisfied condition "running"
Mar 30 14:41:55.725: INFO: Pod "my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57-xqnfc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 14:41:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 14:41:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 14:41:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 14:41:50 +0000 UTC Reason: Message:}])
Mar 30 14:41:55.725: INFO: Trying to dial the pod
Mar 30 14:42:00.732: INFO: Controller my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57: Got expected result from replica 1 [my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57-xqnfc]: "my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57-xqnfc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 30 14:42:00.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2162" for this suite. 03/30/23 14:42:00.734
------------------------------
â€¢ [SLOW TEST] [10.032 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:41:50.705
    Mar 30 14:41:50.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename replicaset 03/30/23 14:41:50.706
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:41:50.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:41:50.714
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Mar 30 14:41:50.715: INFO: Creating ReplicaSet my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57
    Mar 30 14:41:50.719: INFO: Pod name my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57: Found 0 pods out of 1
    Mar 30 14:41:55.724: INFO: Pod name my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57: Found 1 pods out of 1
    Mar 30 14:41:55.724: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57" is running
    Mar 30 14:41:55.724: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57-xqnfc" in namespace "replicaset-2162" to be "running"
    Mar 30 14:41:55.725: INFO: Pod "my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57-xqnfc": Phase="Running", Reason="", readiness=true. Elapsed: 1.403011ms
    Mar 30 14:41:55.725: INFO: Pod "my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57-xqnfc" satisfied condition "running"
    Mar 30 14:41:55.725: INFO: Pod "my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57-xqnfc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 14:41:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 14:41:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 14:41:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-30 14:41:50 +0000 UTC Reason: Message:}])
    Mar 30 14:41:55.725: INFO: Trying to dial the pod
    Mar 30 14:42:00.732: INFO: Controller my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57: Got expected result from replica 1 [my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57-xqnfc]: "my-hostname-basic-9d020adc-79ae-448b-8115-2f8736423b57-xqnfc", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:42:00.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2162" for this suite. 03/30/23 14:42:00.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:42:00.738
Mar 30 14:42:00.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename cronjob 03/30/23 14:42:00.738
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:00.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:00.747
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 03/30/23 14:42:00.748
STEP: creating 03/30/23 14:42:00.748
STEP: getting 03/30/23 14:42:00.751
STEP: listing 03/30/23 14:42:00.752
STEP: watching 03/30/23 14:42:00.753
Mar 30 14:42:00.753: INFO: starting watch
STEP: cluster-wide listing 03/30/23 14:42:00.754
STEP: cluster-wide watching 03/30/23 14:42:00.755
Mar 30 14:42:00.755: INFO: starting watch
STEP: patching 03/30/23 14:42:00.756
STEP: updating 03/30/23 14:42:00.759
Mar 30 14:42:00.763: INFO: waiting for watch events with expected annotations
Mar 30 14:42:00.763: INFO: saw patched and updated annotations
STEP: patching /status 03/30/23 14:42:00.763
STEP: updating /status 03/30/23 14:42:00.766
STEP: get /status 03/30/23 14:42:00.769
STEP: deleting 03/30/23 14:42:00.771
STEP: deleting a collection 03/30/23 14:42:00.776
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 30 14:42:00.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2224" for this suite. 03/30/23 14:42:00.782
------------------------------
â€¢ [0.047 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:42:00.738
    Mar 30 14:42:00.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename cronjob 03/30/23 14:42:00.738
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:00.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:00.747
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 03/30/23 14:42:00.748
    STEP: creating 03/30/23 14:42:00.748
    STEP: getting 03/30/23 14:42:00.751
    STEP: listing 03/30/23 14:42:00.752
    STEP: watching 03/30/23 14:42:00.753
    Mar 30 14:42:00.753: INFO: starting watch
    STEP: cluster-wide listing 03/30/23 14:42:00.754
    STEP: cluster-wide watching 03/30/23 14:42:00.755
    Mar 30 14:42:00.755: INFO: starting watch
    STEP: patching 03/30/23 14:42:00.756
    STEP: updating 03/30/23 14:42:00.759
    Mar 30 14:42:00.763: INFO: waiting for watch events with expected annotations
    Mar 30 14:42:00.763: INFO: saw patched and updated annotations
    STEP: patching /status 03/30/23 14:42:00.763
    STEP: updating /status 03/30/23 14:42:00.766
    STEP: get /status 03/30/23 14:42:00.769
    STEP: deleting 03/30/23 14:42:00.771
    STEP: deleting a collection 03/30/23 14:42:00.776
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:42:00.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2224" for this suite. 03/30/23 14:42:00.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:42:00.785
Mar 30 14:42:00.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pods 03/30/23 14:42:00.786
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:00.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:00.794
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 03/30/23 14:42:00.795
STEP: setting up watch 03/30/23 14:42:00.795
STEP: submitting the pod to kubernetes 03/30/23 14:42:00.897
STEP: verifying the pod is in kubernetes 03/30/23 14:42:00.902
STEP: verifying pod creation was observed 03/30/23 14:42:00.903
Mar 30 14:42:00.903: INFO: Waiting up to 5m0s for pod "pod-submit-remove-a30ddc19-9984-4122-96e9-3b1f242008fc" in namespace "pods-5757" to be "running"
Mar 30 14:42:00.905: INFO: Pod "pod-submit-remove-a30ddc19-9984-4122-96e9-3b1f242008fc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.427569ms
Mar 30 14:42:02.907: INFO: Pod "pod-submit-remove-a30ddc19-9984-4122-96e9-3b1f242008fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003624038s
Mar 30 14:42:02.907: INFO: Pod "pod-submit-remove-a30ddc19-9984-4122-96e9-3b1f242008fc" satisfied condition "running"
STEP: deleting the pod gracefully 03/30/23 14:42:02.909
STEP: verifying pod deletion was observed 03/30/23 14:42:02.912
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 30 14:42:04.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5757" for this suite. 03/30/23 14:42:04.798
------------------------------
â€¢ [4.015 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:42:00.785
    Mar 30 14:42:00.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pods 03/30/23 14:42:00.786
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:00.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:00.794
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 03/30/23 14:42:00.795
    STEP: setting up watch 03/30/23 14:42:00.795
    STEP: submitting the pod to kubernetes 03/30/23 14:42:00.897
    STEP: verifying the pod is in kubernetes 03/30/23 14:42:00.902
    STEP: verifying pod creation was observed 03/30/23 14:42:00.903
    Mar 30 14:42:00.903: INFO: Waiting up to 5m0s for pod "pod-submit-remove-a30ddc19-9984-4122-96e9-3b1f242008fc" in namespace "pods-5757" to be "running"
    Mar 30 14:42:00.905: INFO: Pod "pod-submit-remove-a30ddc19-9984-4122-96e9-3b1f242008fc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.427569ms
    Mar 30 14:42:02.907: INFO: Pod "pod-submit-remove-a30ddc19-9984-4122-96e9-3b1f242008fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003624038s
    Mar 30 14:42:02.907: INFO: Pod "pod-submit-remove-a30ddc19-9984-4122-96e9-3b1f242008fc" satisfied condition "running"
    STEP: deleting the pod gracefully 03/30/23 14:42:02.909
    STEP: verifying pod deletion was observed 03/30/23 14:42:02.912
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:42:04.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5757" for this suite. 03/30/23 14:42:04.798
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:42:04.801
Mar 30 14:42:04.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename container-runtime 03/30/23 14:42:04.802
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:04.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:04.81
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 03/30/23 14:42:04.811
STEP: wait for the container to reach Failed 03/30/23 14:42:04.815
STEP: get the container status 03/30/23 14:42:07.824
STEP: the container should be terminated 03/30/23 14:42:07.826
STEP: the termination message should be set 03/30/23 14:42:07.826
Mar 30 14:42:07.826: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/30/23 14:42:07.826
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 30 14:42:07.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1990" for this suite. 03/30/23 14:42:07.835
------------------------------
â€¢ [3.036 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:42:04.801
    Mar 30 14:42:04.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename container-runtime 03/30/23 14:42:04.802
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:04.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:04.81
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 03/30/23 14:42:04.811
    STEP: wait for the container to reach Failed 03/30/23 14:42:04.815
    STEP: get the container status 03/30/23 14:42:07.824
    STEP: the container should be terminated 03/30/23 14:42:07.826
    STEP: the termination message should be set 03/30/23 14:42:07.826
    Mar 30 14:42:07.826: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/30/23 14:42:07.826
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:42:07.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1990" for this suite. 03/30/23 14:42:07.835
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:42:07.837
Mar 30 14:42:07.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 14:42:07.838
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:07.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:07.846
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 03/30/23 14:42:07.848
Mar 30 14:42:07.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: rename a version 03/30/23 14:42:16.476
STEP: check the new version name is served 03/30/23 14:42:16.49
STEP: check the old version name is removed 03/30/23 14:42:17.975
STEP: check the other version is not changed 03/30/23 14:42:18.627
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:42:21.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-341" for this suite. 03/30/23 14:42:21.551
------------------------------
â€¢ [SLOW TEST] [13.717 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:42:07.837
    Mar 30 14:42:07.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename crd-publish-openapi 03/30/23 14:42:07.838
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:07.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:07.846
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 03/30/23 14:42:07.848
    Mar 30 14:42:07.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: rename a version 03/30/23 14:42:16.476
    STEP: check the new version name is served 03/30/23 14:42:16.49
    STEP: check the old version name is removed 03/30/23 14:42:17.975
    STEP: check the other version is not changed 03/30/23 14:42:18.627
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:42:21.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-341" for this suite. 03/30/23 14:42:21.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:42:21.554
Mar 30 14:42:21.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 14:42:21.555
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:21.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:21.562
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-4c0752dc-ddab-4839-a7ec-eff89d8c6315 03/30/23 14:42:21.564
STEP: Creating a pod to test consume secrets 03/30/23 14:42:21.566
Mar 30 14:42:21.570: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702" in namespace "projected-1278" to be "Succeeded or Failed"
Mar 30 14:42:21.572: INFO: Pod "pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100048ms
Mar 30 14:42:23.575: INFO: Pod "pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005346039s
Mar 30 14:42:25.574: INFO: Pod "pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004099645s
STEP: Saw pod success 03/30/23 14:42:25.574
Mar 30 14:42:25.574: INFO: Pod "pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702" satisfied condition "Succeeded or Failed"
Mar 30 14:42:25.575: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/30/23 14:42:25.585
Mar 30 14:42:25.590: INFO: Waiting for pod pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702 to disappear
Mar 30 14:42:25.591: INFO: Pod pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 30 14:42:25.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1278" for this suite. 03/30/23 14:42:25.593
------------------------------
â€¢ [4.041 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:42:21.554
    Mar 30 14:42:21.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 14:42:21.555
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:21.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:21.562
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-4c0752dc-ddab-4839-a7ec-eff89d8c6315 03/30/23 14:42:21.564
    STEP: Creating a pod to test consume secrets 03/30/23 14:42:21.566
    Mar 30 14:42:21.570: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702" in namespace "projected-1278" to be "Succeeded or Failed"
    Mar 30 14:42:21.572: INFO: Pod "pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100048ms
    Mar 30 14:42:23.575: INFO: Pod "pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005346039s
    Mar 30 14:42:25.574: INFO: Pod "pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004099645s
    STEP: Saw pod success 03/30/23 14:42:25.574
    Mar 30 14:42:25.574: INFO: Pod "pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702" satisfied condition "Succeeded or Failed"
    Mar 30 14:42:25.575: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/30/23 14:42:25.585
    Mar 30 14:42:25.590: INFO: Waiting for pod pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702 to disappear
    Mar 30 14:42:25.591: INFO: Pod pod-projected-secrets-4bdf05cf-3198-456b-9358-bc61864e6702 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:42:25.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1278" for this suite. 03/30/23 14:42:25.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:42:25.596
Mar 30 14:42:25.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename replicaset 03/30/23 14:42:25.597
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:25.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:25.605
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/30/23 14:42:25.606
Mar 30 14:42:25.610: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 30 14:42:30.615: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/30/23 14:42:30.615
STEP: getting scale subresource 03/30/23 14:42:30.615
STEP: updating a scale subresource 03/30/23 14:42:30.616
STEP: verifying the replicaset Spec.Replicas was modified 03/30/23 14:42:30.62
STEP: Patch a scale subresource 03/30/23 14:42:30.622
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 30 14:42:30.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7074" for this suite. 03/30/23 14:42:30.629
------------------------------
â€¢ [SLOW TEST] [5.036 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:42:25.596
    Mar 30 14:42:25.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename replicaset 03/30/23 14:42:25.597
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:25.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:25.605
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/30/23 14:42:25.606
    Mar 30 14:42:25.610: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 30 14:42:30.615: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/30/23 14:42:30.615
    STEP: getting scale subresource 03/30/23 14:42:30.615
    STEP: updating a scale subresource 03/30/23 14:42:30.616
    STEP: verifying the replicaset Spec.Replicas was modified 03/30/23 14:42:30.62
    STEP: Patch a scale subresource 03/30/23 14:42:30.622
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:42:30.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7074" for this suite. 03/30/23 14:42:30.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:42:30.633
Mar 30 14:42:30.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename events 03/30/23 14:42:30.634
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:30.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:30.643
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 03/30/23 14:42:30.644
STEP: listing all events in all namespaces 03/30/23 14:42:30.646
STEP: patching the test event 03/30/23 14:42:30.648
STEP: fetching the test event 03/30/23 14:42:30.65
STEP: updating the test event 03/30/23 14:42:30.652
STEP: getting the test event 03/30/23 14:42:30.655
STEP: deleting the test event 03/30/23 14:42:30.656
STEP: listing all events in all namespaces 03/30/23 14:42:30.659
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Mar 30 14:42:30.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2378" for this suite. 03/30/23 14:42:30.663
------------------------------
â€¢ [0.032 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:42:30.633
    Mar 30 14:42:30.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename events 03/30/23 14:42:30.634
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:30.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:30.643
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 03/30/23 14:42:30.644
    STEP: listing all events in all namespaces 03/30/23 14:42:30.646
    STEP: patching the test event 03/30/23 14:42:30.648
    STEP: fetching the test event 03/30/23 14:42:30.65
    STEP: updating the test event 03/30/23 14:42:30.652
    STEP: getting the test event 03/30/23 14:42:30.655
    STEP: deleting the test event 03/30/23 14:42:30.656
    STEP: listing all events in all namespaces 03/30/23 14:42:30.659
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:42:30.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2378" for this suite. 03/30/23 14:42:30.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:42:30.666
Mar 30 14:42:30.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 14:42:30.667
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:30.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:30.674
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 14:42:30.681
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:42:31.137
STEP: Deploying the webhook pod 03/30/23 14:42:31.14
STEP: Wait for the deployment to be ready 03/30/23 14:42:31.145
Mar 30 14:42:31.148: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 14:42:33.154: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:42:35.157: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:42:37.156: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:42:39.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/30/23 14:42:41.157
STEP: Verifying the service has paired with the endpoint 03/30/23 14:42:41.165
Mar 30 14:42:42.166: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 03/30/23 14:42:42.168
STEP: create a pod that should be denied by the webhook 03/30/23 14:42:42.177
STEP: create a pod that causes the webhook to hang 03/30/23 14:42:42.185
STEP: create a configmap that should be denied by the webhook 03/30/23 14:42:52.189
STEP: create a configmap that should be admitted by the webhook 03/30/23 14:42:52.194
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/30/23 14:42:52.2
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/30/23 14:42:52.204
STEP: create a namespace that bypass the webhook 03/30/23 14:42:52.206
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/30/23 14:42:52.209
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:42:52.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-73" for this suite. 03/30/23 14:42:52.236
STEP: Destroying namespace "webhook-73-markers" for this suite. 03/30/23 14:42:52.24
------------------------------
â€¢ [SLOW TEST] [21.576 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:42:30.666
    Mar 30 14:42:30.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 14:42:30.667
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:30.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:30.674
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 14:42:30.681
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:42:31.137
    STEP: Deploying the webhook pod 03/30/23 14:42:31.14
    STEP: Wait for the deployment to be ready 03/30/23 14:42:31.145
    Mar 30 14:42:31.148: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 30 14:42:33.154: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:42:35.157: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:42:37.156: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:42:39.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 42, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/30/23 14:42:41.157
    STEP: Verifying the service has paired with the endpoint 03/30/23 14:42:41.165
    Mar 30 14:42:42.166: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 03/30/23 14:42:42.168
    STEP: create a pod that should be denied by the webhook 03/30/23 14:42:42.177
    STEP: create a pod that causes the webhook to hang 03/30/23 14:42:42.185
    STEP: create a configmap that should be denied by the webhook 03/30/23 14:42:52.189
    STEP: create a configmap that should be admitted by the webhook 03/30/23 14:42:52.194
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/30/23 14:42:52.2
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/30/23 14:42:52.204
    STEP: create a namespace that bypass the webhook 03/30/23 14:42:52.206
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/30/23 14:42:52.209
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:42:52.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-73" for this suite. 03/30/23 14:42:52.236
    STEP: Destroying namespace "webhook-73-markers" for this suite. 03/30/23 14:42:52.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:42:52.243
Mar 30 14:42:52.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename proxy 03/30/23 14:42:52.243
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:52.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:52.253
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Mar 30 14:42:52.254: INFO: Creating pod...
Mar 30 14:42:52.258: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2947" to be "running"
Mar 30 14:42:52.259: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.60123ms
Mar 30 14:42:54.262: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004411798s
Mar 30 14:42:54.262: INFO: Pod "agnhost" satisfied condition "running"
Mar 30 14:42:54.262: INFO: Creating service...
Mar 30 14:42:54.267: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=DELETE
Mar 30 14:42:54.269: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 30 14:42:54.269: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=OPTIONS
Mar 30 14:42:54.271: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 30 14:42:54.271: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=PATCH
Mar 30 14:42:54.273: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 30 14:42:54.273: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=POST
Mar 30 14:42:54.275: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 30 14:42:54.275: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=PUT
Mar 30 14:42:54.277: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 30 14:42:54.277: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=DELETE
Mar 30 14:42:54.279: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 30 14:42:54.279: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar 30 14:42:54.281: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 30 14:42:54.281: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=PATCH
Mar 30 14:42:54.284: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 30 14:42:54.284: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=POST
Mar 30 14:42:54.287: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 30 14:42:54.287: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=PUT
Mar 30 14:42:54.289: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 30 14:42:54.289: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=GET
Mar 30 14:42:54.290: INFO: http.Client request:GET StatusCode:301
Mar 30 14:42:54.291: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=GET
Mar 30 14:42:54.293: INFO: http.Client request:GET StatusCode:301
Mar 30 14:42:54.293: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=HEAD
Mar 30 14:42:54.294: INFO: http.Client request:HEAD StatusCode:301
Mar 30 14:42:54.294: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=HEAD
Mar 30 14:42:54.296: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 30 14:42:54.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2947" for this suite. 03/30/23 14:42:54.299
------------------------------
â€¢ [2.059 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:42:52.243
    Mar 30 14:42:52.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename proxy 03/30/23 14:42:52.243
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:52.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:52.253
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Mar 30 14:42:52.254: INFO: Creating pod...
    Mar 30 14:42:52.258: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2947" to be "running"
    Mar 30 14:42:52.259: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.60123ms
    Mar 30 14:42:54.262: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004411798s
    Mar 30 14:42:54.262: INFO: Pod "agnhost" satisfied condition "running"
    Mar 30 14:42:54.262: INFO: Creating service...
    Mar 30 14:42:54.267: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=DELETE
    Mar 30 14:42:54.269: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 30 14:42:54.269: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=OPTIONS
    Mar 30 14:42:54.271: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 30 14:42:54.271: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=PATCH
    Mar 30 14:42:54.273: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 30 14:42:54.273: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=POST
    Mar 30 14:42:54.275: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 30 14:42:54.275: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=PUT
    Mar 30 14:42:54.277: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 30 14:42:54.277: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=DELETE
    Mar 30 14:42:54.279: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 30 14:42:54.279: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Mar 30 14:42:54.281: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 30 14:42:54.281: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=PATCH
    Mar 30 14:42:54.284: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 30 14:42:54.284: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=POST
    Mar 30 14:42:54.287: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 30 14:42:54.287: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=PUT
    Mar 30 14:42:54.289: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 30 14:42:54.289: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=GET
    Mar 30 14:42:54.290: INFO: http.Client request:GET StatusCode:301
    Mar 30 14:42:54.291: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=GET
    Mar 30 14:42:54.293: INFO: http.Client request:GET StatusCode:301
    Mar 30 14:42:54.293: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/pods/agnhost/proxy?method=HEAD
    Mar 30 14:42:54.294: INFO: http.Client request:HEAD StatusCode:301
    Mar 30 14:42:54.294: INFO: Starting http.Client for https://172.16.0.1:443/api/v1/namespaces/proxy-2947/services/e2e-proxy-test-service/proxy?method=HEAD
    Mar 30 14:42:54.296: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:42:54.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2947" for this suite. 03/30/23 14:42:54.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:42:54.303
Mar 30 14:42:54.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename services 03/30/23 14:42:54.303
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:54.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:54.312
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-5272 03/30/23 14:42:54.314
STEP: creating service affinity-clusterip in namespace services-5272 03/30/23 14:42:54.314
STEP: creating replication controller affinity-clusterip in namespace services-5272 03/30/23 14:42:54.318
I0330 14:42:54.321482      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5272, replica count: 3
I0330 14:42:57.373187      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 30 14:42:57.376: INFO: Creating new exec pod
Mar 30 14:42:57.379: INFO: Waiting up to 5m0s for pod "execpod-affinityv6f2w" in namespace "services-5272" to be "running"
Mar 30 14:42:57.381: INFO: Pod "execpod-affinityv6f2w": Phase="Pending", Reason="", readiness=false. Elapsed: 1.424981ms
Mar 30 14:42:59.383: INFO: Pod "execpod-affinityv6f2w": Phase="Running", Reason="", readiness=true. Elapsed: 2.003914403s
Mar 30 14:42:59.383: INFO: Pod "execpod-affinityv6f2w" satisfied condition "running"
Mar 30 14:43:00.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-5272 exec execpod-affinityv6f2w -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Mar 30 14:43:00.479: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 30 14:43:00.479: INFO: stdout: ""
Mar 30 14:43:00.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-5272 exec execpod-affinityv6f2w -- /bin/sh -x -c nc -v -z -w 2 172.16.72.61 80'
Mar 30 14:43:00.572: INFO: stderr: "+ nc -v -z -w 2 172.16.72.61 80\nConnection to 172.16.72.61 80 port [tcp/http] succeeded!\n"
Mar 30 14:43:00.572: INFO: stdout: ""
Mar 30 14:43:00.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-5272 exec execpod-affinityv6f2w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.72.61:80/ ; done'
Mar 30 14:43:00.716: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n"
Mar 30 14:43:00.716: INFO: stdout: "\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c"
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
Mar 30 14:43:00.717: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5272, will wait for the garbage collector to delete the pods 03/30/23 14:43:00.721
Mar 30 14:43:00.776: INFO: Deleting ReplicationController affinity-clusterip took: 2.414831ms
Mar 30 14:43:00.877: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.989382ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 30 14:43:02.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5272" for this suite. 03/30/23 14:43:02.988
------------------------------
â€¢ [SLOW TEST] [8.688 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:42:54.303
    Mar 30 14:42:54.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename services 03/30/23 14:42:54.303
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:42:54.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:42:54.312
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-5272 03/30/23 14:42:54.314
    STEP: creating service affinity-clusterip in namespace services-5272 03/30/23 14:42:54.314
    STEP: creating replication controller affinity-clusterip in namespace services-5272 03/30/23 14:42:54.318
    I0330 14:42:54.321482      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5272, replica count: 3
    I0330 14:42:57.373187      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 30 14:42:57.376: INFO: Creating new exec pod
    Mar 30 14:42:57.379: INFO: Waiting up to 5m0s for pod "execpod-affinityv6f2w" in namespace "services-5272" to be "running"
    Mar 30 14:42:57.381: INFO: Pod "execpod-affinityv6f2w": Phase="Pending", Reason="", readiness=false. Elapsed: 1.424981ms
    Mar 30 14:42:59.383: INFO: Pod "execpod-affinityv6f2w": Phase="Running", Reason="", readiness=true. Elapsed: 2.003914403s
    Mar 30 14:42:59.383: INFO: Pod "execpod-affinityv6f2w" satisfied condition "running"
    Mar 30 14:43:00.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-5272 exec execpod-affinityv6f2w -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Mar 30 14:43:00.479: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Mar 30 14:43:00.479: INFO: stdout: ""
    Mar 30 14:43:00.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-5272 exec execpod-affinityv6f2w -- /bin/sh -x -c nc -v -z -w 2 172.16.72.61 80'
    Mar 30 14:43:00.572: INFO: stderr: "+ nc -v -z -w 2 172.16.72.61 80\nConnection to 172.16.72.61 80 port [tcp/http] succeeded!\n"
    Mar 30 14:43:00.572: INFO: stdout: ""
    Mar 30 14:43:00.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=services-5272 exec execpod-affinityv6f2w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.72.61:80/ ; done'
    Mar 30 14:43:00.716: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.72.61:80/\n"
    Mar 30 14:43:00.716: INFO: stdout: "\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c\naffinity-clusterip-9vd9c"
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Received response from host: affinity-clusterip-9vd9c
    Mar 30 14:43:00.717: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-5272, will wait for the garbage collector to delete the pods 03/30/23 14:43:00.721
    Mar 30 14:43:00.776: INFO: Deleting ReplicationController affinity-clusterip took: 2.414831ms
    Mar 30 14:43:00.877: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.989382ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:43:02.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5272" for this suite. 03/30/23 14:43:02.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:43:02.992
Mar 30 14:43:02.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename watch 03/30/23 14:43:02.992
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:43:02.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:43:03.001
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 03/30/23 14:43:03.003
STEP: creating a new configmap 03/30/23 14:43:03.003
STEP: modifying the configmap once 03/30/23 14:43:03.005
STEP: closing the watch once it receives two notifications 03/30/23 14:43:03.009
Mar 30 14:43:03.009: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-869  5c1374c1-9a0e-46ee-ad7c-1fb7298ab5d9 101336 0 2023-03-30 14:43:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-30 14:43:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 14:43:03.009: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-869  5c1374c1-9a0e-46ee-ad7c-1fb7298ab5d9 101337 0 2023-03-30 14:43:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-30 14:43:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 03/30/23 14:43:03.009
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/30/23 14:43:03.013
STEP: deleting the configmap 03/30/23 14:43:03.013
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/30/23 14:43:03.016
Mar 30 14:43:03.016: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-869  5c1374c1-9a0e-46ee-ad7c-1fb7298ab5d9 101338 0 2023-03-30 14:43:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-30 14:43:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 30 14:43:03.016: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-869  5c1374c1-9a0e-46ee-ad7c-1fb7298ab5d9 101339 0 2023-03-30 14:43:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-30 14:43:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 30 14:43:03.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-869" for this suite. 03/30/23 14:43:03.018
------------------------------
â€¢ [0.030 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:43:02.992
    Mar 30 14:43:02.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename watch 03/30/23 14:43:02.992
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:43:02.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:43:03.001
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 03/30/23 14:43:03.003
    STEP: creating a new configmap 03/30/23 14:43:03.003
    STEP: modifying the configmap once 03/30/23 14:43:03.005
    STEP: closing the watch once it receives two notifications 03/30/23 14:43:03.009
    Mar 30 14:43:03.009: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-869  5c1374c1-9a0e-46ee-ad7c-1fb7298ab5d9 101336 0 2023-03-30 14:43:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-30 14:43:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 14:43:03.009: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-869  5c1374c1-9a0e-46ee-ad7c-1fb7298ab5d9 101337 0 2023-03-30 14:43:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-30 14:43:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 03/30/23 14:43:03.009
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/30/23 14:43:03.013
    STEP: deleting the configmap 03/30/23 14:43:03.013
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/30/23 14:43:03.016
    Mar 30 14:43:03.016: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-869  5c1374c1-9a0e-46ee-ad7c-1fb7298ab5d9 101338 0 2023-03-30 14:43:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-30 14:43:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 30 14:43:03.016: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-869  5c1374c1-9a0e-46ee-ad7c-1fb7298ab5d9 101339 0 2023-03-30 14:43:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-30 14:43:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:43:03.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-869" for this suite. 03/30/23 14:43:03.018
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:43:03.021
Mar 30 14:43:03.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename svcaccounts 03/30/23 14:43:03.023
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:43:03.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:43:03.031
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Mar 30 14:43:03.039: INFO: created pod
Mar 30 14:43:03.039: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6854" to be "Succeeded or Failed"
Mar 30 14:43:03.041: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35575ms
Mar 30 14:43:05.043: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003686065s
Mar 30 14:43:07.044: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004214935s
STEP: Saw pod success 03/30/23 14:43:07.044
Mar 30 14:43:07.044: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar 30 14:43:37.045: INFO: polling logs
Mar 30 14:43:37.049: INFO: Pod logs: 
I0330 14:43:03.562407       1 log.go:198] OK: Got token
I0330 14:43:03.562434       1 log.go:198] validating with in-cluster discovery
I0330 14:43:03.562659       1 log.go:198] OK: got issuer https://kubernetes.default.svc
I0330 14:43:03.562678       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6854:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1680187983, NotBefore:1680187383, IssuedAt:1680187383, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6854", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cc80aa76-4e1d-4e9b-b88b-92b6fe01a450"}}}
I0330 14:43:03.569313       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0330 14:43:03.573193       1 log.go:198] OK: Validated signature on JWT
I0330 14:43:03.573245       1 log.go:198] OK: Got valid claims from token!
I0330 14:43:03.573262       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6854:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1680187983, NotBefore:1680187383, IssuedAt:1680187383, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6854", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cc80aa76-4e1d-4e9b-b88b-92b6fe01a450"}}}

Mar 30 14:43:37.049: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 30 14:43:37.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6854" for this suite. 03/30/23 14:43:37.054
------------------------------
â€¢ [SLOW TEST] [34.035 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:43:03.021
    Mar 30 14:43:03.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename svcaccounts 03/30/23 14:43:03.023
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:43:03.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:43:03.031
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Mar 30 14:43:03.039: INFO: created pod
    Mar 30 14:43:03.039: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6854" to be "Succeeded or Failed"
    Mar 30 14:43:03.041: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35575ms
    Mar 30 14:43:05.043: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003686065s
    Mar 30 14:43:07.044: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004214935s
    STEP: Saw pod success 03/30/23 14:43:07.044
    Mar 30 14:43:07.044: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Mar 30 14:43:37.045: INFO: polling logs
    Mar 30 14:43:37.049: INFO: Pod logs: 
    I0330 14:43:03.562407       1 log.go:198] OK: Got token
    I0330 14:43:03.562434       1 log.go:198] validating with in-cluster discovery
    I0330 14:43:03.562659       1 log.go:198] OK: got issuer https://kubernetes.default.svc
    I0330 14:43:03.562678       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6854:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1680187983, NotBefore:1680187383, IssuedAt:1680187383, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6854", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cc80aa76-4e1d-4e9b-b88b-92b6fe01a450"}}}
    I0330 14:43:03.569313       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0330 14:43:03.573193       1 log.go:198] OK: Validated signature on JWT
    I0330 14:43:03.573245       1 log.go:198] OK: Got valid claims from token!
    I0330 14:43:03.573262       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6854:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1680187983, NotBefore:1680187383, IssuedAt:1680187383, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6854", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cc80aa76-4e1d-4e9b-b88b-92b6fe01a450"}}}

    Mar 30 14:43:37.049: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:43:37.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6854" for this suite. 03/30/23 14:43:37.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:43:37.057
Mar 30 14:43:37.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 14:43:37.057
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:43:37.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:43:37.066
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-33172754-6c1e-45e7-adc3-a5eff9ebe01b 03/30/23 14:43:37.069
STEP: Creating the pod 03/30/23 14:43:37.071
Mar 30 14:43:37.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-adf0767e-82cf-4f57-a943-8154b4f2397b" in namespace "projected-7663" to be "running and ready"
Mar 30 14:43:37.076: INFO: Pod "pod-projected-configmaps-adf0767e-82cf-4f57-a943-8154b4f2397b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.459412ms
Mar 30 14:43:37.076: INFO: The phase of Pod pod-projected-configmaps-adf0767e-82cf-4f57-a943-8154b4f2397b is Pending, waiting for it to be Running (with Ready = true)
Mar 30 14:43:39.078: INFO: Pod "pod-projected-configmaps-adf0767e-82cf-4f57-a943-8154b4f2397b": Phase="Running", Reason="", readiness=true. Elapsed: 2.003620614s
Mar 30 14:43:39.078: INFO: The phase of Pod pod-projected-configmaps-adf0767e-82cf-4f57-a943-8154b4f2397b is Running (Ready = true)
Mar 30 14:43:39.078: INFO: Pod "pod-projected-configmaps-adf0767e-82cf-4f57-a943-8154b4f2397b" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-33172754-6c1e-45e7-adc3-a5eff9ebe01b 03/30/23 14:43:39.083
STEP: waiting to observe update in volume 03/30/23 14:43:39.085
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 30 14:44:59.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7663" for this suite. 03/30/23 14:44:59.274
------------------------------
â€¢ [SLOW TEST] [82.220 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:43:37.057
    Mar 30 14:43:37.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 14:43:37.057
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:43:37.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:43:37.066
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-33172754-6c1e-45e7-adc3-a5eff9ebe01b 03/30/23 14:43:37.069
    STEP: Creating the pod 03/30/23 14:43:37.071
    Mar 30 14:43:37.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-adf0767e-82cf-4f57-a943-8154b4f2397b" in namespace "projected-7663" to be "running and ready"
    Mar 30 14:43:37.076: INFO: Pod "pod-projected-configmaps-adf0767e-82cf-4f57-a943-8154b4f2397b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.459412ms
    Mar 30 14:43:37.076: INFO: The phase of Pod pod-projected-configmaps-adf0767e-82cf-4f57-a943-8154b4f2397b is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 14:43:39.078: INFO: Pod "pod-projected-configmaps-adf0767e-82cf-4f57-a943-8154b4f2397b": Phase="Running", Reason="", readiness=true. Elapsed: 2.003620614s
    Mar 30 14:43:39.078: INFO: The phase of Pod pod-projected-configmaps-adf0767e-82cf-4f57-a943-8154b4f2397b is Running (Ready = true)
    Mar 30 14:43:39.078: INFO: Pod "pod-projected-configmaps-adf0767e-82cf-4f57-a943-8154b4f2397b" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-33172754-6c1e-45e7-adc3-a5eff9ebe01b 03/30/23 14:43:39.083
    STEP: waiting to observe update in volume 03/30/23 14:43:39.085
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:44:59.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7663" for this suite. 03/30/23 14:44:59.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:44:59.277
Mar 30 14:44:59.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename sched-preemption 03/30/23 14:44:59.278
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:44:59.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:44:59.286
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 30 14:44:59.293: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 30 14:45:59.320: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 03/30/23 14:45:59.321
Mar 30 14:45:59.331: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 30 14:45:59.334: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 30 14:45:59.343: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 30 14:45:59.346: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 30 14:45:59.354: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 30 14:45:59.357: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/30/23 14:45:59.357
Mar 30 14:45:59.357: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-407" to be "running"
Mar 30 14:45:59.359: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061192ms
Mar 30 14:46:01.362: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005185863s
Mar 30 14:46:01.362: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 30 14:46:01.362: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-407" to be "running"
Mar 30 14:46:01.364: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.761372ms
Mar 30 14:46:01.364: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 30 14:46:01.364: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-407" to be "running"
Mar 30 14:46:01.365: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.472506ms
Mar 30 14:46:01.365: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 30 14:46:01.365: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-407" to be "running"
Mar 30 14:46:01.367: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.521534ms
Mar 30 14:46:01.367: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 30 14:46:01.367: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-407" to be "running"
Mar 30 14:46:01.369: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.527269ms
Mar 30 14:46:01.369: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 30 14:46:01.369: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-407" to be "running"
Mar 30 14:46:01.370: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.288182ms
Mar 30 14:46:01.370: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/30/23 14:46:01.37
Mar 30 14:46:01.373: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-407" to be "running"
Mar 30 14:46:01.374: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.38766ms
Mar 30 14:46:03.377: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003868433s
Mar 30 14:46:05.377: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004165417s
Mar 30 14:46:07.377: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.003620372s
Mar 30 14:46:07.377: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:46:07.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-407" for this suite. 03/30/23 14:46:07.411
------------------------------
â€¢ [SLOW TEST] [68.136 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:44:59.277
    Mar 30 14:44:59.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename sched-preemption 03/30/23 14:44:59.278
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:44:59.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:44:59.286
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 30 14:44:59.293: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 30 14:45:59.320: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 03/30/23 14:45:59.321
    Mar 30 14:45:59.331: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 30 14:45:59.334: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 30 14:45:59.343: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 30 14:45:59.346: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 30 14:45:59.354: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 30 14:45:59.357: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/30/23 14:45:59.357
    Mar 30 14:45:59.357: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-407" to be "running"
    Mar 30 14:45:59.359: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061192ms
    Mar 30 14:46:01.362: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005185863s
    Mar 30 14:46:01.362: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 30 14:46:01.362: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-407" to be "running"
    Mar 30 14:46:01.364: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.761372ms
    Mar 30 14:46:01.364: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 30 14:46:01.364: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-407" to be "running"
    Mar 30 14:46:01.365: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.472506ms
    Mar 30 14:46:01.365: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 30 14:46:01.365: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-407" to be "running"
    Mar 30 14:46:01.367: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.521534ms
    Mar 30 14:46:01.367: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 30 14:46:01.367: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-407" to be "running"
    Mar 30 14:46:01.369: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.527269ms
    Mar 30 14:46:01.369: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 30 14:46:01.369: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-407" to be "running"
    Mar 30 14:46:01.370: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.288182ms
    Mar 30 14:46:01.370: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/30/23 14:46:01.37
    Mar 30 14:46:01.373: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-407" to be "running"
    Mar 30 14:46:01.374: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.38766ms
    Mar 30 14:46:03.377: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003868433s
    Mar 30 14:46:05.377: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004165417s
    Mar 30 14:46:07.377: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.003620372s
    Mar 30 14:46:07.377: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:46:07.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-407" for this suite. 03/30/23 14:46:07.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:46:07.414
Mar 30 14:46:07.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename deployment 03/30/23 14:46:07.414
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:07.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:07.422
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Mar 30 14:46:07.424: INFO: Creating simple deployment test-new-deployment
Mar 30 14:46:07.429: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 03/30/23 14:46:09.439
STEP: updating a scale subresource 03/30/23 14:46:09.441
STEP: verifying the deployment Spec.Replicas was modified 03/30/23 14:46:09.444
STEP: Patch a scale subresource 03/30/23 14:46:09.445
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 30 14:46:09.453: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9629  e0007582-fe06-47e7-bd4b-8e8f9b42c0e7 102402 3 2023-03-30 14:46:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-30 14:46:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 14:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00987a2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-30 14:46:08 +0000 UTC,LastTransitionTime:2023-03-30 14:46:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-03-30 14:46:08 +0000 UTC,LastTransitionTime:2023-03-30 14:46:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 30 14:46:09.455: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9629  07ccc181-8a33-4eaa-b436-61daa68f5090 102407 2 2023-03-30 14:46:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment e0007582-fe06-47e7-bd4b-8e8f9b42c0e7 0xc0047a8677 0xc0047a8678}] [] [{kube-controller-manager Update apps/v1 2023-03-30 14:46:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0007582-fe06-47e7-bd4b-8e8f9b42c0e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 14:46:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047a8708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 30 14:46:09.457: INFO: Pod "test-new-deployment-7f5969cbc7-dd5dw" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-dd5dw test-new-deployment-7f5969cbc7- deployment-9629  9bf24567-f009-4f6d-9ea3-15434e9a7dfe 102406 0 2023-03-30 14:46:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 07ccc181-8a33-4eaa-b436-61daa68f5090 0xc0047a8af7 0xc0047a8af8}] [] [{kube-controller-manager Update v1 2023-03-30 14:46:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07ccc181-8a33-4eaa-b436-61daa68f5090\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4spt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4spt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:46:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 30 14:46:09.457: INFO: Pod "test-new-deployment-7f5969cbc7-zxpjj" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-zxpjj test-new-deployment-7f5969cbc7- deployment-9629  985562e3-f59f-443d-ae93-656efe680875 102388 0 2023-03-30 14:46:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 07ccc181-8a33-4eaa-b436-61daa68f5090 0xc0047a8c50 0xc0047a8c51}] [] [{kube-controller-manager Update v1 2023-03-30 14:46:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07ccc181-8a33-4eaa-b436-61daa68f5090\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 14:46:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5gtqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5gtqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:46:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:46:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:46:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:46:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.51,StartTime:2023-03-30 14:46:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 14:46:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bdc7f935e73eeee57a3a5b3c1b81e0636bdccf64fc7e4d7a3bec2bdf3b271d33,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 30 14:46:09.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9629" for this suite. 03/30/23 14:46:09.46
------------------------------
â€¢ [2.049 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:46:07.414
    Mar 30 14:46:07.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename deployment 03/30/23 14:46:07.414
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:07.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:07.422
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Mar 30 14:46:07.424: INFO: Creating simple deployment test-new-deployment
    Mar 30 14:46:07.429: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 03/30/23 14:46:09.439
    STEP: updating a scale subresource 03/30/23 14:46:09.441
    STEP: verifying the deployment Spec.Replicas was modified 03/30/23 14:46:09.444
    STEP: Patch a scale subresource 03/30/23 14:46:09.445
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 30 14:46:09.453: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9629  e0007582-fe06-47e7-bd4b-8e8f9b42c0e7 102402 3 2023-03-30 14:46:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-30 14:46:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 14:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00987a2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-30 14:46:08 +0000 UTC,LastTransitionTime:2023-03-30 14:46:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-03-30 14:46:08 +0000 UTC,LastTransitionTime:2023-03-30 14:46:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 30 14:46:09.455: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9629  07ccc181-8a33-4eaa-b436-61daa68f5090 102407 2 2023-03-30 14:46:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment e0007582-fe06-47e7-bd4b-8e8f9b42c0e7 0xc0047a8677 0xc0047a8678}] [] [{kube-controller-manager Update apps/v1 2023-03-30 14:46:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0007582-fe06-47e7-bd4b-8e8f9b42c0e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-30 14:46:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047a8708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 30 14:46:09.457: INFO: Pod "test-new-deployment-7f5969cbc7-dd5dw" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-dd5dw test-new-deployment-7f5969cbc7- deployment-9629  9bf24567-f009-4f6d-9ea3-15434e9a7dfe 102406 0 2023-03-30 14:46:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 07ccc181-8a33-4eaa-b436-61daa68f5090 0xc0047a8af7 0xc0047a8af8}] [] [{kube-controller-manager Update v1 2023-03-30 14:46:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07ccc181-8a33-4eaa-b436-61daa68f5090\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4spt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4spt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:46:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 30 14:46:09.457: INFO: Pod "test-new-deployment-7f5969cbc7-zxpjj" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-zxpjj test-new-deployment-7f5969cbc7- deployment-9629  985562e3-f59f-443d-ae93-656efe680875 102388 0 2023-03-30 14:46:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 07ccc181-8a33-4eaa-b436-61daa68f5090 0xc0047a8c50 0xc0047a8c51}] [] [{kube-controller-manager Update v1 2023-03-30 14:46:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07ccc181-8a33-4eaa-b436-61daa68f5090\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-30 14:46:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.29.1.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5gtqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5gtqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:46:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:46:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:46:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-30 14:46:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.29.1.51,StartTime:2023-03-30 14:46:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-30 14:46:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bdc7f935e73eeee57a3a5b3c1b81e0636bdccf64fc7e4d7a3bec2bdf3b271d33,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.29.1.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:46:09.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9629" for this suite. 03/30/23 14:46:09.46
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:46:09.462
Mar 30 14:46:09.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename containers 03/30/23 14:46:09.463
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:09.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:09.481
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 03/30/23 14:46:09.483
Mar 30 14:46:09.488: INFO: Waiting up to 5m0s for pod "client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035" in namespace "containers-187" to be "Succeeded or Failed"
Mar 30 14:46:09.490: INFO: Pod "client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.448522ms
Mar 30 14:46:11.493: INFO: Pod "client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005382116s
Mar 30 14:46:13.493: INFO: Pod "client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004954104s
STEP: Saw pod success 03/30/23 14:46:13.493
Mar 30 14:46:13.493: INFO: Pod "client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035" satisfied condition "Succeeded or Failed"
Mar 30 14:46:13.496: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035 container agnhost-container: <nil>
STEP: delete the pod 03/30/23 14:46:13.499
Mar 30 14:46:13.503: INFO: Waiting for pod client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035 to disappear
Mar 30 14:46:13.504: INFO: Pod client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 30 14:46:13.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-187" for this suite. 03/30/23 14:46:13.507
------------------------------
â€¢ [4.047 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:46:09.462
    Mar 30 14:46:09.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename containers 03/30/23 14:46:09.463
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:09.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:09.481
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 03/30/23 14:46:09.483
    Mar 30 14:46:09.488: INFO: Waiting up to 5m0s for pod "client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035" in namespace "containers-187" to be "Succeeded or Failed"
    Mar 30 14:46:09.490: INFO: Pod "client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.448522ms
    Mar 30 14:46:11.493: INFO: Pod "client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005382116s
    Mar 30 14:46:13.493: INFO: Pod "client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004954104s
    STEP: Saw pod success 03/30/23 14:46:13.493
    Mar 30 14:46:13.493: INFO: Pod "client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035" satisfied condition "Succeeded or Failed"
    Mar 30 14:46:13.496: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035 container agnhost-container: <nil>
    STEP: delete the pod 03/30/23 14:46:13.499
    Mar 30 14:46:13.503: INFO: Waiting for pod client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035 to disappear
    Mar 30 14:46:13.504: INFO: Pod client-containers-2822bfe9-6467-4ce0-b3f2-47172856a035 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:46:13.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-187" for this suite. 03/30/23 14:46:13.507
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:46:13.509
Mar 30 14:46:13.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename secrets 03/30/23 14:46:13.51
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:13.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:13.518
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 03/30/23 14:46:13.52
STEP: listing secrets in all namespaces to ensure that there are more than zero 03/30/23 14:46:13.522
STEP: patching the secret 03/30/23 14:46:13.524
STEP: deleting the secret using a LabelSelector 03/30/23 14:46:13.528
STEP: listing secrets in all namespaces, searching for label name and value in patch 03/30/23 14:46:13.531
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 30 14:46:13.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2261" for this suite. 03/30/23 14:46:13.536
------------------------------
â€¢ [0.029 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:46:13.509
    Mar 30 14:46:13.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename secrets 03/30/23 14:46:13.51
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:13.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:13.518
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 03/30/23 14:46:13.52
    STEP: listing secrets in all namespaces to ensure that there are more than zero 03/30/23 14:46:13.522
    STEP: patching the secret 03/30/23 14:46:13.524
    STEP: deleting the secret using a LabelSelector 03/30/23 14:46:13.528
    STEP: listing secrets in all namespaces, searching for label name and value in patch 03/30/23 14:46:13.531
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:46:13.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2261" for this suite. 03/30/23 14:46:13.536
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:46:13.539
Mar 30 14:46:13.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename ephemeral-containers-test 03/30/23 14:46:13.539
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:13.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:13.547
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 03/30/23 14:46:13.549
Mar 30 14:46:13.552: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6420" to be "running and ready"
Mar 30 14:46:13.554: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.942728ms
Mar 30 14:46:13.554: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 30 14:46:15.556: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004184769s
Mar 30 14:46:15.556: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Mar 30 14:46:15.556: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 03/30/23 14:46:15.558
Mar 30 14:46:15.564: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6420" to be "container debugger running"
Mar 30 14:46:15.566: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.377622ms
Mar 30 14:46:17.568: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003587716s
Mar 30 14:46:17.568: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 03/30/23 14:46:17.568
Mar 30 14:46:17.568: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6420 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 30 14:46:17.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
Mar 30 14:46:17.568: INFO: ExecWithOptions: Clientset creation
Mar 30 14:46:17.568: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/ephemeral-containers-test-6420/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Mar 30 14:46:17.610: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:46:17.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-6420" for this suite. 03/30/23 14:46:17.616
------------------------------
â€¢ [4.081 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:46:13.539
    Mar 30 14:46:13.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename ephemeral-containers-test 03/30/23 14:46:13.539
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:13.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:13.547
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 03/30/23 14:46:13.549
    Mar 30 14:46:13.552: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6420" to be "running and ready"
    Mar 30 14:46:13.554: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.942728ms
    Mar 30 14:46:13.554: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 14:46:15.556: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004184769s
    Mar 30 14:46:15.556: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Mar 30 14:46:15.556: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 03/30/23 14:46:15.558
    Mar 30 14:46:15.564: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6420" to be "container debugger running"
    Mar 30 14:46:15.566: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.377622ms
    Mar 30 14:46:17.568: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003587716s
    Mar 30 14:46:17.568: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 03/30/23 14:46:17.568
    Mar 30 14:46:17.568: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6420 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 30 14:46:17.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    Mar 30 14:46:17.568: INFO: ExecWithOptions: Clientset creation
    Mar 30 14:46:17.568: INFO: ExecWithOptions: execute(POST https://172.16.0.1:443/api/v1/namespaces/ephemeral-containers-test-6420/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Mar 30 14:46:17.610: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:46:17.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-6420" for this suite. 03/30/23 14:46:17.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:46:17.62
Mar 30 14:46:17.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename pods 03/30/23 14:46:17.621
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:17.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:17.63
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Mar 30 14:46:17.634: INFO: Waiting up to 5m0s for pod "server-envvars-cbb24c94-ec02-4a2e-a660-ed8429d12250" in namespace "pods-4883" to be "running and ready"
Mar 30 14:46:17.636: INFO: Pod "server-envvars-cbb24c94-ec02-4a2e-a660-ed8429d12250": Phase="Pending", Reason="", readiness=false. Elapsed: 1.440942ms
Mar 30 14:46:17.636: INFO: The phase of Pod server-envvars-cbb24c94-ec02-4a2e-a660-ed8429d12250 is Pending, waiting for it to be Running (with Ready = true)
Mar 30 14:46:19.639: INFO: Pod "server-envvars-cbb24c94-ec02-4a2e-a660-ed8429d12250": Phase="Running", Reason="", readiness=true. Elapsed: 2.004245575s
Mar 30 14:46:19.639: INFO: The phase of Pod server-envvars-cbb24c94-ec02-4a2e-a660-ed8429d12250 is Running (Ready = true)
Mar 30 14:46:19.639: INFO: Pod "server-envvars-cbb24c94-ec02-4a2e-a660-ed8429d12250" satisfied condition "running and ready"
Mar 30 14:46:19.647: INFO: Waiting up to 5m0s for pod "client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03" in namespace "pods-4883" to be "Succeeded or Failed"
Mar 30 14:46:19.649: INFO: Pod "client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.265639ms
Mar 30 14:46:21.653: INFO: Pod "client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005402742s
Mar 30 14:46:23.652: INFO: Pod "client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005319413s
STEP: Saw pod success 03/30/23 14:46:23.652
Mar 30 14:46:23.653: INFO: Pod "client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03" satisfied condition "Succeeded or Failed"
Mar 30 14:46:23.654: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03 container env3cont: <nil>
STEP: delete the pod 03/30/23 14:46:23.658
Mar 30 14:46:23.663: INFO: Waiting for pod client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03 to disappear
Mar 30 14:46:23.664: INFO: Pod client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 30 14:46:23.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4883" for this suite. 03/30/23 14:46:23.667
------------------------------
â€¢ [SLOW TEST] [6.049 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:46:17.62
    Mar 30 14:46:17.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename pods 03/30/23 14:46:17.621
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:17.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:17.63
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Mar 30 14:46:17.634: INFO: Waiting up to 5m0s for pod "server-envvars-cbb24c94-ec02-4a2e-a660-ed8429d12250" in namespace "pods-4883" to be "running and ready"
    Mar 30 14:46:17.636: INFO: Pod "server-envvars-cbb24c94-ec02-4a2e-a660-ed8429d12250": Phase="Pending", Reason="", readiness=false. Elapsed: 1.440942ms
    Mar 30 14:46:17.636: INFO: The phase of Pod server-envvars-cbb24c94-ec02-4a2e-a660-ed8429d12250 is Pending, waiting for it to be Running (with Ready = true)
    Mar 30 14:46:19.639: INFO: Pod "server-envvars-cbb24c94-ec02-4a2e-a660-ed8429d12250": Phase="Running", Reason="", readiness=true. Elapsed: 2.004245575s
    Mar 30 14:46:19.639: INFO: The phase of Pod server-envvars-cbb24c94-ec02-4a2e-a660-ed8429d12250 is Running (Ready = true)
    Mar 30 14:46:19.639: INFO: Pod "server-envvars-cbb24c94-ec02-4a2e-a660-ed8429d12250" satisfied condition "running and ready"
    Mar 30 14:46:19.647: INFO: Waiting up to 5m0s for pod "client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03" in namespace "pods-4883" to be "Succeeded or Failed"
    Mar 30 14:46:19.649: INFO: Pod "client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.265639ms
    Mar 30 14:46:21.653: INFO: Pod "client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005402742s
    Mar 30 14:46:23.652: INFO: Pod "client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005319413s
    STEP: Saw pod success 03/30/23 14:46:23.652
    Mar 30 14:46:23.653: INFO: Pod "client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03" satisfied condition "Succeeded or Failed"
    Mar 30 14:46:23.654: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03 container env3cont: <nil>
    STEP: delete the pod 03/30/23 14:46:23.658
    Mar 30 14:46:23.663: INFO: Waiting for pod client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03 to disappear
    Mar 30 14:46:23.664: INFO: Pod client-envvars-10c4e2b7-ed1c-4e56-819a-796c09f50e03 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:46:23.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4883" for this suite. 03/30/23 14:46:23.667
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:46:23.669
Mar 30 14:46:23.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename job 03/30/23 14:46:23.67
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:23.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:23.678
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 03/30/23 14:46:23.679
STEP: Ensuring active pods == parallelism 03/30/23 14:46:23.682
STEP: delete a job 03/30/23 14:46:25.684
STEP: deleting Job.batch foo in namespace job-2058, will wait for the garbage collector to delete the pods 03/30/23 14:46:25.684
Mar 30 14:46:25.740: INFO: Deleting Job.batch foo took: 2.627475ms
Mar 30 14:46:25.840: INFO: Terminating Job.batch foo pods took: 100.658338ms
STEP: Ensuring job was deleted 03/30/23 14:46:58.341
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 30 14:46:58.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2058" for this suite. 03/30/23 14:46:58.345
------------------------------
â€¢ [SLOW TEST] [34.678 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:46:23.669
    Mar 30 14:46:23.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename job 03/30/23 14:46:23.67
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:23.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:23.678
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 03/30/23 14:46:23.679
    STEP: Ensuring active pods == parallelism 03/30/23 14:46:23.682
    STEP: delete a job 03/30/23 14:46:25.684
    STEP: deleting Job.batch foo in namespace job-2058, will wait for the garbage collector to delete the pods 03/30/23 14:46:25.684
    Mar 30 14:46:25.740: INFO: Deleting Job.batch foo took: 2.627475ms
    Mar 30 14:46:25.840: INFO: Terminating Job.batch foo pods took: 100.658338ms
    STEP: Ensuring job was deleted 03/30/23 14:46:58.341
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:46:58.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2058" for this suite. 03/30/23 14:46:58.345
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:46:58.347
Mar 30 14:46:58.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename events 03/30/23 14:46:58.348
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:58.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:58.356
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 03/30/23 14:46:58.358
Mar 30 14:46:58.360: INFO: created test-event-1
Mar 30 14:46:58.361: INFO: created test-event-2
Mar 30 14:46:58.363: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 03/30/23 14:46:58.363
STEP: delete collection of events 03/30/23 14:46:58.365
Mar 30 14:46:58.365: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/30/23 14:46:58.373
Mar 30 14:46:58.373: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Mar 30 14:46:58.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8111" for this suite. 03/30/23 14:46:58.377
------------------------------
â€¢ [0.032 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:46:58.347
    Mar 30 14:46:58.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename events 03/30/23 14:46:58.348
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:58.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:58.356
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 03/30/23 14:46:58.358
    Mar 30 14:46:58.360: INFO: created test-event-1
    Mar 30 14:46:58.361: INFO: created test-event-2
    Mar 30 14:46:58.363: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 03/30/23 14:46:58.363
    STEP: delete collection of events 03/30/23 14:46:58.365
    Mar 30 14:46:58.365: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/30/23 14:46:58.373
    Mar 30 14:46:58.373: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:46:58.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8111" for this suite. 03/30/23 14:46:58.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:46:58.38
Mar 30 14:46:58.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename subpath 03/30/23 14:46:58.38
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:58.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:58.389
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/30/23 14:46:58.39
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-qvcg 03/30/23 14:46:58.394
STEP: Creating a pod to test atomic-volume-subpath 03/30/23 14:46:58.394
Mar 30 14:46:58.398: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qvcg" in namespace "subpath-7330" to be "Succeeded or Failed"
Mar 30 14:46:58.399: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.422449ms
Mar 30 14:47:00.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004322417s
Mar 30 14:47:02.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 4.003988624s
Mar 30 14:47:04.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 6.004120898s
Mar 30 14:47:06.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 8.004414432s
Mar 30 14:47:08.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 10.003823957s
Mar 30 14:47:10.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 12.003921471s
Mar 30 14:47:12.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 14.004243166s
Mar 30 14:47:14.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 16.004263192s
Mar 30 14:47:16.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 18.004228658s
Mar 30 14:47:18.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 20.004683152s
Mar 30 14:47:20.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=false. Elapsed: 22.00390527s
Mar 30 14:47:22.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004139481s
STEP: Saw pod success 03/30/23 14:47:22.402
Mar 30 14:47:22.402: INFO: Pod "pod-subpath-test-secret-qvcg" satisfied condition "Succeeded or Failed"
Mar 30 14:47:22.403: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-subpath-test-secret-qvcg container test-container-subpath-secret-qvcg: <nil>
STEP: delete the pod 03/30/23 14:47:22.407
Mar 30 14:47:22.413: INFO: Waiting for pod pod-subpath-test-secret-qvcg to disappear
Mar 30 14:47:22.414: INFO: Pod pod-subpath-test-secret-qvcg no longer exists
STEP: Deleting pod pod-subpath-test-secret-qvcg 03/30/23 14:47:22.414
Mar 30 14:47:22.414: INFO: Deleting pod "pod-subpath-test-secret-qvcg" in namespace "subpath-7330"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 30 14:47:22.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7330" for this suite. 03/30/23 14:47:22.418
------------------------------
â€¢ [SLOW TEST] [24.040 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:46:58.38
    Mar 30 14:46:58.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename subpath 03/30/23 14:46:58.38
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:46:58.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:46:58.389
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/30/23 14:46:58.39
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-qvcg 03/30/23 14:46:58.394
    STEP: Creating a pod to test atomic-volume-subpath 03/30/23 14:46:58.394
    Mar 30 14:46:58.398: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qvcg" in namespace "subpath-7330" to be "Succeeded or Failed"
    Mar 30 14:46:58.399: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.422449ms
    Mar 30 14:47:00.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004322417s
    Mar 30 14:47:02.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 4.003988624s
    Mar 30 14:47:04.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 6.004120898s
    Mar 30 14:47:06.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 8.004414432s
    Mar 30 14:47:08.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 10.003823957s
    Mar 30 14:47:10.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 12.003921471s
    Mar 30 14:47:12.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 14.004243166s
    Mar 30 14:47:14.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 16.004263192s
    Mar 30 14:47:16.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 18.004228658s
    Mar 30 14:47:18.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=true. Elapsed: 20.004683152s
    Mar 30 14:47:20.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Running", Reason="", readiness=false. Elapsed: 22.00390527s
    Mar 30 14:47:22.402: INFO: Pod "pod-subpath-test-secret-qvcg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004139481s
    STEP: Saw pod success 03/30/23 14:47:22.402
    Mar 30 14:47:22.402: INFO: Pod "pod-subpath-test-secret-qvcg" satisfied condition "Succeeded or Failed"
    Mar 30 14:47:22.403: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-subpath-test-secret-qvcg container test-container-subpath-secret-qvcg: <nil>
    STEP: delete the pod 03/30/23 14:47:22.407
    Mar 30 14:47:22.413: INFO: Waiting for pod pod-subpath-test-secret-qvcg to disappear
    Mar 30 14:47:22.414: INFO: Pod pod-subpath-test-secret-qvcg no longer exists
    STEP: Deleting pod pod-subpath-test-secret-qvcg 03/30/23 14:47:22.414
    Mar 30 14:47:22.414: INFO: Deleting pod "pod-subpath-test-secret-qvcg" in namespace "subpath-7330"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:47:22.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7330" for this suite. 03/30/23 14:47:22.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:47:22.42
Mar 30 14:47:22.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename projected 03/30/23 14:47:22.421
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:47:22.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:47:22.429
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 03/30/23 14:47:22.431
Mar 30 14:47:22.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402" in namespace "projected-2190" to be "Succeeded or Failed"
Mar 30 14:47:22.435: INFO: Pod "downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335964ms
Mar 30 14:47:24.438: INFO: Pod "downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003618978s
Mar 30 14:47:26.438: INFO: Pod "downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003929161s
STEP: Saw pod success 03/30/23 14:47:26.438
Mar 30 14:47:26.438: INFO: Pod "downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402" satisfied condition "Succeeded or Failed"
Mar 30 14:47:26.440: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402 container client-container: <nil>
STEP: delete the pod 03/30/23 14:47:26.445
Mar 30 14:47:26.451: INFO: Waiting for pod downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402 to disappear
Mar 30 14:47:26.453: INFO: Pod downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 30 14:47:26.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2190" for this suite. 03/30/23 14:47:26.455
------------------------------
â€¢ [4.037 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:47:22.42
    Mar 30 14:47:22.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename projected 03/30/23 14:47:22.421
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:47:22.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:47:22.429
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 03/30/23 14:47:22.431
    Mar 30 14:47:22.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402" in namespace "projected-2190" to be "Succeeded or Failed"
    Mar 30 14:47:22.435: INFO: Pod "downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335964ms
    Mar 30 14:47:24.438: INFO: Pod "downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003618978s
    Mar 30 14:47:26.438: INFO: Pod "downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003929161s
    STEP: Saw pod success 03/30/23 14:47:26.438
    Mar 30 14:47:26.438: INFO: Pod "downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402" satisfied condition "Succeeded or Failed"
    Mar 30 14:47:26.440: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402 container client-container: <nil>
    STEP: delete the pod 03/30/23 14:47:26.445
    Mar 30 14:47:26.451: INFO: Waiting for pod downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402 to disappear
    Mar 30 14:47:26.453: INFO: Pod downwardapi-volume-33474921-9807-4127-9c2a-4ed6fb527402 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:47:26.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2190" for this suite. 03/30/23 14:47:26.455
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:47:26.458
Mar 30 14:47:26.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename emptydir 03/30/23 14:47:26.458
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:47:26.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:47:26.466
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/30/23 14:47:26.468
Mar 30 14:47:26.472: INFO: Waiting up to 5m0s for pod "pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2" in namespace "emptydir-3499" to be "Succeeded or Failed"
Mar 30 14:47:26.473: INFO: Pod "pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.288623ms
Mar 30 14:47:28.475: INFO: Pod "pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003501107s
Mar 30 14:47:30.475: INFO: Pod "pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003377459s
STEP: Saw pod success 03/30/23 14:47:30.475
Mar 30 14:47:30.475: INFO: Pod "pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2" satisfied condition "Succeeded or Failed"
Mar 30 14:47:30.477: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2 container test-container: <nil>
STEP: delete the pod 03/30/23 14:47:30.48
Mar 30 14:47:30.484: INFO: Waiting for pod pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2 to disappear
Mar 30 14:47:30.485: INFO: Pod pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 30 14:47:30.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3499" for this suite. 03/30/23 14:47:30.488
------------------------------
â€¢ [4.033 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:47:26.458
    Mar 30 14:47:26.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename emptydir 03/30/23 14:47:26.458
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:47:26.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:47:26.466
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/30/23 14:47:26.468
    Mar 30 14:47:26.472: INFO: Waiting up to 5m0s for pod "pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2" in namespace "emptydir-3499" to be "Succeeded or Failed"
    Mar 30 14:47:26.473: INFO: Pod "pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.288623ms
    Mar 30 14:47:28.475: INFO: Pod "pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003501107s
    Mar 30 14:47:30.475: INFO: Pod "pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003377459s
    STEP: Saw pod success 03/30/23 14:47:30.475
    Mar 30 14:47:30.475: INFO: Pod "pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2" satisfied condition "Succeeded or Failed"
    Mar 30 14:47:30.477: INFO: Trying to get logs from node cn-hongkong.192.168.0.5 pod pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2 container test-container: <nil>
    STEP: delete the pod 03/30/23 14:47:30.48
    Mar 30 14:47:30.484: INFO: Waiting for pod pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2 to disappear
    Mar 30 14:47:30.485: INFO: Pod pod-a37d6d03-ea41-4ffb-8e1f-fac97fa792a2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:47:30.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3499" for this suite. 03/30/23 14:47:30.488
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:47:30.49
Mar 30 14:47:30.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename webhook 03/30/23 14:47:30.491
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:47:30.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:47:30.498
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/30/23 14:47:30.505
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:47:31.061
STEP: Deploying the webhook pod 03/30/23 14:47:31.064
STEP: Wait for the deployment to be ready 03/30/23 14:47:31.069
Mar 30 14:47:31.072: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 30 14:47:33.078: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:47:35.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:47:37.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:47:39.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 30 14:47:41.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/30/23 14:47:43.08
STEP: Verifying the service has paired with the endpoint 03/30/23 14:47:43.085
Mar 30 14:47:44.086: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Mar 30 14:47:44.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3224-crds.webhook.example.com via the AdmissionRegistration API 03/30/23 14:47:49.594
STEP: Creating a custom resource while v1 is storage version 03/30/23 14:47:49.603
STEP: Patching Custom Resource Definition to set v2 as storage 03/30/23 14:47:51.636
STEP: Patching the custom resource while v2 is storage version 03/30/23 14:47:51.64
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:47:52.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-883" for this suite. 03/30/23 14:47:52.193
STEP: Destroying namespace "webhook-883-markers" for this suite. 03/30/23 14:47:52.197
------------------------------
â€¢ [SLOW TEST] [21.709 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:47:30.49
    Mar 30 14:47:30.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename webhook 03/30/23 14:47:30.491
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:47:30.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:47:30.498
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/30/23 14:47:30.505
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/30/23 14:47:31.061
    STEP: Deploying the webhook pod 03/30/23 14:47:31.064
    STEP: Wait for the deployment to be ready 03/30/23 14:47:31.069
    Mar 30 14:47:31.072: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 30 14:47:33.078: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:47:35.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:47:37.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:47:39.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 30 14:47:41.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 30, 14, 47, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/30/23 14:47:43.08
    STEP: Verifying the service has paired with the endpoint 03/30/23 14:47:43.085
    Mar 30 14:47:44.086: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Mar 30 14:47:44.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3224-crds.webhook.example.com via the AdmissionRegistration API 03/30/23 14:47:49.594
    STEP: Creating a custom resource while v1 is storage version 03/30/23 14:47:49.603
    STEP: Patching Custom Resource Definition to set v2 as storage 03/30/23 14:47:51.636
    STEP: Patching the custom resource while v2 is storage version 03/30/23 14:47:51.64
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:47:52.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-883" for this suite. 03/30/23 14:47:52.193
    STEP: Destroying namespace "webhook-883-markers" for this suite. 03/30/23 14:47:52.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:47:52.201
Mar 30 14:47:52.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename resourcequota 03/30/23 14:47:52.201
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:47:52.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:47:52.21
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 03/30/23 14:47:52.212
STEP: Ensuring ResourceQuota status is calculated 03/30/23 14:47:52.214
STEP: Creating a ResourceQuota with not best effort scope 03/30/23 14:47:54.217
STEP: Ensuring ResourceQuota status is calculated 03/30/23 14:47:54.22
STEP: Creating a best-effort pod 03/30/23 14:47:56.223
STEP: Ensuring resource quota with best effort scope captures the pod usage 03/30/23 14:47:56.229
STEP: Ensuring resource quota with not best effort ignored the pod usage 03/30/23 14:47:58.232
STEP: Deleting the pod 03/30/23 14:48:00.235
STEP: Ensuring resource quota status released the pod usage 03/30/23 14:48:00.239
STEP: Creating a not best-effort pod 03/30/23 14:48:02.242
STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/30/23 14:48:02.247
STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/30/23 14:48:04.25
STEP: Deleting the pod 03/30/23 14:48:06.253
STEP: Ensuring resource quota status released the pod usage 03/30/23 14:48:06.26
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 30 14:48:08.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6103" for this suite. 03/30/23 14:48:08.264
------------------------------
â€¢ [SLOW TEST] [16.066 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:47:52.201
    Mar 30 14:47:52.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename resourcequota 03/30/23 14:47:52.201
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:47:52.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:47:52.21
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 03/30/23 14:47:52.212
    STEP: Ensuring ResourceQuota status is calculated 03/30/23 14:47:52.214
    STEP: Creating a ResourceQuota with not best effort scope 03/30/23 14:47:54.217
    STEP: Ensuring ResourceQuota status is calculated 03/30/23 14:47:54.22
    STEP: Creating a best-effort pod 03/30/23 14:47:56.223
    STEP: Ensuring resource quota with best effort scope captures the pod usage 03/30/23 14:47:56.229
    STEP: Ensuring resource quota with not best effort ignored the pod usage 03/30/23 14:47:58.232
    STEP: Deleting the pod 03/30/23 14:48:00.235
    STEP: Ensuring resource quota status released the pod usage 03/30/23 14:48:00.239
    STEP: Creating a not best-effort pod 03/30/23 14:48:02.242
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/30/23 14:48:02.247
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/30/23 14:48:04.25
    STEP: Deleting the pod 03/30/23 14:48:06.253
    STEP: Ensuring resource quota status released the pod usage 03/30/23 14:48:06.26
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:48:08.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6103" for this suite. 03/30/23 14:48:08.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:48:08.268
Mar 30 14:48:08.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename sched-preemption 03/30/23 14:48:08.269
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:48:08.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:48:08.277
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 30 14:48:08.284: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 30 14:49:08.309: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:49:08.311
Mar 30 14:49:08.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename sched-preemption-path 03/30/23 14:49:08.311
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:49:08.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:49:08.32
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 03/30/23 14:49:08.322
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/30/23 14:49:08.322
Mar 30 14:49:08.325: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9936" to be "running"
Mar 30 14:49:08.326: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.365336ms
Mar 30 14:49:10.329: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003597607s
Mar 30 14:49:10.329: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/30/23 14:49:10.33
Mar 30 14:49:10.335: INFO: found a healthy node: cn-hongkong.192.168.0.5
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Mar 30 14:49:16.369: INFO: pods created so far: [1 1 1]
Mar 30 14:49:16.369: INFO: length of pods created so far: 3
Mar 30 14:49:18.374: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Mar 30 14:49:25.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 30 14:49:25.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-9936" for this suite. 03/30/23 14:49:25.423
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-2432" for this suite. 03/30/23 14:49:25.426
------------------------------
â€¢ [SLOW TEST] [77.161 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:48:08.268
    Mar 30 14:48:08.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename sched-preemption 03/30/23 14:48:08.269
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:48:08.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:48:08.277
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 30 14:48:08.284: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 30 14:49:08.309: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:49:08.311
    Mar 30 14:49:08.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename sched-preemption-path 03/30/23 14:49:08.311
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:49:08.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:49:08.32
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 03/30/23 14:49:08.322
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/30/23 14:49:08.322
    Mar 30 14:49:08.325: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9936" to be "running"
    Mar 30 14:49:08.326: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.365336ms
    Mar 30 14:49:10.329: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003597607s
    Mar 30 14:49:10.329: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/30/23 14:49:10.33
    Mar 30 14:49:10.335: INFO: found a healthy node: cn-hongkong.192.168.0.5
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Mar 30 14:49:16.369: INFO: pods created so far: [1 1 1]
    Mar 30 14:49:16.369: INFO: length of pods created so far: 3
    Mar 30 14:49:18.374: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:49:25.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:49:25.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-9936" for this suite. 03/30/23 14:49:25.423
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-2432" for this suite. 03/30/23 14:49:25.426
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:49:25.429
Mar 30 14:49:25.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename kubectl 03/30/23 14:49:25.43
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:49:25.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:49:25.437
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/30/23 14:49:25.439
Mar 30 14:49:25.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-5131 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 30 14:49:25.495: INFO: stderr: ""
Mar 30 14:49:25.495: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 03/30/23 14:49:25.495
STEP: verifying the pod e2e-test-httpd-pod was created 03/30/23 14:49:30.549
Mar 30 14:49:30.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-5131 get pod e2e-test-httpd-pod -o json'
Mar 30 14:49:30.597: INFO: stderr: ""
Mar 30 14:49:30.597: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-03-30T14:49:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5131\",\n        \"resourceVersion\": \"103931\",\n        \"uid\": \"c73a35a3-b7e3-40f7-b08e-1859f7e2b4af\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-ql2r8\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cn-hongkong.192.168.0.5\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-ql2r8\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-30T14:49:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-30T14:49:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-30T14:49:26Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-30T14:49:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://942135b3e868f83cc6aa91431dd2bf9376a9e65f71185a3681d3aa92b2b48c30\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-30T14:49:26Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.29.1.7\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.29.1.7\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-30T14:49:25Z\"\n    }\n}\n"
STEP: replace the image in the pod 03/30/23 14:49:30.597
Mar 30 14:49:30.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-5131 replace -f -'
Mar 30 14:49:31.158: INFO: stderr: ""
Mar 30 14:49:31.158: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 03/30/23 14:49:31.158
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Mar 30 14:49:31.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-5131 delete pods e2e-test-httpd-pod'
Mar 30 14:49:32.487: INFO: stderr: ""
Mar 30 14:49:32.487: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 30 14:49:32.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5131" for this suite. 03/30/23 14:49:32.489
------------------------------
â€¢ [SLOW TEST] [7.063 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:49:25.429
    Mar 30 14:49:25.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename kubectl 03/30/23 14:49:25.43
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:49:25.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:49:25.437
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/30/23 14:49:25.439
    Mar 30 14:49:25.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-5131 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 30 14:49:25.495: INFO: stderr: ""
    Mar 30 14:49:25.495: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 03/30/23 14:49:25.495
    STEP: verifying the pod e2e-test-httpd-pod was created 03/30/23 14:49:30.549
    Mar 30 14:49:30.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-5131 get pod e2e-test-httpd-pod -o json'
    Mar 30 14:49:30.597: INFO: stderr: ""
    Mar 30 14:49:30.597: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-03-30T14:49:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5131\",\n        \"resourceVersion\": \"103931\",\n        \"uid\": \"c73a35a3-b7e3-40f7-b08e-1859f7e2b4af\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-ql2r8\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cn-hongkong.192.168.0.5\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-ql2r8\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-30T14:49:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-30T14:49:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-30T14:49:26Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-30T14:49:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://942135b3e868f83cc6aa91431dd2bf9376a9e65f71185a3681d3aa92b2b48c30\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-30T14:49:26Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.29.1.7\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.29.1.7\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-30T14:49:25Z\"\n    }\n}\n"
    STEP: replace the image in the pod 03/30/23 14:49:30.597
    Mar 30 14:49:30.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-5131 replace -f -'
    Mar 30 14:49:31.158: INFO: stderr: ""
    Mar 30 14:49:31.158: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 03/30/23 14:49:31.158
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Mar 30 14:49:31.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1182850278 --namespace=kubectl-5131 delete pods e2e-test-httpd-pod'
    Mar 30 14:49:32.487: INFO: stderr: ""
    Mar 30 14:49:32.487: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:49:32.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5131" for this suite. 03/30/23 14:49:32.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:49:32.493
Mar 30 14:49:32.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename csiinlinevolumes 03/30/23 14:49:32.493
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:49:32.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:49:32.501
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 03/30/23 14:49:32.503
STEP: getting 03/30/23 14:49:32.509
STEP: listing in namespace 03/30/23 14:49:32.511
STEP: patching 03/30/23 14:49:32.512
STEP: deleting 03/30/23 14:49:32.515
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Mar 30 14:49:32.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-5059" for this suite. 03/30/23 14:49:32.522
------------------------------
â€¢ [0.031 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:49:32.493
    Mar 30 14:49:32.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename csiinlinevolumes 03/30/23 14:49:32.493
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:49:32.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:49:32.501
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 03/30/23 14:49:32.503
    STEP: getting 03/30/23 14:49:32.509
    STEP: listing in namespace 03/30/23 14:49:32.511
    STEP: patching 03/30/23 14:49:32.512
    STEP: deleting 03/30/23 14:49:32.515
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:49:32.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-5059" for this suite. 03/30/23 14:49:32.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/30/23 14:49:32.532
Mar 30 14:49:32.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
STEP: Building a namespace api object, basename gc 03/30/23 14:49:32.532
STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:49:32.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:49:32.54
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Mar 30 14:49:32.552: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"0a2b48b9-0a83-443e-9dd0-d4429641155f", Controller:(*bool)(0xc004ba15b2), BlockOwnerDeletion:(*bool)(0xc004ba15b3)}}
Mar 30 14:49:32.554: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"87bf9b2d-1820-4b25-9258-c8b73dd17df3", Controller:(*bool)(0xc003d8cd22), BlockOwnerDeletion:(*bool)(0xc003d8cd23)}}
Mar 30 14:49:32.557: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a935a327-c964-42d4-a3c4-4c72f6137ad0", Controller:(*bool)(0xc003d8cf4a), BlockOwnerDeletion:(*bool)(0xc003d8cf4b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 30 14:49:37.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8745" for this suite. 03/30/23 14:49:37.565
------------------------------
â€¢ [SLOW TEST] [5.036 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/30/23 14:49:32.532
    Mar 30 14:49:32.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1182850278
    STEP: Building a namespace api object, basename gc 03/30/23 14:49:32.532
    STEP: Waiting for a default service account to be provisioned in namespace 03/30/23 14:49:32.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/30/23 14:49:32.54
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Mar 30 14:49:32.552: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"0a2b48b9-0a83-443e-9dd0-d4429641155f", Controller:(*bool)(0xc004ba15b2), BlockOwnerDeletion:(*bool)(0xc004ba15b3)}}
    Mar 30 14:49:32.554: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"87bf9b2d-1820-4b25-9258-c8b73dd17df3", Controller:(*bool)(0xc003d8cd22), BlockOwnerDeletion:(*bool)(0xc003d8cd23)}}
    Mar 30 14:49:32.557: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a935a327-c964-42d4-a3c4-4c72f6137ad0", Controller:(*bool)(0xc003d8cf4a), BlockOwnerDeletion:(*bool)(0xc003d8cf4b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 30 14:49:37.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8745" for this suite. 03/30/23 14:49:37.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Mar 30 14:49:37.570: INFO: Running AfterSuite actions on node 1
Mar 30 14:49:37.570: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Mar 30 14:49:37.570: INFO: Running AfterSuite actions on node 1
    Mar 30 14:49:37.570: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.077 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6055.894 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h40m56.127420001s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

